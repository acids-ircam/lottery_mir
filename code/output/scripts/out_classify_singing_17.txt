Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41289095.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, kiwisolver, python-dateutil, cycler, pyparsing, matplotlib, google-pasta, h5py, keras-applications, opt-einsum, grpcio, astor, tensorflow-estimator, oauthlib, certifi, idna, urllib3, chardet, requests, requests-oauthlib, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, google-auth-oauthlib, markdown, werkzeug, absl-py, protobuf, tensorboard, termcolor, gast, wrapt, keras-preprocessing, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41289095.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 05:01:09.035651: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 05:01:09.511773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is singing_classify_cnn_xavier_trimming_info_target_rewind_local_0.
*******
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9669]
[Starting training]
Epoch 0 	 0.842716 	 0.703125 	 0.692831
Epoch 10 	 0.304802 	 0.346048 	 0.217463
Epoch 20 	 0.145565 	 0.228401 	 0.091728
Epoch 30 	 0.092486 	 0.185202 	 0.058272
Epoch 40 	 0.066981 	 0.169577 	 0.044118
Epoch 50 	 0.044692 	 0.152574 	 0.035110
Epoch 60 	 0.046645 	 0.153952 	 0.033824
Epoch 70 	 0.024242 	 0.152574 	 0.031434
Epoch 80 	 0.016659 	 0.147059 	 0.029963
Epoch 90 	 0.009306 	 0.153952 	 0.031066
Epoch 100 	 0.011604 	 0.150735 	 0.030515
Epoch 110 	 0.009306 	 0.149816 	 0.030331
[Model stopped early]
Train loss       : 0.007812
Best valid loss  : 0.144761
Best test loss   : 0.029779
Pruning          : 1.00
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 774,827
--------------------------------
Total memory      : 6.42 MB
Total Flops       : 398.98 MFlops
Total Mem (Read)  : 8.28 MB
Total Mem (Write) : 4.82 MB
[Supermasks testing]
[Untrained loss : 0.4040]
[Starting training]
Epoch 0 	 0.131319 	 0.159467 	 0.039890
Epoch 10 	 0.059628 	 0.155331 	 0.035478
Epoch 20 	 0.041820 	 0.154412 	 0.033088
Epoch 30 	 0.032744 	 0.153952 	 0.032353
Epoch 40 	 0.029642 	 0.154412 	 0.032537
Epoch 50 	 0.022518 	 0.149816 	 0.030331
Epoch 60 	 0.017578 	 0.147978 	 0.030147
[Model stopped early]
Train loss       : 0.017578
Best valid loss  : 0.146140
Best test loss   : 0.030147
Pruning          : 0.75
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 442,883
--------------------------------
Total memory      : 4.82 MB
Total Flops       : 243.8 MFlops
Total Mem (Read)  : 5.81 MB
Total Mem (Write) : 3.61 MB
[Supermasks testing]
[Untrained loss : 0.6762]
[Starting training]
Epoch 0 	 0.279412 	 0.231618 	 0.100551
Epoch 10 	 0.107077 	 0.167739 	 0.045588
Epoch 20 	 0.084444 	 0.163603 	 0.043934
Epoch 30 	 0.062730 	 0.162684 	 0.038603
Epoch 40 	 0.054458 	 0.159007 	 0.036397
Epoch 50 	 0.043313 	 0.160846 	 0.037868
Epoch 60 	 0.039407 	 0.157169 	 0.036397
Epoch 70 	 0.036765 	 0.158088 	 0.034559
Epoch 80 	 0.034697 	 0.155331 	 0.034375
Epoch 90 	 0.031710 	 0.153952 	 0.033272
Epoch 100 	 0.029182 	 0.151654 	 0.031801
Epoch 110 	 0.030101 	 0.150735 	 0.031801
Epoch 120 	 0.028378 	 0.152114 	 0.031801
Epoch 130 	 0.028033 	 0.158088 	 0.034375
[Model stopped early]
Train loss       : 0.031710
Best valid loss  : 0.149357
Best test loss   : 0.031250
Pruning          : 0.56
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 254,405
--------------------------------
Total memory      : 3.61 MB
Total Flops       : 151.67 MFlops
Total Mem (Read)  : 4.19 MB
Total Mem (Write) : 2.71 MB
[Supermasks testing]
[Untrained loss : 0.8915]
[Starting training]
Epoch 0 	 0.422564 	 0.335018 	 0.218566
Epoch 10 	 0.183938 	 0.224265 	 0.085478
Epoch 20 	 0.152918 	 0.197151 	 0.064614
Epoch 30 	 0.122817 	 0.183364 	 0.055882
Epoch 40 	 0.109949 	 0.176930 	 0.050551
Epoch 50 	 0.106733 	 0.174173 	 0.046691
Epoch 60 	 0.076861 	 0.168199 	 0.044669
Epoch 70 	 0.071576 	 0.165901 	 0.040993
Epoch 80 	 0.067096 	 0.163603 	 0.038971
Epoch 90 	 0.056066 	 0.157629 	 0.036213
Epoch 100 	 0.056411 	 0.163143 	 0.037132
Epoch 110 	 0.052045 	 0.164982 	 0.038051
Epoch 120 	 0.056756 	 0.162224 	 0.038971
[Model stopped early]
Train loss       : 0.050781
Best valid loss  : 0.157629
Best test loss   : 0.036213
Pruning          : 0.42
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 144,361
--------------------------------
Total memory      : 2.68 MB
Total Flops       : 94.38 MFlops
Total Mem (Read)  : 3.06 MB
Total Mem (Write) : 2.01 MB
[Supermasks testing]
[Untrained loss : 0.9118]
[Starting training]
Epoch 0 	 0.545381 	 0.452206 	 0.357813
Epoch 10 	 0.306411 	 0.298254 	 0.169669
Epoch 20 	 0.244256 	 0.251379 	 0.123346
Epoch 30 	 0.214614 	 0.236673 	 0.099081
Epoch 40 	 0.186236 	 0.214614 	 0.082721
Epoch 50 	 0.175896 	 0.215533 	 0.087868
Epoch 60 	 0.159582 	 0.205882 	 0.072794
Epoch 70 	 0.142923 	 0.183364 	 0.062500
Epoch 80 	 0.138327 	 0.176930 	 0.057813
Epoch 90 	 0.134076 	 0.178768 	 0.059191
Epoch 100 	 0.115809 	 0.171875 	 0.050000
Epoch 110 	 0.112132 	 0.170956 	 0.048713
Epoch 120 	 0.111558 	 0.165901 	 0.047610
Epoch 130 	 0.101677 	 0.172335 	 0.053676
Epoch 140 	 0.100414 	 0.164062 	 0.045404
Epoch 150 	 0.089384 	 0.170496 	 0.050735
[Model stopped early]
Train loss       : 0.092486
Best valid loss  : 0.161305
Best test loss   : 0.045772
Pruning          : 0.32
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 84,015
--------------------------------
Total memory      : 2.01 MB
Total Flops       : 61.16 MFlops
Total Mem (Read)  : 2.33 MB
Total Mem (Write) : 1.51 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.633157 	 0.552849 	 0.498438
Epoch 10 	 0.382353 	 0.396599 	 0.261949
Epoch 20 	 0.334099 	 0.337316 	 0.205147
Epoch 30 	 0.308134 	 0.312500 	 0.172978
Epoch 40 	 0.274012 	 0.296415 	 0.160110
Epoch 50 	 0.260800 	 0.271140 	 0.143015
Epoch 60 	 0.252528 	 0.259191 	 0.129779
Epoch 70 	 0.231962 	 0.248162 	 0.112132
Epoch 80 	 0.216912 	 0.245864 	 0.114706
Epoch 90 	 0.218750 	 0.241268 	 0.105055
Epoch 100 	 0.210708 	 0.224724 	 0.086765
Epoch 110 	 0.188074 	 0.214154 	 0.086213
Epoch 120 	 0.178309 	 0.217831 	 0.085662
Epoch 130 	 0.177160 	 0.217831 	 0.088603
Epoch 140 	 0.171186 	 0.207721 	 0.076195
Epoch 150 	 0.163718 	 0.204963 	 0.072335
Train loss       : 0.157858
Best valid loss  : 0.199449
Best test loss   : 0.069577
Pruning          : 0.24
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 47,755
--------------------------------
Total memory      : 1.47 MB
Total Flops       : 39.21 MFlops
Total Mem (Read)  : 1.79 MB
Total Mem (Write) : 1.1 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.718750 	 0.698529 	 0.652206
Epoch 10 	 0.476907 	 0.485294 	 0.375919
Epoch 20 	 0.421875 	 0.418199 	 0.302665
Epoch 30 	 0.402574 	 0.378217 	 0.270864
Epoch 40 	 0.387983 	 0.380055 	 0.259283
Epoch 50 	 0.367417 	 0.366728 	 0.251838
Epoch 60 	 0.346048 	 0.355699 	 0.237132
Epoch 70 	 0.340303 	 0.339614 	 0.222610
Epoch 80 	 0.331112 	 0.339614 	 0.218015
Epoch 90 	 0.326517 	 0.317096 	 0.193566
Epoch 100 	 0.321347 	 0.316636 	 0.191176
Epoch 110 	 0.318244 	 0.306985 	 0.188051
Epoch 120 	 0.305377 	 0.302849 	 0.170221
Epoch 130 	 0.297105 	 0.296875 	 0.175184
Epoch 140 	 0.288948 	 0.293199 	 0.173162
Epoch 150 	 0.281135 	 0.287224 	 0.171507
Train loss       : 0.277114
Best valid loss  : 0.273438
Best test loss   : 0.154779
Pruning          : 0.18
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 27,239
--------------------------------
Total memory      : 1.07 MB
Total Flops       : 25.43 MFlops
Total Mem (Read)  : 1.41 MB
Total Mem (Write) : 822.46 KB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.801815 	 0.787684 	 0.762684
Epoch 10 	 0.553768 	 0.557904 	 0.450092
Epoch 20 	 0.498736 	 0.515625 	 0.396048
Epoch 30 	 0.484835 	 0.479320 	 0.353585
Epoch 40 	 0.451861 	 0.465993 	 0.338419
Epoch 50 	 0.440602 	 0.458180 	 0.321048
Epoch 60 	 0.431756 	 0.435202 	 0.309743
Epoch 70 	 0.423483 	 0.433364 	 0.302114
Epoch 80 	 0.411190 	 0.426011 	 0.294393
Epoch 90 	 0.397748 	 0.416820 	 0.287960
Epoch 100 	 0.405216 	 0.415441 	 0.283548
Epoch 110 	 0.396369 	 0.408088 	 0.278125
Epoch 120 	 0.395680 	 0.402574 	 0.272243
Epoch 130 	 0.394301 	 0.407169 	 0.277206
Epoch 140 	 0.398323 	 0.412684 	 0.284559
Epoch 150 	 0.389821 	 0.404871 	 0.268750
Train loss       : 0.386029
Best valid loss  : 0.397059
Best test loss   : 0.267463
Pruning          : 0.13
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 16,485
--------------------------------
Total memory      : 0.80 MB
Total Flops       : 17.54 MFlops
Total Mem (Read)  : 1.17 MB
Total Mem (Write) : 616.86 KB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.850528 	 0.826746 	 0.810110
Epoch 10 	 0.615349 	 0.619026 	 0.540901
Epoch 20 	 0.576402 	 0.596048 	 0.480882
Epoch 30 	 0.548943 	 0.562500 	 0.448162
Epoch 40 	 0.528378 	 0.556066 	 0.430423
Epoch 50 	 0.513902 	 0.542739 	 0.417188
Epoch 60 	 0.499311 	 0.521599 	 0.393566
Epoch 70 	 0.497243 	 0.513327 	 0.384375
Epoch 80 	 0.494600 	 0.495404 	 0.377757
Epoch 90 	 0.489315 	 0.498621 	 0.371232
Epoch 100 	 0.475528 	 0.492647 	 0.362776
Epoch 110 	 0.476792 	 0.488051 	 0.359283
Epoch 120 	 0.473001 	 0.475643 	 0.348254
Epoch 130 	 0.474035 	 0.475184 	 0.348897
Epoch 140 	 0.465188 	 0.474265 	 0.348437
Epoch 150 	 0.466452 	 0.484835 	 0.359467
Train loss       : 0.461972
Best valid loss  : 0.461857
Best test loss   : 0.335570
Pruning          : 0.10
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 8,769
--------------------------------
Total memory      : 0.54 MB
Total Flops       : 10.66 MFlops
Total Mem (Read)  : 962.38 KB
Total Mem (Write) : 411.36 KB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.875115 	 0.864430 	 0.854228
Epoch 10 	 0.693704 	 0.676930 	 0.647059
Epoch 20 	 0.662684 	 0.652574 	 0.592463
Epoch 30 	 0.636604 	 0.640625 	 0.575552
Epoch 40 	 0.620979 	 0.631893 	 0.553309
Epoch 50 	 0.611098 	 0.614430 	 0.532261
Epoch 60 	 0.599035 	 0.620864 	 0.537776
Epoch 70 	 0.591912 	 0.607537 	 0.518842
Epoch 80 	 0.580308 	 0.585938 	 0.499449
Epoch 90 	 0.589040 	 0.612132 	 0.520221
Epoch 100 	 0.584329 	 0.590993 	 0.500368
Epoch 110 	 0.574334 	 0.591912 	 0.495129
[Model stopped early]
Train loss       : 0.581457
Best valid loss  : 0.585938
Best test loss   : 0.499449
Pruning          : 0.08
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 5,489
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 846.73 KB
Total Mem (Write) : 308.53 KB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.898782 	 0.886949 	 0.879044
Epoch 10 	 0.738511 	 0.750000 	 0.742279
Epoch 20 	 0.698759 	 0.695772 	 0.680882
Epoch 30 	 0.694623 	 0.681526 	 0.663603
Epoch 40 	 0.676471 	 0.672335 	 0.650368
Epoch 50 	 0.684168 	 0.669577 	 0.632904
Epoch 60 	 0.674632 	 0.664982 	 0.631618
Epoch 70 	 0.665901 	 0.646599 	 0.608272
Epoch 80 	 0.663833 	 0.638327 	 0.597059
Epoch 90 	 0.664637 	 0.647059 	 0.598529
Epoch 100 	 0.663143 	 0.640165 	 0.599265
Epoch 110 	 0.659352 	 0.649816 	 0.600184
[Model stopped early]
Train loss       : 0.659926
Best valid loss  : 0.638327
Best test loss   : 0.597059
Pruning          : 0.06
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 4,929
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 844.38 KB
Total Mem (Write) : 308.37 KB
[Supermasks testing]
[Untrained loss : 0.9667]
[Starting training]
Epoch 0 	 0.926126 	 0.935202 	 0.932353
Epoch 10 	 0.762408 	 0.764246 	 0.760110
Epoch 20 	 0.708065 	 0.687960 	 0.675368
Epoch 30 	 0.699219 	 0.674632 	 0.658088
Epoch 40 	 0.701517 	 0.697151 	 0.670772
Epoch 50 	 0.688419 	 0.679228 	 0.654228
Epoch 60 	 0.683019 	 0.678309 	 0.649816
Epoch 70 	 0.682445 	 0.655790 	 0.627573
Epoch 80 	 0.678883 	 0.666820 	 0.632721
Epoch 90 	 0.674403 	 0.671875 	 0.629963
Epoch 100 	 0.681411 	 0.663143 	 0.630515
[Model stopped early]
Train loss       : 0.671875
Best valid loss  : 0.648897
Best test loss   : 0.619485
Pruning          : 0.04
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 4,589
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 842.93 KB
Total Mem (Write) : 308.25 KB
[Supermasks testing]
[Untrained loss : 0.9669]
[Starting training]
Epoch 0 	 0.940832 	 0.939338 	 0.937316
Epoch 10 	 0.791590 	 0.786305 	 0.776471
Epoch 20 	 0.721622 	 0.688419 	 0.681618
Epoch 30 	 0.705997 	 0.683364 	 0.669669
Epoch 40 	 0.696461 	 0.669118 	 0.656801
Epoch 50 	 0.688419 	 0.645221 	 0.629228
Epoch 60 	 0.684513 	 0.642004 	 0.620404
Epoch 70 	 0.678309 	 0.645221 	 0.623529
Epoch 80 	 0.679573 	 0.643382 	 0.615074
Epoch 90 	 0.682560 	 0.642463 	 0.610846
Epoch 100 	 0.670152 	 0.630515 	 0.602757
Epoch 110 	 0.672679 	 0.634651 	 0.606802
Epoch 120 	 0.669347 	 0.633732 	 0.601654
Epoch 130 	 0.669577 	 0.632812 	 0.606434
Epoch 140 	 0.670496 	 0.632812 	 0.602206
Epoch 150 	 0.664177 	 0.636489 	 0.606066
Train loss       : 0.665556
Best valid loss  : 0.627298
Best test loss   : 0.599081
Pruning          : 0.03
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 4,353
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 841.92 KB
Total Mem (Write) : 308.16 KB
[Supermasks testing]
[Untrained loss : 0.9669]
[Starting training]
Epoch 0 	 0.961282 	 0.971048 	 0.966912
Epoch 10 	 0.802619 	 0.799173 	 0.790074
Epoch 20 	 0.741383 	 0.726562 	 0.715441
Epoch 30 	 0.711627 	 0.691636 	 0.673529
Epoch 40 	 0.697381 	 0.667739 	 0.653860
Epoch 50 	 0.692555 	 0.657169 	 0.639154
Epoch 60 	 0.697151 	 0.655790 	 0.636765
Epoch 70 	 0.688074 	 0.650276 	 0.634926
Epoch 80 	 0.686581 	 0.643382 	 0.626287
Epoch 90 	 0.682100 	 0.650276 	 0.625919
Epoch 100 	 0.686811 	 0.640625 	 0.621875
Epoch 110 	 0.678539 	 0.643382 	 0.621691
Epoch 120 	 0.687960 	 0.637408 	 0.622610
Epoch 130 	 0.683594 	 0.638327 	 0.619669
Epoch 140 	 0.685547 	 0.635110 	 0.617647
Epoch 150 	 0.684053 	 0.637408 	 0.616728
Train loss       : 0.687385
Best valid loss  : 0.635110
Best test loss   : 0.617647
Pruning          : 0.02
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 4,197
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 841.24 KB
Total Mem (Write) : 308.09 KB
[Supermasks testing]
[Untrained loss : 0.9669]
[Starting training]
Epoch 0 	 0.964844 	 0.971048 	 0.966912
Epoch 10 	 0.835248 	 0.844669 	 0.830699
Epoch 20 	 0.781020 	 0.770680 	 0.764338
Epoch 30 	 0.728401 	 0.697151 	 0.686949
Epoch 40 	 0.721507 	 0.685202 	 0.670956
Epoch 50 	 0.709444 	 0.652114 	 0.639522
Epoch 60 	 0.708640 	 0.658548 	 0.647978
Epoch 70 	 0.695887 	 0.660386 	 0.644485
Epoch 80 	 0.703240 	 0.652574 	 0.625735
Epoch 90 	 0.698759 	 0.653033 	 0.627941
[Model stopped early]
Train loss       : 0.698759
Best valid loss  : 0.647059
Best test loss   : 0.636397
Pruning          : 0.02
