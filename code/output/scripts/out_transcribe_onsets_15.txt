Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288805.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, kiwisolver, python-dateutil, pyparsing, cycler, matplotlib, opt-einsum, google-pasta, astor, termcolor, gast, tensorflow-estimator, h5py, keras-applications, protobuf, certifi, idna, urllib3, chardet, requests, absl-py, oauthlib, requests-oauthlib, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, grpcio, werkzeug, markdown, tensorboard, wrapt, keras-preprocessing, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288805.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:52:37.109880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:52:37.381548: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_trimming_activation_rewind_local_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288805.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7055]
[Starting training]
/localscratch/esling.41288805.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
Epoch 0 	 22.772970 	 0.635902 	 0.652606
Epoch 10 	 21.768745 	 0.548333 	 0.565280
Epoch 20 	 20.431215 	 0.390911 	 0.397868
Epoch 30 	 19.289036 	 0.281392 	 0.288309
Epoch 40 	 18.401907 	 0.223723 	 0.231736
Epoch 50 	 17.838535 	 0.191924 	 0.189209
Epoch 60 	 17.429014 	 0.173184 	 0.175641
Epoch 70 	 17.190998 	 0.170023 	 0.158980
Epoch 80 	 16.956640 	 0.158230 	 0.151739
Epoch 90 	 16.785904 	 0.158318 	 0.153681
Epoch 100 	 16.645199 	 0.154539 	 0.148011
Epoch 110 	 16.517889 	 0.152812 	 0.148798
Epoch 120 	 16.410524 	 0.146836 	 0.143224
Epoch 130 	 16.351742 	 0.144375 	 0.142852
Epoch 140 	 16.213589 	 0.142659 	 0.137140
Epoch 150 	 16.156839 	 0.140510 	 0.138172
Epoch 160 	 16.117371 	 0.139728 	 0.138214
Epoch 170 	 16.099949 	 0.140400 	 0.136229
[Model stopped early]
Train loss       : 16.088522
Best valid loss  : 0.134925
Best test loss   : 0.138012
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,031,141
--------------------------------
Total memory      : 15.85 MB
Total Flops       : 1.5 GFlops
Total Mem (Read)  : 23.9 MB
Total Mem (Write) : 12.33 MB
[Supermasks testing]
[Untrained loss : 0.3910]
[Starting training]
Epoch 0 	 17.743002 	 0.174269 	 0.174549
Epoch 10 	 17.239805 	 0.156417 	 0.154292
Epoch 20 	 17.016403 	 0.151905 	 0.151358
Epoch 30 	 16.798239 	 0.145425 	 0.139831
Epoch 40 	 16.654697 	 0.145858 	 0.139688
Epoch 50 	 16.457445 	 0.140690 	 0.133597
Epoch 60 	 16.367672 	 0.142448 	 0.133975
Epoch 70 	 16.302195 	 0.137359 	 0.131759
[Model stopped early]
Train loss       : 16.263161
Best valid loss  : 0.136395
Best test loss   : 0.134115
Pruning          : 0.75
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,015,845
--------------------------------
Total memory      : 11.89 MB
Total Flops       : 848.79 MFlops
Total Mem (Read)  : 16.95 MB
Total Mem (Write) : 9.25 MB
[Supermasks testing]
[Untrained loss : 0.5616]
[Starting training]
Epoch 0 	 18.219357 	 0.185442 	 0.181992
Epoch 10 	 17.409834 	 0.161046 	 0.155461
Epoch 20 	 17.119137 	 0.153449 	 0.146020
Epoch 30 	 16.946781 	 0.145127 	 0.139544
Epoch 40 	 16.796864 	 0.145200 	 0.138287
Epoch 50 	 16.657211 	 0.141174 	 0.135482
Epoch 60 	 16.472162 	 0.140843 	 0.135637
Epoch 70 	 16.421055 	 0.140187 	 0.132112
Epoch 80 	 16.320330 	 0.136757 	 0.132398
Epoch 90 	 16.286648 	 0.139364 	 0.133896
Epoch 100 	 16.255825 	 0.137852 	 0.131234
Epoch 110 	 16.220577 	 0.137189 	 0.130461
[Model stopped early]
Train loss       : 16.216791
Best valid loss  : 0.134809
Best test loss   : 0.132283
Pruning          : 0.56
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,378,357
--------------------------------
Total memory      : 8.92 MB
Total Flops       : 481.03 MFlops
Total Mem (Read)  : 12.21 MB
Total Mem (Write) : 6.94 MB
[Supermasks testing]
[Untrained loss : 0.6634]
[Starting training]
Epoch 0 	 18.983805 	 0.197371 	 0.196342
Epoch 10 	 17.631924 	 0.163190 	 0.158780
Epoch 20 	 17.328880 	 0.159994 	 0.153335
Epoch 30 	 17.137508 	 0.151178 	 0.142390
Epoch 40 	 16.974993 	 0.149010 	 0.140563
Epoch 50 	 16.738195 	 0.145444 	 0.136883
Epoch 60 	 16.654495 	 0.145926 	 0.134829
Epoch 70 	 16.571188 	 0.140513 	 0.135349
Epoch 80 	 16.496805 	 0.138925 	 0.134077
Epoch 90 	 16.453367 	 0.139362 	 0.134836
Epoch 100 	 16.380098 	 0.138669 	 0.131438
Epoch 110 	 16.329329 	 0.138772 	 0.131354
[Model stopped early]
Train loss       : 16.312906
Best valid loss  : 0.134916
Best test loss   : 0.132111
Pruning          : 0.42
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 969,982
--------------------------------
Total memory      : 6.69 MB
Total Flops       : 273.29 MFlops
Total Mem (Read)  : 8.92 MB
Total Mem (Write) : 5.21 MB
[Supermasks testing]
[Untrained loss : 0.6755]
[Starting training]
Epoch 0 	 19.825552 	 0.230363 	 0.224000
Epoch 10 	 17.917078 	 0.167650 	 0.161514
Epoch 20 	 17.631104 	 0.166739 	 0.152397
Epoch 30 	 17.428562 	 0.155378 	 0.147277
Epoch 40 	 17.272766 	 0.150343 	 0.142803
Epoch 50 	 17.099508 	 0.151068 	 0.144001
Epoch 60 	 16.878839 	 0.143045 	 0.135652
Epoch 70 	 16.819818 	 0.146399 	 0.137982
Epoch 80 	 16.727356 	 0.142824 	 0.136847
Epoch 90 	 16.652033 	 0.143827 	 0.137858
[Model stopped early]
Train loss       : 16.652033
Best valid loss  : 0.139107
Best test loss   : 0.136312
Pruning          : 0.32
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 700,624
--------------------------------
Total memory      : 4.96 MB
Total Flops       : 152.05 MFlops
Total Mem (Read)  : 6.54 MB
Total Mem (Write) : 3.86 MB
[Supermasks testing]
[Untrained loss : 0.7101]
[Starting training]
Epoch 0 	 20.561836 	 0.271582 	 0.267622
Epoch 10 	 18.253199 	 0.178015 	 0.168889
Epoch 20 	 17.997553 	 0.167370 	 0.164547
Epoch 30 	 17.750851 	 0.165900 	 0.160978
Epoch 40 	 17.645679 	 0.162180 	 0.154863
Epoch 50 	 17.495371 	 0.159866 	 0.156810
Epoch 60 	 17.349356 	 0.157119 	 0.148617
Epoch 70 	 17.238211 	 0.149597 	 0.146598
Epoch 80 	 17.072500 	 0.147889 	 0.142922
Epoch 90 	 17.000872 	 0.146157 	 0.145122
Epoch 100 	 16.933796 	 0.148616 	 0.145502
Epoch 110 	 16.838621 	 0.146507 	 0.142242
Epoch 120 	 16.823124 	 0.148191 	 0.141900
[Model stopped early]
Train loss       : 16.830570
Best valid loss  : 0.142413
Best test loss   : 0.142048
Pruning          : 0.24
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 522,854
--------------------------------
Total memory      : 3.72 MB
Total Flops       : 87.05 MFlops
Total Mem (Read)  : 4.9 MB
Total Mem (Write) : 2.9 MB
[Supermasks testing]
[Untrained loss : 0.7341]
[Starting training]
Epoch 0 	 21.417103 	 0.337983 	 0.345634
Epoch 10 	 18.719456 	 0.193950 	 0.186421
Epoch 20 	 18.377113 	 0.178078 	 0.168063
Epoch 30 	 18.163504 	 0.170334 	 0.165977
Epoch 40 	 18.012522 	 0.168900 	 0.158570
Epoch 50 	 17.827200 	 0.162259 	 0.154306
Epoch 60 	 17.696337 	 0.158922 	 0.154639
Epoch 70 	 17.642881 	 0.159120 	 0.153540
Epoch 80 	 17.539227 	 0.156635 	 0.151629
Epoch 90 	 17.550030 	 0.156988 	 0.151383
Epoch 100 	 17.435383 	 0.154450 	 0.149570
Epoch 110 	 17.383501 	 0.152649 	 0.149146
Epoch 120 	 17.339354 	 0.153368 	 0.145403
Epoch 130 	 17.303038 	 0.149627 	 0.146411
Epoch 140 	 17.237562 	 0.147285 	 0.143678
Epoch 150 	 17.213369 	 0.149810 	 0.144104
Epoch 160 	 17.222219 	 0.147062 	 0.144374
[Model stopped early]
Train loss       : 17.217100
Best valid loss  : 0.145615
Best test loss   : 0.144629
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 399,974
--------------------------------
Total memory      : 2.73 MB
Total Flops       : 48.02 MFlops
Total Mem (Read)  : 3.66 MB
Total Mem (Write) : 2.13 MB
[Supermasks testing]
[Untrained loss : 0.7018]
[Starting training]
Epoch 0 	 21.977930 	 0.455388 	 0.470685
Epoch 10 	 19.225622 	 0.225735 	 0.222465
Epoch 20 	 18.880779 	 0.200764 	 0.195948
Epoch 30 	 18.641605 	 0.189129 	 0.182071
Epoch 40 	 18.537928 	 0.184848 	 0.176193
Epoch 50 	 18.364805 	 0.176961 	 0.169594
Epoch 60 	 18.270201 	 0.171079 	 0.168211
Epoch 70 	 18.192942 	 0.172244 	 0.170518
Epoch 80 	 18.147495 	 0.168170 	 0.166771
Epoch 90 	 17.952549 	 0.165326 	 0.159405
Epoch 100 	 17.882330 	 0.166227 	 0.159239
Epoch 110 	 17.865589 	 0.158422 	 0.157732
Epoch 120 	 17.838602 	 0.161858 	 0.156480
Epoch 130 	 17.764339 	 0.161840 	 0.157648
Epoch 140 	 17.745687 	 0.156767 	 0.157443
Epoch 150 	 17.737844 	 0.158497 	 0.154859
Epoch 160 	 17.729990 	 0.162592 	 0.156322
Epoch 170 	 17.703100 	 0.160354 	 0.156161
[Model stopped early]
Train loss       : 17.705803
Best valid loss  : 0.156767
Best test loss   : 0.157443
Pruning          : 0.13
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 316,333
--------------------------------
Total memory      : 1.99 MB
Total Flops       : 26.32 MFlops
Total Mem (Read)  : 2.77 MB
Total Mem (Write) : 1.55 MB
[Supermasks testing]
[Untrained loss : 0.7677]
[Starting training]
Epoch 0 	 22.312057 	 0.568990 	 0.581658
Epoch 10 	 19.769358 	 0.262254 	 0.263092
Epoch 20 	 19.381760 	 0.220781 	 0.219826
Epoch 30 	 19.154123 	 0.217400 	 0.210286
Epoch 40 	 19.058672 	 0.204650 	 0.200491
Epoch 50 	 18.929617 	 0.207462 	 0.199068
Epoch 60 	 18.820210 	 0.196349 	 0.191154
Epoch 70 	 18.779682 	 0.192296 	 0.185536
Epoch 80 	 18.695253 	 0.188245 	 0.188281
Epoch 90 	 18.609539 	 0.185647 	 0.184602
Epoch 100 	 18.575262 	 0.193816 	 0.179948
Epoch 110 	 18.439613 	 0.187738 	 0.177233
Epoch 120 	 18.374411 	 0.185532 	 0.178177
Epoch 130 	 18.295071 	 0.183347 	 0.176067
[Model stopped early]
Train loss       : 18.313126
Best valid loss  : 0.180971
Best test loss   : 0.180699
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 257,489
--------------------------------
Total memory      : 1.49 MB
Total Flops       : 15.44 MFlops
Total Mem (Read)  : 2.16 MB
Total Mem (Write) : 1.16 MB
[Supermasks testing]
[Untrained loss : 0.7791]
[Starting training]
Epoch 0 	 22.477707 	 0.582481 	 0.596193
Epoch 10 	 20.299503 	 0.323795 	 0.326295
Epoch 20 	 19.900742 	 0.267199 	 0.267112
Epoch 30 	 19.696964 	 0.249234 	 0.247677
Epoch 40 	 19.552774 	 0.232966 	 0.232892
Epoch 50 	 19.425426 	 0.229002 	 0.224650
Epoch 60 	 19.330515 	 0.228029 	 0.223079
Epoch 70 	 19.232468 	 0.218835 	 0.217349
Epoch 80 	 19.160154 	 0.220007 	 0.214997
Epoch 90 	 19.090506 	 0.218607 	 0.213746
Epoch 100 	 19.067617 	 0.220164 	 0.214123
Epoch 110 	 19.009068 	 0.214383 	 0.213054
Epoch 120 	 18.957237 	 0.213675 	 0.210497
Epoch 130 	 18.989349 	 0.207809 	 0.209294
Epoch 140 	 18.973080 	 0.213798 	 0.207858
Epoch 150 	 18.937225 	 0.215008 	 0.208742
Epoch 160 	 18.942413 	 0.211541 	 0.208870
[Model stopped early]
Train loss       : 18.944393
Best valid loss  : 0.207809
Best test loss   : 0.209294
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 216,037
--------------------------------
Total memory      : 1.08 MB
Total Flops       : 8.58 MFlops
Total Mem (Read)  : 1.68 MB
Total Mem (Write) : 861.98 KB
[Supermasks testing]
[Untrained loss : 0.7307]
[Starting training]
Epoch 0 	 22.597380 	 0.619455 	 0.633338
Epoch 10 	 20.921558 	 0.392948 	 0.408953
Epoch 20 	 20.494616 	 0.347092 	 0.355572
Epoch 30 	 20.338070 	 0.331106 	 0.342717
Epoch 40 	 20.280119 	 0.318761 	 0.326476
Epoch 50 	 20.169859 	 0.317051 	 0.320553
Epoch 60 	 20.112606 	 0.311794 	 0.318129
Epoch 70 	 20.038630 	 0.302784 	 0.310926
Epoch 80 	 19.966228 	 0.296187 	 0.304118
Epoch 90 	 19.915623 	 0.295239 	 0.301698
Epoch 100 	 19.791843 	 0.297433 	 0.297341
Epoch 110 	 19.730852 	 0.283531 	 0.289721
Epoch 120 	 19.718002 	 0.283950 	 0.288277
Epoch 130 	 19.698568 	 0.285364 	 0.292192
Epoch 140 	 19.613205 	 0.276680 	 0.284978
Epoch 150 	 19.588337 	 0.273832 	 0.283103
Epoch 160 	 19.583467 	 0.276210 	 0.282651
Epoch 170 	 19.573193 	 0.274595 	 0.281758
Epoch 180 	 19.562670 	 0.273697 	 0.282609
Epoch 190 	 19.576340 	 0.273965 	 0.280881
Train loss       : 19.565939
Best valid loss  : 0.272356
Best test loss   : 0.282523
Pruning          : 0.06
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 184,678
--------------------------------
Total memory      : 0.75 MB
Total Flops       : 4.54 MFlops
Total Mem (Read)  : 1.3 MB
Total Mem (Write) : 599.12 KB
[Supermasks testing]
[Untrained loss : 0.7656]
[Starting training]
Epoch 0 	 22.711420 	 0.644535 	 0.663669
Epoch 10 	 21.566755 	 0.488914 	 0.510052
Epoch 20 	 21.244577 	 0.450561 	 0.472154
Epoch 30 	 21.067814 	 0.410643 	 0.432691
Epoch 40 	 20.936428 	 0.401199 	 0.406299
Epoch 50 	 20.731852 	 0.372300 	 0.382533
Epoch 60 	 20.636837 	 0.370657 	 0.381119
Epoch 70 	 20.556101 	 0.349214 	 0.364120
Epoch 80 	 20.514370 	 0.345636 	 0.354642
Epoch 90 	 20.505159 	 0.348229 	 0.355503
Epoch 100 	 20.380512 	 0.340959 	 0.348180
Epoch 110 	 20.381079 	 0.343129 	 0.350030
Epoch 120 	 20.319242 	 0.336197 	 0.349118
Epoch 130 	 20.307657 	 0.332092 	 0.345296
Epoch 140 	 20.265036 	 0.331876 	 0.344003
Epoch 150 	 20.223185 	 0.336145 	 0.345209
Epoch 160 	 20.232325 	 0.331528 	 0.341419
Epoch 170 	 20.224398 	 0.331142 	 0.342430
Epoch 180 	 20.225847 	 0.330088 	 0.341669
Epoch 190 	 20.239943 	 0.326349 	 0.340747
Train loss       : 20.244732
Best valid loss  : 0.326349
Best test loss   : 0.340747
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 162,452
--------------------------------
Total memory      : 0.50 MB
Total Flops       : 2.34 MFlops
Total Mem (Read)  : 1.02 MB
Total Mem (Write) : 401.99 KB
[Supermasks testing]
[Untrained loss : 0.7266]
[Starting training]
Epoch 0 	 22.806992 	 0.664181 	 0.679043
Epoch 10 	 21.911577 	 0.540969 	 0.560027
Epoch 20 	 21.692951 	 0.524150 	 0.531510
Epoch 30 	 21.573288 	 0.513143 	 0.520769
Epoch 40 	 21.446491 	 0.498756 	 0.508139
Epoch 50 	 21.411469 	 0.492070 	 0.501199
Epoch 60 	 21.343929 	 0.490397 	 0.500943
Epoch 70 	 21.256134 	 0.474053 	 0.485153
Epoch 80 	 21.239506 	 0.461891 	 0.480672
Epoch 90 	 21.231211 	 0.463856 	 0.473902
Epoch 100 	 21.160557 	 0.465930 	 0.477137
Epoch 110 	 21.149380 	 0.462599 	 0.473594
Epoch 120 	 21.108341 	 0.457093 	 0.469663
Epoch 130 	 21.082111 	 0.446373 	 0.456185
Epoch 140 	 21.066572 	 0.445276 	 0.456065
Epoch 150 	 21.051382 	 0.444986 	 0.458112
Epoch 160 	 21.041994 	 0.439821 	 0.453743
Epoch 170 	 21.025904 	 0.436977 	 0.453753
Epoch 180 	 21.017374 	 0.439889 	 0.453456
Epoch 190 	 21.077801 	 0.439713 	 0.452115
[Model stopped early]
Train loss       : 21.077801
Best valid loss  : 0.435531
Best test loss   : 0.449854
Pruning          : 0.03
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 146,768
--------------------------------
Total memory      : 0.34 MB
Total Flops       : 1.28 MFlops
Total Mem (Read)  : 855.98 KB
Total Mem (Write) : 270.55 KB
[Supermasks testing]
[Untrained loss : 0.6481]
[Starting training]
Epoch 0 	 22.890892 	 0.666032 	 0.685678
Epoch 10 	 22.064619 	 0.557676 	 0.579818
Epoch 20 	 21.914944 	 0.547907 	 0.564907
Epoch 30 	 21.817526 	 0.538368 	 0.550095
Epoch 40 	 21.732132 	 0.525298 	 0.533368
Epoch 50 	 21.680584 	 0.519717 	 0.534503
Epoch 60 	 21.650394 	 0.521563 	 0.531172
Epoch 70 	 21.651142 	 0.518431 	 0.528609
Epoch 80 	 21.605207 	 0.514529 	 0.528453
Epoch 90 	 21.559072 	 0.514066 	 0.527401
Epoch 100 	 21.548244 	 0.511909 	 0.526562
Epoch 110 	 21.534365 	 0.507948 	 0.523143
Epoch 120 	 21.516705 	 0.510005 	 0.525246
Epoch 130 	 21.473423 	 0.516717 	 0.522915
Epoch 140 	 21.479330 	 0.512211 	 0.523090
Epoch 150 	 21.472481 	 0.508189 	 0.520058
[Model stopped early]
Train loss       : 21.482450
Best valid loss  : 0.504972
Best test loss   : 0.520921
Pruning          : 0.02
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 135,317
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 866.6 KFlops
Total Mem (Read)  : 745.48 KB
Total Mem (Write) : 204.79 KB
[Supermasks testing]
[Untrained loss : 0.6826]
[Starting training]
Epoch 0 	 22.910078 	 0.675439 	 0.689273
Epoch 10 	 22.190413 	 0.574531 	 0.599145
Epoch 20 	 22.023571 	 0.561801 	 0.581016
Epoch 30 	 21.947422 	 0.555015 	 0.568682
Epoch 40 	 21.878437 	 0.541523 	 0.558371
Epoch 50 	 21.844706 	 0.536771 	 0.550471
Epoch 60 	 21.818462 	 0.538912 	 0.552655
Epoch 70 	 21.739922 	 0.530952 	 0.546609
Epoch 80 	 21.703947 	 0.530371 	 0.539712
Epoch 90 	 21.704380 	 0.524002 	 0.537647
Epoch 100 	 21.696144 	 0.517309 	 0.535033
Epoch 110 	 21.693325 	 0.517528 	 0.532482
Epoch 120 	 21.662048 	 0.516196 	 0.528902
Epoch 130 	 21.647726 	 0.514889 	 0.529505
Epoch 140 	 21.633657 	 0.513014 	 0.525658
Epoch 150 	 21.626881 	 0.508395 	 0.527047
Epoch 160 	 21.607613 	 0.511672 	 0.525857
Epoch 170 	 21.615807 	 0.509433 	 0.528540
Epoch 180 	 21.610775 	 0.514411 	 0.523474
Epoch 190 	 21.617760 	 0.513568 	 0.526801
[Model stopped early]
Train loss       : 21.619751
Best valid loss  : 0.507594
Best test loss   : 0.528399
Pruning          : 0.02
