Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288770.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, pillow-simd, future, torch, torchvision, tqdm, kiwisolver, pyparsing, python-dateutil, cycler, matplotlib, termcolor, protobuf, google-pasta, opt-einsum, wrapt, gast, tensorflow-estimator, astor, oauthlib, urllib3, idna, chardet, certifi, requests, requests-oauthlib, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, absl-py, markdown, grpcio, werkzeug, tensorboard, h5py, keras-applications, keras-preprocessing, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288770.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:51:08.751040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:51:08.762503: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_trimming_magnitude_reinit_local_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288770.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7642]
[Starting training]
/localscratch/esling.41288770.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
Epoch 0 	 22.645504 	 0.553892 	 0.554985
Epoch 10 	 21.372223 	 0.499417 	 0.511663
Epoch 20 	 20.102512 	 0.344612 	 0.352662
Epoch 30 	 18.621841 	 0.226899 	 0.233584
Epoch 40 	 17.744089 	 0.185547 	 0.182762
Epoch 50 	 17.272144 	 0.162498 	 0.160758
Epoch 60 	 16.911652 	 0.151125 	 0.142494
Epoch 70 	 16.705404 	 0.148600 	 0.141030
Epoch 80 	 16.559195 	 0.148881 	 0.140949
Epoch 90 	 16.367851 	 0.143552 	 0.135610
Epoch 100 	 16.269564 	 0.140925 	 0.133839
Epoch 110 	 16.224783 	 0.140575 	 0.132934
Epoch 120 	 16.188644 	 0.142313 	 0.131044
Epoch 130 	 16.176304 	 0.136839 	 0.130934
Epoch 140 	 16.166128 	 0.141654 	 0.130287
Epoch 150 	 16.135593 	 0.140685 	 0.131415
Epoch 160 	 16.127481 	 0.140780 	 0.130700
[Model stopped early]
Train loss       : 16.122145
Best valid loss  : 0.134816
Best test loss   : 0.131807
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 3,031,141
--------------------------------
Total memory      : 15.85 MB
Total Flops       : 1.5 GFlops
Total Mem (Read)  : 23.9 MB
Total Mem (Write) : 12.33 MB
[Supermasks testing]
[Untrained loss : 0.7952]
[Starting training]
Epoch 0 	 22.749666 	 0.594063 	 0.597168
Epoch 10 	 21.272966 	 0.478957 	 0.491425
Epoch 20 	 20.285229 	 0.362948 	 0.377822
Epoch 30 	 18.519506 	 0.216669 	 0.219507
Epoch 40 	 17.714693 	 0.173976 	 0.173635
Epoch 50 	 17.368055 	 0.166131 	 0.161850
Epoch 60 	 17.096018 	 0.166192 	 0.159945
Epoch 70 	 16.866978 	 0.147831 	 0.150828
Epoch 80 	 16.629194 	 0.144510 	 0.142781
Epoch 90 	 16.400517 	 0.141883 	 0.136019
Epoch 100 	 16.288870 	 0.142122 	 0.135196
Epoch 110 	 16.205975 	 0.138364 	 0.135270
Epoch 120 	 16.152395 	 0.139193 	 0.134591
Epoch 130 	 16.123343 	 0.139940 	 0.134272
Epoch 140 	 16.110020 	 0.139525 	 0.134535
Epoch 150 	 16.097158 	 0.137333 	 0.133921
Epoch 160 	 16.092384 	 0.139244 	 0.133845
Epoch 170 	 16.067617 	 0.140536 	 0.133092
Epoch 180 	 16.067757 	 0.139595 	 0.132694
Epoch 190 	 16.063957 	 0.140477 	 0.132451
Train loss       : 16.065908
Best valid loss  : 0.133901
Best test loss   : 0.131986
Pruning          : 0.75
0.001
0.001
[Current model size]
================================
Total params      : 2,015,845
--------------------------------
Total memory      : 11.89 MB
Total Flops       : 848.79 MFlops
Total Mem (Read)  : 16.95 MB
Total Mem (Write) : 9.25 MB
[Supermasks testing]
[Untrained loss : 0.7481]
[Starting training]
Epoch 0 	 22.953590 	 0.618847 	 0.619006
Epoch 10 	 21.582968 	 0.517252 	 0.522049
Epoch 20 	 20.668900 	 0.418611 	 0.427859
Epoch 30 	 19.537951 	 0.302645 	 0.307747
/localscratch/esling.41288770.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 40 	 18.614950 	 0.235053 	 0.240687
Epoch 50 	 18.064724 	 0.209322 	 0.211183
Epoch 60 	 17.610935 	 0.190093 	 0.187540
Epoch 70 	 17.242399 	 0.176809 	 0.171304
Epoch 80 	 16.956570 	 0.160663 	 0.158678
Epoch 90 	 16.777222 	 0.154463 	 0.151535
Epoch 100 	 16.626188 	 0.144005 	 0.135013
Epoch 110 	 16.499693 	 0.143779 	 0.133795
Epoch 120 	 16.417192 	 0.140663 	 0.135713
Epoch 130 	 16.243740 	 0.133644 	 0.127995
Epoch 140 	 16.187727 	 0.131802 	 0.132195
Epoch 150 	 16.151310 	 0.139874 	 0.127806
Epoch 160 	 16.105637 	 0.133296 	 0.129273
Epoch 170 	 16.068426 	 0.136106 	 0.127760
[Model stopped early]
Train loss       : 16.064192
Best valid loss  : 0.131802
Best test loss   : 0.132195
Pruning          : 0.56
0.001
0.001
[Current model size]
================================
Total params      : 1,378,357
--------------------------------
Total memory      : 8.92 MB
Total Flops       : 481.03 MFlops
Total Mem (Read)  : 12.21 MB
Total Mem (Write) : 6.94 MB
[Supermasks testing]
[Untrained loss : 0.8065]
[Starting training]
Epoch 0 	 23.023260 	 0.640293 	 0.639243
Epoch 10 	 21.430935 	 0.504611 	 0.515278
Epoch 20 	 20.898151 	 0.439936 	 0.449422
Epoch 30 	 19.558836 	 0.311493 	 0.318750
Epoch 40 	 18.395639 	 0.207825 	 0.208185
Epoch 50 	 17.716507 	 0.164304 	 0.168660
Epoch 60 	 17.300089 	 0.157348 	 0.157274
Epoch 70 	 17.061209 	 0.149644 	 0.150463
Epoch 80 	 16.944021 	 0.147601 	 0.146070
Epoch 90 	 16.735189 	 0.148097 	 0.144589
Epoch 100 	 16.615242 	 0.141982 	 0.140409
Epoch 110 	 16.452044 	 0.135843 	 0.138461
Epoch 120 	 16.406620 	 0.137325 	 0.136693
Epoch 130 	 16.313129 	 0.139037 	 0.135460
Epoch 140 	 16.269430 	 0.138252 	 0.135012
Epoch 150 	 16.243893 	 0.137644 	 0.134563
Epoch 160 	 16.234814 	 0.135955 	 0.132120
[Model stopped early]
Train loss       : 16.234814
Best valid loss  : 0.135092
Best test loss   : 0.135760
Pruning          : 0.42
0.001
0.001
[Current model size]
================================
Total params      : 969,982
--------------------------------
Total memory      : 6.69 MB
Total Flops       : 273.29 MFlops
Total Mem (Read)  : 8.92 MB
Total Mem (Write) : 5.21 MB
[Supermasks testing]
[Untrained loss : 0.7477]
[Starting training]
Epoch 0 	 23.201239 	 0.618662 	 0.619142
Epoch 10 	 21.973335 	 0.589004 	 0.589290
Epoch 20 	 21.598471 	 0.523181 	 0.529385
Epoch 30 	 20.546635 	 0.412616 	 0.412744
Epoch 40 	 19.443521 	 0.290498 	 0.288423
Epoch 50 	 18.598995 	 0.219566 	 0.216552
Epoch 60 	 18.109114 	 0.195249 	 0.191267
Epoch 70 	 17.774979 	 0.188341 	 0.177660
Epoch 80 	 17.534264 	 0.176189 	 0.169298
Epoch 90 	 17.330694 	 0.165864 	 0.158030
Epoch 100 	 17.200031 	 0.166125 	 0.157402
Epoch 110 	 16.995771 	 0.160334 	 0.150872
Epoch 120 	 16.896544 	 0.150169 	 0.147534
Epoch 130 	 16.814774 	 0.154649 	 0.143671
Epoch 140 	 16.716715 	 0.147557 	 0.143097
Epoch 150 	 16.692400 	 0.150719 	 0.141878
[Model stopped early]
Train loss       : 16.669851
Best valid loss  : 0.147437
Best test loss   : 0.146815
Pruning          : 0.32
0.001
0.001
[Current model size]
================================
Total params      : 700,624
--------------------------------
Total memory      : 4.96 MB
Total Flops       : 152.05 MFlops
Total Mem (Read)  : 6.54 MB
Total Mem (Write) : 3.86 MB
[Supermasks testing]
[Untrained loss : 0.7160]
[Starting training]
Epoch 0 	 23.327814 	 0.661331 	 0.663614
Epoch 10 	 21.750381 	 0.545093 	 0.547358
Epoch 20 	 21.202520 	 0.484687 	 0.485740
Epoch 30 	 20.128742 	 0.346237 	 0.358257
Epoch 40 	 19.261728 	 0.278836 	 0.276617
Epoch 50 	 18.635927 	 0.217691 	 0.223723
Epoch 60 	 18.219046 	 0.195496 	 0.186673
Epoch 70 	 17.898098 	 0.173956 	 0.172863
Epoch 80 	 17.667673 	 0.160176 	 0.157559
Epoch 90 	 17.480528 	 0.158812 	 0.153638
Epoch 100 	 17.330423 	 0.161033 	 0.151031
Epoch 110 	 17.119230 	 0.152528 	 0.146032
Epoch 120 	 17.026724 	 0.153117 	 0.142898
Epoch 130 	 16.958389 	 0.153552 	 0.144885
Epoch 140 	 16.920256 	 0.149616 	 0.144179
Epoch 150 	 16.896830 	 0.146020 	 0.143141
Epoch 160 	 16.868984 	 0.149356 	 0.142678
Epoch 170 	 16.816099 	 0.149659 	 0.141591
Epoch 180 	 16.818600 	 0.148455 	 0.141783
Epoch 190 	 16.794884 	 0.149263 	 0.142163
[Model stopped early]
Train loss       : 16.794884
Best valid loss  : 0.143315
Best test loss   : 0.142627
Pruning          : 0.24
0.001
0.001
[Current model size]
================================
Total params      : 522,854
--------------------------------
Total memory      : 3.72 MB
Total Flops       : 87.05 MFlops
Total Mem (Read)  : 4.9 MB
Total Mem (Write) : 2.9 MB
[Supermasks testing]
[Untrained loss : 0.7338]
[Starting training]
Epoch 0 	 23.480509 	 0.680090 	 0.688793
Epoch 10 	 22.132683 	 0.594056 	 0.592015
Epoch 20 	 21.530916 	 0.525397 	 0.527483
Epoch 30 	 20.901138 	 0.444020 	 0.450489
Epoch 40 	 19.933327 	 0.332808 	 0.336308
Epoch 50 	 19.418642 	 0.275604 	 0.283979
Epoch 60 	 18.899912 	 0.243417 	 0.241031
Epoch 70 	 18.587036 	 0.233193 	 0.226491
Epoch 80 	 18.391134 	 0.216726 	 0.215221
Epoch 90 	 18.173330 	 0.200454 	 0.194145
Epoch 100 	 17.961363 	 0.195198 	 0.193586
Epoch 110 	 17.883738 	 0.184915 	 0.184325
Epoch 120 	 17.792847 	 0.187061 	 0.177624
Epoch 130 	 17.628525 	 0.183388 	 0.177042
Epoch 140 	 17.550589 	 0.184114 	 0.178364
Epoch 150 	 17.493900 	 0.179255 	 0.174629
Epoch 160 	 17.434530 	 0.181463 	 0.176199
Epoch 170 	 17.373428 	 0.178757 	 0.170956
Epoch 180 	 17.359426 	 0.173603 	 0.170178
Epoch 190 	 17.339548 	 0.170037 	 0.170746
Train loss       : 17.324301
Best valid loss  : 0.163599
Best test loss   : 0.166202
Pruning          : 0.18
0.001
0.001
[Current model size]
================================
Total params      : 399,974
--------------------------------
Total memory      : 2.73 MB
Total Flops       : 48.02 MFlops
Total Mem (Read)  : 3.66 MB
Total Mem (Write) : 2.13 MB
[Supermasks testing]
[Untrained loss : 0.7042]
[Starting training]
Epoch 0 	 23.513754 	 0.702370 	 0.707680
Epoch 10 	 21.757116 	 0.553263 	 0.551519
Epoch 20 	 21.186682 	 0.479027 	 0.487548
Epoch 30 	 20.576422 	 0.393234 	 0.409841
Epoch 40 	 20.113829 	 0.348567 	 0.362325
Epoch 50 	 19.745657 	 0.308003 	 0.309258
Epoch 60 	 19.466318 	 0.285579 	 0.282741
Epoch 70 	 19.257195 	 0.274573 	 0.269584
Epoch 80 	 19.036972 	 0.253403 	 0.250441
Epoch 90 	 18.817205 	 0.226806 	 0.225346
Epoch 100 	 18.620121 	 0.222024 	 0.216576
Epoch 110 	 18.497402 	 0.209907 	 0.207906
Epoch 120 	 18.343964 	 0.214410 	 0.206142
Epoch 130 	 18.234510 	 0.208387 	 0.197054
Epoch 140 	 18.150520 	 0.192923 	 0.191397
Epoch 150 	 18.071230 	 0.198192 	 0.190180
Epoch 160 	 17.932867 	 0.194047 	 0.189653
Epoch 170 	 17.852322 	 0.187119 	 0.181932
Epoch 180 	 17.821178 	 0.189222 	 0.183556
Epoch 190 	 17.824881 	 0.192340 	 0.183511
Train loss       : 17.756784
Best valid loss  : 0.184092
Best test loss   : 0.181820
Pruning          : 0.13
0.001
0.001
[Current model size]
================================
Total params      : 316,333
--------------------------------
Total memory      : 1.99 MB
Total Flops       : 26.32 MFlops
Total Mem (Read)  : 2.77 MB
Total Mem (Write) : 1.55 MB
[Supermasks testing]
[Untrained loss : 0.7360]
[Starting training]
Epoch 0 	 23.630569 	 0.698796 	 0.705537
Epoch 10 	 21.793325 	 0.555668 	 0.560692
Epoch 20 	 21.528608 	 0.522318 	 0.525771
Epoch 30 	 21.111412 	 0.471870 	 0.473855
Epoch 40 	 20.774439 	 0.417777 	 0.420707
Epoch 50 	 20.390785 	 0.373879 	 0.379851
Epoch 60 	 20.135361 	 0.354016 	 0.353445
Epoch 70 	 19.923071 	 0.326952 	 0.332863
Epoch 80 	 19.719612 	 0.304774 	 0.312085
Epoch 90 	 19.572165 	 0.285513 	 0.295834
Epoch 100 	 19.377377 	 0.272772 	 0.274140
Epoch 110 	 19.211256 	 0.263791 	 0.266207
Epoch 120 	 19.137674 	 0.264279 	 0.262521
Epoch 130 	 19.018089 	 0.244449 	 0.248335
Epoch 140 	 18.863810 	 0.237313 	 0.237251
Epoch 150 	 18.781855 	 0.232861 	 0.223897
Epoch 160 	 18.687624 	 0.226309 	 0.219275
Epoch 170 	 18.604548 	 0.222829 	 0.214357
Epoch 180 	 18.527433 	 0.216287 	 0.209345
Epoch 190 	 18.422131 	 0.218728 	 0.207412
Train loss       : 18.399509
Best valid loss  : 0.208718
Best test loss   : 0.201601
Pruning          : 0.10
0.001
0.001
[Current model size]
================================
Total params      : 257,489
--------------------------------
Total memory      : 1.49 MB
Total Flops       : 15.44 MFlops
Total Mem (Read)  : 2.16 MB
Total Mem (Write) : 1.16 MB
[Supermasks testing]
[Untrained loss : 0.7250]
[Starting training]
Epoch 0 	 23.685759 	 0.679969 	 0.675321
Epoch 10 	 22.019852 	 0.585120 	 0.587999
Epoch 20 	 21.986723 	 0.579440 	 0.582811
Epoch 30 	 21.607016 	 0.538308 	 0.536937
Epoch 40 	 21.331835 	 0.492412 	 0.500803
Epoch 50 	 21.057077 	 0.457051 	 0.467891
Epoch 60 	 20.812946 	 0.420317 	 0.434292
Epoch 70 	 20.644638 	 0.394513 	 0.405875
Epoch 80 	 20.460232 	 0.380876 	 0.395326
Epoch 90 	 20.289335 	 0.361552 	 0.376299
Epoch 100 	 20.179287 	 0.358661 	 0.365085
Epoch 110 	 20.098227 	 0.352098 	 0.361209
Epoch 120 	 19.969975 	 0.328881 	 0.338809
Epoch 130 	 19.904348 	 0.315971 	 0.320286
Epoch 140 	 19.819120 	 0.311876 	 0.312643
Epoch 150 	 19.747679 	 0.301768 	 0.305271
Epoch 160 	 19.655291 	 0.296109 	 0.303376
Epoch 170 	 19.584576 	 0.294096 	 0.298832
Epoch 180 	 19.478037 	 0.290609 	 0.295750
Epoch 190 	 19.426481 	 0.283303 	 0.294005
Train loss       : 19.435221
Best valid loss  : 0.283303
Best test loss   : 0.294005
Pruning          : 0.08
0.001
0.001
[Current model size]
================================
Total params      : 216,037
--------------------------------
Total memory      : 1.08 MB
Total Flops       : 8.58 MFlops
Total Mem (Read)  : 1.68 MB
Total Mem (Write) : 861.98 KB
[Supermasks testing]
[Untrained loss : 0.7661]
[Starting training]
Epoch 0 	 23.546623 	 0.715779 	 0.718799
Epoch 10 	 21.950754 	 0.559292 	 0.565340
Epoch 20 	 21.672611 	 0.528189 	 0.533888
Epoch 30 	 21.349541 	 0.490049 	 0.499942
Epoch 40 	 21.077871 	 0.442423 	 0.447305
Epoch 50 	 20.925398 	 0.429943 	 0.431327
Epoch 60 	 20.765600 	 0.408210 	 0.408275
Epoch 70 	 20.613705 	 0.393357 	 0.398024
Epoch 80 	 20.462460 	 0.383174 	 0.387194
Epoch 90 	 20.351385 	 0.354811 	 0.361986
Epoch 100 	 20.239050 	 0.348399 	 0.356405
Epoch 110 	 20.157980 	 0.348188 	 0.351112
Epoch 120 	 20.051229 	 0.340537 	 0.341260
Epoch 130 	 20.023043 	 0.337219 	 0.338500
Epoch 140 	 19.981100 	 0.322091 	 0.323231
Epoch 150 	 19.830105 	 0.309274 	 0.314947
Epoch 160 	 19.775854 	 0.304852 	 0.312345
Epoch 170 	 19.783737 	 0.307357 	 0.312380
Epoch 180 	 19.716049 	 0.305902 	 0.310775
Epoch 190 	 19.652971 	 0.298773 	 0.305660
Train loss       : 19.659218
Best valid loss  : 0.291833
Best test loss   : 0.303309
Pruning          : 0.06
0.001
0.001
[Current model size]
================================
Total params      : 184,678
--------------------------------
Total memory      : 0.75 MB
Total Flops       : 4.54 MFlops
Total Mem (Read)  : 1.3 MB
Total Mem (Write) : 599.12 KB
[Supermasks testing]
[Untrained loss : 0.7464]
[Starting training]
Epoch 0 	 23.652645 	 0.711703 	 0.715967
Epoch 10 	 22.108780 	 0.578161 	 0.582226
Epoch 20 	 21.754189 	 0.542050 	 0.537924
Epoch 30 	 21.571613 	 0.504675 	 0.516684
Epoch 40 	 21.397932 	 0.483240 	 0.495036
Epoch 50 	 21.277262 	 0.455614 	 0.467034
Epoch 60 	 21.168137 	 0.455289 	 0.460256
Epoch 70 	 21.029842 	 0.450759 	 0.455164
Epoch 80 	 21.009968 	 0.449568 	 0.449995
Epoch 90 	 20.961863 	 0.439040 	 0.446681
Epoch 100 	 20.896873 	 0.442526 	 0.445409
Epoch 110 	 20.856390 	 0.420378 	 0.425552
Epoch 120 	 20.827366 	 0.421642 	 0.425959
Epoch 130 	 20.787458 	 0.425734 	 0.428235
Epoch 140 	 20.757059 	 0.415831 	 0.425979
Epoch 150 	 20.722269 	 0.417702 	 0.422743
Epoch 160 	 20.717583 	 0.415170 	 0.418769
Epoch 170 	 20.716549 	 0.414232 	 0.417237
Epoch 180 	 20.683289 	 0.416824 	 0.418035
Epoch 190 	 20.656397 	 0.411401 	 0.414937
Train loss       : 20.678471
Best valid loss  : 0.404203
Best test loss   : 0.415280
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 162,452
--------------------------------
Total memory      : 0.50 MB
Total Flops       : 2.34 MFlops
Total Mem (Read)  : 1.02 MB
Total Mem (Write) : 401.99 KB
[Supermasks testing]
[Untrained loss : 0.8126]
[Starting training]
Epoch 0 	 23.868948 	 0.766203 	 0.767987
Epoch 10 	 22.109659 	 0.588774 	 0.592271
Epoch 20 	 22.067093 	 0.578263 	 0.584295
Epoch 30 	 21.927061 	 0.564821 	 0.564384
Epoch 40 	 21.737675 	 0.528740 	 0.539588
Epoch 50 	 21.588840 	 0.509815 	 0.519060
Epoch 60 	 21.503977 	 0.498815 	 0.506061
Epoch 70 	 21.430685 	 0.479644 	 0.490877
Epoch 80 	 21.359232 	 0.480563 	 0.482757
Epoch 90 	 21.335318 	 0.478058 	 0.482001
Epoch 100 	 21.239464 	 0.471893 	 0.476144
Epoch 110 	 21.205847 	 0.475268 	 0.476897
Epoch 120 	 21.163622 	 0.467568 	 0.474179
Epoch 130 	 21.120220 	 0.468303 	 0.474211
Epoch 140 	 21.075718 	 0.463286 	 0.469522
Epoch 150 	 21.064079 	 0.459578 	 0.468075
Epoch 160 	 21.050571 	 0.457933 	 0.467566
Epoch 170 	 21.053719 	 0.461140 	 0.462189
Epoch 180 	 21.050182 	 0.457132 	 0.459467
Epoch 190 	 20.999966 	 0.456488 	 0.461395
Train loss       : 21.004160
Best valid loss  : 0.449667
Best test loss   : 0.458731
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 146,768
--------------------------------
Total memory      : 0.34 MB
Total Flops       : 1.28 MFlops
Total Mem (Read)  : 855.98 KB
Total Mem (Write) : 270.55 KB
[Supermasks testing]
[Untrained loss : 0.7026]
[Starting training]
Epoch 0 	 23.852209 	 0.776504 	 0.782012
Epoch 10 	 22.178537 	 0.596170 	 0.602728
Epoch 20 	 21.966887 	 0.560351 	 0.559869
Epoch 30 	 21.847168 	 0.529355 	 0.540428
Epoch 40 	 21.767021 	 0.534370 	 0.540595
Epoch 50 	 21.723038 	 0.522754 	 0.532486
Epoch 60 	 21.677181 	 0.513015 	 0.518412
Epoch 70 	 21.622335 	 0.507991 	 0.512662
Epoch 80 	 21.612762 	 0.494906 	 0.505819
Epoch 90 	 21.526731 	 0.499177 	 0.503900
Epoch 100 	 21.531960 	 0.494574 	 0.500321
Epoch 110 	 21.531712 	 0.491876 	 0.499208
Epoch 120 	 21.481627 	 0.481101 	 0.488538
Epoch 130 	 21.477915 	 0.487270 	 0.490963
Epoch 140 	 21.438465 	 0.480149 	 0.486093
Epoch 150 	 21.397615 	 0.485812 	 0.484339
[Model stopped early]
Train loss       : 21.444515
Best valid loss  : 0.476430
Best test loss   : 0.485422
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 135,317
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 866.6 KFlops
Total Mem (Read)  : 745.48 KB
Total Mem (Write) : 204.79 KB
[Supermasks testing]
[Untrained loss : 0.7465]
[Starting training]
Epoch 0 	 23.741873 	 0.707532 	 0.714866
Epoch 10 	 22.285547 	 0.603554 	 0.600843
Epoch 20 	 22.179325 	 0.585434 	 0.586449
Epoch 30 	 22.077000 	 0.561972 	 0.568573
Epoch 40 	 21.987228 	 0.561222 	 0.563881
Epoch 50 	 21.942345 	 0.546232 	 0.552313
Epoch 60 	 21.883865 	 0.548714 	 0.551616
Epoch 70 	 21.891329 	 0.543079 	 0.546924
Epoch 80 	 21.866539 	 0.540639 	 0.543178
Epoch 90 	 21.816940 	 0.542620 	 0.543987
Epoch 100 	 21.814699 	 0.538900 	 0.538031
Epoch 110 	 21.805592 	 0.540144 	 0.545461
Epoch 120 	 21.809650 	 0.538242 	 0.541565
Epoch 130 	 21.820320 	 0.543473 	 0.542335
[Model stopped early]
Train loss       : 21.810905
Best valid loss  : 0.533023
Best test loss   : 0.540299
Pruning          : 0.02
