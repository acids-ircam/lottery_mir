Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288824.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, cycler, python-dateutil, pyparsing, kiwisolver, matplotlib, absl-py, gast, protobuf, astor, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, oauthlib, urllib3, chardet, certifi, idna, requests, requests-oauthlib, google-auth-oauthlib, grpcio, markdown, werkzeug, tensorboard, wrapt, google-pasta, keras-preprocessing, tensorflow-estimator, h5py, keras-applications, termcolor, opt-einsum, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288824.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:53:40.730458: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:53:40.784424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_masking_gradient_min_rewind_local_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288824.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7373]
[Starting training]
/localscratch/esling.41288824.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
Epoch 0 	 22.694883 	 0.602783 	 0.600365
Epoch 10 	 21.469646 	 0.528714 	 0.520510
Epoch 20 	 20.158903 	 0.379248 	 0.363926
Epoch 30 	 18.309399 	 0.214915 	 0.205655
Epoch 40 	 17.507046 	 0.166497 	 0.155718
Epoch 50 	 17.069822 	 0.147937 	 0.141943
Epoch 60 	 16.810123 	 0.147549 	 0.137039
Epoch 70 	 16.655100 	 0.142657 	 0.136814
Epoch 80 	 16.535788 	 0.145080 	 0.135300
Epoch 90 	 16.332331 	 0.140920 	 0.129267
Epoch 100 	 16.278379 	 0.143229 	 0.130823
Epoch 110 	 16.205507 	 0.141880 	 0.128601
Epoch 120 	 16.173124 	 0.141687 	 0.128423
[Model stopped early]
Train loss       : 16.173124
Best valid loss  : 0.137993
Best test loss   : 0.130059
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.6415]
[Starting training]
Epoch 0 	 17.210783 	 0.152107 	 0.138363
Epoch 10 	 16.742582 	 0.144929 	 0.137067
Epoch 20 	 16.498684 	 0.138857 	 0.132501
Epoch 30 	 16.403385 	 0.143384 	 0.136292
Epoch 40 	 16.287180 	 0.140977 	 0.131003
Epoch 50 	 16.250950 	 0.140753 	 0.132221
Epoch 60 	 16.217546 	 0.140024 	 0.130874
Epoch 70 	 16.197269 	 0.139427 	 0.131248
Epoch 80 	 16.183826 	 0.134759 	 0.131560
Epoch 90 	 16.176971 	 0.139035 	 0.129828
Epoch 100 	 16.172464 	 0.140783 	 0.129714
Epoch 110 	 16.162363 	 0.138382 	 0.129909
[Model stopped early]
Train loss       : 16.185209
Best valid loss  : 0.134759
Best test loss   : 0.131560
Pruning          : 0.70
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7781]
[Starting training]
Epoch 0 	 17.778666 	 0.163041 	 0.148316
Epoch 10 	 16.927378 	 0.147281 	 0.138395
Epoch 20 	 16.735043 	 0.146202 	 0.136893
Epoch 30 	 16.598370 	 0.143144 	 0.136481
Epoch 40 	 16.523972 	 0.146609 	 0.135995
Epoch 50 	 16.351496 	 0.142394 	 0.133727
Epoch 60 	 16.269257 	 0.140744 	 0.133204
Epoch 70 	 16.205997 	 0.138716 	 0.133029
Epoch 80 	 16.170698 	 0.137382 	 0.132324
Epoch 90 	 16.141964 	 0.138560 	 0.131310
Epoch 100 	 16.130714 	 0.139786 	 0.130998
[Model stopped early]
Train loss       : 16.123390
Best valid loss  : 0.136144
Best test loss   : 0.131700
Pruning          : 0.49
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7781]
[Starting training]
Epoch 0 	 18.651892 	 0.220673 	 0.219434
Epoch 10 	 17.192265 	 0.148045 	 0.140241
Epoch 20 	 16.945488 	 0.144458 	 0.135842
Epoch 30 	 16.778236 	 0.143299 	 0.137911
Epoch 40 	 16.574833 	 0.142850 	 0.131131
Epoch 50 	 16.498434 	 0.140841 	 0.132318
Epoch 60 	 16.417316 	 0.143244 	 0.131303
Epoch 70 	 16.401690 	 0.143169 	 0.133033
Epoch 80 	 16.365681 	 0.138773 	 0.132821
Epoch 90 	 16.330790 	 0.140759 	 0.133673
Epoch 100 	 16.238680 	 0.142907 	 0.133678
Epoch 110 	 16.237827 	 0.139057 	 0.130808
Epoch 120 	 16.207447 	 0.139007 	 0.132115
Epoch 130 	 16.180815 	 0.138371 	 0.131653
Epoch 140 	 16.164162 	 0.138334 	 0.131794
Epoch 150 	 16.166988 	 0.137016 	 0.131724
[Model stopped early]
Train loss       : 16.156733
Best valid loss  : 0.133598
Best test loss   : 0.133352
Pruning          : 0.34
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7712]
[Starting training]
Epoch 0 	 19.667067 	 0.392214 	 0.379277
Epoch 10 	 17.598789 	 0.186005 	 0.180202
Epoch 20 	 17.260494 	 0.150023 	 0.139363
Epoch 30 	 17.035355 	 0.150438 	 0.137904
Epoch 40 	 16.954182 	 0.143479 	 0.138988
/localscratch/esling.41288824.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 50 	 16.801964 	 0.140248 	 0.136252
Epoch 60 	 16.734734 	 0.138657 	 0.131173
Epoch 70 	 16.508987 	 0.135301 	 0.128407
Epoch 80 	 16.419210 	 0.134814 	 0.126109
Epoch 90 	 16.382608 	 0.133983 	 0.126988
Epoch 100 	 16.338354 	 0.133720 	 0.127035
Epoch 110 	 16.320333 	 0.131644 	 0.124990
Epoch 120 	 16.309277 	 0.128703 	 0.126463
Epoch 130 	 16.281322 	 0.132745 	 0.126206
Epoch 140 	 16.274494 	 0.133308 	 0.126256
Epoch 150 	 16.266045 	 0.133510 	 0.125794
[Model stopped early]
Train loss       : 16.269917
Best valid loss  : 0.128703
Best test loss   : 0.126463
Pruning          : 0.24
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.6927]
[Starting training]
Epoch 0 	 20.858109 	 0.720170 	 0.718958
Epoch 10 	 18.002926 	 0.218307 	 0.213740
Epoch 20 	 17.591995 	 0.160628 	 0.151796
Epoch 30 	 17.399490 	 0.144801 	 0.140683
Epoch 40 	 17.246910 	 0.145576 	 0.138232
Epoch 50 	 17.128531 	 0.142740 	 0.133570
Epoch 60 	 17.022007 	 0.141927 	 0.136286
Epoch 70 	 16.963278 	 0.145329 	 0.135898
Epoch 80 	 16.878090 	 0.142052 	 0.137301
Epoch 90 	 16.747147 	 0.137560 	 0.135272
Epoch 100 	 16.657906 	 0.139662 	 0.131235
Epoch 110 	 16.624674 	 0.141149 	 0.133071
Epoch 120 	 16.536007 	 0.141182 	 0.133749
Epoch 130 	 16.473099 	 0.140806 	 0.134118
Epoch 140 	 16.474531 	 0.140286 	 0.132294
Epoch 150 	 16.435553 	 0.139521 	 0.133476
Epoch 160 	 16.445841 	 0.139071 	 0.132491
[Model stopped early]
Train loss       : 16.430906
Best valid loss  : 0.135800
Best test loss   : 0.132858
Pruning          : 0.17
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7410]
[Starting training]
Epoch 0 	 22.847445 	 0.715069 	 0.710526
Epoch 10 	 18.514160 	 0.206117 	 0.193996
Epoch 20 	 18.028843 	 0.162913 	 0.156777
Epoch 30 	 17.782265 	 0.153202 	 0.145952
Epoch 40 	 17.601639 	 0.146156 	 0.142378
Epoch 50 	 17.539232 	 0.146455 	 0.138578
Epoch 60 	 17.282881 	 0.145914 	 0.134866
Epoch 70 	 17.208227 	 0.142018 	 0.132061
Epoch 80 	 17.113403 	 0.142321 	 0.133229
Epoch 90 	 17.049444 	 0.139921 	 0.131843
Epoch 100 	 17.018503 	 0.139176 	 0.132526
Epoch 110 	 16.990221 	 0.140004 	 0.133781
Epoch 120 	 16.966040 	 0.141714 	 0.133432
Epoch 130 	 16.938450 	 0.140506 	 0.131683
Epoch 140 	 16.899441 	 0.139375 	 0.131378
Epoch 150 	 16.884266 	 0.135938 	 0.133528
[Model stopped early]
Train loss       : 16.884266
Best valid loss  : 0.135495
Best test loss   : 0.132535
Pruning          : 0.12
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7495]
[Starting training]
Epoch 0 	 23.161894 	 0.775131 	 0.778110
Epoch 10 	 23.148623 	 0.720935 	 0.725324
Epoch 20 	 23.140438 	 0.721678 	 0.725324
Epoch 30 	 23.150854 	 0.720565 	 0.725324
Epoch 40 	 23.150110 	 0.720841 	 0.725324
[Model stopped early]
Train loss       : 23.146389
Best valid loss  : 0.719392
Best test loss   : 0.725324
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7753]
[Starting training]
Epoch 0 	 23.161942 	 0.776789 	 0.778110
Epoch 10 	 23.155697 	 0.722591 	 0.725324
Epoch 20 	 23.144934 	 0.721900 	 0.725324
Epoch 30 	 23.146763 	 0.722417 	 0.725324
[Model stopped early]
Train loss       : 23.149645
Best valid loss  : 0.720825
Best test loss   : 0.725324
Pruning          : 0.06
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.6599]
[Starting training]
Epoch 0 	 23.176853 	 0.778424 	 0.778110
Epoch 10 	 23.146038 	 0.741510 	 0.737407
Epoch 20 	 23.132626 	 0.775984 	 0.778110
Epoch 30 	 23.133738 	 0.724563 	 0.725324
Epoch 40 	 23.142672 	 0.720889 	 0.725324
[Model stopped early]
Train loss       : 23.150854
Best valid loss  : 0.693401
Best test loss   : 0.692918
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7115]
[Starting training]
Epoch 0 	 23.154667 	 0.776043 	 0.778376
Epoch 10 	 23.138578 	 0.722036 	 0.725324
Epoch 20 	 23.146017 	 0.720782 	 0.725324
Epoch 30 	 23.152344 	 0.723275 	 0.725324
Epoch 40 	 23.146391 	 0.721842 	 0.725324
Epoch 50 	 23.151226 	 0.721797 	 0.725324
Epoch 60 	 23.138950 	 0.722548 	 0.725324
Epoch 70 	 23.138205 	 0.721991 	 0.725324
[Model stopped early]
Train loss       : 23.140066
Best valid loss  : 0.719646
Best test loss   : 0.725324
Pruning          : 0.03
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7716]
[Starting training]
Epoch 0 	 23.192629 	 0.750580 	 0.744756
Epoch 10 	 23.121281 	 0.775575 	 0.778110
Epoch 20 	 23.107632 	 0.777708 	 0.778110
Epoch 30 	 23.106270 	 0.778318 	 0.778110
Epoch 40 	 23.108429 	 0.775823 	 0.778110
[Model stopped early]
Train loss       : 23.109598
Best valid loss  : 0.679276
Best test loss   : 0.674158
Pruning          : 0.02
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7090]
[Starting training]
Epoch 0 	 23.209789 	 0.725801 	 0.724603
Epoch 10 	 23.133757 	 0.776537 	 0.778110
Epoch 20 	 23.133013 	 0.776000 	 0.778110
Epoch 30 	 23.124172 	 0.775986 	 0.778110
Epoch 40 	 23.124510 	 0.776405 	 0.778110
[Model stopped early]
Train loss       : 23.123598
Best valid loss  : 0.714739
Best test loss   : 0.716307
Pruning          : 0.01
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7099]
[Starting training]
Epoch 0 	 23.192450 	 0.710933 	 0.707836
Epoch 10 	 23.132198 	 0.742706 	 0.737407
Epoch 20 	 23.120621 	 0.776147 	 0.778110
Epoch 30 	 23.118374 	 0.776613 	 0.778110
[Model stopped early]
Train loss       : 23.114542
Best valid loss  : 0.710933
Best test loss   : 0.707836
Pruning          : 0.01
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7374]
[Starting training]
Epoch 0 	 23.189255 	 0.671588 	 0.669234
Epoch 10 	 23.106340 	 0.742911 	 0.737407
Epoch 20 	 23.095480 	 0.777368 	 0.778110
Epoch 30 	 23.114895 	 0.775510 	 0.778110
[Model stopped early]
Train loss       : 23.112778
Best valid loss  : 0.642613
Best test loss   : 0.649769
Pruning          : 0.01
