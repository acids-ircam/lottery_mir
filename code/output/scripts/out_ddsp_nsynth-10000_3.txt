Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41146326.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, pillow-simd, future, torch, torchvision, tqdm, python-dateutil, pyparsing, kiwisolver, cycler, matplotlib, wrapt, h5py, keras-applications, keras-preprocessing, tensorflow-estimator, google-pasta, grpcio, termcolor, opt-einsum, absl-py, gast, markdown, protobuf, chardet, urllib3, idna, certifi, requests, werkzeug, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, astor, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146326.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-27 03:02:40.449816: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-27 03:02:40.807481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_ddsp_cnn_xavier_trimming_activation_reinit_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41146326.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 83.3808]
[Starting training]
Epoch 0 	 75.719818 	 20264.539062 	 74.765198
Epoch 10 	 62.613171 	 56.480545 	 57.344727
Epoch 20 	 47.356941 	 42.243252 	 43.797501
Epoch 30 	 38.859871 	 37.076595 	 38.797180
Epoch 40 	 36.029636 	 33.227566 	 35.110973
Epoch 50 	 32.958885 	 30.910521 	 32.606182
Epoch 60 	 31.809387 	 29.823164 	 31.573853
Epoch 70 	 30.417040 	 29.117821 	 31.214840
Epoch 80 	 29.994551 	 28.139883 	 30.241163
Epoch 90 	 28.633869 	 29.419413 	 31.287752
Epoch 100 	 27.865170 	 26.850540 	 28.638540
Epoch 110 	 27.770594 	 26.432333 	 28.576670
Epoch 120 	 26.784060 	 26.391117 	 28.417778
Epoch 130 	 26.297426 	 25.790716 	 27.472786
Epoch 140 	 26.563749 	 26.140821 	 28.137215
Epoch 150 	 26.219738 	 25.824320 	 27.533962
Epoch 160 	 30.347324 	 32.005272 	 34.042030
Epoch 170 	 25.453255 	 24.792139 	 26.583504
Epoch 180 	 24.911886 	 24.624495 	 26.581526
Epoch 190 	 29.149778 	 28.000093 	 29.909174
Train loss       : 24.270340
Best valid loss  : 24.191982
Best test loss   : 26.080297
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,909,528
--------------------------------
Total memory      : 35.03 MB
Total Flops       : 326.2 MFlops
Total Mem (Read)  : 31.67 MB
Total Mem (Write) : 24.49 MB
[Supermasks testing]
[Untrained loss : 94.0898]
[Starting training]
Epoch 0 	 68.570099 	 54.615139 	 55.114193
Epoch 10 	 46.936741 	 40.008911 	 41.513226
Epoch 20 	 38.860394 	 36.846256 	 38.319706
Epoch 30 	 35.476105 	 32.020557 	 33.985821
Epoch 40 	 33.782410 	 31.329786 	 33.446335
Epoch 50 	 31.731239 	 29.355373 	 31.482985
Epoch 60 	 31.124929 	 28.984741 	 30.833113
Epoch 70 	 32.127953 	 29.427250 	 31.067257
Epoch 80 	 29.225996 	 27.621069 	 29.518461
Epoch 90 	 28.282173 	 27.486113 	 29.121433
Epoch 100 	 27.648108 	 26.782589 	 28.691912
Epoch 110 	 27.046888 	 26.779127 	 28.554422
Epoch 120 	 26.721899 	 26.375772 	 27.934504
Epoch 130 	 26.292929 	 26.617374 	 28.369137
Epoch 140 	 25.997202 	 25.996048 	 27.676092
Epoch 150 	 25.625154 	 26.024767 	 27.810406
Epoch 160 	 25.194393 	 25.869257 	 27.400074
Epoch 170 	 25.038059 	 25.598974 	 27.294231
Epoch 180 	 24.859795 	 25.489416 	 27.192869
Epoch 190 	 24.501249 	 25.094936 	 26.926371
Train loss       : 24.242599
Best valid loss  : 24.912426
Best test loss   : 26.747871
Pruning          : 0.72
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,898,752
--------------------------------
Total memory      : 29.65 MB
Total Flops       : 171.22 MFlops
Total Mem (Read)  : 22.92 MB
Total Mem (Write) : 17.34 MB
[Supermasks testing]
[Untrained loss : 75.3855]
[Starting training]
Epoch 0 	 70.296295 	 55.856766 	 55.457497
Epoch 10 	 46.628452 	 42.708843 	 44.463917
Epoch 20 	 39.305859 	 36.256966 	 37.590481
Epoch 30 	 35.755749 	 34.177483 	 36.072479
Epoch 40 	 34.197865 	 32.417015 	 33.980907
Epoch 50 	 32.125824 	 30.304804 	 32.057598
Epoch 60 	 31.020039 	 29.661745 	 31.773615
Epoch 70 	 29.933828 	 28.653246 	 30.813093
Epoch 80 	 28.844807 	 27.942125 	 29.691116
Epoch 90 	 28.234451 	 28.175489 	 29.749552
Epoch 100 	 27.691853 	 26.897949 	 28.701191
Epoch 110 	 27.429796 	 26.339365 	 28.069040
Epoch 120 	 26.897623 	 26.045752 	 27.853750
Epoch 130 	 26.668507 	 26.431772 	 28.162451
Epoch 140 	 26.389189 	 26.759781 	 28.494150
Epoch 150 	 26.091452 	 26.392300 	 28.356016
Epoch 160 	 24.828136 	 25.037649 	 26.903368
Epoch 170 	 24.631676 	 25.360847 	 27.092945
Epoch 180 	 24.479832 	 25.084436 	 26.756575
Epoch 190 	 24.234026 	 25.196800 	 26.783783
Train loss       : 23.896675
Best valid loss  : 24.533686
Best test loss   : 26.274176
Pruning          : 0.52
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,282,494
--------------------------------
Total memory      : 25.75 MB
Total Flops       : 89.46 MFlops
Total Mem (Read)  : 17.01 MB
Total Mem (Write) : 12.62 MB
[Supermasks testing]
[Untrained loss : 85.9747]
[Starting training]
Epoch 0 	 71.967316 	 54.785927 	 54.809101
Epoch 10 	 46.678162 	 43.527428 	 43.648857
Epoch 20 	 39.406376 	 37.697193 	 39.006927
Epoch 30 	 36.977924 	 34.201912 	 36.257137
Epoch 40 	 33.732487 	 32.062199 	 33.714199
Epoch 50 	 31.910975 	 30.218184 	 32.233585
Epoch 60 	 31.026024 	 30.180063 	 31.720600
Epoch 70 	 32.211220 	 30.399881 	 31.884876
Epoch 80 	 28.869942 	 28.119637 	 29.865679
Epoch 90 	 28.166384 	 28.683374 	 30.275196
Epoch 100 	 27.767471 	 27.626093 	 29.684530
Epoch 110 	 27.470121 	 27.404285 	 29.044914
Epoch 120 	 26.934589 	 26.798536 	 28.678425
Epoch 130 	 26.735920 	 26.778179 	 28.487837
Epoch 140 	 26.512974 	 27.032454 	 28.729599
Epoch 150 	 25.831022 	 26.466711 	 28.154278
Epoch 160 	 25.808641 	 26.388891 	 28.017574
Epoch 170 	 25.871750 	 26.293127 	 28.141504
Epoch 180 	 25.554522 	 26.182051 	 27.902588
Epoch 190 	 25.440140 	 26.095591 	 27.875298
Train loss       : 25.008085
Best valid loss  : 25.860855
Best test loss   : 27.636345
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 887,398
--------------------------------
Total memory      : 22.89 MB
Total Flops       : 46.15 MFlops
Total Mem (Read)  : 12.89 MB
Total Mem (Write) : 9.4 MB
[Supermasks testing]
[Untrained loss : 83.3042]
[Starting training]
Epoch 0 	 71.193092 	 56.964489 	 57.190022
Epoch 10 	 53.698753 	 50.618374 	 51.556061
Epoch 20 	 48.128162 	 45.867664 	 45.077633
Epoch 30 	 44.896370 	 43.323986 	 43.331738
Epoch 40 	 42.555664 	 40.761192 	 41.439655
Epoch 50 	 39.702278 	 39.344982 	 39.921085
Epoch 60 	 37.787682 	 39.664322 	 40.171192
Epoch 70 	 36.655910 	 38.267426 	 38.632587
Epoch 80 	 35.080147 	 36.986599 	 37.079212
Epoch 90 	 34.017998 	 36.832924 	 37.085545
Epoch 100 	 34.007427 	 36.184010 	 36.624767
Epoch 110 	 32.917191 	 35.865940 	 36.227531
Epoch 120 	 31.959782 	 35.728558 	 36.234283
Epoch 130 	 31.507675 	 35.798725 	 36.177383
Epoch 140 	 30.960222 	 34.512447 	 35.082359
Epoch 150 	 30.449280 	 34.367466 	 34.902046
Epoch 160 	 29.994583 	 34.199619 	 34.537514
Epoch 170 	 30.604967 	 33.808655 	 34.659233
Epoch 180 	 29.256168 	 33.987488 	 34.755287
Epoch 190 	 28.396503 	 33.549946 	 34.097679
Train loss       : 28.096113
Best valid loss  : 33.535027
Best test loss   : 34.057972
Pruning          : 0.27
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 630,951
--------------------------------
Total memory      : 20.84 MB
Total Flops       : 23.99 MFlops
Total Mem (Read)  : 10.04 MB
Total Mem (Write) : 7.21 MB
[Supermasks testing]
[Untrained loss : 85.5805]
[Starting training]
Epoch 0 	 72.830002 	 112.600746 	 210.750549
Epoch 10 	 51.906689 	 49.152195 	 49.595547
Epoch 20 	 47.792671 	 44.799854 	 44.591953
Epoch 30 	 45.440804 	 43.356556 	 43.197460
Epoch 40 	 43.080891 	 42.116356 	 42.041897
Epoch 50 	 41.146473 	 40.298820 	 40.674393
Epoch 60 	 39.969601 	 39.792992 	 40.776657
Epoch 70 	 37.710915 	 39.014500 	 39.739529
Epoch 80 	 37.237659 	 37.751774 	 38.629337
Epoch 90 	 36.023354 	 37.817196 	 38.591747
Epoch 100 	 36.247929 	 40.477703 	 41.151325
Epoch 110 	 34.388695 	 36.340145 	 37.066471
Epoch 120 	 35.676563 	 37.267803 	 37.447571
Epoch 130 	 32.063660 	 35.300415 	 36.066196
Epoch 140 	 31.634426 	 35.463921 	 35.946621
Epoch 150 	 31.057985 	 34.857441 	 35.649002
Epoch 160 	 30.808390 	 34.490871 	 35.530792
Epoch 170 	 30.680067 	 34.891994 	 35.446484
Epoch 180 	 30.262676 	 34.853470 	 35.290913
Epoch 190 	 30.165901 	 34.468391 	 35.357819
Train loss       : 30.104687
Best valid loss  : 34.468391
Best test loss   : 35.357819
Pruning          : 0.19
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 461,817
--------------------------------
Total memory      : 19.39 MB
Total Flops       : 12.83 MFlops
Total Mem (Read)  : 8.07 MB
Total Mem (Write) : 5.74 MB
[Supermasks testing]
[Untrained loss : 83.2990]
[Starting training]
Epoch 0 	 73.975662 	 63.720474 	 62.892887
Epoch 10 	 48.575146 	 46.846035 	 47.187431
Epoch 20 	 45.771717 	 43.458603 	 43.679619
Epoch 30 	 43.480640 	 42.163807 	 42.475163
Epoch 40 	 42.392159 	 40.548141 	 41.721474
Epoch 50 	 40.759537 	 40.507225 	 41.747627
Epoch 60 	 39.449123 	 38.138004 	 39.970589
Epoch 70 	 38.221493 	 37.753521 	 38.615250
Epoch 80 	 37.393524 	 36.854691 	 38.393589
Epoch 90 	 36.806164 	 37.586220 	 38.486328
Epoch 100 	 36.510460 	 36.398594 	 37.710194
Epoch 110 	 35.597191 	 35.761211 	 36.841808
Epoch 120 	 35.274231 	 36.126915 	 37.854424
Epoch 130 	 34.049591 	 34.871311 	 35.732826
Epoch 140 	 33.581654 	 35.102646 	 35.805534
Epoch 150 	 33.006882 	 34.551003 	 35.434814
Epoch 160 	 33.045799 	 35.166054 	 35.993958
Epoch 170 	 32.673599 	 34.368042 	 35.358223
Epoch 180 	 32.574883 	 34.414242 	 35.284286
Epoch 190 	 32.505356 	 34.533024 	 35.325356
Train loss       : 32.282509
Best valid loss  : 34.014339
Best test loss   : 35.322079
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 348,567
--------------------------------
Total memory      : 18.35 MB
Total Flops       : 7.06 MFlops
Total Mem (Read)  : 6.7 MB
Total Mem (Write) : 4.71 MB
[Supermasks testing]
[Untrained loss : 101.0800]
[Starting training]
Epoch 0 	 76.958130 	 62.418251 	 63.741268
Epoch 10 	 49.713654 	 47.118954 	 46.689003
Epoch 20 	 46.040852 	 44.595726 	 44.435383
Epoch 30 	 44.577732 	 49.305363 	 44.023014
Epoch 40 	 43.347382 	 41.773628 	 42.483917
Epoch 50 	 41.935204 	 40.825916 	 41.692909
Epoch 60 	 41.583141 	 39.910831 	 40.650578
Epoch 70 	 40.549477 	 39.587044 	 40.475319
Epoch 80 	 39.096783 	 39.485676 	 39.923580
Epoch 90 	 39.379856 	 39.129623 	 39.430889
Epoch 100 	 38.005371 	 39.979874 	 39.462833
Epoch 110 	 37.605011 	 38.292767 	 39.008854
Epoch 120 	 37.492516 	 38.836468 	 39.552456
Epoch 130 	 37.331379 	 38.480923 	 39.561073
Epoch 140 	 36.413372 	 37.483421 	 38.515636
Epoch 150 	 36.152973 	 37.683220 	 38.485432
Epoch 160 	 36.043571 	 37.508488 	 38.376938
Epoch 170 	 35.965893 	 37.235111 	 38.264187
Epoch 180 	 35.755371 	 37.394375 	 38.197548
Epoch 190 	 35.673088 	 37.400211 	 38.129929
[Model stopped early]
Train loss       : 35.676121
Best valid loss  : 37.099976
Best test loss   : 38.222263
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 272,819
--------------------------------
Total memory      : 17.56 MB
Total Flops       : 3.79 MFlops
Total Mem (Read)  : 5.68 MB
Total Mem (Write) : 3.94 MB
[Supermasks testing]
[Untrained loss : 88.3418]
[Starting training]
Epoch 0 	 77.241814 	 77.136139 	 88.048622
Epoch 10 	 49.557838 	 46.477489 	 46.627895
Epoch 20 	 46.822102 	 46.575226 	 47.655571
Epoch 30 	 45.781330 	 42.951244 	 43.758602
Epoch 40 	 44.665890 	 42.486801 	 43.823761
Epoch 50 	 43.857033 	 41.737228 	 42.621700
Epoch 60 	 43.276211 	 41.473370 	 41.993229
Epoch 70 	 42.575314 	 40.648575 	 41.891205
Epoch 80 	 42.040718 	 40.486549 	 41.450989
Epoch 90 	 41.905071 	 39.803974 	 41.342918
Epoch 100 	 40.437027 	 39.375984 	 39.920326
Epoch 110 	 39.998993 	 39.046825 	 40.087910
Epoch 120 	 40.032234 	 39.572735 	 40.096195
Epoch 130 	 39.944393 	 39.247456 	 40.008869
Epoch 140 	 39.715176 	 38.612274 	 39.725262
Epoch 150 	 39.436768 	 38.855511 	 39.482643
Epoch 160 	 38.983704 	 38.191082 	 39.224525
Epoch 170 	 38.763302 	 38.349678 	 39.319366
Epoch 180 	 38.543503 	 38.188179 	 39.250278
[Model stopped early]
Train loss       : 38.417114
Best valid loss  : 38.150505
Best test loss   : 39.273613
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 220,983
--------------------------------
Total memory      : 17.11 MB
Total Flops       : 2.56 MFlops
Total Mem (Read)  : 5.08 MB
Total Mem (Write) : 3.52 MB
[Supermasks testing]
[Untrained loss : 84.8067]
[Starting training]
Epoch 0 	 77.510345 	 67.887917 	 70.044395
Epoch 10 	 56.010075 	 53.201218 	 53.503288
Epoch 20 	 51.769867 	 49.503407 	 49.881451
Epoch 30 	 49.277111 	 47.752995 	 47.518219
Epoch 40 	 47.781021 	 45.356884 	 45.716595
Epoch 50 	 46.606930 	 44.049911 	 44.288433
Epoch 60 	 46.407738 	 44.937298 	 45.397713
Epoch 70 	 45.390518 	 43.419323 	 44.004677
Epoch 80 	 44.858318 	 43.313686 	 43.578915
Epoch 90 	 44.712406 	 42.681393 	 43.099174
Epoch 100 	 44.344185 	 42.165115 	 43.094948
Epoch 110 	 43.778210 	 42.355068 	 43.064579
Epoch 120 	 42.797398 	 41.319832 	 41.944778
Epoch 130 	 42.689117 	 41.846458 	 41.945126
Epoch 140 	 41.740772 	 41.617382 	 41.970684
Epoch 150 	 41.966892 	 41.469700 	 41.945103
Epoch 160 	 41.737774 	 41.055229 	 41.385792
Epoch 170 	 41.693142 	 41.040756 	 41.572292
Epoch 180 	 41.432690 	 40.909782 	 41.286465
Epoch 190 	 41.475380 	 40.758644 	 41.276123
Train loss       : 41.360054
Best valid loss  : 40.622631
Best test loss   : 41.288425
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 185,398
--------------------------------
Total memory      : 16.72 MB
Total Flops       : 1.66 MFlops
Total Mem (Read)  : 4.58 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 96.4541]
[Starting training]
Epoch 0 	 78.846596 	 69.168556 	 71.206627
Epoch 10 	 55.634068 	 50.686024 	 50.815472
Epoch 20 	 52.337643 	 48.965134 	 48.828762
Epoch 30 	 51.437954 	 48.910259 	 48.945343
Epoch 40 	 50.911255 	 47.425388 	 47.569786
Epoch 50 	 50.197330 	 47.512943 	 47.095955
Epoch 60 	 50.064827 	 46.860523 	 46.498226
Epoch 70 	 49.338043 	 46.171032 	 46.017685
Epoch 80 	 48.686077 	 46.290470 	 46.109554
Epoch 90 	 48.404427 	 45.380020 	 44.969814
Epoch 100 	 47.708782 	 44.917801 	 44.774643
Epoch 110 	 47.393509 	 45.067070 	 45.083115
Epoch 120 	 47.104660 	 45.025436 	 44.894058
Epoch 130 	 46.684227 	 44.196575 	 44.234867
Epoch 140 	 46.251484 	 44.251686 	 43.944851
Epoch 150 	 46.273521 	 44.136219 	 44.040646
Epoch 160 	 46.373013 	 43.995678 	 44.164955
Epoch 170 	 46.100964 	 43.366890 	 43.916000
Epoch 180 	 46.023945 	 43.581608 	 44.174656
Epoch 190 	 46.032810 	 43.758072 	 44.038311
Train loss       : 45.769745
Best valid loss  : 43.265587
Best test loss   : 44.046902
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 156,046
--------------------------------
Total memory      : 16.63 MB
Total Flops       : 1.63 MFlops
Total Mem (Read)  : 4.41 MB
Total Mem (Write) : 3.08 MB
[Supermasks testing]
[Untrained loss : 76.9140]
[Starting training]
Epoch 0 	 75.417847 	 63.531906 	 65.743980
Epoch 10 	 56.627773 	 52.542065 	 52.502949
Epoch 20 	 55.349098 	 51.084305 	 51.311905
Epoch 30 	 52.293289 	 48.672127 	 48.827858
Epoch 40 	 51.204510 	 47.337811 	 47.808453
Epoch 50 	 50.172901 	 47.408283 	 47.783409
Epoch 60 	 49.289665 	 45.594978 	 46.035904
Epoch 70 	 48.835545 	 45.312141 	 45.541054
Epoch 80 	 48.481480 	 45.166405 	 45.232906
Epoch 90 	 47.913788 	 45.070183 	 45.453209
Epoch 100 	 47.630383 	 45.197399 	 45.386745
Epoch 110 	 47.307858 	 44.525669 	 44.995064
Epoch 120 	 47.110008 	 44.596725 	 44.924049
[Model stopped early]
Train loss       : 47.261921
Best valid loss  : 44.192936
Best test loss   : 44.995621
Pruning          : 0.03
[Performing one full cumulative epoch]
Traceback (most recent call last):
  File "main.py", line 261, in <module>
    model = pruning.reset(model)
  File "/scratch/esling/lottery/pruning.py", line 781, in reset
    replace_recurrent(m, l, m.unprune_idx[l], prev_kept)
  File "/scratch/esling/lottery/pruning.py", line 752, in replace_recurrent
    cur_ih = nn.Parameter(cur_ih[rep_id0])#torch.from_numpy(cur_ih[rep_id0]).to(self.args.device))
IndexError: too many indices for tensor of dimension 2
