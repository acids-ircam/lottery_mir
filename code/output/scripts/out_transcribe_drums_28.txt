Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288884.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, pillow-simd, six, torchvision, tqdm, kiwisolver, pyparsing, cycler, python-dateutil, matplotlib, protobuf, astor, gast, google-pasta, certifi, chardet, urllib3, idna, requests, oauthlib, requests-oauthlib, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, google-auth-oauthlib, grpcio, werkzeug, absl-py, markdown, tensorboard, keras-preprocessing, termcolor, wrapt, opt-einsum, h5py, keras-applications, tensorflow-estimator, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288884.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 09:36:55.367789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 09:36:55.381348: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is drums_transcribe_cnn_xavier_masking_magnitude_rewind_local_0.
*******
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
/localscratch/esling.41288884.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.8782]
[Starting training]
/localscratch/esling.41288884.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
/localscratch/esling.41288884.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 0 	 57.494671 	 0.887834 	 0.886540
Epoch 10 	 56.006744 	 0.887711 	 0.886540
Epoch 20 	 56.009720 	 0.888101 	 0.886540
Epoch 30 	 56.011208 	 0.887385 	 0.886540
Epoch 40 	 56.013439 	 0.887438 	 0.886540
Epoch 50 	 56.003765 	 0.888310 	 0.886540
Epoch 60 	 56.013439 	 0.887782 	 0.886540
Epoch 70 	 56.008976 	 0.888670 	 0.886540
Epoch 80 	 56.009720 	 0.886603 	 0.886540
Epoch 90 	 56.000046 	 0.887654 	 0.886540
Epoch 100 	 55.979958 	 0.888008 	 0.886540
[Model stopped early]
Train loss       : 56.026459
Best valid loss  : 0.886515
Best test loss   : 0.886540
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.963142 	 0.887856 	 0.886540
Epoch 10 	 55.938751 	 0.886937 	 0.886540
Epoch 20 	 55.913120 	 0.888149 	 0.886540
Epoch 30 	 55.920734 	 0.887779 	 0.886540
Epoch 40 	 55.923553 	 0.887263 	 0.885839
Epoch 50 	 55.896286 	 0.886711 	 0.884791
Epoch 60 	 55.921055 	 0.889126 	 0.886440
Epoch 70 	 55.902081 	 0.887624 	 0.886017
Epoch 80 	 55.897820 	 0.888709 	 0.886340
[Model stopped early]
Train loss       : 55.891666
Best valid loss  : 0.884522
Best test loss   : 0.881164
Pruning          : 0.70
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.960880 	 0.888117 	 0.886540
Epoch 10 	 55.885654 	 0.866723 	 0.863397
Epoch 20 	 55.866055 	 0.873004 	 0.865672
Epoch 30 	 55.860039 	 0.872689 	 0.869283
Epoch 40 	 55.840630 	 0.872327 	 0.866036
[Model stopped early]
Train loss       : 55.830002
Best valid loss  : 0.864104
Best test loss   : 0.860473
Pruning          : 0.49
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.930531 	 0.888017 	 0.886407
Epoch 10 	 55.677280 	 0.866983 	 0.860298
Epoch 20 	 55.624046 	 0.863784 	 0.855429
Epoch 30 	 55.580845 	 0.864635 	 0.854396
Epoch 40 	 55.512871 	 0.859421 	 0.850119
[Model stopped early]
Train loss       : 55.522392
Best valid loss  : 0.854678
Best test loss   : 0.846473
Pruning          : 0.34
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.938774 	 0.871624 	 0.867835
Epoch 10 	 55.766491 	 0.859015 	 0.850257
Epoch 20 	 55.721344 	 0.865076 	 0.854891
Epoch 30 	 55.712933 	 0.861777 	 0.854500
Epoch 40 	 55.671673 	 0.858235 	 0.851391
Epoch 50 	 55.664120 	 0.860160 	 0.853817
Epoch 60 	 55.624226 	 0.856517 	 0.851713
Epoch 70 	 55.618942 	 0.859563 	 0.851742
Epoch 80 	 55.589432 	 0.856799 	 0.850738
Epoch 90 	 55.571674 	 0.856836 	 0.850575
Epoch 100 	 55.564022 	 0.855770 	 0.850078
Epoch 110 	 55.542511 	 0.855543 	 0.849302
Epoch 120 	 55.525711 	 0.856802 	 0.849701
Epoch 130 	 55.515877 	 0.858341 	 0.849307
Epoch 140 	 55.528694 	 0.858065 	 0.849194
[Model stopped early]
Train loss       : 55.531570
Best valid loss  : 0.854971
Best test loss   : 0.849282
Pruning          : 0.24
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.895649 	 0.874116 	 0.866007
Epoch 10 	 55.344151 	 0.815363 	 0.798128
Epoch 20 	 55.229382 	 0.810586 	 0.793953
Epoch 30 	 55.051739 	 0.799968 	 0.784225
Epoch 40 	 54.984306 	 0.799904 	 0.779585
Epoch 50 	 54.846123 	 0.790845 	 0.773573
Epoch 60 	 54.754650 	 0.784441 	 0.768512
Epoch 70 	 54.747379 	 0.780724 	 0.761353
Epoch 80 	 54.660088 	 0.777919 	 0.759343
Epoch 90 	 54.608540 	 0.777812 	 0.764893
Epoch 100 	 54.546539 	 0.770345 	 0.757502
Epoch 110 	 54.502277 	 0.767075 	 0.753395
Epoch 120 	 54.470493 	 0.766968 	 0.749388
Epoch 130 	 54.431377 	 0.763662 	 0.746130
Epoch 140 	 54.373055 	 0.764807 	 0.743144
Epoch 150 	 54.355015 	 0.760972 	 0.739959
Epoch 160 	 54.306995 	 0.763986 	 0.745395
Epoch 170 	 54.254917 	 0.758420 	 0.737574
Epoch 180 	 54.174438 	 0.746100 	 0.727855
Epoch 190 	 54.165329 	 0.749821 	 0.729396
Train loss       : 54.126564
Best valid loss  : 0.738801
Best test loss   : 0.725697
Pruning          : 0.17
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.859306 	 0.858301 	 0.848984
Epoch 10 	 55.104950 	 0.802197 	 0.788448
Epoch 20 	 54.876930 	 0.794453 	 0.778963
Epoch 30 	 54.689911 	 0.774112 	 0.760470
Epoch 40 	 54.642342 	 0.775702 	 0.754452
Epoch 50 	 54.576538 	 0.762367 	 0.750816
Epoch 60 	 54.488503 	 0.767506 	 0.754076
Epoch 70 	 54.470028 	 0.764502 	 0.746583
Epoch 80 	 54.390503 	 0.752932 	 0.738765
Epoch 90 	 54.343262 	 0.756855 	 0.737694
Epoch 100 	 54.302399 	 0.747819 	 0.731565
Epoch 110 	 54.291538 	 0.748507 	 0.734432
Epoch 120 	 54.278473 	 0.748902 	 0.729486
Epoch 130 	 54.240265 	 0.748317 	 0.730263
Epoch 140 	 54.212395 	 0.748624 	 0.730718
Epoch 150 	 54.203217 	 0.745099 	 0.728641
Epoch 160 	 54.184319 	 0.747761 	 0.728489
Epoch 170 	 54.169834 	 0.742480 	 0.726767
[Model stopped early]
Train loss       : 54.175327
Best valid loss  : 0.741415
Best test loss   : 0.728681
Pruning          : 0.12
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.887493 	 0.858787 	 0.851318
Epoch 10 	 55.141178 	 0.788860 	 0.778145
Epoch 20 	 54.906307 	 0.780655 	 0.760281
Epoch 30 	 54.762287 	 0.775727 	 0.756455
Epoch 40 	 54.629086 	 0.758806 	 0.742232
Epoch 50 	 54.569954 	 0.764538 	 0.743325
Epoch 60 	 54.415104 	 0.754639 	 0.738069
Epoch 70 	 54.337635 	 0.752376 	 0.735831
Epoch 80 	 54.295418 	 0.754387 	 0.735836
Epoch 90 	 54.244030 	 0.745227 	 0.729969
Epoch 100 	 54.208996 	 0.747480 	 0.732901
Epoch 110 	 54.184383 	 0.750495 	 0.730331
Epoch 120 	 54.178402 	 0.747712 	 0.730116
Epoch 130 	 54.157017 	 0.748320 	 0.730007
Epoch 140 	 54.157970 	 0.747540 	 0.730098
Epoch 150 	 54.160076 	 0.745093 	 0.730481
[Model stopped early]
Train loss       : 54.141155
Best valid loss  : 0.744140
Best test loss   : 0.729736
Pruning          : 0.08
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.811775 	 0.866904 	 0.856419
Epoch 10 	 54.460728 	 0.755644 	 0.739254
Epoch 20 	 54.278820 	 0.750666 	 0.730428
Epoch 30 	 54.214825 	 0.752194 	 0.734567
Epoch 40 	 54.154842 	 0.736456 	 0.722130
Epoch 50 	 54.140728 	 0.747317 	 0.727459
Epoch 60 	 54.069424 	 0.736784 	 0.720106
Epoch 70 	 54.038155 	 0.739367 	 0.721013
Epoch 80 	 53.999519 	 0.733584 	 0.715002
Epoch 90 	 53.934437 	 0.727607 	 0.714096
Epoch 100 	 53.889236 	 0.729385 	 0.709599
Epoch 110 	 53.870026 	 0.721285 	 0.708395
Epoch 120 	 53.858063 	 0.725274 	 0.710905
Epoch 130 	 53.832451 	 0.718807 	 0.705838
Epoch 140 	 53.822109 	 0.721118 	 0.705500
Epoch 150 	 53.820942 	 0.719836 	 0.706970
Epoch 160 	 53.801201 	 0.715305 	 0.705195
Epoch 170 	 53.815063 	 0.718909 	 0.705427
Epoch 180 	 53.802254 	 0.718764 	 0.704258
Epoch 190 	 53.793671 	 0.714691 	 0.705604
[Model stopped early]
Train loss       : 53.810349
Best valid loss  : 0.712494
Best test loss   : 0.705037
Pruning          : 0.06
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.789227 	 0.818149 	 0.803355
Epoch 10 	 54.245117 	 0.739725 	 0.723447
Epoch 20 	 54.082844 	 0.727056 	 0.713608
Epoch 30 	 54.019817 	 0.710112 	 0.703068
Epoch 40 	 53.979568 	 0.713803 	 0.704871
Epoch 50 	 53.855366 	 0.706529 	 0.694333
Epoch 60 	 53.800648 	 0.706200 	 0.694013
Epoch 70 	 53.771004 	 0.707200 	 0.693097
Epoch 80 	 53.697002 	 0.703000 	 0.690430
Epoch 90 	 53.692879 	 0.701969 	 0.689407
Epoch 100 	 53.682529 	 0.697120 	 0.688581
Epoch 110 	 53.655350 	 0.699477 	 0.687294
Epoch 120 	 53.638020 	 0.699162 	 0.686798
Epoch 130 	 53.617229 	 0.699398 	 0.687647
Epoch 140 	 53.600777 	 0.696859 	 0.686405
Epoch 150 	 53.593128 	 0.697535 	 0.686368
Epoch 160 	 53.576984 	 0.701091 	 0.686970
Epoch 170 	 53.582153 	 0.698476 	 0.687706
Epoch 180 	 53.599255 	 0.697803 	 0.685456
Epoch 190 	 53.593628 	 0.698221 	 0.686167
Train loss       : 53.579567
Best valid loss  : 0.692946
Best test loss   : 0.687025
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.910908 	 0.837782 	 0.834307
Epoch 10 	 54.390984 	 0.754640 	 0.738525
Epoch 20 	 54.250965 	 0.739222 	 0.725310
Epoch 30 	 54.187557 	 0.739203 	 0.721398
Epoch 40 	 54.160522 	 0.738366 	 0.720825
Epoch 50 	 54.079414 	 0.737720 	 0.716414
Epoch 60 	 54.011482 	 0.736040 	 0.715794
Epoch 70 	 53.963345 	 0.731692 	 0.714425
Epoch 80 	 53.938812 	 0.733530 	 0.714346
Epoch 90 	 53.937687 	 0.727970 	 0.715017
Epoch 100 	 53.902046 	 0.731273 	 0.713316
Epoch 110 	 53.888771 	 0.730769 	 0.712846
Epoch 120 	 53.892742 	 0.726493 	 0.711577
[Model stopped early]
Train loss       : 53.885822
Best valid loss  : 0.726072
Best test loss   : 0.711969
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.771038 	 0.807792 	 0.797749
Epoch 10 	 54.201706 	 0.732451 	 0.720833
Epoch 20 	 54.025238 	 0.718359 	 0.707581
Epoch 30 	 53.916016 	 0.714153 	 0.696195
Epoch 40 	 53.883347 	 0.712342 	 0.697860
Epoch 50 	 53.855545 	 0.711851 	 0.700581
Epoch 60 	 53.765205 	 0.708948 	 0.697681
Epoch 70 	 53.756802 	 0.704856 	 0.696051
Epoch 80 	 53.769386 	 0.705025 	 0.695369
Epoch 90 	 53.742222 	 0.707604 	 0.694796
Epoch 100 	 53.730949 	 0.711308 	 0.694386
Epoch 110 	 53.709229 	 0.708560 	 0.693783
Epoch 120 	 53.717987 	 0.706542 	 0.693864
Epoch 130 	 53.712627 	 0.702449 	 0.694336
[Model stopped early]
Train loss       : 53.694916
Best valid loss  : 0.702370
Best test loss   : 0.693652
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.957664 	 0.887453 	 0.886540
Epoch 10 	 54.418148 	 0.753197 	 0.734329
Epoch 20 	 54.153179 	 0.728276 	 0.710627
Epoch 30 	 54.093204 	 0.728382 	 0.711156
Epoch 40 	 54.074173 	 0.730803 	 0.713944
Epoch 50 	 54.055756 	 0.727388 	 0.709720
Epoch 60 	 53.957821 	 0.723737 	 0.708596
Epoch 70 	 53.928169 	 0.715312 	 0.706231
Epoch 80 	 53.903423 	 0.722892 	 0.705783
Epoch 90 	 53.892975 	 0.718545 	 0.705807
Epoch 100 	 53.838589 	 0.717339 	 0.702915
Epoch 110 	 53.815952 	 0.716503 	 0.703901
[Model stopped early]
Train loss       : 53.818230
Best valid loss  : 0.712538
Best test loss   : 0.701432
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.912388 	 0.867160 	 0.863987
Epoch 10 	 54.324127 	 0.744545 	 0.726781
Epoch 20 	 54.162006 	 0.729252 	 0.714830
Epoch 30 	 54.143963 	 0.733822 	 0.712740
Epoch 40 	 54.086060 	 0.723372 	 0.706481
Epoch 50 	 54.024059 	 0.722007 	 0.705121
Epoch 60 	 53.960751 	 0.720253 	 0.705549
Epoch 70 	 53.948639 	 0.717934 	 0.704923
Epoch 80 	 53.917480 	 0.720498 	 0.704903
Epoch 90 	 53.881966 	 0.717272 	 0.701546
Epoch 100 	 53.843830 	 0.717466 	 0.700063
Epoch 110 	 53.821621 	 0.712632 	 0.697656
Epoch 120 	 53.810215 	 0.714512 	 0.699883
Epoch 130 	 53.774223 	 0.711658 	 0.697471
Epoch 140 	 53.736931 	 0.708760 	 0.694428
Epoch 150 	 53.749176 	 0.709072 	 0.695047
Epoch 160 	 53.724010 	 0.709473 	 0.693507
Epoch 170 	 53.698441 	 0.710849 	 0.692039
Epoch 180 	 53.680618 	 0.707830 	 0.692027
Epoch 190 	 53.696987 	 0.709114 	 0.692423
Train loss       : 53.707329
Best valid loss  : 0.704419
Best test loss   : 0.692781
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
[Untrained loss : 0.8865]
[Starting training]
Epoch 0 	 55.849407 	 0.832722 	 0.814659
Epoch 10 	 54.359455 	 0.740894 	 0.725643
Epoch 20 	 54.223476 	 0.731939 	 0.713377
Epoch 30 	 54.118084 	 0.724262 	 0.709123
Epoch 40 	 54.075081 	 0.722949 	 0.707401
Epoch 50 	 54.007614 	 0.722288 	 0.705125
Epoch 60 	 53.971546 	 0.715425 	 0.701735
Epoch 70 	 53.935249 	 0.713387 	 0.697558
Epoch 80 	 53.876499 	 0.716790 	 0.695720
Epoch 90 	 53.892658 	 0.712467 	 0.696190
Epoch 100 	 53.837360 	 0.714760 	 0.694445
Epoch 110 	 53.829754 	 0.706989 	 0.693633
Epoch 120 	 53.809425 	 0.709603 	 0.692818
Epoch 130 	 53.808105 	 0.707116 	 0.693028
Epoch 140 	 53.801346 	 0.707595 	 0.692847
[Model stopped early]
Train loss       : 53.793270
Best valid loss  : 0.706989
Best test loss   : 0.693633
Pruning          : 0.01
