Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41289081.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, six, future, torch, torchvision, tqdm, python-dateutil, kiwisolver, pyparsing, cycler, matplotlib, markdown, protobuf, absl-py, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, werkzeug, chardet, idna, urllib3, certifi, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, tensorboard, google-pasta, keras-preprocessing, tensorflow-estimator, astor, gast, h5py, keras-applications, wrapt, termcolor, opt-einsum, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41289081.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 05:00:16.487735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 05:00:16.817100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is singing_classify_cnn_xavier_trimming_magnitude_rewind_local_0.
*******
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9421]
[Starting training]
Epoch 0 	 0.810202 	 0.700827 	 0.692647
Epoch 10 	 0.316406 	 0.385570 	 0.255331
Epoch 20 	 0.153493 	 0.264706 	 0.129779
Epoch 30 	 0.091567 	 0.202665 	 0.065809
Epoch 40 	 0.066751 	 0.183824 	 0.049449
Epoch 50 	 0.054688 	 0.173254 	 0.043382
Epoch 60 	 0.039407 	 0.163143 	 0.035478
Epoch 70 	 0.021829 	 0.169118 	 0.037684
Epoch 80 	 0.019072 	 0.165441 	 0.034743
Epoch 90 	 0.019991 	 0.158088 	 0.031985
Epoch 100 	 0.012293 	 0.160386 	 0.033088
Epoch 110 	 0.011029 	 0.160846 	 0.032537
[Model stopped early]
Train loss       : 0.008732
Best valid loss  : 0.152574
Best test loss   : 0.031066
Pruning          : 1.00
0.0001
0.0001
[Current model size]
================================
Total params      : 774,827
--------------------------------
Total memory      : 6.42 MB
Total Flops       : 398.98 MFlops
Total Mem (Read)  : 8.28 MB
Total Mem (Write) : 4.82 MB
[Supermasks testing]
[Untrained loss : 0.1380]
[Starting training]
Epoch 0 	 0.103401 	 0.172335 	 0.038603
Epoch 10 	 0.049288 	 0.169118 	 0.038603
Epoch 20 	 0.042854 	 0.164982 	 0.034926
Epoch 30 	 0.038948 	 0.159467 	 0.032537
Epoch 40 	 0.038948 	 0.168658 	 0.036765
Epoch 50 	 0.020450 	 0.170496 	 0.035662
Epoch 60 	 0.016199 	 0.168199 	 0.034191
[Model stopped early]
Train loss       : 0.014246
Best valid loss  : 0.159467
Best test loss   : 0.032537
Pruning          : 0.75
0.0001
0.0001
[Current model size]
================================
Total params      : 442,883
--------------------------------
Total memory      : 4.82 MB
Total Flops       : 243.8 MFlops
Total Mem (Read)  : 5.81 MB
Total Mem (Write) : 3.61 MB
[Supermasks testing]
[Untrained loss : 0.4548]
[Starting training]
Epoch 0 	 0.211857 	 0.204044 	 0.063971
Epoch 10 	 0.086742 	 0.178768 	 0.044301
Epoch 20 	 0.065947 	 0.177390 	 0.041360
Epoch 30 	 0.065602 	 0.165901 	 0.035662
Epoch 40 	 0.043888 	 0.169118 	 0.037868
Epoch 50 	 0.032973 	 0.168199 	 0.036397
Epoch 60 	 0.032284 	 0.167739 	 0.034007
Epoch 70 	 0.023897 	 0.164522 	 0.033456
Epoch 80 	 0.025276 	 0.163603 	 0.033456
Epoch 90 	 0.022518 	 0.165441 	 0.033640
Epoch 100 	 0.022978 	 0.165441 	 0.034375
Epoch 110 	 0.024242 	 0.166820 	 0.034375
[Model stopped early]
Train loss       : 0.022289
Best valid loss  : 0.161305
Best test loss   : 0.033640
Pruning          : 0.56
0.0001
0.0001
[Current model size]
================================
Total params      : 254,405
--------------------------------
Total memory      : 3.61 MB
Total Flops       : 151.67 MFlops
Total Mem (Read)  : 4.19 MB
Total Mem (Write) : 2.71 MB
[Supermasks testing]
[Untrained loss : 0.5688]
[Starting training]
Epoch 0 	 0.353631 	 0.286305 	 0.142739
Epoch 10 	 0.139936 	 0.188419 	 0.051654
Epoch 20 	 0.119715 	 0.184743 	 0.048713
Epoch 30 	 0.093176 	 0.172335 	 0.041544
Epoch 40 	 0.086972 	 0.165901 	 0.041544
Epoch 50 	 0.076402 	 0.168658 	 0.038971
Epoch 60 	 0.074219 	 0.167739 	 0.039154
Epoch 70 	 0.048369 	 0.163603 	 0.034559
Epoch 80 	 0.048139 	 0.166360 	 0.036029
Epoch 90 	 0.040211 	 0.158088 	 0.033272
Epoch 100 	 0.039407 	 0.161305 	 0.033088
[Model stopped early]
Train loss       : 0.039982
Best valid loss  : 0.154871
Best test loss   : 0.031618
Pruning          : 0.42
0.0001
0.0001
[Current model size]
================================
Total params      : 144,361
--------------------------------
Total memory      : 2.68 MB
Total Flops       : 94.38 MFlops
Total Mem (Read)  : 3.06 MB
Total Mem (Write) : 2.01 MB
[Supermasks testing]
[Untrained loss : 0.6377]
[Starting training]
Epoch 0 	 0.491843 	 0.433824 	 0.303493
Epoch 10 	 0.229320 	 0.243566 	 0.086397
Epoch 20 	 0.184053 	 0.221507 	 0.069853
Epoch 30 	 0.152344 	 0.204044 	 0.061397
Epoch 40 	 0.141085 	 0.198989 	 0.056618
Epoch 50 	 0.134421 	 0.191176 	 0.048346
Epoch 60 	 0.105239 	 0.179688 	 0.043382
Epoch 70 	 0.102367 	 0.181066 	 0.046691
Epoch 80 	 0.093635 	 0.179688 	 0.043199
Epoch 90 	 0.084214 	 0.172794 	 0.039890
Epoch 100 	 0.083525 	 0.177849 	 0.041544
Epoch 110 	 0.082606 	 0.175551 	 0.040257
Epoch 120 	 0.080423 	 0.175551 	 0.041912
[Model stopped early]
Train loss       : 0.083065
Best valid loss  : 0.172794
Best test loss   : 0.039890
Pruning          : 0.32
0.0001
0.0001
[Current model size]
================================
Total params      : 84,015
--------------------------------
Total memory      : 2.01 MB
Total Flops       : 61.16 MFlops
Total Mem (Read)  : 2.33 MB
Total Mem (Write) : 1.51 MB
[Supermasks testing]
[Untrained loss : 0.9033]
[Starting training]
Epoch 0 	 0.615119 	 0.545037 	 0.454320
Epoch 10 	 0.342946 	 0.340993 	 0.161673
Epoch 20 	 0.281020 	 0.297335 	 0.125184
Epoch 30 	 0.246209 	 0.265625 	 0.102114
Epoch 40 	 0.220129 	 0.244026 	 0.088235
Epoch 50 	 0.214959 	 0.246783 	 0.093199
Epoch 60 	 0.193704 	 0.233915 	 0.086581
Epoch 70 	 0.194049 	 0.220588 	 0.074632
Epoch 80 	 0.182330 	 0.213235 	 0.069118
Epoch 90 	 0.175896 	 0.212316 	 0.069118
Epoch 100 	 0.157399 	 0.201287 	 0.061213
Epoch 110 	 0.155446 	 0.193474 	 0.056801
Epoch 120 	 0.145565 	 0.196232 	 0.056434
Epoch 130 	 0.143957 	 0.191636 	 0.054963
Epoch 140 	 0.138212 	 0.193015 	 0.054412
Epoch 150 	 0.133157 	 0.202665 	 0.061581
[Model stopped early]
Train loss       : 0.128676
Best valid loss  : 0.181985
Best test loss   : 0.049265
Pruning          : 0.24
0.0001
0.0001
[Current model size]
================================
Total params      : 47,755
--------------------------------
Total memory      : 1.47 MB
Total Flops       : 39.21 MFlops
Total Mem (Read)  : 1.79 MB
Total Mem (Write) : 1.1 MB
[Supermasks testing]
[Untrained loss : 0.7563]
[Starting training]
Epoch 0 	 0.722082 	 0.681066 	 0.632721
Epoch 10 	 0.451976 	 0.466452 	 0.312960
Epoch 20 	 0.385570 	 0.408088 	 0.229044
Epoch 30 	 0.355813 	 0.371324 	 0.193382
Epoch 40 	 0.325253 	 0.345129 	 0.179320
Epoch 50 	 0.309972 	 0.328585 	 0.164154
Epoch 60 	 0.295726 	 0.311121 	 0.144577
Epoch 70 	 0.295496 	 0.315717 	 0.151838
Epoch 80 	 0.281020 	 0.300551 	 0.136121
Epoch 90 	 0.261949 	 0.281250 	 0.121048
Epoch 100 	 0.260110 	 0.273897 	 0.120037
Epoch 110 	 0.257238 	 0.272059 	 0.119577
Epoch 120 	 0.250345 	 0.259651 	 0.108640
Epoch 130 	 0.257812 	 0.264246 	 0.113235
Epoch 140 	 0.244141 	 0.261949 	 0.112132
Epoch 150 	 0.234490 	 0.261489 	 0.111121
Train loss       : 0.235639
Best valid loss  : 0.246783
Best test loss   : 0.102390
Pruning          : 0.18
0.0001
0.0001
[Current model size]
================================
Total params      : 27,239
--------------------------------
Total memory      : 1.07 MB
Total Flops       : 25.43 MFlops
Total Mem (Read)  : 1.41 MB
Total Mem (Write) : 822.46 KB
[Supermasks testing]
[Untrained loss : 0.7739]
[Starting training]
Epoch 0 	 0.810432 	 0.805607 	 0.789246
Epoch 10 	 0.539177 	 0.569853 	 0.454412
Epoch 20 	 0.493222 	 0.524816 	 0.377022
Epoch 30 	 0.451631 	 0.475184 	 0.309651
Epoch 40 	 0.434972 	 0.460018 	 0.285754
Epoch 50 	 0.416360 	 0.432904 	 0.259743
Epoch 60 	 0.411305 	 0.426930 	 0.260110
Epoch 70 	 0.400276 	 0.412684 	 0.236949
Epoch 80 	 0.379366 	 0.411765 	 0.232904
Epoch 90 	 0.376149 	 0.392004 	 0.225643
Epoch 100 	 0.377872 	 0.402114 	 0.225368
Epoch 110 	 0.364660 	 0.390625 	 0.215993
Epoch 120 	 0.363396 	 0.375919 	 0.205331
Epoch 130 	 0.366383 	 0.370404 	 0.205239
Epoch 140 	 0.356388 	 0.375919 	 0.210110
Epoch 150 	 0.352367 	 0.371324 	 0.204044
Train loss       : 0.354205
Best valid loss  : 0.362592
Best test loss   : 0.199081
Pruning          : 0.13
0.0001
0.0001
[Current model size]
================================
Total params      : 16,485
--------------------------------
Total memory      : 0.80 MB
Total Flops       : 17.54 MFlops
Total Mem (Read)  : 1.17 MB
Total Mem (Write) : 616.86 KB
[Supermasks testing]
[Untrained loss : 0.7772]
[Starting training]
Epoch 0 	 0.834674 	 0.857077 	 0.847059
Epoch 10 	 0.618451 	 0.624081 	 0.548713
Epoch 20 	 0.568589 	 0.580423 	 0.476195
Epoch 30 	 0.539177 	 0.557904 	 0.435662
Epoch 40 	 0.519072 	 0.551930 	 0.419945
Epoch 50 	 0.507927 	 0.535846 	 0.397886
Epoch 60 	 0.499311 	 0.527574 	 0.376930
Epoch 70 	 0.479435 	 0.522518 	 0.366360
Epoch 80 	 0.472197 	 0.503217 	 0.356158
Epoch 90 	 0.459099 	 0.508272 	 0.351562
Epoch 100 	 0.471507 	 0.498621 	 0.348529
Epoch 110 	 0.462201 	 0.484375 	 0.337960
Epoch 120 	 0.455078 	 0.478860 	 0.325919
Epoch 130 	 0.443015 	 0.476562 	 0.323162
Epoch 140 	 0.448185 	 0.477022 	 0.324632
Epoch 150 	 0.445312 	 0.468750 	 0.314246
Train loss       : 0.442440
Best valid loss  : 0.457261
Best test loss   : 0.306985
Pruning          : 0.10
0.0001
0.0001
[Current model size]
================================
Total params      : 8,769
--------------------------------
Total memory      : 0.54 MB
Total Flops       : 10.66 MFlops
Total Mem (Read)  : 962.38 KB
Total Mem (Write) : 411.36 KB
[Supermasks testing]
[Untrained loss : 0.9368]
[Starting training]
Epoch 0 	 0.852597 	 0.871783 	 0.867463
Epoch 10 	 0.682790 	 0.667739 	 0.643934
Epoch 20 	 0.654527 	 0.642004 	 0.598805
Epoch 30 	 0.638902 	 0.631434 	 0.565349
Epoch 40 	 0.621783 	 0.629136 	 0.542923
Epoch 50 	 0.611903 	 0.625000 	 0.527022
Epoch 60 	 0.593635 	 0.621783 	 0.519485
Epoch 70 	 0.592486 	 0.613511 	 0.511581
Epoch 80 	 0.591452 	 0.615809 	 0.511489
Epoch 90 	 0.584559 	 0.614890 	 0.505790
Epoch 100 	 0.592716 	 0.612132 	 0.504136
Epoch 110 	 0.586282 	 0.608456 	 0.498346
Epoch 120 	 0.585363 	 0.607537 	 0.501195
[Model stopped early]
Train loss       : 0.583525
Best valid loss  : 0.597426
Best test loss   : 0.490349
Pruning          : 0.08
0.0001
0.0001
[Current model size]
================================
Total params      : 5,489
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 846.73 KB
Total Mem (Write) : 308.53 KB
[Supermasks testing]
[Untrained loss : 0.7535]
[Starting training]
Epoch 0 	 0.872128 	 0.880055 	 0.876471
Epoch 10 	 0.711857 	 0.672335 	 0.662132
Epoch 20 	 0.682215 	 0.654871 	 0.627757
Epoch 30 	 0.676471 	 0.644761 	 0.608180
Epoch 40 	 0.661075 	 0.636949 	 0.589246
Epoch 50 	 0.652459 	 0.642004 	 0.583272
Epoch 60 	 0.654182 	 0.636489 	 0.578860
Epoch 70 	 0.649127 	 0.642004 	 0.578217
Epoch 80 	 0.649816 	 0.627298 	 0.567647
Epoch 90 	 0.650850 	 0.633732 	 0.568934
Epoch 100 	 0.637638 	 0.641085 	 0.578125
Epoch 110 	 0.651425 	 0.634651 	 0.568199
[Model stopped early]
Train loss       : 0.645450
Best valid loss  : 0.627298
Best test loss   : 0.567647
Pruning          : 0.06
0.0001
0.0001
[Current model size]
================================
Total params      : 4,929
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 844.38 KB
Total Mem (Write) : 308.37 KB
[Supermasks testing]
[Untrained loss : 0.7537]
[Starting training]
Epoch 0 	 0.875460 	 0.880055 	 0.876471
Epoch 10 	 0.737017 	 0.726562 	 0.720588
Epoch 20 	 0.704733 	 0.672794 	 0.660662
Epoch 30 	 0.687730 	 0.664062 	 0.635202
Epoch 40 	 0.679113 	 0.654412 	 0.612132
Epoch 50 	 0.662799 	 0.648897 	 0.603125
Epoch 60 	 0.651769 	 0.648438 	 0.595037
Epoch 70 	 0.665441 	 0.647059 	 0.580147
Epoch 80 	 0.656824 	 0.658088 	 0.591177
Epoch 90 	 0.648208 	 0.650735 	 0.576471
Epoch 100 	 0.648782 	 0.648438 	 0.577206
Epoch 110 	 0.654412 	 0.640625 	 0.567463
Epoch 120 	 0.649472 	 0.639706 	 0.565441
Epoch 130 	 0.641774 	 0.650276 	 0.573713
Epoch 140 	 0.637178 	 0.646140 	 0.565993
[Model stopped early]
Train loss       : 0.645221
Best valid loss  : 0.632812
Best test loss   : 0.561213
Pruning          : 0.04
0.0001
0.0001
[Current model size]
================================
Total params      : 4,589
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 842.93 KB
Total Mem (Write) : 308.25 KB
[Supermasks testing]
[Untrained loss : 0.8765]
[Starting training]
Epoch 0 	 0.876034 	 0.880055 	 0.876471
Epoch 10 	 0.770795 	 0.762408 	 0.757721
Epoch 20 	 0.712086 	 0.685662 	 0.663419
Epoch 30 	 0.698185 	 0.655790 	 0.629779
Epoch 40 	 0.688994 	 0.648438 	 0.615901
Epoch 50 	 0.673483 	 0.648438 	 0.608456
Epoch 60 	 0.674173 	 0.651654 	 0.607169
Epoch 70 	 0.674058 	 0.643382 	 0.600184
Epoch 80 	 0.675092 	 0.640625 	 0.588143
Epoch 90 	 0.664522 	 0.638787 	 0.590257
[Model stopped early]
Train loss       : 0.664522
Best valid loss  : 0.635570
Best test loss   : 0.596691
Pruning          : 0.03
0.0001
0.0001
[Current model size]
================================
Total params      : 4,353
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 841.92 KB
Total Mem (Write) : 308.16 KB
[Supermasks testing]
[Untrained loss : 0.8765]
[Starting training]
Epoch 0 	 0.875574 	 0.880515 	 0.876471
Epoch 10 	 0.805951 	 0.820312 	 0.811765
Epoch 20 	 0.724954 	 0.709559 	 0.696507
Epoch 30 	 0.712776 	 0.673713 	 0.646507
Epoch 40 	 0.701287 	 0.670496 	 0.640074
Epoch 50 	 0.690602 	 0.671415 	 0.632353
Epoch 60 	 0.683249 	 0.664522 	 0.624265
Epoch 70 	 0.685202 	 0.659007 	 0.611857
Epoch 80 	 0.681870 	 0.662684 	 0.615074
Epoch 90 	 0.678309 	 0.660386 	 0.610386
Epoch 100 	 0.673943 	 0.656710 	 0.606710
Epoch 110 	 0.674288 	 0.645221 	 0.593199
Epoch 120 	 0.678309 	 0.663143 	 0.607537
Epoch 130 	 0.671990 	 0.654412 	 0.599357
Epoch 140 	 0.672679 	 0.648438 	 0.594853
[Model stopped early]
Train loss       : 0.672449
Best valid loss  : 0.645221
Best test loss   : 0.593199
Pruning          : 0.02
0.0001
0.0001
[Current model size]
================================
Total params      : 4,197
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 841.24 KB
Total Mem (Write) : 308.09 KB
[Supermasks testing]
[Untrained loss : 0.8765]
[Starting training]
Epoch 0 	 0.875460 	 0.880055 	 0.876471
Epoch 10 	 0.819738 	 0.835938 	 0.829228
Epoch 20 	 0.743336 	 0.760570 	 0.744485
Epoch 30 	 0.725988 	 0.706342 	 0.674448
Epoch 40 	 0.711972 	 0.684283 	 0.650919
Epoch 50 	 0.699219 	 0.689338 	 0.656801
Epoch 60 	 0.697151 	 0.694853 	 0.648529
Epoch 70 	 0.690372 	 0.667279 	 0.627390
Epoch 80 	 0.688764 	 0.666360 	 0.620221
Epoch 90 	 0.684053 	 0.658548 	 0.613603
Epoch 100 	 0.681756 	 0.660846 	 0.613419
Epoch 110 	 0.677275 	 0.654871 	 0.609007
Epoch 120 	 0.678079 	 0.650735 	 0.606618
Epoch 130 	 0.673943 	 0.650276 	 0.603493
Epoch 140 	 0.673254 	 0.663143 	 0.613051
Epoch 150 	 0.673483 	 0.648438 	 0.600735
Train loss       : 0.674862
Best valid loss  : 0.648438
Best test loss   : 0.600735
Pruning          : 0.02
