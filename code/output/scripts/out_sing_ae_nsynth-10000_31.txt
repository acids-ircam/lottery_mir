Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40871943.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, future, torch, six, torchvision, tqdm, kiwisolver, python-dateutil, cycler, pyparsing, matplotlib, h5py, keras-applications, wrapt, gast, tensorflow-estimator, absl-py, markdown, chardet, certifi, idna, urllib3, requests, oauthlib, requests-oauthlib, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, grpcio, werkzeug, protobuf, tensorboard, opt-einsum, termcolor, astor, keras-preprocessing, google-pasta, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871943.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-22 07:38:54.181144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-22 07:38:54.513024: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_sing_ae_cnn_xavier_masking_gradient_min_rewind_global_0.
*******
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5732]
[Starting training]
Epoch 0 	 0.472435 	 0.431485 	 0.425954
Epoch 10 	 0.211378 	 0.212912 	 0.213559
Epoch 20 	 0.172595 	 0.180901 	 0.183690
Epoch 30 	 0.162242 	 0.174387 	 0.176420
Epoch 40 	 0.151458 	 0.163845 	 0.164572
Epoch 50 	 0.143231 	 0.156475 	 0.156173
Epoch 60 	 0.136455 	 0.153540 	 0.153175
Epoch 70 	 0.135769 	 0.150449 	 0.153314
Epoch 80 	 0.130028 	 0.147645 	 0.149595
Epoch 90 	 0.129661 	 0.145819 	 0.145210
Epoch 100 	 0.126985 	 0.142140 	 0.143696
Epoch 110 	 0.111984 	 0.134458 	 0.134013
Epoch 120 	 0.103661 	 0.128789 	 0.129327
Epoch 130 	 0.102186 	 0.128393 	 0.128622
Epoch 140 	 0.098120 	 0.124500 	 0.126285
Epoch 150 	 0.097322 	 0.124636 	 0.125875
Train loss       : 0.096667
Best valid loss  : 0.121378
Best test loss   : 0.126143
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.467209 	 0.425097 	 0.424666
Epoch 10 	 0.214890 	 0.221764 	 0.223917
Epoch 20 	 0.170570 	 0.179622 	 0.181717
Epoch 30 	 0.154505 	 0.170161 	 0.167232
Epoch 40 	 0.146456 	 0.162396 	 0.160142
Epoch 50 	 0.142241 	 0.156594 	 0.156482
Epoch 60 	 0.141007 	 0.151176 	 0.150998
Epoch 70 	 0.121308 	 0.138633 	 0.138756
Epoch 80 	 0.119325 	 0.138564 	 0.137154
Epoch 90 	 0.117524 	 0.141314 	 0.137913
Epoch 100 	 0.115257 	 0.132877 	 0.132906
Epoch 110 	 0.112174 	 0.133411 	 0.133535
Epoch 120 	 0.111117 	 0.132085 	 0.133100
Epoch 130 	 0.103006 	 0.126882 	 0.126962
Epoch 140 	 0.102400 	 0.126559 	 0.126975
Epoch 150 	 0.101409 	 0.125150 	 0.126332
Train loss       : 0.101148
Best valid loss  : 0.123849
Best test loss   : 0.126770
Pruning          : 0.70
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.493010 	 0.447337 	 0.446831
Epoch 10 	 0.238302 	 0.237443 	 0.239634
Epoch 20 	 0.189511 	 0.193343 	 0.194858
Epoch 30 	 0.169097 	 0.177964 	 0.177707
Epoch 40 	 0.159795 	 0.173712 	 0.172860
Epoch 50 	 0.154607 	 0.164786 	 0.162913
Epoch 60 	 0.149152 	 0.159334 	 0.157126
Epoch 70 	 0.143913 	 0.156009 	 0.156172
Epoch 80 	 0.140225 	 0.155768 	 0.153921
Epoch 90 	 0.138383 	 0.154530 	 0.153505
Epoch 100 	 0.128213 	 0.145168 	 0.143943
Epoch 110 	 0.126108 	 0.142616 	 0.142412
Epoch 120 	 0.125335 	 0.142449 	 0.141791
Epoch 130 	 0.124534 	 0.140941 	 0.141253
Epoch 140 	 0.124025 	 0.141554 	 0.140238
Epoch 150 	 0.117469 	 0.136801 	 0.135792
Train loss       : 0.116976
Best valid loss  : 0.135746
Best test loss   : 0.135825
Pruning          : 0.49
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.509839 	 0.504351 	 0.500775
Epoch 10 	 0.503039 	 0.510500 	 0.500570
Epoch 20 	 0.502333 	 0.507327 	 0.500643
Epoch 30 	 0.502123 	 0.507236 	 0.500436
[Model stopped early]
Train loss       : 0.502677
Best valid loss  : 0.504351
Best test loss   : 0.500775
Pruning          : 0.34
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.512103 	 0.508828 	 0.501051
Epoch 10 	 0.502320 	 0.508938 	 0.500995
Epoch 20 	 0.502535 	 0.506158 	 0.500427
Epoch 30 	 0.502285 	 0.506954 	 0.500432
[Model stopped early]
Train loss       : 0.502337
Best valid loss  : 0.503456
Best test loss   : 0.500947
Pruning          : 0.24
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.529752 	 0.536648 	 0.526324
Epoch 10 	 0.529984 	 0.533135 	 0.526422
Epoch 20 	 0.529422 	 0.533674 	 0.526399
Epoch 30 	 0.529325 	 0.534182 	 0.526429
Epoch 40 	 0.529663 	 0.532574 	 0.526316
Epoch 50 	 0.528824 	 0.534002 	 0.526316
[Model stopped early]
Train loss       : 0.529036
Best valid loss  : 0.529710
Best test loss   : 0.526341
Pruning          : 0.17
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.529261 	 0.533962 	 0.526351
Epoch 10 	 0.502204 	 0.505591 	 0.501358
Epoch 20 	 0.502768 	 0.507539 	 0.501438
Epoch 30 	 0.502595 	 0.507837 	 0.500810
Epoch 40 	 0.502528 	 0.506362 	 0.500392
[Model stopped early]
Train loss       : 0.502528
Best valid loss  : 0.498350
Best test loss   : 0.501072
Pruning          : 0.12
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.529397 	 0.531045 	 0.526315
Epoch 10 	 0.529372 	 0.538188 	 0.526316
Epoch 20 	 0.529542 	 0.533735 	 0.526314
Epoch 30 	 0.529466 	 0.532263 	 0.526322
[Model stopped early]
Train loss       : 0.529473
Best valid loss  : 0.531045
Best test loss   : 0.526315
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.514875 	 0.506058 	 0.501327
Epoch 10 	 0.502996 	 0.509111 	 0.500628
Epoch 20 	 0.501948 	 0.505672 	 0.500521
Epoch 30 	 0.502616 	 0.508609 	 0.500968
Epoch 40 	 0.502165 	 0.505415 	 0.500543
Epoch 50 	 0.502514 	 0.505831 	 0.500370
Epoch 60 	 0.502159 	 0.505812 	 0.500399
Epoch 70 	 0.502425 	 0.505876 	 0.500369
Epoch 80 	 0.502606 	 0.508119 	 0.500365
Epoch 90 	 0.502286 	 0.506893 	 0.500379
Epoch 100 	 0.502529 	 0.506712 	 0.500377
Epoch 110 	 0.502057 	 0.499830 	 0.500362
Epoch 120 	 0.502668 	 0.506060 	 0.500361
Epoch 130 	 0.502882 	 0.509497 	 0.500365
Epoch 140 	 0.502279 	 0.510344 	 0.500364
[Model stopped early]
Train loss       : 0.501945
Best valid loss  : 0.499830
Best test loss   : 0.500362
Pruning          : 0.06
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.515454 	 0.507374 	 0.500673
Epoch 10 	 0.502876 	 0.509766 	 0.501809
Epoch 20 	 0.502486 	 0.506239 	 0.500492
Epoch 30 	 0.502644 	 0.505426 	 0.500625
Epoch 40 	 0.502058 	 0.503894 	 0.500366
[Model stopped early]
Train loss       : 0.502070
Best valid loss  : 0.499133
Best test loss   : 0.500518
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.508698 	 0.506508 	 0.501181
Epoch 10 	 0.503015 	 0.504110 	 0.501041
Epoch 20 	 0.502632 	 0.504426 	 0.500634
Epoch 30 	 0.502557 	 0.505026 	 0.501159
Epoch 40 	 0.502135 	 0.499681 	 0.500413
Epoch 50 	 0.503046 	 0.506116 	 0.500508
Epoch 60 	 0.502441 	 0.508656 	 0.500388
Epoch 70 	 0.501900 	 0.509832 	 0.500351
[Model stopped early]
Train loss       : 0.502378
Best valid loss  : 0.499681
Best test loss   : 0.500413
Pruning          : 0.03
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.523168 	 0.510203 	 0.501769
Epoch 10 	 0.503287 	 0.506161 	 0.500713
Epoch 20 	 0.502453 	 0.507169 	 0.500476
Epoch 30 	 0.502743 	 0.507831 	 0.500522
Epoch 40 	 0.501845 	 0.509063 	 0.500412
Epoch 50 	 0.502676 	 0.507411 	 0.500510
Epoch 60 	 0.502420 	 0.503613 	 0.500439
Epoch 70 	 0.502297 	 0.506910 	 0.500375
[Model stopped early]
Train loss       : 0.501640
Best valid loss  : 0.497518
Best test loss   : 0.500380
Pruning          : 0.02
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.513420 	 0.505431 	 0.501669
Epoch 10 	 0.502454 	 0.505381 	 0.500519
Epoch 20 	 0.502712 	 0.506485 	 0.500830
Epoch 30 	 0.502843 	 0.502483 	 0.500481
Epoch 40 	 0.502750 	 0.505775 	 0.500559
Epoch 50 	 0.502488 	 0.506128 	 0.500418
Epoch 60 	 0.502273 	 0.503714 	 0.500434
Epoch 70 	 0.502037 	 0.505013 	 0.500380
[Model stopped early]
Train loss       : 0.502004
Best valid loss  : 0.501672
Best test loss   : 0.500352
Pruning          : 0.01
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.517744 	 0.508272 	 0.501350
Epoch 10 	 0.502798 	 0.508763 	 0.500617
Epoch 20 	 0.502778 	 0.505642 	 0.500588
Epoch 30 	 0.501979 	 0.503867 	 0.500485
Epoch 40 	 0.502821 	 0.501047 	 0.500495
Epoch 50 	 0.502082 	 0.507118 	 0.500581
Epoch 60 	 0.502785 	 0.505888 	 0.500825
Epoch 70 	 0.502568 	 0.505912 	 0.500463
Epoch 80 	 0.502811 	 0.505323 	 0.500385
Epoch 90 	 0.502892 	 0.505097 	 0.500359
Epoch 100 	 0.502298 	 0.507261 	 0.500362
Epoch 110 	 0.502966 	 0.504168 	 0.500362
[Model stopped early]
Train loss       : 0.502347
Best valid loss  : 0.500291
Best test loss   : 0.500432
Pruning          : 0.01
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5267]
[Starting training]
Epoch 0 	 0.512141 	 0.509878 	 0.501000
Epoch 10 	 0.502776 	 0.510060 	 0.500510
Epoch 20 	 0.502083 	 0.509055 	 0.500477
Epoch 30 	 0.502541 	 0.510100 	 0.500391
[Model stopped early]
Train loss       : 0.502746
Best valid loss  : 0.502134
Best test loss   : 0.500846
Pruning          : 0.01
