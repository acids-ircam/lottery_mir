Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41146339.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, kiwisolver, python-dateutil, pyparsing, cycler, matplotlib, opt-einsum, termcolor, wrapt, grpcio, h5py, keras-applications, absl-py, protobuf, markdown, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, werkzeug, chardet, urllib3, idna, certifi, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, astor, gast, keras-preprocessing, tensorflow-estimator, google-pasta, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146339.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-27 03:01:59.664248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-27 03:01:59.675056: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_ddsp_cnn_xavier_trimming_activation_rewind_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41146339.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 83.9136]
[Starting training]
Epoch 0 	 75.014153 	 71.533516 	 73.474716
Epoch 10 	 61.200230 	 56.805988 	 57.386677
Epoch 20 	 48.346012 	 42.980453 	 44.231846
Epoch 30 	 41.719761 	 39.077873 	 40.723545
Epoch 40 	 37.768841 	 34.129509 	 36.300083
Epoch 50 	 35.210445 	 32.588612 	 38.502941
Epoch 60 	 33.373230 	 30.666693 	 32.579456
Epoch 70 	 33.934784 	 31.309689 	 33.145077
Epoch 80 	 30.600124 	 29.105980 	 30.728981
Epoch 90 	 32.890087 	 33.850670 	 35.577244
Epoch 100 	 30.826580 	 31.345963 	 33.033936
Epoch 110 	 29.842417 	 31.369726 	 32.879353
[Model stopped early]
Train loss       : 29.788975
Best valid loss  : 29.105980
Best test loss   : 30.728981
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,909,528
--------------------------------
Total memory      : 35.03 MB
Total Flops       : 326.2 MFlops
Total Mem (Read)  : 31.67 MB
Total Mem (Write) : 24.49 MB
[Supermasks testing]
[Untrained loss : 64.0956]
[Starting training]
Epoch 0 	 36.062923 	 30.734470 	 32.546253
Epoch 10 	 29.571087 	 28.271809 	 30.061123
Epoch 20 	 29.439444 	 27.971975 	 29.529839
Epoch 30 	 27.954002 	 27.506968 	 29.019682
Epoch 40 	 27.424086 	 26.671341 	 28.337639
Epoch 50 	 25.858936 	 25.715082 	 27.306028
Epoch 60 	 25.620321 	 25.721081 	 27.364292
Epoch 70 	 24.808390 	 25.096453 	 26.621315
Epoch 80 	 24.613695 	 24.983711 	 26.543875
Epoch 90 	 24.424559 	 24.895746 	 26.593584
Epoch 100 	 24.067017 	 24.599527 	 26.264811
Epoch 110 	 23.971313 	 24.628035 	 26.290998
Epoch 120 	 23.847591 	 24.517635 	 26.225063
Epoch 130 	 23.751127 	 24.397377 	 26.190460
Epoch 140 	 23.585609 	 24.418631 	 26.111843
Epoch 150 	 23.515839 	 24.245377 	 26.133207
Epoch 160 	 23.467165 	 24.192387 	 26.106852
Epoch 170 	 23.400801 	 24.317673 	 26.029087
Epoch 180 	 23.337732 	 24.303635 	 26.043741
Epoch 190 	 23.353592 	 24.296741 	 26.063484
Train loss       : 23.376123
Best valid loss  : 23.983938
Best test loss   : 26.056023
Pruning          : 0.72
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,898,752
--------------------------------
Total memory      : 29.65 MB
Total Flops       : 171.22 MFlops
Total Mem (Read)  : 22.92 MB
Total Mem (Write) : 17.34 MB
[Supermasks testing]
[Untrained loss : 68.6235]
[Starting training]
Epoch 0 	 41.037010 	 32.811214 	 34.660240
Epoch 10 	 30.888964 	 28.784048 	 30.425461
Epoch 20 	 29.285891 	 28.068987 	 29.781818
Epoch 30 	 28.565140 	 27.533833 	 29.087345
Epoch 40 	 27.744766 	 26.709757 	 28.557976
Epoch 50 	 27.167276 	 26.974699 	 28.604355
Epoch 60 	 27.209139 	 26.220318 	 28.192989
Epoch 70 	 26.629637 	 26.745365 	 28.436689
Epoch 80 	 26.369986 	 25.839098 	 27.595703
Epoch 90 	 25.867914 	 25.773634 	 27.327679
Epoch 100 	 27.431976 	 25.858545 	 27.520144
Epoch 110 	 24.669991 	 25.403193 	 27.028345
Epoch 120 	 24.760895 	 24.782351 	 26.582516
Epoch 130 	 24.286604 	 24.907341 	 26.517778
Epoch 140 	 24.062334 	 24.737158 	 26.447416
Epoch 150 	 24.031559 	 24.692091 	 26.270697
Epoch 160 	 23.906988 	 24.576538 	 26.202175
Epoch 170 	 23.888304 	 24.611582 	 26.151461
Epoch 180 	 23.852875 	 24.753498 	 26.209013
Epoch 190 	 23.707598 	 24.495594 	 26.225616
Train loss       : 23.822809
Best valid loss  : 24.296124
Best test loss   : 26.178516
Pruning          : 0.52
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,282,494
--------------------------------
Total memory      : 25.75 MB
Total Flops       : 89.46 MFlops
Total Mem (Read)  : 17.01 MB
Total Mem (Write) : 12.62 MB
[Supermasks testing]
[Untrained loss : 86.5871]
[Starting training]
Epoch 0 	 43.083298 	 538.612732 	 65.890602
Epoch 10 	 32.292732 	 30.985395 	 32.909508
Epoch 20 	 30.604666 	 29.773994 	 31.897943
Epoch 30 	 29.788927 	 27.822969 	 29.704849
Epoch 40 	 29.149176 	 27.762800 	 29.401402
Epoch 50 	 28.286961 	 27.173298 	 28.724323
Epoch 60 	 28.347363 	 28.060322 	 29.781298
Epoch 70 	 27.602793 	 26.519852 	 28.165266
Epoch 80 	 27.895514 	 27.339302 	 28.910809
Epoch 90 	 26.663132 	 25.877987 	 27.525820
Epoch 100 	 26.529106 	 25.922831 	 27.661100
Epoch 110 	 26.035967 	 25.710913 	 27.345541
Epoch 120 	 25.942621 	 25.649681 	 27.174379
Epoch 130 	 25.602358 	 25.199516 	 27.077497
Epoch 140 	 25.440226 	 25.576290 	 27.114679
Epoch 150 	 24.997118 	 25.185001 	 26.795048
Epoch 160 	 24.882505 	 25.079975 	 26.710922
Epoch 170 	 24.598215 	 24.824020 	 26.500875
Epoch 180 	 24.501038 	 24.771585 	 26.555639
Epoch 190 	 24.358387 	 24.900738 	 26.473036
Train loss       : 24.363884
Best valid loss  : 24.681791
Best test loss   : 26.438572
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 887,398
--------------------------------
Total memory      : 22.89 MB
Total Flops       : 46.15 MFlops
Total Mem (Read)  : 12.89 MB
Total Mem (Write) : 9.4 MB
[Supermasks testing]
[Untrained loss : 85.1255]
[Starting training]
Epoch 0 	 46.843658 	 38.053825 	 40.102890
Epoch 10 	 34.366257 	 31.252378 	 33.372181
Epoch 20 	 32.344288 	 30.366808 	 32.509403
Epoch 30 	 31.196131 	 28.981995 	 31.023874
Epoch 40 	 30.412182 	 29.008575 	 30.906202
Epoch 50 	 29.488010 	 27.751926 	 29.770308
Epoch 60 	 29.148197 	 28.961323 	 30.737251
Epoch 70 	 27.847210 	 27.205297 	 29.211969
Epoch 80 	 27.652733 	 26.791334 	 28.526566
Epoch 90 	 27.448441 	 26.597054 	 28.282362
Epoch 100 	 27.198490 	 26.412315 	 28.232904
Epoch 110 	 26.631519 	 26.409426 	 28.036619
Epoch 120 	 26.509232 	 26.214489 	 28.045000
Epoch 130 	 26.306341 	 25.828840 	 27.807131
Epoch 140 	 26.144714 	 25.852154 	 27.583210
Epoch 150 	 26.160973 	 25.718767 	 27.655840
Epoch 160 	 26.016285 	 25.851063 	 27.654879
Epoch 170 	 26.004404 	 25.822607 	 27.511093
Epoch 180 	 25.873638 	 25.713036 	 27.507435
Epoch 190 	 25.779377 	 25.619690 	 27.410858
Train loss       : 25.764322
Best valid loss  : 25.437479
Best test loss   : 27.416792
Pruning          : 0.27
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 630,951
--------------------------------
Total memory      : 20.84 MB
Total Flops       : 23.99 MFlops
Total Mem (Read)  : 10.04 MB
Total Mem (Write) : 7.21 MB
[Supermasks testing]
[Untrained loss : 74.5477]
[Starting training]
Epoch 0 	 168919.046875 	 53.844276 	 53.437923
Epoch 10 	 49.221970 	 48.924091 	 47.650013
Epoch 20 	 46.368813 	 44.305470 	 45.190117
Epoch 30 	 44.589962 	 42.501587 	 236.790436
Epoch 40 	 43.557999 	 87.778008 	 46.060509
Epoch 50 	 42.601521 	 682.890320 	 87.954796
Epoch 60 	 40.271114 	 39.824291 	 67.496063
Epoch 70 	 39.467358 	 38.966934 	 73.696220
Epoch 80 	 38.977749 	 459.439606 	 68.418259
Epoch 90 	 38.372513 	 36.768524 	 39.784130
Epoch 100 	 37.585430 	 36.279964 	 46.320717
Epoch 110 	 36.952736 	 36.154606 	 37.695957
Epoch 120 	 36.242859 	 35.455517 	 37.299263
Epoch 130 	 35.489353 	 34.254059 	 36.104477
Epoch 140 	 34.462833 	 33.394630 	 35.234730
Epoch 150 	 33.725525 	 295.194214 	 53.859825
Epoch 160 	 32.757183 	 32.278332 	 33.975986
Epoch 170 	 32.327393 	 31.872841 	 33.429764
Epoch 180 	 31.822016 	 31.911098 	 33.399696
Epoch 190 	 31.271206 	 31.246572 	 32.883163
Train loss       : 30.932014
Best valid loss  : 30.451920
Best test loss   : 32.345608
Pruning          : 0.19
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 461,817
--------------------------------
Total memory      : 19.39 MB
Total Flops       : 12.83 MFlops
Total Mem (Read)  : 8.07 MB
Total Mem (Write) : 5.74 MB
[Supermasks testing]
[Untrained loss : 2810.3574]
[Starting training]
Epoch 0 	 66.662331 	 53.700909 	 53.953754
Epoch 10 	 42.801846 	 37.902206 	 40.317165
Epoch 20 	 38.735291 	 35.099628 	 37.079041
Epoch 30 	 37.001888 	 33.503906 	 35.394276
Epoch 40 	 35.440357 	 32.428802 	 34.387211
Epoch 50 	 34.586437 	 31.937813 	 33.980610
Epoch 60 	 32.868565 	 31.467211 	 33.196648
Epoch 70 	 32.586098 	 30.911791 	 32.672604
Epoch 80 	 32.419312 	 30.374205 	 32.139648
Epoch 90 	 31.915514 	 30.107517 	 31.854902
Epoch 100 	 30.987228 	 29.779898 	 31.444008
Epoch 110 	 30.920923 	 30.151028 	 31.878498
Epoch 120 	 30.742189 	 29.312387 	 31.131575
Epoch 130 	 30.544165 	 29.293358 	 31.090727
Epoch 140 	 30.331932 	 29.291178 	 31.305044
Epoch 150 	 30.066467 	 28.712866 	 30.663576
Epoch 160 	 30.027822 	 28.865576 	 30.735853
Epoch 170 	 29.846979 	 28.756207 	 30.543119
Epoch 180 	 29.713324 	 28.535038 	 30.496847
[Model stopped early]
Train loss       : 29.743486
Best valid loss  : 28.404127
Best test loss   : 30.699566
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 348,567
--------------------------------
Total memory      : 18.35 MB
Total Flops       : 7.06 MFlops
Total Mem (Read)  : 6.7 MB
Total Mem (Write) : 4.71 MB
[Supermasks testing]
[Untrained loss : 173653688215142400.0000]
[Starting training]
Epoch 0 	 68.883232 	 71.872375 	 65.158287
Epoch 10 	 46.627357 	 45.430069 	 44.271000
Epoch 20 	 41.153805 	 39.227112 	 40.859982
Epoch 30 	 38.902554 	 37.726933 	 39.405746
Epoch 40 	 37.391205 	 34.079029 	 36.411476
Epoch 50 	 36.989639 	 34.155506 	 36.313038
Epoch 60 	 36.207695 	 33.148033 	 35.426556
Epoch 70 	 35.037373 	 33.116714 	 35.102818
Epoch 80 	 34.630508 	 31.764891 	 34.124378
Epoch 90 	 34.293655 	 31.939541 	 34.104355
Epoch 100 	 33.140877 	 30.884279 	 33.028233
Epoch 110 	 32.637363 	 30.513281 	 32.738415
Epoch 120 	 32.589027 	 30.643126 	 32.952927
Epoch 130 	 32.263203 	 30.267813 	 32.511902
Epoch 140 	 32.250088 	 30.001518 	 32.360054
Epoch 150 	 32.084148 	 30.059542 	 32.252880
Epoch 160 	 32.057278 	 30.110836 	 32.295853
Epoch 170 	 31.981909 	 30.078945 	 32.189350
Epoch 180 	 31.925636 	 30.049103 	 32.149097
Epoch 190 	 31.810610 	 29.698647 	 32.120422
Train loss       : 31.733011
Best valid loss  : 29.366329
Best test loss   : 32.221439
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 272,819
--------------------------------
Total memory      : 17.56 MB
Total Flops       : 3.79 MFlops
Total Mem (Read)  : 5.68 MB
Total Mem (Write) : 3.94 MB
[Supermasks testing]
[Untrained loss : 24406472.0000]
[Starting training]
Epoch 0 	 46803.242188 	 455932160.000000 	 1602075.375000
Epoch 10 	 61.200779 	 169.273605 	 1253.341064
Epoch 20 	 60.005226 	 132.064316 	 4472.790039
Epoch 30 	 209.818115 	 68.498573 	 64.198074
Epoch 40 	 57.894859 	 309.651520 	 774.407471
Epoch 50 	 69.427307 	 56.682526 	 60.638741
Epoch 60 	 55.681808 	 346.406433 	 2350.045898
Epoch 70 	 56.426651 	 144.749084 	 337.360687
[Model stopped early]
Train loss       : 57.360497
Best valid loss  : 53.682167
Best test loss   : 54.093891
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 220,983
--------------------------------
Total memory      : 17.11 MB
Total Flops       : 2.56 MFlops
Total Mem (Read)  : 5.08 MB
Total Mem (Write) : 3.52 MB
[Supermasks testing]
[Untrained loss : 159.8833]
[Starting training]
Epoch 0 	 24017.927734 	 86.523048 	 48516.601562
Epoch 10 	 62.545551 	 92.925415 	 354261.875000
Epoch 20 	 56.591854 	 54.047688 	 54.014694
Epoch 30 	 55.284813 	 53.363358 	 53.206348
Epoch 40 	 54.142380 	 52.199299 	 52.021732
Epoch 50 	 53.787106 	 51.219669 	 51.280449
Epoch 60 	 53.356339 	 50.912083 	 51.087002
Epoch 70 	 52.604660 	 51.043289 	 51.134315
Epoch 80 	 52.489765 	 50.121094 	 50.764484
Epoch 90 	 51.244518 	 49.421360 	 49.837357
Epoch 100 	 50.793552 	 49.550480 	 49.701015
Epoch 110 	 50.106457 	 48.169823 	 48.659115
Epoch 120 	 49.620110 	 48.209259 	 48.878780
Epoch 130 	 48.087177 	 46.900822 	 47.483410
Epoch 140 	 47.425549 	 46.183872 	 46.726212
Epoch 150 	 46.496326 	 46.109192 	 45.815487
Epoch 160 	 45.551086 	 45.475834 	 45.599861
Epoch 170 	 44.670536 	 44.617275 	 45.184319
Epoch 180 	 44.390617 	 45.039429 	 45.833477
Epoch 190 	 44.144669 	 44.177490 	 45.127266
Train loss       : 43.820206
Best valid loss  : 43.605827
Best test loss   : 45.521931
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 185,398
--------------------------------
Total memory      : 16.72 MB
Total Flops       : 1.66 MFlops
Total Mem (Read)  : 4.58 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 291.1211]
[Starting training]
Epoch 0 	 75.767822 	 62.545731 	 64.445908
Epoch 10 	 54.811882 	 52.452297 	 51.760628
Epoch 20 	 51.439518 	 49.244186 	 49.347759
Epoch 30 	 48.205891 	 43.918434 	 45.107311
Epoch 40 	 46.349030 	 43.430534 	 45.951935
Epoch 50 	 44.837261 	 42.255959 	 43.810577
Epoch 60 	 44.170212 	 41.452583 	 43.456501
Epoch 70 	 43.125946 	 39.815975 	 41.935314
Epoch 80 	 42.915634 	 39.765377 	 41.749004
Epoch 90 	 42.411110 	 39.525986 	 41.825016
Epoch 100 	 43.166592 	 38.525894 	 40.869839
Epoch 110 	 40.993351 	 39.111237 	 41.193565
Epoch 120 	 40.904179 	 37.800190 	 39.873882
Epoch 130 	 40.421299 	 37.387123 	 39.749836
Epoch 140 	 39.670807 	 37.313152 	 39.387207
Epoch 150 	 39.547104 	 36.860298 	 38.819969
Epoch 160 	 39.539970 	 36.731136 	 39.102066
Epoch 170 	 39.540974 	 36.858730 	 38.829651
Epoch 180 	 39.284092 	 36.902950 	 38.885544
Epoch 190 	 38.826710 	 36.267338 	 38.437050
Train loss       : 38.999882
Best valid loss  : 36.187656
Best test loss   : 38.448334
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 156,046
--------------------------------
Total memory      : 16.63 MB
Total Flops       : 1.63 MFlops
Total Mem (Read)  : 4.41 MB
Total Mem (Write) : 3.08 MB
[Supermasks testing]
[Untrained loss : 260.8403]
[Starting training]
Epoch 0 	 79.203827 	 63.004848 	 64.204399
Epoch 10 	 57.862976 	 54.635658 	 54.705799
Epoch 20 	 53.883743 	 52.085701 	 51.883968
Epoch 30 	 51.798111 	 49.984093 	 49.592937
Epoch 40 	 51.184090 	 49.845688 	 50.053158
Epoch 50 	 50.307480 	 48.768513 	 48.738934
Epoch 60 	 49.739574 	 48.256348 	 48.119049
Epoch 70 	 49.176525 	 47.696777 	 47.664345
Epoch 80 	 48.760975 	 46.697670 	 46.678612
Epoch 90 	 48.241196 	 46.433681 	 46.314186
Epoch 100 	 47.933868 	 46.569210 	 46.010651
Epoch 110 	 47.083176 	 45.908279 	 45.976772
Epoch 120 	 46.547089 	 45.598984 	 46.036278
Epoch 130 	 46.070675 	 45.639221 	 45.797325
Epoch 140 	 45.933289 	 45.747803 	 45.552498
Epoch 150 	 45.646332 	 45.265041 	 45.691761
[Model stopped early]
Train loss       : 45.520329
Best valid loss  : 45.006607
Best test loss   : 45.677200
Pruning          : 0.03
[Performing one full cumulative epoch]
Traceback (most recent call last):
  File "main.py", line 261, in <module>
    model = pruning.reset(model)
  File "/scratch/esling/lottery/pruning.py", line 781, in reset
    replace_recurrent(m, l, m.unprune_idx[l], prev_kept)
  File "/scratch/esling/lottery/pruning.py", line 752, in replace_recurrent
    cur_ih = nn.Parameter(cur_ih[rep_id0])#torch.from_numpy(cur_ih[rep_id0]).to(self.args.device))
IndexError: too many indices for tensor of dimension 2
