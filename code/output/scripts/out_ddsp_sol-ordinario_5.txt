Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41281310.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, six, pillow-simd, torchvision, tqdm, pyparsing, cycler, kiwisolver, python-dateutil, matplotlib, google-pasta, markdown, grpcio, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, oauthlib, urllib3, idna, certifi, chardet, requests, requests-oauthlib, google-auth-oauthlib, protobuf, absl-py, werkzeug, tensorboard, wrapt, opt-einsum, astor, tensorflow-estimator, h5py, keras-applications, gast, keras-preprocessing, termcolor, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281310.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-29 02:24:35.498260: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 02:24:35.828571: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is sol-ordinario_ddsp_cnn_xavier_trimming_info_target_reinit_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41281310.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 95.3708]
[Starting training]
Epoch 0 	 89.724823 	 85.465416 	 88.774803
Epoch 10 	 82.212532 	 72.917130 	 79.969658
Epoch 20 	 76.457085 	 79.032669 	 82.720963
Epoch 30 	 69.968246 	 69.218658 	 74.330399
Epoch 40 	 67.264832 	 68.294159 	 67.627861
Epoch 50 	 67.404533 	 64.171349 	 67.774773
Epoch 60 	 63.086510 	 62.676300 	 65.175491
Epoch 70 	 62.378330 	 60.877907 	 62.729675
Epoch 80 	 58.533932 	 60.448151 	 61.766895
Epoch 90 	 58.360519 	 57.251869 	 59.207836
Epoch 100 	 57.059666 	 55.523373 	 58.635098
Epoch 110 	 54.586849 	 53.758949 	 55.342793
Epoch 120 	 51.945641 	 54.882629 	 54.536598
Epoch 130 	 51.584293 	 49.855762 	 52.698200
Epoch 140 	 51.035904 	 50.450630 	 52.127399
Epoch 150 	 48.065792 	 45.339939 	 50.669155
Epoch 160 	 46.716835 	 46.699009 	 46.071033
Epoch 170 	 45.556744 	 46.187511 	 47.946461
Epoch 180 	 44.789371 	 45.281425 	 47.636795
Epoch 190 	 44.717682 	 46.049477 	 47.467335
Train loss       : 44.431812
Best valid loss  : 41.524849
Best test loss   : 46.631672
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,909,528
--------------------------------
Total memory      : 35.03 MB
Total Flops       : 326.2 MFlops
Total Mem (Read)  : 31.67 MB
Total Mem (Write) : 24.49 MB
[Supermasks testing]
[Untrained loss : 96.4893]
[Starting training]
Epoch 0 	 88.707947 	 83.804108 	 86.504463
Epoch 10 	 65.251030 	 70.124763 	 69.992920
Epoch 20 	 54.156487 	 52.810616 	 54.958149
Epoch 30 	 46.255058 	 45.775986 	 48.183128
Epoch 40 	 41.766266 	 40.167862 	 42.900051
Epoch 50 	 39.840523 	 38.834652 	 41.299313
Epoch 60 	 36.359467 	 36.333649 	 38.736191
Epoch 70 	 35.791817 	 35.211441 	 37.589886
Epoch 80 	 32.384727 	 33.259972 	 34.894814
Epoch 90 	 30.840782 	 32.756477 	 35.233994
Epoch 100 	 29.308334 	 31.748823 	 34.033089
Epoch 110 	 29.678480 	 32.677681 	 34.869442
Epoch 120 	 27.678326 	 32.366516 	 33.508591
Epoch 130 	 28.004602 	 30.549686 	 32.070538
Epoch 140 	 25.522064 	 28.543379 	 30.739807
Epoch 150 	 24.848822 	 28.379578 	 29.425787
Epoch 160 	 23.897644 	 27.542011 	 29.303085
Epoch 170 	 23.457590 	 27.711884 	 29.087351
Epoch 180 	 23.230989 	 27.264799 	 28.580481
Epoch 190 	 22.407118 	 26.949085 	 28.240570
Train loss       : 22.628752
Best valid loss  : 26.443552
Best test loss   : 28.337027
Pruning          : 0.72
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,898,752
--------------------------------
Total memory      : 29.65 MB
Total Flops       : 171.22 MFlops
Total Mem (Read)  : 22.92 MB
Total Mem (Write) : 17.34 MB
[Supermasks testing]
[Untrained loss : 95.2617]
[Starting training]
Epoch 0 	 88.230194 	 85.965958 	 88.707718
Epoch 10 	 63.645718 	 63.087936 	 68.113945
Epoch 20 	 49.645863 	 49.897957 	 53.502106
Epoch 30 	 43.554829 	 42.423561 	 45.219830
Epoch 40 	 41.945641 	 41.964890 	 45.008408
Epoch 50 	 34.326637 	 34.181103 	 37.742554
Epoch 60 	 32.048183 	 33.185608 	 36.290527
Epoch 70 	 33.037640 	 34.301815 	 39.212383
Epoch 80 	 29.585716 	 31.742104 	 34.511898
Epoch 90 	 29.571281 	 33.377831 	 35.261826
Epoch 100 	 27.753788 	 30.001446 	 31.703146
Epoch 110 	 27.221521 	 28.524958 	 30.940754
Epoch 120 	 26.877863 	 30.163767 	 31.626163
Epoch 130 	 25.073828 	 28.244419 	 30.298399
Epoch 140 	 23.656082 	 27.751886 	 29.219849
Epoch 150 	 23.464186 	 27.523844 	 29.406614
Epoch 160 	 23.214725 	 27.236168 	 28.837614
Epoch 170 	 22.775343 	 27.100327 	 28.609207
Epoch 180 	 22.503851 	 27.057888 	 28.606831
Epoch 190 	 22.369377 	 27.013515 	 28.443640
[Model stopped early]
Train loss       : 22.336889
Best valid loss  : 26.212421
Best test loss   : 28.682600
Pruning          : 0.52
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,282,494
--------------------------------
Total memory      : 25.75 MB
Total Flops       : 89.46 MFlops
Total Mem (Read)  : 17.01 MB
Total Mem (Write) : 12.62 MB
[Supermasks testing]
[Untrained loss : 102.2046]
[Starting training]
Epoch 0 	 92.610161 	 88.313774 	 90.391075
Epoch 10 	 60.798481 	 58.668179 	 64.921059
Epoch 20 	 51.796597 	 49.760410 	 54.806496
Epoch 30 	 48.558338 	 47.295399 	 50.898403
Epoch 40 	 42.715160 	 43.457851 	 47.512428
Epoch 50 	 39.767570 	 41.120499 	 46.245785
Epoch 60 	 38.289894 	 38.034458 	 43.179234
Epoch 70 	 35.861057 	 38.752029 	 42.327904
Epoch 80 	 36.468853 	 43.588497 	 46.150043
Epoch 90 	 32.826286 	 37.502228 	 40.856838
Epoch 100 	 31.923733 	 36.198406 	 40.417797
Epoch 110 	 31.012737 	 33.968597 	 41.104309
Epoch 120 	 29.869604 	 34.099728 	 40.436768
Epoch 130 	 28.907429 	 33.230240 	 39.601433
Epoch 140 	 28.315235 	 31.843683 	 39.158688
Epoch 150 	 27.883314 	 32.162712 	 38.666332
Epoch 160 	 27.917387 	 31.729845 	 38.472767
Epoch 170 	 26.914055 	 31.625603 	 38.426090
Epoch 180 	 26.711596 	 32.042480 	 37.806828
Epoch 190 	 26.381218 	 31.097645 	 37.775387
Train loss       : 26.359518
Best valid loss  : 30.610102
Best test loss   : 37.777824
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 887,398
--------------------------------
Total memory      : 22.89 MB
Total Flops       : 46.15 MFlops
Total Mem (Read)  : 12.89 MB
Total Mem (Write) : 9.4 MB
[Supermasks testing]
[Untrained loss : 104.0190]
[Starting training]
Epoch 0 	 94.444702 	 88.033096 	 92.137474
Epoch 10 	 63.331936 	 60.445885 	 63.582195
Epoch 20 	 56.576218 	 55.119019 	 59.373474
Epoch 30 	 50.938400 	 47.447235 	 50.357582
Epoch 40 	 42.621098 	 43.797806 	 48.067284
Epoch 50 	 38.536694 	 40.746582 	 44.124073
Epoch 60 	 38.956356 	 41.363430 	 44.625313
Epoch 70 	 36.714268 	 43.875683 	 45.539669
Epoch 80 	 34.981335 	 37.924622 	 41.428108
Epoch 90 	 37.873146 	 39.812576 	 42.611740
Epoch 100 	 32.891994 	 35.450520 	 38.854523
Epoch 110 	 31.807522 	 34.762749 	 37.547859
Epoch 120 	 31.045404 	 35.541706 	 38.329185
Epoch 130 	 30.663897 	 33.956734 	 36.361954
Epoch 140 	 30.179430 	 33.669621 	 35.877502
Epoch 150 	 29.550323 	 33.481239 	 36.332508
Epoch 160 	 29.916786 	 33.394459 	 35.888439
Epoch 170 	 29.223438 	 32.345398 	 35.007366
Epoch 180 	 28.630142 	 32.789818 	 34.907772
Epoch 190 	 28.422028 	 32.066406 	 34.678997
Train loss       : 28.318377
Best valid loss  : 31.605026
Best test loss   : 34.777752
Pruning          : 0.27
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 630,951
--------------------------------
Total memory      : 20.84 MB
Total Flops       : 23.99 MFlops
Total Mem (Read)  : 10.04 MB
Total Mem (Write) : 7.21 MB
[Supermasks testing]
[Untrained loss : 95.4469]
[Starting training]
Epoch 0 	 91.738831 	 88.321968 	 90.946083
Epoch 10 	 63.688366 	 60.190216 	 63.754681
Epoch 20 	 56.101093 	 52.031559 	 56.153564
Epoch 30 	 51.653061 	 48.978977 	 54.076332
Epoch 40 	 45.878937 	 46.519962 	 49.667976
Epoch 50 	 46.102005 	 46.863625 	 49.186672
Epoch 60 	 40.652519 	 43.690819 	 46.526264
Epoch 70 	 37.216000 	 39.952236 	 44.056503
Epoch 80 	 37.634686 	 41.163589 	 44.623089
Epoch 90 	 36.042439 	 38.980381 	 44.346169
Epoch 100 	 36.569641 	 42.204796 	 42.932384
Epoch 110 	 35.777576 	 41.529957 	 42.977245
Epoch 120 	 35.617470 	 39.680984 	 43.141720
[Model stopped early]
Train loss       : 35.235001
Best valid loss  : 38.980381
Best test loss   : 44.346169
Pruning          : 0.19
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 461,817
--------------------------------
Total memory      : 19.39 MB
Total Flops       : 12.83 MFlops
Total Mem (Read)  : 8.07 MB
Total Mem (Write) : 5.74 MB
[Supermasks testing]
[Untrained loss : 96.1721]
[Starting training]
Epoch 0 	 92.071854 	 89.253494 	 92.446915
Epoch 10 	 65.114601 	 63.412560 	 67.265907
Epoch 20 	 56.943748 	 55.421993 	 59.823719
Epoch 30 	 53.364948 	 50.644714 	 56.574951
Epoch 40 	 47.984856 	 47.462234 	 53.361671
Epoch 50 	 47.396393 	 45.348557 	 50.407085
Epoch 60 	 43.231781 	 44.438717 	 48.617771
Epoch 70 	 41.224827 	 41.991653 	 46.193577
Epoch 80 	 39.593052 	 40.887997 	 45.235512
Epoch 90 	 38.763126 	 40.121521 	 43.559204
Epoch 100 	 36.551773 	 38.798363 	 41.992046
Epoch 110 	 35.149792 	 37.439381 	 39.206081
Epoch 120 	 35.010494 	 37.483803 	 39.257599
Epoch 130 	 33.751942 	 36.507504 	 38.748745
Epoch 140 	 33.635365 	 37.394638 	 38.336540
Epoch 150 	 33.004677 	 36.355701 	 38.237396
Epoch 160 	 33.179916 	 35.398937 	 37.867092
Epoch 170 	 32.857716 	 35.907021 	 38.099819
Epoch 180 	 33.055550 	 36.650681 	 37.683590
[Model stopped early]
Train loss       : 33.012192
Best valid loss  : 35.363667
Best test loss   : 38.437572
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 348,567
--------------------------------
Total memory      : 18.35 MB
Total Flops       : 7.06 MFlops
Total Mem (Read)  : 6.7 MB
Total Mem (Write) : 4.71 MB
[Supermasks testing]
[Untrained loss : 97.0090]
[Starting training]
Epoch 0 	 93.074104 	 90.134048 	 93.095741
Epoch 10 	 66.244423 	 65.225174 	 67.585342
Epoch 20 	 61.579437 	 59.373528 	 63.832214
Epoch 30 	 56.805981 	 54.837021 	 58.084072
Epoch 40 	 53.018383 	 54.086845 	 58.134651
Epoch 50 	 52.010456 	 54.283360 	 58.770458
Epoch 60 	 48.430824 	 49.479805 	 53.858135
Epoch 70 	 46.967598 	 49.914917 	 54.457947
Epoch 80 	 45.154751 	 48.018707 	 53.573162
Epoch 90 	 43.756390 	 47.264965 	 52.966824
Epoch 100 	 42.590801 	 46.931343 	 51.327595
Epoch 110 	 41.923531 	 47.420174 	 52.192593
Epoch 120 	 41.342365 	 46.945770 	 52.894905
Epoch 130 	 41.173363 	 46.957008 	 51.413296
Epoch 140 	 40.841022 	 46.679390 	 51.133835
Epoch 150 	 40.590527 	 46.992081 	 51.227112
[Model stopped early]
Train loss       : 40.811852
Best valid loss  : 45.993244
Best test loss   : 51.369648
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 272,819
--------------------------------
Total memory      : 17.56 MB
Total Flops       : 3.79 MFlops
Total Mem (Read)  : 5.68 MB
Total Mem (Write) : 3.94 MB
[Supermasks testing]
[Untrained loss : 101.8187]
[Starting training]
Epoch 0 	 95.180656 	 89.993088 	 94.220840
Epoch 10 	 65.459023 	 60.087177 	 67.197357
Epoch 20 	 61.473183 	 57.831280 	 63.814571
Epoch 30 	 57.944820 	 55.370621 	 61.383259
Epoch 40 	 57.440311 	 55.176537 	 60.098763
Epoch 50 	 55.737247 	 53.162201 	 58.705502
Epoch 60 	 52.138535 	 50.919731 	 55.362762
Epoch 70 	 48.364525 	 47.940689 	 52.217445
Epoch 80 	 46.904167 	 46.667873 	 52.765968
Epoch 90 	 46.951893 	 46.630360 	 51.638927
Epoch 100 	 45.426247 	 46.161095 	 51.020901
Epoch 110 	 44.811039 	 49.846806 	 55.737148
Epoch 120 	 43.295532 	 43.649395 	 50.095211
Epoch 130 	 41.637669 	 43.644215 	 49.852734
Epoch 140 	 41.396351 	 43.783772 	 49.714615
Epoch 150 	 40.418972 	 43.746170 	 49.811569
Epoch 160 	 41.050846 	 43.260811 	 48.972176
Epoch 170 	 40.274380 	 43.079632 	 49.089863
Epoch 180 	 40.077484 	 42.672310 	 49.078926
Epoch 190 	 40.281734 	 43.163506 	 49.350216
Train loss       : 39.976875
Best valid loss  : 42.007862
Best test loss   : 49.477081
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 220,983
--------------------------------
Total memory      : 17.11 MB
Total Flops       : 2.56 MFlops
Total Mem (Read)  : 5.08 MB
Total Mem (Write) : 3.52 MB
[Supermasks testing]
[Untrained loss : 1409860461658112.0000]
[Starting training]
Epoch 0 	 93.033745 	 90.554031 	 93.566971
Epoch 10 	 68.959656 	 70.450150 	 72.841690
Epoch 20 	 67.323822 	 67.083282 	 70.869934
Epoch 30 	 63.988571 	 63.841927 	 68.859154
Epoch 40 	 61.681091 	 61.489754 	 65.838867
Epoch 50 	 62.311741 	 60.257103 	 64.904182
Epoch 60 	 59.325695 	 58.049976 	 63.011311
Epoch 70 	 57.323975 	 57.038124 	 62.738049
Epoch 80 	 57.149574 	 56.477802 	 61.088745
Epoch 90 	 56.109489 	 54.586258 	 58.973682
Epoch 100 	 52.862625 	 54.340549 	 58.796143
Epoch 110 	 51.447803 	 54.048847 	 58.033455
Epoch 120 	 50.039066 	 53.554436 	 57.748913
Epoch 130 	 50.201027 	 52.839443 	 57.632435
Epoch 140 	 50.749378 	 52.709286 	 57.352222
Epoch 150 	 50.623817 	 52.176079 	 57.389912
Epoch 160 	 49.218124 	 52.880474 	 57.097820
[Model stopped early]
Train loss       : 49.587345
Best valid loss  : 51.928223
Best test loss   : 57.807655
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 185,398
--------------------------------
Total memory      : 16.72 MB
Total Flops       : 1.66 MFlops
Total Mem (Read)  : 4.58 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 99.0384]
[Starting training]
Epoch 0 	 93.271942 	 89.795685 	 93.213997
Epoch 10 	 70.096977 	 68.815941 	 71.659317
Epoch 20 	 68.446861 	 68.023430 	 72.777733
Epoch 30 	 64.267311 	 63.192665 	 67.376793
Epoch 40 	 63.802181 	 61.558716 	 65.934769
Epoch 50 	 60.651066 	 56.841221 	 62.666908
Epoch 60 	 58.877274 	 58.174179 	 62.048428
Epoch 70 	 56.908150 	 56.894249 	 61.138958
Epoch 80 	 56.795391 	 55.407455 	 59.414368
Epoch 90 	 56.450054 	 55.453705 	 59.141567
Epoch 100 	 55.164043 	 55.582268 	 58.195526
Epoch 110 	 55.340790 	 54.888714 	 58.770626
Epoch 120 	 54.644958 	 54.724777 	 57.661293
Epoch 130 	 54.598858 	 54.572876 	 57.511669
Epoch 140 	 55.318165 	 54.682384 	 57.235931
Epoch 150 	 54.032116 	 53.617241 	 57.381855
Epoch 160 	 54.069870 	 53.868824 	 57.234718
[Model stopped early]
Train loss       : 54.169777
Best valid loss  : 52.509823
Best test loss   : 57.246387
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 156,046
--------------------------------
Total memory      : 16.63 MB
Total Flops       : 1.63 MFlops
Total Mem (Read)  : 4.41 MB
Total Mem (Write) : 3.08 MB
[Supermasks testing]
[Untrained loss : 100.6316]
[Starting training]
Epoch 0 	 94.366318 	 89.709953 	 93.801582
Epoch 10 	 69.551689 	 67.625610 	 70.196136
Epoch 20 	 67.008476 	 69.291679 	 74.144455
Epoch 30 	 64.439384 	 62.479595 	 66.997787
Epoch 40 	 64.187988 	 60.368214 	 65.828300
Epoch 50 	 61.654896 	 60.634132 	 65.017494
Epoch 60 	 63.361980 	 60.410854 	 64.529625
Epoch 70 	 59.189041 	 56.809082 	 62.322155
Epoch 80 	 57.493088 	 58.829838 	 62.967384
Epoch 90 	 57.533810 	 56.144707 	 60.814075
Epoch 100 	 56.978077 	 55.645248 	 60.975147
Epoch 110 	 56.115891 	 55.051765 	 60.145782
Epoch 120 	 55.637520 	 55.483662 	 60.087032
Epoch 130 	 55.936035 	 55.042648 	 60.283333
Epoch 140 	 55.375469 	 54.633911 	 59.850842
Epoch 150 	 54.820507 	 54.752884 	 59.633106
Epoch 160 	 55.284168 	 54.863731 	 59.597118
Epoch 170 	 54.952602 	 54.977913 	 59.557140
Epoch 180 	 54.387882 	 54.284294 	 59.483345
Epoch 190 	 54.759804 	 54.245655 	 59.528858
Train loss       : 55.181385
Best valid loss  : 53.839252
Best test loss   : 59.526428
Pruning          : 0.03
[Performing one full cumulative epoch]
Traceback (most recent call last):
  File "main.py", line 261, in <module>
    if (args.prune_selection in ['activation', 'information', 'info_target']):
  File "/scratch/esling/lottery/pruning.py", line 781, in reset
    replace_recurrent(m, l, m.unprune_idx[l], prev_kept)
  File "/scratch/esling/lottery/pruning.py", line 752, in replace_recurrent
    cur_ih = nn.Parameter(cur_ih[rep_id0])#torch.from_numpy(cur_ih[rep_id0]).to(self.args.device))
IndexError: too many indices for tensor of dimension 2
