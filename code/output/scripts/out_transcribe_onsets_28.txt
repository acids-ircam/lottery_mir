Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288823.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, future, torch, six, torchvision, tqdm, pyparsing, python-dateutil, kiwisolver, cycler, matplotlib, protobuf, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, markdown, grpcio, oauthlib, idna, chardet, urllib3, certifi, requests, requests-oauthlib, google-auth-oauthlib, werkzeug, absl-py, tensorboard, tensorflow-estimator, gast, wrapt, h5py, keras-applications, astor, termcolor, keras-preprocessing, google-pasta, opt-einsum, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288823.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:53:02.111447: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:53:02.122661: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_masking_magnitude_rewind_local_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288823.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7963]
[Starting training]
/localscratch/esling.41288823.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
Epoch 0 	 22.808035 	 0.604807 	 0.586737
Epoch 10 	 21.385635 	 0.505042 	 0.495322
Epoch 20 	 20.041491 	 0.364372 	 0.355013
Epoch 30 	 18.512768 	 0.232778 	 0.222171
Epoch 40 	 17.751339 	 0.188586 	 0.185453
/localscratch/esling.41288823.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 50 	 17.273336 	 0.173513 	 0.168024
Epoch 60 	 16.889286 	 0.156285 	 0.148498
Epoch 70 	 16.693222 	 0.140130 	 0.143197
Epoch 80 	 16.554235 	 0.137052 	 0.138283
Epoch 90 	 16.333719 	 0.136588 	 0.132259
Epoch 100 	 16.262093 	 0.136106 	 0.136327
Epoch 110 	 16.172453 	 0.125079 	 0.127379
Epoch 120 	 16.138474 	 0.125770 	 0.124042
Epoch 130 	 16.066475 	 0.124179 	 0.122960
Epoch 140 	 16.045197 	 0.121178 	 0.122496
Epoch 150 	 16.015238 	 0.120451 	 0.122414
Epoch 160 	 16.009504 	 0.117819 	 0.122433
Epoch 170 	 15.999228 	 0.122174 	 0.123657
Epoch 180 	 15.972404 	 0.120233 	 0.122932
Epoch 190 	 15.971274 	 0.118619 	 0.122353
Train loss       : 15.968101
Best valid loss  : 0.115796
Best test loss   : 0.122184
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.1518]
[Starting training]
Epoch 0 	 16.827332 	 0.144515 	 0.146460
Epoch 10 	 16.457310 	 0.128628 	 0.126470
Epoch 20 	 16.223570 	 0.124571 	 0.126076
Epoch 30 	 16.127676 	 0.125160 	 0.126230
Epoch 40 	 16.090242 	 0.124953 	 0.128398
Epoch 50 	 16.049135 	 0.128534 	 0.124957
Epoch 60 	 15.984471 	 0.119622 	 0.122267
Epoch 70 	 15.958566 	 0.121139 	 0.120880
Epoch 80 	 15.936742 	 0.118614 	 0.121230
Epoch 90 	 15.931533 	 0.121778 	 0.121750
Epoch 100 	 15.921384 	 0.118815 	 0.120354
Epoch 110 	 15.909688 	 0.118579 	 0.120610
Epoch 120 	 15.907491 	 0.119958 	 0.120772
Epoch 130 	 15.912767 	 0.121746 	 0.120519
Epoch 140 	 15.914379 	 0.116990 	 0.120916
Epoch 150 	 15.905842 	 0.119011 	 0.120835
Epoch 160 	 15.916412 	 0.119560 	 0.120468
[Model stopped early]
Train loss       : 15.907152
Best valid loss  : 0.115814
Best test loss   : 0.120708
Pruning          : 0.70
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.1451]
[Starting training]
Epoch 0 	 16.711798 	 0.146670 	 0.143211
Epoch 10 	 16.318031 	 0.124410 	 0.127270
Epoch 20 	 16.172092 	 0.125597 	 0.127967
Epoch 30 	 16.016439 	 0.118688 	 0.124014
Epoch 40 	 15.992310 	 0.120761 	 0.124661
Epoch 50 	 15.925142 	 0.121198 	 0.123370
Epoch 60 	 15.924061 	 0.119415 	 0.121435
Epoch 70 	 15.910701 	 0.119747 	 0.120050
Epoch 80 	 15.904949 	 0.121660 	 0.122077
Epoch 90 	 15.899877 	 0.119249 	 0.120775
[Model stopped early]
Train loss       : 15.893414
Best valid loss  : 0.117576
Best test loss   : 0.121287
Pruning          : 0.49
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.1458]
[Starting training]
Epoch 0 	 16.689608 	 0.138971 	 0.138299
Epoch 10 	 16.249899 	 0.125532 	 0.130774
Epoch 20 	 16.135599 	 0.126062 	 0.124407
Epoch 30 	 15.998095 	 0.120123 	 0.122901
Epoch 40 	 15.955366 	 0.119294 	 0.123718
Epoch 50 	 15.922933 	 0.120595 	 0.122823
Epoch 60 	 15.904400 	 0.119948 	 0.119931
[Model stopped early]
Train loss       : 15.902125
Best valid loss  : 0.116976
Best test loss   : 0.123610
Pruning          : 0.34
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.1464]
[Starting training]
Epoch 0 	 16.716457 	 0.142433 	 0.144031
Epoch 10 	 16.227825 	 0.126814 	 0.128398
Epoch 20 	 16.076420 	 0.119972 	 0.122370
Epoch 30 	 15.992426 	 0.124673 	 0.124525
Epoch 40 	 15.955264 	 0.119479 	 0.121982
Epoch 50 	 15.940080 	 0.120304 	 0.119424
Epoch 60 	 15.931292 	 0.120841 	 0.120243
[Model stopped early]
Train loss       : 15.911893
Best valid loss  : 0.115738
Best test loss   : 0.120760
Pruning          : 0.24
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.1522]
[Starting training]
Epoch 0 	 16.786684 	 0.142167 	 0.144130
Epoch 10 	 16.285582 	 0.124242 	 0.127086
Epoch 20 	 16.155289 	 0.116544 	 0.123333
Epoch 30 	 16.108917 	 0.125261 	 0.126635
Epoch 40 	 16.004110 	 0.123714 	 0.124812
Epoch 50 	 15.951971 	 0.124386 	 0.124074
[Model stopped early]
Train loss       : 15.948040
Best valid loss  : 0.116544
Best test loss   : 0.123333
Pruning          : 0.17
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.1631]
[Starting training]
Epoch 0 	 16.872313 	 0.139963 	 0.142665
Epoch 10 	 16.378799 	 0.121847 	 0.125156
Epoch 20 	 16.249863 	 0.123517 	 0.125913
Epoch 30 	 16.128698 	 0.124322 	 0.125077
Epoch 40 	 16.062246 	 0.123786 	 0.125714
Epoch 50 	 16.009964 	 0.123881 	 0.124133
Epoch 60 	 15.984645 	 0.123969 	 0.123543
[Model stopped early]
Train loss       : 15.980954
Best valid loss  : 0.118019
Best test loss   : 0.124416
Pruning          : 0.12
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.1769]
[Starting training]
Epoch 0 	 16.987667 	 0.146793 	 0.145289
Epoch 10 	 16.387827 	 0.125641 	 0.130217
Epoch 20 	 16.264807 	 0.122560 	 0.126544
Epoch 30 	 16.125553 	 0.122262 	 0.125609
Epoch 40 	 16.072474 	 0.122278 	 0.125338
Epoch 50 	 16.053766 	 0.120834 	 0.125231
Epoch 60 	 16.046417 	 0.121808 	 0.124263
Epoch 70 	 16.035524 	 0.123149 	 0.124029
Epoch 80 	 16.002625 	 0.120226 	 0.122177
Epoch 90 	 15.986853 	 0.121791 	 0.121919
[Model stopped early]
Train loss       : 16.004766
Best valid loss  : 0.117996
Best test loss   : 0.123162
Pruning          : 0.08
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.2037]
[Starting training]
Epoch 0 	 17.103256 	 0.140646 	 0.144580
Epoch 10 	 16.453642 	 0.127823 	 0.131736
Epoch 20 	 16.337395 	 0.126206 	 0.128298
Epoch 30 	 16.179770 	 0.122341 	 0.124417
Epoch 40 	 16.125826 	 0.121686 	 0.124753
Epoch 50 	 16.095570 	 0.120952 	 0.122937
Epoch 60 	 16.079569 	 0.121858 	 0.123069
Epoch 70 	 16.068033 	 0.119165 	 0.122657
Epoch 80 	 16.066519 	 0.122842 	 0.122549
Epoch 90 	 16.056248 	 0.120706 	 0.123069
[Model stopped early]
Train loss       : 16.056248
Best valid loss  : 0.117845
Best test loss   : 0.122858
Pruning          : 0.06
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.3586]
[Starting training]
Epoch 0 	 17.380411 	 0.150169 	 0.150914
Epoch 10 	 16.609715 	 0.123756 	 0.127786
Epoch 20 	 16.450012 	 0.126486 	 0.123902
Epoch 30 	 16.368275 	 0.122808 	 0.121972
Epoch 40 	 16.267927 	 0.120441 	 0.123164
Epoch 50 	 16.205719 	 0.120502 	 0.121530
[Model stopped early]
Train loss       : 16.165237
Best valid loss  : 0.117533
Best test loss   : 0.122805
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.5339]
[Starting training]
Epoch 0 	 17.655491 	 0.149300 	 0.148480
Epoch 10 	 16.851065 	 0.126589 	 0.127138
Epoch 20 	 16.698891 	 0.122128 	 0.124184
Epoch 30 	 16.502481 	 0.120914 	 0.122183
Epoch 40 	 16.469536 	 0.119232 	 0.122278
Epoch 50 	 16.363955 	 0.116406 	 0.121136
Epoch 60 	 16.345770 	 0.119973 	 0.122881
Epoch 70 	 16.331310 	 0.117933 	 0.121408
Epoch 80 	 16.284723 	 0.117121 	 0.121900
[Model stopped early]
Train loss       : 16.295803
Best valid loss  : 0.113436
Best test loss   : 0.120792
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.6050]
[Starting training]
Epoch 0 	 17.936527 	 0.156486 	 0.160033
Epoch 10 	 17.008106 	 0.128236 	 0.130170
Epoch 20 	 16.845205 	 0.121181 	 0.121041
Epoch 30 	 16.666183 	 0.121224 	 0.122453
Epoch 40 	 16.530710 	 0.122064 	 0.118406
Epoch 50 	 16.535643 	 0.119951 	 0.121606
Epoch 60 	 16.508398 	 0.117729 	 0.118258
Epoch 70 	 16.487125 	 0.119909 	 0.119735
Epoch 80 	 16.450233 	 0.117888 	 0.118001
Epoch 90 	 16.430326 	 0.119985 	 0.118042
Epoch 100 	 16.449682 	 0.119327 	 0.118479
Epoch 110 	 16.407383 	 0.117246 	 0.117914
[Model stopped early]
Train loss       : 16.457422
Best valid loss  : 0.113560
Best test loss   : 0.117872
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.6113]
[Starting training]
Epoch 0 	 18.204494 	 0.176961 	 0.172610
Epoch 10 	 17.207088 	 0.122900 	 0.129610
Epoch 20 	 17.032709 	 0.118345 	 0.126621
Epoch 30 	 16.907253 	 0.115751 	 0.120965
Epoch 40 	 16.840864 	 0.123450 	 0.124481
Epoch 50 	 16.693981 	 0.117145 	 0.122253
Epoch 60 	 16.649883 	 0.118782 	 0.120685
Epoch 70 	 16.616449 	 0.115178 	 0.119588
Epoch 80 	 16.594467 	 0.116403 	 0.119928
Epoch 90 	 16.571712 	 0.116707 	 0.118972
Epoch 100 	 16.582594 	 0.119522 	 0.119907
Epoch 110 	 16.569483 	 0.118432 	 0.118797
Epoch 120 	 16.582535 	 0.119630 	 0.120242
[Model stopped early]
Train loss       : 16.572145
Best valid loss  : 0.114249
Best test loss   : 0.118770
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7620]
[Starting training]
Epoch 0 	 18.701168 	 0.191784 	 0.192428
Epoch 10 	 17.436562 	 0.132595 	 0.130978
Epoch 20 	 17.213184 	 0.123342 	 0.125976
Epoch 30 	 17.141201 	 0.124899 	 0.121386
Epoch 40 	 17.028006 	 0.119486 	 0.122469
Epoch 50 	 17.031197 	 0.119419 	 0.120229
Epoch 60 	 16.870230 	 0.119632 	 0.120042
Epoch 70 	 16.773914 	 0.116995 	 0.117067
Epoch 80 	 16.767073 	 0.117592 	 0.118121
Epoch 90 	 16.746704 	 0.116187 	 0.117983
Epoch 100 	 16.740440 	 0.119085 	 0.117704
Epoch 110 	 16.716681 	 0.118224 	 0.119255
Epoch 120 	 16.712376 	 0.117932 	 0.118834
[Model stopped early]
Train loss       : 16.724264
Best valid loss  : 0.113320
Best test loss   : 0.118249
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7655]
[Starting training]
Epoch 0 	 18.980963 	 0.193451 	 0.182842
Epoch 10 	 17.603781 	 0.132402 	 0.131084
Epoch 20 	 17.381439 	 0.122497 	 0.124592
Epoch 30 	 17.311295 	 0.120286 	 0.121782
Epoch 40 	 17.229195 	 0.118686 	 0.123291
Epoch 50 	 17.109812 	 0.118844 	 0.120162
Epoch 60 	 17.010921 	 0.117583 	 0.118938
[Model stopped early]
Train loss       : 16.970972
Best valid loss  : 0.116267
Best test loss   : 0.122723
Pruning          : 0.01
