Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41281306.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, future, torch, six, torchvision, tqdm, cycler, pyparsing, python-dateutil, kiwisolver, matplotlib, grpcio, absl-py, idna, urllib3, chardet, certifi, requests, werkzeug, protobuf, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, tensorboard, tensorflow-estimator, gast, wrapt, google-pasta, keras-preprocessing, opt-einsum, termcolor, h5py, keras-applications, astor, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281306.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-29 01:58:18.636250: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 01:58:18.977013: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is sol-ordinario_ddsp_cnn_xavier_trimming_activation_reinit_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41281306.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 113.9551]
[Starting training]
Epoch 0 	 91.451012 	 87.693024 	 89.602432
Epoch 10 	 82.625069 	 79.028603 	 84.303238
Epoch 20 	 77.174683 	 77.273544 	 79.918900
Epoch 30 	 73.679955 	 69.394852 	 75.808128
Epoch 40 	 70.152901 	 68.881042 	 69.777649
Epoch 50 	 64.927711 	 62.975681 	 66.561150
Epoch 60 	 58.704388 	 55.429321 	 58.130665
Epoch 70 	 51.525894 	 51.305515 	 55.737556
Epoch 80 	 54.712353 	 53.963081 	 57.620605
Epoch 90 	 45.001984 	 45.322910 	 48.932228
Epoch 100 	 42.179169 	 41.416325 	 42.927422
Epoch 110 	 40.710556 	 38.224468 	 40.196430
Epoch 120 	 38.181320 	 38.003349 	 39.398083
Epoch 130 	 37.625744 	 38.733479 	 39.658775
Epoch 140 	 37.516605 	 35.425571 	 37.529430
Epoch 150 	 34.129459 	 36.300407 	 37.912457
Epoch 160 	 32.901184 	 32.238449 	 35.036995
Epoch 170 	 31.846458 	 33.339390 	 34.560295
Epoch 180 	 29.503614 	 30.579247 	 32.281471
Epoch 190 	 27.669340 	 29.780777 	 31.763494
Train loss       : 27.268644
Best valid loss  : 29.780777
Best test loss   : 31.763494
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,909,528
--------------------------------
Total memory      : 35.03 MB
Total Flops       : 326.2 MFlops
Total Mem (Read)  : 31.67 MB
Total Mem (Write) : 24.49 MB
[Supermasks testing]
[Untrained loss : 97.7257]
[Starting training]
Epoch 0 	 88.889107 	 81.908127 	 85.843422
Epoch 10 	 57.900845 	 52.647373 	 57.385139
Epoch 20 	 45.957191 	 45.170025 	 47.598591
Epoch 30 	 42.071461 	 49.783371 	 52.859955
Epoch 40 	 37.133736 	 37.935860 	 38.552677
Epoch 50 	 35.374187 	 35.938934 	 37.637691
Epoch 60 	 32.696808 	 33.139980 	 35.803833
Epoch 70 	 30.707613 	 32.007362 	 33.667610
Epoch 80 	 29.807192 	 30.859867 	 32.092556
Epoch 90 	 29.546352 	 30.665789 	 32.308613
Epoch 100 	 25.930134 	 28.168503 	 29.988785
Epoch 110 	 24.954098 	 27.372185 	 29.435751
Epoch 120 	 24.507244 	 27.648651 	 29.362995
Epoch 130 	 24.132587 	 27.102249 	 29.460617
Epoch 140 	 23.153635 	 26.779411 	 28.142618
Epoch 150 	 23.115704 	 26.402824 	 28.357725
Epoch 160 	 22.667559 	 26.005999 	 27.953775
Epoch 170 	 21.572710 	 25.406242 	 27.279911
Epoch 180 	 21.284229 	 25.452469 	 27.429876
Epoch 190 	 20.732740 	 25.240376 	 27.220173
Train loss       : 20.557163
Best valid loss  : 24.631287
Best test loss   : 26.815704
Pruning          : 0.72
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,898,752
--------------------------------
Total memory      : 29.65 MB
Total Flops       : 171.22 MFlops
Total Mem (Read)  : 22.92 MB
Total Mem (Write) : 17.34 MB
[Supermasks testing]
[Untrained loss : 95.4649]
[Starting training]
Epoch 0 	 88.611153 	 83.515587 	 87.262596
Epoch 10 	 60.283428 	 56.821770 	 61.385220
Epoch 20 	 47.959724 	 47.188335 	 50.517334
Epoch 30 	 41.770599 	 42.123863 	 44.659832
Epoch 40 	 40.457306 	 39.640984 	 41.906906
Epoch 50 	 35.374584 	 35.016148 	 37.731091
Epoch 60 	 33.395157 	 34.446270 	 36.343971
Epoch 70 	 31.580490 	 33.743107 	 35.728146
Epoch 80 	 31.988686 	 32.843002 	 35.202488
Epoch 90 	 29.123224 	 30.998692 	 34.177219
Epoch 100 	 27.574902 	 30.065174 	 33.041786
Epoch 110 	 29.010590 	 31.853279 	 34.950569
Epoch 120 	 26.740486 	 30.245325 	 32.755108
Epoch 130 	 24.958412 	 28.306093 	 30.448431
Epoch 140 	 24.399700 	 28.607155 	 30.099344
Epoch 150 	 24.238897 	 28.123764 	 30.098181
Epoch 160 	 22.839111 	 28.002686 	 29.470993
Epoch 170 	 22.577009 	 27.370632 	 29.376799
Epoch 180 	 22.511806 	 27.820618 	 29.041916
Epoch 190 	 21.908373 	 27.303801 	 28.827570
Train loss       : 21.793673
Best valid loss  : 26.368935
Best test loss   : 28.583282
Pruning          : 0.52
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,282,494
--------------------------------
Total memory      : 25.75 MB
Total Flops       : 89.46 MFlops
Total Mem (Read)  : 17.01 MB
Total Mem (Write) : 12.62 MB
[Supermasks testing]
[Untrained loss : 97.7119]
[Starting training]
Epoch 0 	 90.626320 	 86.117088 	 88.844063
Epoch 10 	 62.417786 	 55.258362 	 60.623699
Epoch 20 	 55.378410 	 48.462730 	 52.424587
Epoch 30 	 45.438629 	 45.019409 	 49.168510
Epoch 40 	 43.741844 	 43.329288 	 47.710918
Epoch 50 	 36.829723 	 37.986755 	 42.267502
Epoch 60 	 34.296509 	 37.941517 	 41.659340
Epoch 70 	 32.138165 	 35.368389 	 39.643894
Epoch 80 	 30.968082 	 33.828682 	 37.824532
Epoch 90 	 30.165928 	 33.479401 	 37.985016
Epoch 100 	 27.768848 	 32.120743 	 36.442059
Epoch 110 	 27.287668 	 31.810411 	 36.448738
Epoch 120 	 26.282436 	 31.842474 	 36.061832
Epoch 130 	 25.906359 	 30.371492 	 35.151573
Epoch 140 	 25.762190 	 30.603947 	 34.579906
Epoch 150 	 25.594889 	 30.472462 	 34.718929
Epoch 160 	 25.298626 	 30.366455 	 34.800434
Epoch 170 	 25.261303 	 30.493799 	 34.435253
Epoch 180 	 25.146477 	 30.272120 	 34.520992
Epoch 190 	 25.042688 	 29.835014 	 34.344753
Train loss       : 24.902088
Best valid loss  : 29.123415
Best test loss   : 33.911930
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 887,398
--------------------------------
Total memory      : 22.89 MB
Total Flops       : 46.15 MFlops
Total Mem (Read)  : 12.89 MB
Total Mem (Write) : 9.4 MB
[Supermasks testing]
[Untrained loss : 99.3026]
[Starting training]
Epoch 0 	 92.532982 	 88.228477 	 91.630424
Epoch 10 	 61.372185 	 65.730583 	 70.491394
Epoch 20 	 50.937843 	 46.749435 	 51.017757
Epoch 30 	 44.333897 	 44.519661 	 51.171947
Epoch 40 	 39.741177 	 41.806816 	 46.491062
Epoch 50 	 38.293705 	 39.612007 	 45.765350
Epoch 60 	 38.038120 	 39.102562 	 44.463833
Epoch 70 	 34.933613 	 37.283569 	 42.839054
Epoch 80 	 32.885555 	 35.391262 	 40.891933
Epoch 90 	 32.122475 	 35.496830 	 39.694820
Epoch 100 	 31.385233 	 36.064632 	 39.737000
Epoch 110 	 30.966913 	 35.475372 	 40.132374
Epoch 120 	 29.844755 	 34.287312 	 38.822266
Epoch 130 	 29.130064 	 34.542305 	 38.732159
Epoch 140 	 28.467102 	 33.633839 	 38.233730
Epoch 150 	 28.379705 	 34.156464 	 38.229492
Epoch 160 	 27.970284 	 33.850807 	 38.174034
Epoch 170 	 27.779299 	 32.673454 	 38.077297
Epoch 180 	 27.779642 	 33.458118 	 37.935577
Epoch 190 	 27.783150 	 33.536674 	 37.706844
Train loss       : 27.462046
Best valid loss  : 32.565605
Best test loss   : 37.861305
Pruning          : 0.27
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 630,951
--------------------------------
Total memory      : 20.84 MB
Total Flops       : 23.99 MFlops
Total Mem (Read)  : 10.04 MB
Total Mem (Write) : 7.21 MB
[Supermasks testing]
[Untrained loss : 97.4087]
[Starting training]
Epoch 0 	 92.495567 	 87.561279 	 91.844841
Epoch 10 	 61.802731 	 59.773647 	 65.123192
Epoch 20 	 54.539413 	 50.917259 	 55.687263
Epoch 30 	 48.205456 	 51.122231 	 53.935036
Epoch 40 	 45.260048 	 45.533283 	 50.489651
Epoch 50 	 39.416706 	 41.982887 	 46.396687
Epoch 60 	 37.916435 	 45.490326 	 48.415447
Epoch 70 	 35.252037 	 39.303696 	 43.724087
Epoch 80 	 34.232334 	 39.984310 	 43.772778
Epoch 90 	 33.555618 	 38.948971 	 41.310741
Epoch 100 	 31.847565 	 38.390099 	 41.430458
Epoch 110 	 32.141552 	 38.221195 	 40.818226
Epoch 120 	 31.286308 	 37.677502 	 39.806797
Epoch 130 	 30.913860 	 36.306377 	 40.801826
Epoch 140 	 30.054949 	 35.717335 	 38.925598
Epoch 150 	 29.727282 	 35.821671 	 38.943165
Epoch 160 	 29.493111 	 34.950191 	 38.528404
Epoch 170 	 29.223616 	 35.476746 	 39.244312
Epoch 180 	 29.318956 	 35.476967 	 39.216614
Epoch 190 	 29.036659 	 35.418674 	 39.188431
Train loss       : 28.951872
Best valid loss  : 34.580700
Best test loss   : 38.907513
Pruning          : 0.19
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 461,817
--------------------------------
Total memory      : 19.39 MB
Total Flops       : 12.83 MFlops
Total Mem (Read)  : 8.07 MB
Total Mem (Write) : 5.74 MB
[Supermasks testing]
[Untrained loss : 95.6196]
[Starting training]
Epoch 0 	 92.353981 	 88.171951 	 91.955376
Epoch 10 	 64.457573 	 65.089874 	 69.139374
Epoch 20 	 58.388889 	 54.149727 	 58.730297
Epoch 30 	 50.476200 	 48.683544 	 51.382427
Epoch 40 	 45.802536 	 46.061390 	 47.999081
Epoch 50 	 43.351280 	 45.561653 	 49.543495
Epoch 60 	 42.772717 	 42.570335 	 46.080959
Epoch 70 	 39.472065 	 43.715160 	 47.036289
Epoch 80 	 40.338684 	 41.701397 	 44.686459
Epoch 90 	 37.877296 	 40.996632 	 44.751221
Epoch 100 	 38.860752 	 39.616318 	 44.190083
Epoch 110 	 37.574993 	 39.974163 	 43.383785
Epoch 120 	 36.137268 	 38.658882 	 41.487076
Epoch 130 	 35.664417 	 38.584908 	 41.651947
Epoch 140 	 34.124443 	 37.391083 	 40.575970
Epoch 150 	 33.624992 	 37.298939 	 40.240932
Epoch 160 	 33.175148 	 37.080536 	 40.028400
Epoch 170 	 32.709225 	 36.896164 	 40.188236
Epoch 180 	 34.300003 	 37.575302 	 41.853207
Epoch 190 	 32.803547 	 37.310127 	 40.523136
Train loss       : 32.462639
Best valid loss  : 35.635025
Best test loss   : 40.457500
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 348,567
--------------------------------
Total memory      : 18.35 MB
Total Flops       : 7.06 MFlops
Total Mem (Read)  : 6.7 MB
Total Mem (Write) : 4.71 MB
[Supermasks testing]
[Untrained loss : 97.2735]
[Starting training]
Epoch 0 	 92.713249 	 89.091545 	 92.284447
Epoch 10 	 65.546204 	 62.314430 	 67.810310
Epoch 20 	 57.727150 	 54.245140 	 61.272839
Epoch 30 	 52.150005 	 52.246372 	 56.837490
Epoch 40 	 49.810921 	 49.026863 	 54.638973
Epoch 50 	 47.534565 	 47.666805 	 52.885242
Epoch 60 	 44.135525 	 46.688709 	 51.285610
Epoch 70 	 42.630623 	 44.635639 	 49.108738
Epoch 80 	 41.131393 	 44.535431 	 48.034279
Epoch 90 	 40.242378 	 42.395302 	 46.161293
Epoch 100 	 37.699146 	 40.371414 	 43.832787
Epoch 110 	 37.109589 	 41.056366 	 44.076694
Epoch 120 	 35.721149 	 38.390133 	 41.739452
Epoch 130 	 33.651611 	 38.451172 	 41.210037
Epoch 140 	 33.521614 	 38.304676 	 40.985535
Epoch 150 	 33.797153 	 38.367378 	 40.614250
Epoch 160 	 33.177578 	 37.374210 	 40.493801
Epoch 170 	 32.764473 	 37.197315 	 40.028316
Epoch 180 	 32.866100 	 37.055626 	 39.637241
Epoch 190 	 32.973011 	 37.448235 	 40.045986
Train loss       : 32.355026
Best valid loss  : 36.632847
Best test loss   : 39.875896
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 272,819
--------------------------------
Total memory      : 17.56 MB
Total Flops       : 3.79 MFlops
Total Mem (Read)  : 5.68 MB
Total Mem (Write) : 3.94 MB
[Supermasks testing]
[Untrained loss : 61558005449467791147008.0000]
[Starting training]
Epoch 0 	 95.268700 	 91.966431 	 94.116058
Epoch 10 	 68.055611 	 68.457756 	 71.409958
Epoch 20 	 62.403553 	 59.738483 	 64.757378
Epoch 30 	 60.927563 	 58.404552 	 62.821125
Epoch 40 	 56.811176 	 56.274555 	 58.393311
Epoch 50 	 57.180462 	 53.257637 	 57.310810
Epoch 60 	 52.225327 	 53.407051 	 56.366638
Epoch 70 	 51.347519 	 49.304108 	 54.039425
Epoch 80 	 49.752510 	 50.627617 	 54.967354
Epoch 90 	 48.863472 	 49.071182 	 52.223652
Epoch 100 	 46.397388 	 48.047478 	 52.157825
Epoch 110 	 47.262314 	 47.154797 	 51.443050
Epoch 120 	 45.879623 	 48.105762 	 51.849098
Epoch 130 	 44.873959 	 48.810925 	 52.199245
Epoch 140 	 43.879658 	 47.533257 	 51.128628
Epoch 150 	 44.079712 	 48.467167 	 50.967148
Epoch 160 	 43.444267 	 47.083847 	 51.283100
Epoch 170 	 42.871357 	 46.785213 	 51.018059
Epoch 180 	 43.412151 	 48.230995 	 51.079601
Epoch 190 	 42.066452 	 48.130035 	 50.922047
Train loss       : 43.184685
Best valid loss  : 45.907768
Best test loss   : 50.811024
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 220,983
--------------------------------
Total memory      : 17.11 MB
Total Flops       : 2.56 MFlops
Total Mem (Read)  : 5.08 MB
Total Mem (Write) : 3.52 MB
[Supermasks testing]
[Untrained loss : 96.0634]
[Starting training]
Epoch 0 	 92.590599 	 89.860283 	 92.611099
Epoch 10 	 67.562698 	 64.827988 	 69.574593
Epoch 20 	 63.967770 	 62.195522 	 67.143166
Epoch 30 	 62.315231 	 60.285919 	 65.308296
Epoch 40 	 59.429867 	 60.117214 	 65.144821
Epoch 50 	 56.196514 	 54.902798 	 59.797119
Epoch 60 	 55.480301 	 53.470619 	 58.739277
Epoch 70 	 52.731091 	 54.597252 	 59.203419
Epoch 80 	 51.704193 	 52.164192 	 56.783569
Epoch 90 	 51.150578 	 51.923103 	 55.141052
Epoch 100 	 49.373398 	 50.804138 	 54.891041
Epoch 110 	 48.530220 	 48.658821 	 54.782013
Epoch 120 	 48.471619 	 48.919270 	 55.015533
Epoch 130 	 46.742664 	 47.855904 	 53.539978
Epoch 140 	 45.809399 	 49.150768 	 53.476986
Epoch 150 	 46.069427 	 49.166679 	 53.108795
Epoch 160 	 46.202965 	 47.422977 	 52.217804
Epoch 170 	 46.030701 	 48.089745 	 53.459969
Epoch 180 	 45.650333 	 47.506351 	 51.681831
Epoch 190 	 45.666470 	 48.048336 	 52.072662
[Model stopped early]
Train loss       : 45.343201
Best valid loss  : 47.084511
Best test loss   : 52.321167
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 185,398
--------------------------------
Total memory      : 16.72 MB
Total Flops       : 1.66 MFlops
Total Mem (Read)  : 4.58 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 94.5213]
[Starting training]
Epoch 0 	 91.959824 	 89.504631 	 92.376762
Epoch 10 	 67.628960 	 66.214600 	 70.603935
Epoch 20 	 65.066475 	 64.270081 	 68.232704
Epoch 30 	 63.711140 	 65.053680 	 69.242706
Epoch 40 	 60.906494 	 59.629967 	 62.845806
Epoch 50 	 58.784603 	 57.592735 	 62.471844
Epoch 60 	 57.404884 	 58.027248 	 62.080410
Epoch 70 	 56.334084 	 54.119751 	 59.473949
Epoch 80 	 55.139347 	 53.665993 	 58.974506
Epoch 90 	 54.599930 	 55.142143 	 59.095486
Epoch 100 	 53.663395 	 55.863449 	 58.376801
Epoch 110 	 53.930790 	 55.090542 	 58.668560
[Model stopped early]
Train loss       : 53.714363
Best valid loss  : 53.665993
Best test loss   : 58.974506
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 156,046
--------------------------------
Total memory      : 16.63 MB
Total Flops       : 1.63 MFlops
Total Mem (Read)  : 4.41 MB
Total Mem (Write) : 3.08 MB
[Supermasks testing]
[Untrained loss : inf]
[Starting training]
Epoch 0 	 91.981064 	 89.543030 	 92.212242
Epoch 10 	 70.776314 	 70.686432 	 75.541557
Epoch 20 	 68.832420 	 66.249329 	 70.685478
Epoch 30 	 66.354538 	 67.572754 	 71.236565
Epoch 40 	 66.404480 	 67.470436 	 69.893814
Epoch 50 	 66.400345 	 66.533241 	 69.366875
Epoch 60 	 64.976082 	 65.557915 	 69.007484
Epoch 70 	 63.796478 	 65.213448 	 68.487755
Epoch 80 	 63.849506 	 64.851212 	 68.018150
Epoch 90 	 62.394230 	 64.379921 	 67.831749
Epoch 100 	 62.836777 	 63.571682 	 67.722923
[Model stopped early]
Train loss       : 61.825504
Best valid loss  : 62.949455
Best test loss   : 68.055138
Pruning          : 0.03
[Performing one full cumulative epoch]
Traceback (most recent call last):
  File "main.py", line 261, in <module>
    if (args.prune_selection in ['activation', 'information', 'info_target']):
  File "/scratch/esling/lottery/pruning.py", line 781, in reset
    replace_recurrent(m, l, m.unprune_idx[l], prev_kept)
  File "/scratch/esling/lottery/pruning.py", line 752, in replace_recurrent
    cur_ih = nn.Parameter(cur_ih[rep_id0])#torch.from_numpy(cur_ih[rep_id0]).to(self.args.device))
IndexError: too many indices for tensor of dimension 2
