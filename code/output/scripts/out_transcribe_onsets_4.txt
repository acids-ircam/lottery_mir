Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288784.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, future, torch, six, torchvision, tqdm, pyparsing, cycler, kiwisolver, python-dateutil, matplotlib, termcolor, absl-py, astor, h5py, keras-applications, opt-einsum, wrapt, gast, grpcio, tensorflow-estimator, protobuf, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, urllib3, idna, certifi, chardet, requests, requests-oauthlib, google-auth-oauthlib, markdown, werkzeug, tensorboard, keras-preprocessing, google-pasta, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288784.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:52:06.635016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:52:06.647697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_trimming_information_reinit_local_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288784.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7916]
[Starting training]
/localscratch/esling.41288784.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
Epoch 0 	 22.711878 	 0.598655 	 0.609229
Epoch 10 	 21.562920 	 0.546821 	 0.550834
Epoch 20 	 20.789431 	 0.437958 	 0.445849
Epoch 30 	 19.727219 	 0.334922 	 0.343134
Epoch 40 	 18.699614 	 0.254556 	 0.263007
Epoch 50 	 18.000504 	 0.193531 	 0.200645
Epoch 60 	 17.494669 	 0.171633 	 0.178645
Epoch 70 	 17.154760 	 0.154427 	 0.163702
Epoch 80 	 16.880304 	 0.145163 	 0.155265
Epoch 90 	 16.698532 	 0.144457 	 0.154397
Epoch 100 	 16.549177 	 0.140292 	 0.149345
Epoch 110 	 16.459888 	 0.142816 	 0.146921
Epoch 120 	 16.258989 	 0.129276 	 0.140108
Epoch 130 	 16.223469 	 0.132072 	 0.139288
Epoch 140 	 16.168341 	 0.133588 	 0.138864
Epoch 150 	 16.144550 	 0.133804 	 0.137993
[Model stopped early]
Train loss       : 16.133854
Best valid loss  : 0.129276
Best test loss   : 0.140108
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,031,141
--------------------------------
Total memory      : 15.85 MB
Total Flops       : 1.5 GFlops
Total Mem (Read)  : 23.9 MB
Total Mem (Write) : 12.33 MB
[Supermasks testing]
[Untrained loss : 0.7240]
[Starting training]
Epoch 0 	 22.708942 	 0.597923 	 0.605358
Epoch 10 	 21.776661 	 0.570230 	 0.572409
Epoch 20 	 21.603231 	 0.552255 	 0.564276
Epoch 30 	 20.581577 	 0.408250 	 0.414596
Epoch 40 	 19.262449 	 0.292333 	 0.294431
Epoch 50 	 18.506929 	 0.248520 	 0.258976
/localscratch/esling.41288784.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 60 	 17.835281 	 0.207672 	 0.215801
Epoch 70 	 17.476461 	 0.169859 	 0.179463
Epoch 80 	 17.126322 	 0.159172 	 0.166851
Epoch 90 	 16.966408 	 0.151389 	 0.155074
Epoch 100 	 16.699499 	 0.140167 	 0.143586
Epoch 110 	 16.595047 	 0.139293 	 0.145630
Epoch 120 	 16.538820 	 0.142740 	 0.143712
Epoch 130 	 16.448826 	 0.140915 	 0.143115
Epoch 140 	 16.401455 	 0.141299 	 0.143119
Epoch 150 	 16.385458 	 0.141583 	 0.140623
Epoch 160 	 16.363798 	 0.139959 	 0.140809
Epoch 170 	 16.355186 	 0.140250 	 0.141985
Epoch 180 	 16.343847 	 0.140769 	 0.140941
[Model stopped early]
Train loss       : 16.325556
Best valid loss  : 0.137754
Best test loss   : 0.139857
Pruning          : 0.75
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,015,845
--------------------------------
Total memory      : 11.89 MB
Total Flops       : 848.79 MFlops
Total Mem (Read)  : 16.95 MB
Total Mem (Write) : 9.25 MB
[Supermasks testing]
[Untrained loss : 0.7684]
[Starting training]
Epoch 0 	 23.125082 	 0.700136 	 0.710422
Epoch 10 	 22.332600 	 0.635409 	 0.641701
Epoch 20 	 21.561300 	 0.516704 	 0.530216
Epoch 30 	 20.094553 	 0.351692 	 0.362184
Epoch 40 	 18.983591 	 0.267079 	 0.274229
Epoch 50 	 18.233683 	 0.196598 	 0.203842
Epoch 60 	 17.689840 	 0.165534 	 0.170544
Epoch 70 	 17.346886 	 0.151777 	 0.160119
Epoch 80 	 17.096132 	 0.142869 	 0.149007
Epoch 90 	 16.964212 	 0.137990 	 0.146604
Epoch 100 	 16.787724 	 0.131219 	 0.139533
Epoch 110 	 16.663099 	 0.138687 	 0.141024
Epoch 120 	 16.461231 	 0.128958 	 0.136005
Epoch 130 	 16.364298 	 0.128436 	 0.133404
Epoch 140 	 16.337572 	 0.131709 	 0.132192
Epoch 150 	 16.296591 	 0.127848 	 0.131790
Epoch 160 	 16.274189 	 0.129611 	 0.133397
Epoch 170 	 16.241613 	 0.125174 	 0.132381
Epoch 180 	 16.243908 	 0.127099 	 0.132939
Epoch 190 	 16.225225 	 0.127060 	 0.132990
Train loss       : 16.220203
Best valid loss  : 0.124003
Best test loss   : 0.133104
Pruning          : 0.56
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,378,357
--------------------------------
Total memory      : 8.92 MB
Total Flops       : 481.03 MFlops
Total Mem (Read)  : 12.21 MB
Total Mem (Write) : 6.94 MB
[Supermasks testing]
[Untrained loss : 0.7255]
[Starting training]
Epoch 0 	 23.067913 	 0.668612 	 0.681739
Epoch 10 	 21.683304 	 0.554339 	 0.566691
Epoch 20 	 20.798527 	 0.422668 	 0.434400
Epoch 30 	 19.661009 	 0.330035 	 0.338131
Epoch 40 	 18.662933 	 0.231882 	 0.235681
Epoch 50 	 18.044096 	 0.204959 	 0.212030
Epoch 60 	 17.692898 	 0.175361 	 0.187414
Epoch 70 	 17.376425 	 0.166673 	 0.174637
Epoch 80 	 17.106321 	 0.156632 	 0.163831
Epoch 90 	 16.922144 	 0.153814 	 0.159670
Epoch 100 	 16.758795 	 0.141702 	 0.154251
Epoch 110 	 16.622311 	 0.145339 	 0.149016
Epoch 120 	 16.434780 	 0.137402 	 0.141722
Epoch 130 	 16.363230 	 0.132543 	 0.138265
Epoch 140 	 16.326115 	 0.133971 	 0.141024
Epoch 150 	 16.267967 	 0.134218 	 0.138300
Epoch 160 	 16.220791 	 0.134725 	 0.138610
Epoch 170 	 16.200651 	 0.135468 	 0.137498
Epoch 180 	 16.175808 	 0.133251 	 0.136842
Epoch 190 	 16.164261 	 0.130983 	 0.135977
Train loss       : 16.142656
Best valid loss  : 0.129822
Best test loss   : 0.134855
Pruning          : 0.42
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 969,982
--------------------------------
Total memory      : 6.69 MB
Total Flops       : 273.29 MFlops
Total Mem (Read)  : 8.92 MB
Total Mem (Write) : 5.21 MB
[Supermasks testing]
[Untrained loss : 0.7995]
[Starting training]
Epoch 0 	 23.055752 	 0.666033 	 0.677872
Epoch 10 	 21.916103 	 0.587206 	 0.593417
Epoch 20 	 21.321417 	 0.501768 	 0.512187
Epoch 30 	 20.192095 	 0.373894 	 0.384417
Epoch 40 	 19.254160 	 0.279276 	 0.285947
Epoch 50 	 18.574841 	 0.227668 	 0.232175
Epoch 60 	 18.085423 	 0.189326 	 0.197977
Epoch 70 	 17.685263 	 0.162335 	 0.166955
Epoch 80 	 17.398552 	 0.150273 	 0.154570
Epoch 90 	 17.196344 	 0.143839 	 0.153448
Epoch 100 	 17.073053 	 0.145943 	 0.147449
Epoch 110 	 16.837183 	 0.139274 	 0.147633
Epoch 120 	 16.784691 	 0.137760 	 0.147631
Epoch 130 	 16.722694 	 0.137733 	 0.147810
Epoch 140 	 16.655968 	 0.134281 	 0.145988
Epoch 150 	 16.603313 	 0.135186 	 0.145834
Epoch 160 	 16.522354 	 0.136402 	 0.142323
Epoch 170 	 16.483810 	 0.132335 	 0.139663
Epoch 180 	 16.483927 	 0.131057 	 0.140415
Epoch 190 	 16.472969 	 0.136091 	 0.139459
Train loss       : 16.468348
Best valid loss  : 0.129663
Best test loss   : 0.138480
Pruning          : 0.32
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 700,624
--------------------------------
Total memory      : 4.96 MB
Total Flops       : 152.05 MFlops
Total Mem (Read)  : 6.54 MB
Total Mem (Write) : 3.86 MB
[Supermasks testing]
[Untrained loss : 0.8058]
[Starting training]
Epoch 0 	 22.992516 	 0.629821 	 0.642818
Epoch 10 	 21.749378 	 0.557879 	 0.563936
Epoch 20 	 21.165903 	 0.475229 	 0.489769
Epoch 30 	 19.846523 	 0.340579 	 0.342245
Epoch 40 	 19.026417 	 0.258253 	 0.268650
Epoch 50 	 18.523947 	 0.221000 	 0.228589
Epoch 60 	 18.165565 	 0.185692 	 0.197148
Epoch 70 	 17.902792 	 0.174414 	 0.177883
Epoch 80 	 17.691053 	 0.171578 	 0.176526
Epoch 90 	 17.538794 	 0.160827 	 0.168369
Epoch 100 	 17.400772 	 0.159072 	 0.165178
Epoch 110 	 17.259621 	 0.152259 	 0.158709
Epoch 120 	 17.171864 	 0.150951 	 0.157097
Epoch 130 	 16.974960 	 0.142244 	 0.149896
Epoch 140 	 16.892040 	 0.137229 	 0.144836
Epoch 150 	 16.841156 	 0.136040 	 0.143819
Epoch 160 	 16.770399 	 0.134737 	 0.144805
Epoch 170 	 16.747236 	 0.136044 	 0.142821
Epoch 180 	 16.708254 	 0.135058 	 0.142624
Epoch 190 	 16.681793 	 0.132193 	 0.141282
Train loss       : 16.676931
Best valid loss  : 0.130955
Best test loss   : 0.142347
Pruning          : 0.24
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 522,854
--------------------------------
Total memory      : 3.72 MB
Total Flops       : 87.05 MFlops
Total Mem (Read)  : 4.9 MB
Total Mem (Write) : 2.9 MB
[Supermasks testing]
[Untrained loss : 0.7661]
[Starting training]
Epoch 0 	 23.559673 	 0.678473 	 0.692252
Epoch 10 	 21.759323 	 0.555336 	 0.568511
Epoch 20 	 21.281862 	 0.505569 	 0.516934
Epoch 30 	 20.269735 	 0.382744 	 0.388339
Epoch 40 	 19.589596 	 0.299612 	 0.307914
Epoch 50 	 18.994558 	 0.247743 	 0.256729
Epoch 60 	 18.566916 	 0.210883 	 0.224494
Epoch 70 	 18.327539 	 0.197453 	 0.205165
Epoch 80 	 18.131056 	 0.184104 	 0.192068
Epoch 90 	 17.903273 	 0.175592 	 0.181282
Epoch 100 	 17.782314 	 0.173318 	 0.177814
Epoch 110 	 17.584328 	 0.165952 	 0.170188
Epoch 120 	 17.504848 	 0.163957 	 0.169059
Epoch 130 	 17.414265 	 0.161392 	 0.166181
Epoch 140 	 17.343765 	 0.156101 	 0.165128
Epoch 150 	 17.308819 	 0.157347 	 0.163158
Epoch 160 	 17.242689 	 0.158892 	 0.162960
Epoch 170 	 17.253149 	 0.157270 	 0.161489
Epoch 180 	 17.227812 	 0.156369 	 0.162325
Epoch 190 	 17.194374 	 0.158891 	 0.163086
Train loss       : 17.225056
Best valid loss  : 0.154838
Best test loss   : 0.161912
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 399,974
--------------------------------
Total memory      : 2.73 MB
Total Flops       : 48.02 MFlops
Total Mem (Read)  : 3.66 MB
Total Mem (Write) : 2.13 MB
[Supermasks testing]
[Untrained loss : 0.7335]
[Starting training]
Epoch 0 	 23.361162 	 0.658071 	 0.681832
Epoch 10 	 21.901789 	 0.578522 	 0.583820
Epoch 20 	 21.528896 	 0.537139 	 0.548947
Epoch 30 	 20.959682 	 0.452451 	 0.460817
Epoch 40 	 20.150852 	 0.362873 	 0.365106
Epoch 50 	 19.708746 	 0.306105 	 0.313619
Epoch 60 	 19.346003 	 0.257641 	 0.265653
Epoch 70 	 19.051849 	 0.241546 	 0.255433
Epoch 80 	 18.852549 	 0.224444 	 0.237971
Epoch 90 	 18.703791 	 0.217478 	 0.228281
Epoch 100 	 18.578856 	 0.213406 	 0.222969
Epoch 110 	 18.464281 	 0.209935 	 0.219342
Epoch 120 	 18.316092 	 0.196320 	 0.206844
Epoch 130 	 18.211065 	 0.193446 	 0.203516
Epoch 140 	 18.112616 	 0.179782 	 0.196257
Epoch 150 	 17.952267 	 0.180631 	 0.188679
Epoch 160 	 17.893770 	 0.183153 	 0.192456
Epoch 170 	 17.843857 	 0.176681 	 0.191016
Epoch 180 	 17.817961 	 0.178138 	 0.185890
Epoch 190 	 17.726536 	 0.171816 	 0.182997
Train loss       : 17.763260
Best valid loss  : 0.164054
Best test loss   : 0.180212
Pruning          : 0.13
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 316,333
--------------------------------
Total memory      : 1.99 MB
Total Flops       : 26.32 MFlops
Total Mem (Read)  : 2.77 MB
Total Mem (Write) : 1.55 MB
[Supermasks testing]
[Untrained loss : 0.7555]
[Starting training]
Epoch 0 	 23.557566 	 0.708562 	 0.718593
Epoch 10 	 21.837961 	 0.569663 	 0.579835
Epoch 20 	 21.596792 	 0.542349 	 0.551003
Epoch 30 	 20.933098 	 0.448973 	 0.455840
Epoch 40 	 20.410536 	 0.392486 	 0.393725
Epoch 50 	 20.068579 	 0.342379 	 0.343101
Epoch 60 	 19.846714 	 0.322778 	 0.329108
Epoch 70 	 19.661572 	 0.298281 	 0.308911
Epoch 80 	 19.511786 	 0.279009 	 0.288961
Epoch 90 	 19.300795 	 0.268175 	 0.278990
Epoch 100 	 19.135612 	 0.255817 	 0.266547
Epoch 110 	 19.029865 	 0.249499 	 0.261247
Epoch 120 	 18.911661 	 0.238398 	 0.253541
Epoch 130 	 18.879908 	 0.236340 	 0.248504
Epoch 140 	 18.750444 	 0.224124 	 0.237931
Epoch 150 	 18.693327 	 0.221445 	 0.240730
Epoch 160 	 18.598061 	 0.220079 	 0.232553
Epoch 170 	 18.571560 	 0.211267 	 0.225102
Epoch 180 	 18.500795 	 0.214593 	 0.224325
Epoch 190 	 18.465261 	 0.205835 	 0.219656
Train loss       : 18.431875
Best valid loss  : 0.203327
Best test loss   : 0.214668
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 257,489
--------------------------------
Total memory      : 1.49 MB
Total Flops       : 15.44 MFlops
Total Mem (Read)  : 2.16 MB
Total Mem (Write) : 1.16 MB
[Supermasks testing]
[Untrained loss : 0.7849]
[Starting training]
Epoch 0 	 23.550220 	 0.704527 	 0.711711
Epoch 10 	 21.970266 	 0.576433 	 0.586355
Epoch 20 	 21.682138 	 0.545761 	 0.555811
Epoch 30 	 21.263687 	 0.490068 	 0.500264
Epoch 40 	 20.951344 	 0.450040 	 0.457637
Epoch 50 	 20.664942 	 0.412093 	 0.424497
Epoch 60 	 20.475309 	 0.393636 	 0.409626
Epoch 70 	 20.269655 	 0.374650 	 0.382649
Epoch 80 	 20.124582 	 0.362858 	 0.377281
Epoch 90 	 20.028402 	 0.353575 	 0.373923
Epoch 100 	 19.899967 	 0.354260 	 0.364136
Epoch 110 	 19.791281 	 0.342971 	 0.355509
Epoch 120 	 19.741926 	 0.334429 	 0.342577
Epoch 130 	 19.628201 	 0.328674 	 0.340397
Epoch 140 	 19.578583 	 0.322471 	 0.336584
Epoch 150 	 19.444981 	 0.314037 	 0.332240
Epoch 160 	 19.394545 	 0.309688 	 0.327070
Epoch 170 	 19.407274 	 0.310080 	 0.324195
Epoch 180 	 19.333035 	 0.304921 	 0.321899
Epoch 190 	 19.294699 	 0.307034 	 0.320447
Train loss       : 19.247677
Best valid loss  : 0.301504
Best test loss   : 0.320649
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 216,037
--------------------------------
Total memory      : 1.08 MB
Total Flops       : 8.58 MFlops
Total Mem (Read)  : 1.68 MB
Total Mem (Write) : 861.98 KB
[Supermasks testing]
[Untrained loss : 0.7511]
[Starting training]
Epoch 0 	 23.636951 	 0.679490 	 0.695486
Epoch 10 	 21.923811 	 0.561528 	 0.571779
Epoch 20 	 21.771303 	 0.553913 	 0.563072
Epoch 30 	 21.419504 	 0.505731 	 0.513129
Epoch 40 	 21.165928 	 0.469749 	 0.474813
Epoch 50 	 20.781370 	 0.419665 	 0.418834
Epoch 60 	 20.577099 	 0.403307 	 0.402316
Epoch 70 	 20.448744 	 0.392195 	 0.388192
Epoch 80 	 20.359152 	 0.385556 	 0.377584
Epoch 90 	 20.319241 	 0.373515 	 0.379537
Epoch 100 	 20.227901 	 0.374593 	 0.369698
Epoch 110 	 20.155130 	 0.361802 	 0.365314
Epoch 120 	 20.056631 	 0.353006 	 0.353152
Epoch 130 	 19.979319 	 0.343303 	 0.344794
Epoch 140 	 19.923855 	 0.336213 	 0.337169
Epoch 150 	 19.878874 	 0.334689 	 0.335566
Epoch 160 	 19.874769 	 0.322707 	 0.326127
Epoch 170 	 19.797226 	 0.315634 	 0.323282
Epoch 180 	 19.695564 	 0.310375 	 0.313934
Epoch 190 	 19.700430 	 0.303741 	 0.310410
Train loss       : 19.593655
Best valid loss  : 0.300557
Best test loss   : 0.308294
Pruning          : 0.06
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 184,678
--------------------------------
Total memory      : 0.75 MB
Total Flops       : 4.54 MFlops
Total Mem (Read)  : 1.3 MB
Total Mem (Write) : 599.12 KB
[Supermasks testing]
[Untrained loss : 0.7611]
[Starting training]
Epoch 0 	 23.667746 	 0.774011 	 0.780337
Epoch 10 	 22.125479 	 0.593565 	 0.607023
Epoch 20 	 21.886410 	 0.565618 	 0.574175
Epoch 30 	 21.592014 	 0.525087 	 0.537668
Epoch 40 	 21.431572 	 0.490471 	 0.502164
Epoch 50 	 21.239460 	 0.472786 	 0.483805
Epoch 60 	 21.086098 	 0.449230 	 0.462244
Epoch 70 	 20.930668 	 0.440314 	 0.447682
Epoch 80 	 20.872314 	 0.435153 	 0.444879
Epoch 90 	 20.809946 	 0.432906 	 0.439738
Epoch 100 	 20.739210 	 0.432549 	 0.434981
Epoch 110 	 20.710304 	 0.423159 	 0.432917
Epoch 120 	 20.650686 	 0.421193 	 0.428817
Epoch 130 	 20.588734 	 0.413487 	 0.427281
Epoch 140 	 20.513876 	 0.412792 	 0.420001
Epoch 150 	 20.509537 	 0.416252 	 0.416536
slurmstepd: error: *** JOB 41288784 ON cdr345 CANCELLED AT 2020-04-29T16:49:02 DUE TO TIME LIMIT ***
