Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41146330.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, future, torch, six, torchvision, tqdm, cycler, kiwisolver, pyparsing, python-dateutil, matplotlib, google-pasta, keras-preprocessing, gast, wrapt, h5py, keras-applications, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, absl-py, grpcio, markdown, protobuf, certifi, urllib3, chardet, idna, requests, werkzeug, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, termcolor, tensorflow-estimator, astor, opt-einsum, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146330.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-27 03:02:02.642329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-27 03:02:02.652729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_ddsp_cnn_xavier_trimming_batchnorm_reinit_global_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41146330.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 97.9045]
[Starting training]
Epoch 0 	 76.293808 	 67.546814 	 69.704094
Epoch 10 	 134.810455 	 60.913448 	 61.402287
Epoch 20 	 54.629742 	 50.275066 	 50.307209
Epoch 30 	 45.602982 	 45.283092 	 46.015945
Epoch 40 	 42.051750 	 38.700806 	 40.713074
Epoch 50 	 130.840897 	 41.041733 	 42.777229
Epoch 60 	 39.243515 	 33.974148 	 36.323593
Epoch 70 	 37.734337 	 33.408627 	 35.334930
Epoch 80 	 36.118069 	 32.520084 	 34.569798
Epoch 90 	 34.163837 	 30.909758 	 32.780441
Epoch 100 	 31.758503 	 30.269320 	 32.408684
Epoch 110 	 30.383287 	 28.518370 	 30.643600
Epoch 120 	 28.799891 	 27.645678 	 29.644180
Epoch 130 	 28.097166 	 27.721529 	 29.485184
Epoch 140 	 27.715227 	 27.650242 	 29.518482
Epoch 150 	 26.744951 	 26.949917 	 28.752068
Epoch 160 	 27.141796 	 26.409735 	 28.316462
Epoch 170 	 25.970217 	 26.301109 	 28.368147
Epoch 180 	 25.463652 	 25.776922 	 27.632969
Epoch 190 	 25.871218 	 26.333361 	 28.047932
Train loss       : 24.144348
Best valid loss  : 25.066429
Best test loss   : 26.863716
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 3,789,825
--------------------------------
Total memory      : 40.26 MB
Total Flops       : 622.57 MFlops
Total Mem (Read)  : 40.33 MB
Total Mem (Write) : 34.15 MB
[Supermasks testing]
[Untrained loss : 78.1268]
[Starting training]
Epoch 0 	 68.105400 	 59.435596 	 58.798298
Epoch 10 	 41.538509 	 36.407513 	 38.077824
Epoch 20 	 37.254616 	 33.254223 	 34.992031
Epoch 30 	 33.631233 	 31.757803 	 34.011650
Epoch 40 	 33.201015 	 30.020267 	 31.686638
Epoch 50 	 31.225603 	 29.173243 	 31.296202
Epoch 60 	 29.951950 	 27.886793 	 30.032322
Epoch 70 	 28.985909 	 28.086781 	 29.909035
Epoch 80 	 28.359467 	 27.636700 	 29.479269
Epoch 90 	 27.555876 	 27.393778 	 29.336115
Epoch 100 	 27.012020 	 27.248837 	 28.720171
Epoch 110 	 26.489637 	 25.944942 	 28.041786
Epoch 120 	 25.968594 	 25.990753 	 27.685446
Epoch 130 	 25.105110 	 25.345381 	 27.227524
Epoch 140 	 24.686178 	 25.364309 	 27.185637
Epoch 150 	 24.269836 	 25.271399 	 26.922953
Epoch 160 	 24.174467 	 25.163553 	 26.891426
Epoch 170 	 23.967276 	 25.195553 	 26.900946
Epoch 180 	 23.786003 	 25.032532 	 26.709122
Epoch 190 	 23.562578 	 24.782934 	 26.612793
Train loss       : 23.530266
Best valid loss  : 24.719217
Best test loss   : 26.560692
Pruning          : 0.72
0.001
0.001
[Current model size]
================================
Total params      : 3,328,407
--------------------------------
Total memory      : 38.66 MB
Total Flops       : 622.44 MFlops
Total Mem (Read)  : 37.52 MB
Total Mem (Write) : 33.09 MB
[Supermasks testing]
[Untrained loss : 86.1900]
[Starting training]
Epoch 0 	 70.094162 	 58.380203 	 58.561512
Epoch 10 	 42.834579 	 37.959953 	 39.557297
Epoch 20 	 36.485298 	 34.538208 	 36.603542
Epoch 30 	 34.790508 	 31.972830 	 33.780975
Epoch 40 	 31.833576 	 29.871283 	 32.066532
Epoch 50 	 30.713034 	 28.480455 	 30.451405
Epoch 60 	 29.575533 	 28.266724 	 30.134037
Epoch 70 	 28.138903 	 27.648624 	 29.416107
Epoch 80 	 33.276447 	 29.371246 	 30.984005
Epoch 90 	 27.582626 	 26.955793 	 28.780489
Epoch 100 	 26.824617 	 26.734549 	 28.347876
Epoch 110 	 26.219133 	 26.158903 	 28.102552
Epoch 120 	 25.849407 	 25.604536 	 27.407925
Epoch 130 	 24.825190 	 25.267744 	 26.914085
Epoch 140 	 24.462208 	 25.072582 	 26.717806
Epoch 150 	 24.332125 	 24.960730 	 26.683147
Epoch 160 	 23.850775 	 24.977901 	 26.541180
Epoch 170 	 23.710754 	 24.831787 	 26.403641
Epoch 180 	 23.634480 	 24.878815 	 26.440504
Epoch 190 	 23.325600 	 24.631372 	 26.349667
Train loss       : 23.280066
Best valid loss  : 24.444824
Best test loss   : 26.211332
Pruning          : 0.52
0.001
0.001
[Current model size]
================================
Total params      : 3,114,119
--------------------------------
Total memory      : 37.73 MB
Total Flops       : 622.35 MFlops
Total Mem (Read)  : 36.08 MB
Total Mem (Write) : 32.46 MB
[Supermasks testing]
[Untrained loss : 86.5447]
[Starting training]
Epoch 0 	 70.654205 	 55.713745 	 55.736237
Epoch 10 	 47.500568 	 45.349010 	 46.052280
Epoch 20 	 43.991428 	 42.922306 	 43.776955
Epoch 30 	 40.022575 	 39.297894 	 39.613995
Epoch 40 	 38.558086 	 36.584637 	 37.726662
Epoch 50 	 35.367786 	 36.974407 	 37.771152
Epoch 60 	 33.911156 	 35.282078 	 36.176598
Epoch 70 	 32.736897 	 34.322090 	 34.954746
Epoch 80 	 31.008436 	 32.933319 	 33.805618
Epoch 90 	 30.486807 	 32.592098 	 33.437187
Epoch 100 	 31.144352 	 35.493134 	 36.890541
Epoch 110 	 28.108538 	 31.377157 	 32.662827
Epoch 120 	 27.139921 	 31.032337 	 32.358925
Epoch 130 	 26.532524 	 30.767687 	 32.168705
Epoch 140 	 26.294931 	 30.888960 	 32.150208
Epoch 150 	 25.333887 	 30.196775 	 31.910120
Epoch 160 	 25.150097 	 30.137300 	 31.838566
Epoch 170 	 24.886339 	 30.360958 	 31.628468
Epoch 180 	 24.521780 	 30.263050 	 31.607460
Epoch 190 	 24.308523 	 30.209332 	 31.698248
Train loss       : 24.219744
Best valid loss  : 29.906532
Best test loss   : 31.697151
Pruning          : 0.37
0.001
0.001
[Current model size]
================================
Total params      : 2,965,823
--------------------------------
Total memory      : 35.88 MB
Total Flops       : 500.91 MFlops
Total Mem (Read)  : 33.87 MB
Total Mem (Write) : 30.82 MB
[Supermasks testing]
[Untrained loss : 90.6457]
[Starting training]
Epoch 0 	 71.605583 	 56.573135 	 57.137459
Epoch 10 	 46.130146 	 42.811413 	 42.794792
Epoch 20 	 45.592102 	 43.847420 	 43.716423
Epoch 30 	 39.898918 	 39.436836 	 39.575356
Epoch 40 	 38.048176 	 38.831791 	 38.961994
Epoch 50 	 36.722580 	 37.670692 	 38.151405
Epoch 60 	 35.330364 	 37.204094 	 37.376568
Epoch 70 	 34.188732 	 37.726814 	 37.942520
Epoch 80 	 33.266090 	 36.108822 	 37.005157
Epoch 90 	 32.266380 	 35.937336 	 36.269142
Epoch 100 	 31.468208 	 35.342304 	 35.647301
Epoch 110 	 30.777197 	 34.582390 	 35.082310
Epoch 120 	 30.184404 	 35.139374 	 35.640499
Epoch 130 	 29.444296 	 34.893391 	 35.235603
Epoch 140 	 27.851080 	 33.760506 	 34.139496
Epoch 150 	 27.341969 	 33.491535 	 34.002972
Epoch 160 	 26.996830 	 33.216030 	 33.805618
Epoch 170 	 26.625887 	 33.071117 	 33.588573
Epoch 180 	 26.196167 	 33.138657 	 33.491642
Epoch 190 	 25.690804 	 32.820518 	 33.421017
Train loss       : 25.398687
Best valid loss  : 32.542530
Best test loss   : 33.223579
Pruning          : 0.27
0.001
0.001
[Current model size]
================================
Total params      : 2,881,580
--------------------------------
Total memory      : 35.40 MB
Total Flops       : 500.87 MFlops
Total Mem (Read)  : 33.23 MB
Total Mem (Write) : 30.5 MB
[Supermasks testing]
[Untrained loss : 82.8562]
[Starting training]
Epoch 0 	 70.303917 	 54.927940 	 55.182461
Epoch 10 	 46.894867 	 43.760708 	 43.917213
Epoch 20 	 41.934200 	 39.990826 	 40.529049
Epoch 30 	 47.344746 	 46.005737 	 47.625710
Epoch 40 	 40.620003 	 39.057552 	 39.462402
Epoch 50 	 38.485283 	 37.595509 	 38.658070
[Model stopped early]
Train loss       : 37.909828
Best valid loss  : 37.315887
Best test loss   : 38.339821
Pruning          : 0.19
0.001
0.001
[Current model size]
================================
Total params      : 2,822,190
--------------------------------
Total memory      : 34.83 MB
Total Flops       : 500.82 MFlops
Total Mem (Read)  : 32.63 MB
Total Mem (Write) : 30.12 MB
[Supermasks testing]
[Untrained loss : 93.2994]
[Starting training]
Epoch 0 	 71.786362 	 57.379768 	 57.950077
Epoch 10 	 51.403774 	 46.959751 	 46.921406
Epoch 20 	 47.184353 	 43.279583 	 43.608559
Epoch 30 	 49.089931 	 45.048714 	 45.455986
Epoch 40 	 45.305901 	 42.706635 	 42.993271
Epoch 50 	 43.174858 	 42.059433 	 41.754944
Epoch 60 	 41.717976 	 41.066994 	 40.858101
Epoch 70 	 40.251484 	 39.635239 	 40.742130
Epoch 80 	 38.849731 	 39.380772 	 40.322201
Epoch 90 	 37.666000 	 38.921421 	 39.499760
Epoch 100 	 37.164501 	 38.219139 	 39.057877
Epoch 110 	 36.092514 	 38.624107 	 38.809891
Epoch 120 	 34.577755 	 37.618954 	 38.171158
Epoch 130 	 33.785362 	 37.357235 	 38.482922
Epoch 140 	 33.289215 	 37.358147 	 37.925819
Epoch 150 	 32.363739 	 36.869064 	 37.650143
Epoch 160 	 31.733160 	 37.009090 	 37.537655
Epoch 170 	 31.140348 	 36.667812 	 37.151398
Epoch 180 	 30.949989 	 36.747780 	 37.427834
Epoch 190 	 30.648169 	 36.307915 	 37.211414
Train loss       : 30.209936
Best valid loss  : 36.031227
Best test loss   : 37.026478
Pruning          : 0.14
0.001
0.001
[Current model size]
================================
Total params      : 2,747,632
--------------------------------
Total memory      : 34.38 MB
Total Flops       : 497.87 MFlops
Total Mem (Read)  : 32.02 MB
Total Mem (Write) : 29.79 MB
[Supermasks testing]
[Untrained loss : 91.4149]
[Starting training]
Epoch 0 	 62409.406250 	 98.491219 	 82.869987
Epoch 10 	 54.572937 	 52.230862 	 53.031456
Epoch 20 	 51.066250 	 46.958355 	 47.586140
Epoch 30 	 50.125256 	 49.364582 	 49.405872
Epoch 40 	 47.258488 	 46.170212 	 46.119976
Epoch 50 	 44.921829 	 43.395973 	 43.728428
Epoch 60 	 43.210766 	 44.401634 	 45.527939
Epoch 70 	 41.319099 	 40.442211 	 41.537373
Epoch 80 	 39.134819 	 39.954491 	 40.490582
Epoch 90 	 36.657200 	 38.048950 	 38.328430
Epoch 100 	 37.277668 	 39.010754 	 39.370380
Epoch 110 	 34.446201 	 36.504280 	 37.258877
Epoch 120 	 33.401524 	 37.009129 	 37.264076
Epoch 130 	 31.759296 	 35.763290 	 36.402050
Epoch 140 	 31.673588 	 34.600258 	 35.075947
Epoch 150 	 30.721651 	 34.072243 	 35.017937
Epoch 160 	 28.615417 	 32.757504 	 33.428135
Epoch 170 	 27.869980 	 33.272404 	 33.488811
Epoch 180 	 27.495470 	 32.306324 	 33.100937
Epoch 190 	 27.120348 	 32.243233 	 32.883965
Train loss       : 26.696074
Best valid loss  : 31.853485
Best test loss   : 32.733070
Pruning          : 0.10
0.001
0.001
[Current model size]
================================
Total params      : 2,666,978
--------------------------------
Total memory      : 28.39 MB
Total Flops       : 288.81 MFlops
Total Mem (Read)  : 25.75 MB
Total Mem (Write) : 23.83 MB
[Supermasks testing]
[Untrained loss : 81.7695]
[Starting training]
Epoch 0 	 79.083908 	 59.991463 	 59.850960
Epoch 10 	 47.737946 	 44.122150 	 44.273739
Epoch 20 	 43.583237 	 41.250267 	 41.521248
Epoch 30 	 49.056606 	 45.409969 	 45.905632
Epoch 40 	 50.112991 	 42.718227 	 43.613472
Epoch 50 	 41.837364 	 42.019127 	 42.470291
[Model stopped early]
Train loss       : 41.980350
Best valid loss  : 41.250267
Best test loss   : 41.521248
Pruning          : 0.07
0.001
0.001
[Current model size]
================================
Total params      : 2,614,223
--------------------------------
Total memory      : 27.77 MB
Total Flops       : 255.26 MFlops
Total Mem (Read)  : 24.93 MB
Total Mem (Write) : 23.22 MB
[Supermasks testing]
[Untrained loss : 85.1802]
[Starting training]
Epoch 0 	 75.469841 	 108.722694 	 70.283142
Epoch 10 	 52.533123 	 48.488846 	 48.338135
Epoch 20 	 50.526867 	 47.805668 	 47.762005
Epoch 30 	 47.884838 	 44.642635 	 45.310650
Epoch 40 	 46.433628 	 43.671104 	 44.402946
Epoch 50 	 45.025547 	 42.963882 	 43.403191
Epoch 60 	 42.989971 	 43.415009 	 43.948711
Epoch 70 	 41.949375 	 41.017544 	 41.467720
Epoch 80 	 39.657276 	 39.669613 	 40.661312
Epoch 90 	 38.245777 	 38.669785 	 39.428764
Epoch 100 	 36.925072 	 38.068516 	 38.817585
Epoch 110 	 47.246239 	 43.602070 	 44.210247
Epoch 120 	 40.867245 	 39.283283 	 40.532158
Epoch 130 	 39.057594 	 39.199970 	 39.916687
[Model stopped early]
Train loss       : 38.048557
Best valid loss  : 36.784134
Best test loss   : 37.805817
Pruning          : 0.05
0.001
0.001
[Current model size]
================================
Total params      : 2,574,749
--------------------------------
Total memory      : 25.16 MB
Total Flops       : 161.26 MFlops
Total Mem (Read)  : 22.19 MB
Total Mem (Write) : 20.63 MB
[Supermasks testing]
[Untrained loss : 80.3364]
[Starting training]
Epoch 0 	 74.115959 	 132.679718 	 107.105110
Epoch 10 	 56.596584 	 51.800480 	 51.921635
Epoch 20 	 53.918018 	 49.658970 	 49.928810
Epoch 30 	 51.487480 	 48.277027 	 48.380634
Epoch 40 	 49.321308 	 46.561180 	 46.730141
Epoch 50 	 47.445599 	 47.324169 	 47.593002
Epoch 60 	 45.932217 	 46.236076 	 47.005619
Epoch 70 	 44.389999 	 44.329105 	 44.558239
Epoch 80 	 43.028774 	 42.655708 	 43.115479
Epoch 90 	 42.056862 	 41.981533 	 42.875397
Epoch 100 	 40.890491 	 43.071274 	 43.520149
Epoch 110 	 40.935009 	 42.250874 	 42.920135
Epoch 120 	 38.609810 	 40.786434 	 40.965900
Epoch 130 	 37.104225 	 40.567104 	 40.850967
Epoch 140 	 36.455189 	 40.554863 	 41.102489
Epoch 150 	 35.552963 	 39.345711 	 40.335869
Epoch 160 	 34.922565 	 39.826248 	 40.272198
Epoch 170 	 34.781284 	 39.258888 	 40.261700
Epoch 180 	 33.983311 	 38.972027 	 39.775295
Epoch 190 	 33.572281 	 38.313370 	 39.682846
Train loss       : 33.089851
Best valid loss  : 37.870819
Best test loss   : 39.292965
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 2,550,944
--------------------------------
Total memory      : 24.01 MB
Total Flops       : 131.59 MFlops
Total Mem (Read)  : 20.96 MB
Total Mem (Write) : 19.48 MB
[Supermasks testing]
[Untrained loss : 74.5842]
[Starting training]
Epoch 0 	 74.834724 	 61.019173 	 62.695427
Epoch 10 	 52.852875 	 50.398949 	 49.485653
Epoch 20 	 48.821136 	 48.583103 	 48.054001
Epoch 30 	 49.508041 	 47.445705 	 47.122292
Epoch 40 	 45.401867 	 44.504337 	 44.153046
Epoch 50 	 44.437168 	 45.064583 	 45.054996
Epoch 60 	 43.293640 	 43.227184 	 43.321396
Epoch 70 	 43.468342 	 43.718170 	 44.036835
Epoch 80 	 43.292465 	 43.195126 	 43.616009
Epoch 90 	 41.435692 	 41.813282 	 42.178307
Epoch 100 	 41.483841 	 42.060539 	 42.287930
Epoch 110 	 42.197990 	 42.334286 	 42.682087
Epoch 120 	 41.026566 	 41.907349 	 42.300705
Epoch 130 	 40.763634 	 41.532455 	 41.951527
Epoch 140 	 40.980423 	 41.743526 	 41.898472
Epoch 150 	 40.668850 	 41.531654 	 41.872395
Epoch 160 	 40.289062 	 41.713367 	 41.728779
Epoch 170 	 40.210636 	 41.618717 	 41.532692
Epoch 180 	 39.996017 	 41.370399 	 41.480927
Epoch 190 	 39.952393 	 41.277149 	 41.500164
Train loss       : 39.924469
Best valid loss  : 40.972420
Best test loss   : 41.503616
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 2,531,735
--------------------------------
Total memory      : 23.24 MB
Total Flops       : 107.29 MFlops
Total Mem (Read)  : 20.11 MB
Total Mem (Write) : 18.71 MB
[Supermasks testing]
[Untrained loss : 85.3708]
[Starting training]
Epoch 0 	 78.413483 	 71.070717 	 73.304497
Epoch 10 	 54.533733 	 54.095188 	 49.020195
Epoch 20 	 51.183586 	 48.193592 	 48.221352
Epoch 30 	 49.765591 	 49.115925 	 48.743454
Epoch 40 	 47.460678 	 46.983421 	 47.385109
Epoch 50 	 46.130100 	 44.807709 	 44.742661
Epoch 60 	 45.383293 	 44.865482 	 45.123444
Epoch 70 	 48.669098 	 46.526688 	 46.847157
Epoch 80 	 45.188976 	 44.404507 	 44.638802
Epoch 90 	 43.715656 	 42.932655 	 43.771271
Epoch 100 	 42.749413 	 42.518913 	 43.494633
Epoch 110 	 43.275425 	 48.539696 	 49.584129
Epoch 120 	 50.261208 	 47.709545 	 47.808598
Epoch 130 	 42.005287 	 41.966366 	 42.747005
Epoch 140 	 41.550449 	 41.613476 	 43.100494
Epoch 150 	 41.513668 	 40.989071 	 42.140293
Epoch 160 	 40.838299 	 42.206699 	 43.530315
Epoch 170 	 40.280266 	 40.525383 	 42.291050
Epoch 180 	 39.953838 	 41.149048 	 42.656334
Epoch 190 	 39.720325 	 40.725807 	 41.894070
Train loss       : 39.822388
Best valid loss  : 40.209179
Best test loss   : 41.779556
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 2,525,346
--------------------------------
Total memory      : 21.63 MB
Total Flops       : 81.46 MFlops
Total Mem (Read)  : 18.48 MB
Total Mem (Write) : 17.1 MB
[Supermasks testing]
[Untrained loss : 90.0954]
[Starting training]
Epoch 0 	 77.079201 	 64.710930 	 67.149292
Epoch 10 	 50.810802 	 46.553673 	 46.673256
Epoch 20 	 47.229294 	 45.770542 	 45.606823
Epoch 30 	 45.620441 	 43.687485 	 43.931129
Epoch 40 	 45.263634 	 43.634872 	 43.592655
Epoch 50 	 44.512749 	 44.168259 	 44.417385
Epoch 60 	 44.196583 	 42.578888 	 43.347912
Epoch 70 	 41.629162 	 41.166122 	 42.517712
Epoch 80 	 41.328671 	 39.996464 	 41.630993
Epoch 90 	 42.981823 	 42.126099 	 42.946922
Epoch 100 	 40.692364 	 40.595924 	 41.552277
Epoch 110 	 40.203979 	 40.181305 	 41.307701
Epoch 120 	 39.919491 	 39.963032 	 41.019650
Epoch 130 	 39.871788 	 39.981720 	 41.287678
Epoch 140 	 39.544559 	 39.571705 	 40.941959
Epoch 150 	 39.446362 	 39.748730 	 40.948570
[Model stopped early]
Train loss       : 39.340595
Best valid loss  : 39.251659
Best test loss   : 41.014709
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 2,520,732
--------------------------------
Total memory      : 20.72 MB
Total Flops       : 66.86 MFlops
Total Mem (Read)  : 17.54 MB
Total Mem (Write) : 16.19 MB
[Supermasks testing]
[Untrained loss : 92.5586]
[Starting training]
Epoch 0 	 78.913429 	 69.864845 	 71.835548
Epoch 10 	 53.399849 	 49.176376 	 48.866276
Epoch 20 	 49.043388 	 46.808731 	 46.685993
Epoch 30 	 46.403343 	 44.540932 	 44.471031
Epoch 40 	 48.637012 	 47.373398 	 47.062626
Epoch 50 	 45.832886 	 44.302025 	 44.108192
Epoch 60 	 43.635544 	 42.593338 	 43.183594
Epoch 70 	 43.398460 	 43.410416 	 44.414650
Epoch 80 	 42.065605 	 43.234406 	 43.750977
Epoch 90 	 42.289986 	 42.600327 	 42.964241
Epoch 100 	 40.655300 	 42.370304 	 42.482620
Epoch 110 	 40.197975 	 41.232193 	 41.707462
Epoch 120 	 40.935810 	 41.791740 	 42.387444
Epoch 130 	 39.766499 	 40.921631 	 41.724915
Epoch 140 	 39.469135 	 40.986839 	 41.483189
Epoch 150 	 39.260647 	 40.757912 	 41.306725
Epoch 160 	 39.127308 	 40.755199 	 41.302788
Epoch 170 	 39.048523 	 40.688900 	 41.297234
Epoch 180 	 38.985477 	 40.365067 	 41.366173
Epoch 190 	 39.005962 	 40.535099 	 41.355782
Train loss       : 38.984966
Best valid loss  : 40.308945
Best test loss   : 41.344059
Pruning          : 0.01
