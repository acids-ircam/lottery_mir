Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41281301.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, pillow-simd, six, torchvision, tqdm, kiwisolver, pyparsing, python-dateutil, cycler, matplotlib, wrapt, opt-einsum, protobuf, keras-preprocessing, gast, termcolor, astor, google-pasta, tensorflow-estimator, grpcio, werkzeug, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, urllib3, idna, certifi, chardet, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, markdown, tensorboard, h5py, keras-applications, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281301.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-29 01:59:01.154425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 01:59:01.166045: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is sol-ordinario_ddsp_cnn_xavier_trimming_batchnorm_reinit_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41281301.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 96.3894]
[Starting training]
Epoch 0 	 90.448196 	 85.697701 	 88.320946
Epoch 10 	 78.859840 	 77.893784 	 78.678841
Epoch 20 	 80.390305 	 75.122375 	 80.128265
Epoch 30 	 74.092255 	 72.467392 	 15441.625000
Epoch 40 	 67.968025 	 68.463219 	 95.253807
Epoch 50 	 188.154205 	 67.762123 	 69.102211
Epoch 60 	 67.943504 	 67.398232 	 69.389977
Epoch 70 	 62.222149 	 61.009571 	 61.356533
Epoch 80 	 60.449432 	 56.381367 	 61.314148
Epoch 90 	 57.180393 	 56.335617 	 58.153194
Epoch 100 	 53.790867 	 52.669613 	 56.759857
Epoch 110 	 52.119110 	 51.049469 	 53.884407
Epoch 120 	 53.338364 	 52.220863 	 54.886238
Epoch 130 	 49.912056 	 50.069328 	 54.876465
Epoch 140 	 51.141495 	 50.392616 	 51.307331
Epoch 150 	 49.074413 	 49.363392 	 51.528717
Epoch 160 	 49.812050 	 49.516918 	 51.880054
Epoch 170 	 48.462440 	 50.379307 	 49.310703
Epoch 180 	 48.009018 	 48.082882 	 51.709576
[Model stopped early]
Train loss       : 48.906700
Best valid loss  : 46.487457
Best test loss   : 52.603401
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 3,792,824
--------------------------------
Total memory      : 35.14 MB
Total Flops       : 339.89 MFlops
Total Mem (Read)  : 35.15 MB
Total Mem (Write) : 28.95 MB
[Supermasks testing]
[Untrained loss : 95.4250]
[Starting training]
Epoch 0 	 88.740631 	 81.829605 	 86.351616
Epoch 10 	 64.590149 	 68.788025 	 72.507141
Epoch 20 	 52.558514 	 54.276619 	 57.264198
Epoch 30 	 52.677170 	 47.249100 	 53.796124
Epoch 40 	 45.950924 	 45.596497 	 47.371479
Epoch 50 	 45.038849 	 42.557281 	 46.271618
Epoch 60 	 36.758518 	 38.788326 	 41.844967
Epoch 70 	 34.450634 	 33.812653 	 36.523617
Epoch 80 	 32.830429 	 33.446941 	 36.444397
Epoch 90 	 31.074726 	 32.788090 	 34.545242
Epoch 100 	 29.433577 	 31.483929 	 33.919655
Epoch 110 	 28.772686 	 32.460114 	 33.702938
Epoch 120 	 26.411884 	 29.486258 	 31.538580
Epoch 130 	 26.317320 	 28.965536 	 31.236906
Epoch 140 	 24.512457 	 28.009439 	 29.954679
Epoch 150 	 24.966297 	 29.042162 	 31.074076
Epoch 160 	 23.273125 	 27.322227 	 29.200882
Epoch 170 	 22.659050 	 27.613949 	 29.533792
Epoch 180 	 22.528864 	 27.756447 	 29.145491
Epoch 190 	 22.002525 	 26.847538 	 28.924961
Train loss       : 21.759878
Best valid loss  : 26.491314
Best test loss   : 28.845427
Pruning          : 0.72
0.001
0.001
[Current model size]
================================
Total params      : 3,316,816
--------------------------------
Total memory      : 29.84 MB
Total Flops       : 192.02 MFlops
Total Mem (Read)  : 28.52 MB
Total Mem (Write) : 24.13 MB
[Supermasks testing]
[Untrained loss : 99.9919]
[Starting training]
Epoch 0 	 90.364891 	 84.175095 	 87.554298
Epoch 10 	 59.351883 	 56.575481 	 60.255917
Epoch 20 	 45.631519 	 43.464237 	 45.971603
Epoch 30 	 42.036526 	 41.503960 	 43.254440
Epoch 40 	 38.173820 	 36.862465 	 38.591042
Epoch 50 	 36.326729 	 35.886604 	 37.548794
Epoch 60 	 35.396240 	 35.236584 	 37.595337
Epoch 70 	 32.231640 	 32.826443 	 35.283443
Epoch 80 	 31.034203 	 33.055416 	 35.670605
Epoch 90 	 28.091972 	 30.401342 	 32.515560
Epoch 100 	 27.569895 	 29.704561 	 32.294960
Epoch 110 	 26.707546 	 28.873077 	 31.744995
Epoch 120 	 25.812067 	 29.189837 	 31.578476
Epoch 130 	 25.645508 	 28.233799 	 30.982000
Epoch 140 	 25.364172 	 28.778400 	 31.006170
Epoch 150 	 24.786942 	 28.053122 	 30.540060
Epoch 160 	 24.603312 	 27.889214 	 30.388721
Epoch 170 	 24.428339 	 28.067009 	 30.596180
Epoch 180 	 24.151876 	 27.829517 	 30.363266
Epoch 190 	 23.853142 	 27.512285 	 30.307499
Train loss       : 23.819368
Best valid loss  : 27.173281
Best test loss   : 30.598225
Pruning          : 0.52
0.001
0.001
[Current model size]
================================
Total params      : 3,028,378
--------------------------------
Total memory      : 26.00 MB
Total Flops       : 113.88 MFlops
Total Mem (Read)  : 23.92 MB
Total Mem (Write) : 20.63 MB
[Supermasks testing]
[Untrained loss : 102.1474]
[Starting training]
Epoch 0 	 91.776886 	 85.571823 	 88.790321
Epoch 10 	 62.087181 	 59.363968 	 62.114285
Epoch 20 	 53.916092 	 48.887691 	 52.143353
Epoch 30 	 45.383591 	 41.435604 	 43.958870
Epoch 40 	 38.269897 	 37.944336 	 41.534496
Epoch 50 	 36.081417 	 35.263836 	 38.186623
Epoch 60 	 33.200321 	 34.545303 	 37.144577
Epoch 70 	 32.140751 	 35.303448 	 36.999287
Epoch 80 	 32.312550 	 33.343773 	 35.639198
Epoch 90 	 27.736288 	 30.610106 	 32.499489
Epoch 100 	 27.579124 	 30.012005 	 32.574200
Epoch 110 	 26.232174 	 28.864323 	 32.262325
Epoch 120 	 25.919674 	 29.307333 	 31.498762
Epoch 130 	 24.600075 	 28.328678 	 30.847462
Epoch 140 	 24.243563 	 29.240406 	 31.033960
Epoch 150 	 23.595392 	 28.642847 	 30.320021
Epoch 160 	 23.267427 	 28.365482 	 30.303440
[Model stopped early]
Train loss       : 23.250645
Best valid loss  : 28.064795
Best test loss   : 30.502567
Pruning          : 0.37
0.001
0.001
[Current model size]
================================
Total params      : 2,844,854
--------------------------------
Total memory      : 23.18 MB
Total Flops       : 72.48 MFlops
Total Mem (Read)  : 20.64 MB
Total Mem (Write) : 18.05 MB
[Supermasks testing]
[Untrained loss : 104.3721]
[Starting training]
Epoch 0 	 93.455933 	 86.853500 	 90.980782
Epoch 10 	 61.436829 	 59.994823 	 65.680939
Epoch 20 	 53.812584 	 51.575356 	 56.207733
Epoch 30 	 50.132416 	 48.439545 	 53.099133
Epoch 40 	 42.940731 	 44.872475 	 50.024323
Epoch 50 	 38.523064 	 39.160770 	 44.060360
Epoch 60 	 35.099514 	 37.900272 	 42.515621
Epoch 70 	 31.899918 	 37.441750 	 41.239891
Epoch 80 	 30.665257 	 35.207588 	 40.652966
Epoch 90 	 34.258492 	 39.423450 	 44.510189
Epoch 100 	 28.334904 	 34.870766 	 39.065182
Epoch 110 	 27.361639 	 36.173370 	 39.812984
Epoch 120 	 26.560225 	 33.125084 	 38.051666
Epoch 130 	 26.022808 	 35.094696 	 37.038666
Epoch 140 	 25.257597 	 34.007980 	 37.094845
Epoch 150 	 24.970331 	 33.855865 	 36.849560
Epoch 160 	 24.723581 	 33.652813 	 36.723179
Epoch 170 	 24.583738 	 33.792118 	 36.507839
[Model stopped early]
Train loss       : 24.230574
Best valid loss  : 32.534958
Best test loss   : 36.964737
Pruning          : 0.27
0.001
0.001
[Current model size]
================================
Total params      : 2,726,701
--------------------------------
Total memory      : 21.15 MB
Total Flops       : 51.3 MFlops
Total Mem (Read)  : 18.35 MB
Total Mem (Write) : 16.21 MB
[Supermasks testing]
[Untrained loss : 94.6943]
[Starting training]
Epoch 0 	 89.981087 	 85.615601 	 89.012001
Epoch 10 	 62.275616 	 58.352161 	 63.025379
Epoch 20 	 54.729443 	 54.702007 	 58.370117
Epoch 30 	 45.501518 	 44.579487 	 50.119835
Epoch 40 	 46.708969 	 46.911549 	 51.246967
Epoch 50 	 37.457306 	 39.332924 	 44.834423
Epoch 60 	 34.767159 	 38.925529 	 42.765530
Epoch 70 	 33.158054 	 37.408020 	 42.438351
Epoch 80 	 31.998007 	 37.551739 	 41.434654
Epoch 90 	 30.727583 	 36.987251 	 40.365223
Epoch 100 	 29.706114 	 36.060783 	 40.729794
Epoch 110 	 28.877037 	 35.712601 	 40.191082
Epoch 120 	 27.899141 	 35.379627 	 39.495804
Epoch 130 	 26.126301 	 34.571014 	 38.987354
Epoch 140 	 25.948164 	 33.761902 	 37.831539
Epoch 150 	 25.788685 	 33.795197 	 37.849957
Epoch 160 	 24.308569 	 34.826080 	 37.978001
Epoch 170 	 24.429550 	 34.096474 	 37.386501
Epoch 180 	 24.092304 	 34.248573 	 36.982811
Epoch 190 	 24.065462 	 33.425247 	 37.359688
Train loss       : 23.919378
Best valid loss  : 33.370247
Best test loss   : 37.395184
Pruning          : 0.19
0.001
0.001
[Current model size]
================================
Total params      : 2,649,351
--------------------------------
Total memory      : 19.73 MB
Total Flops       : 40.64 MFlops
Total Mem (Read)  : 16.76 MB
Total Mem (Write) : 14.91 MB
[Supermasks testing]
[Untrained loss : 97.3006]
[Starting training]
Epoch 0 	 91.795235 	 85.720848 	 90.408127
Epoch 10 	 61.730812 	 64.423271 	 68.456474
Epoch 20 	 53.657017 	 50.143375 	 56.749981
Epoch 30 	 47.838470 	 48.707043 	 52.579601
Epoch 40 	 42.884373 	 44.726357 	 48.314518
Epoch 50 	 42.696526 	 46.712929 	 50.238586
Epoch 60 	 39.739414 	 42.602997 	 46.594879
Epoch 70 	 35.463940 	 37.874214 	 42.723980
Epoch 80 	 34.139198 	 37.343307 	 41.226711
Epoch 90 	 33.152504 	 38.146233 	 41.666286
Epoch 100 	 32.126350 	 36.068710 	 40.569733
Epoch 110 	 30.810843 	 35.549431 	 40.513298
Epoch 120 	 29.294746 	 37.331779 	 40.500465
Epoch 130 	 28.346951 	 34.609009 	 39.691681
Epoch 140 	 27.531065 	 35.002720 	 38.468079
Epoch 150 	 26.609081 	 35.194801 	 38.899960
Epoch 160 	 28.825741 	 36.208603 	 39.764584
Epoch 170 	 25.204075 	 34.313007 	 37.601696
Epoch 180 	 24.275024 	 34.496227 	 37.730465
Epoch 190 	 24.177109 	 33.243797 	 37.165497
Train loss       : 23.180506
Best valid loss  : 32.793625
Best test loss   : 37.658863
Pruning          : 0.14
0.001
0.001
[Current model size]
================================
Total params      : 2,597,821
--------------------------------
Total memory      : 18.71 MB
Total Flops       : 35.13 MFlops
Total Mem (Read)  : 15.63 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 97.0642]
[Starting training]
Epoch 0 	 92.613312 	 87.689980 	 91.973534
Epoch 10 	 63.839817 	 60.390518 	 66.146324
Epoch 20 	 58.500790 	 53.680069 	 60.290283
Epoch 30 	 51.254066 	 50.475838 	 55.638245
Epoch 40 	 49.190296 	 52.198612 	 55.817486
Epoch 50 	 45.447147 	 49.761650 	 52.996864
Epoch 60 	 43.843048 	 49.021759 	 51.920410
Epoch 70 	 47.695126 	 47.360504 	 52.263302
Epoch 80 	 37.140457 	 46.810211 	 53.655731
Epoch 90 	 36.295250 	 44.698383 	 51.229626
Epoch 100 	 34.016857 	 45.425751 	 50.170780
Epoch 110 	 33.274097 	 44.671566 	 51.195095
Epoch 120 	 30.719189 	 44.288101 	 49.522499
Epoch 130 	 30.077375 	 44.680660 	 49.767147
[Model stopped early]
Train loss       : 29.549917
Best valid loss  : 42.921646
Best test loss   : 50.591885
Pruning          : 0.10
0.001
0.001
[Current model size]
================================
Total params      : 2,563,373
--------------------------------
Total memory      : 17.92 MB
Total Flops       : 31.98 MFlops
Total Mem (Read)  : 14.78 MB
Total Mem (Write) : 13.26 MB
[Supermasks testing]
[Untrained loss : 99.4365]
[Starting training]
Epoch 0 	 93.407539 	 90.464134 	 93.058914
Epoch 10 	 67.210236 	 66.433395 	 70.550446
Epoch 20 	 61.969009 	 59.891914 	 63.849495
Epoch 30 	 60.086880 	 60.107002 	 63.447945
Epoch 40 	 55.650074 	 54.258793 	 57.632748
Epoch 50 	 49.512653 	 52.100685 	 57.659908
Epoch 60 	 44.973881 	 49.187637 	 54.428883
Epoch 70 	 41.045155 	 45.550686 	 46.964214
Epoch 80 	 37.227146 	 41.458141 	 43.437744
Epoch 90 	 35.897720 	 41.076500 	 43.871174
Epoch 100 	 35.215652 	 41.063301 	 43.938465
Epoch 110 	 32.606754 	 41.153786 	 43.079327
Epoch 120 	 32.655121 	 40.884754 	 43.828880
Epoch 130 	 32.256683 	 39.882965 	 42.952782
Epoch 140 	 31.920492 	 40.351273 	 43.675903
Epoch 150 	 31.762119 	 40.262657 	 42.516705
[Model stopped early]
Train loss       : 32.510086
Best valid loss  : 39.270477
Best test loss   : 42.575130
Pruning          : 0.07
0.001
0.001
[Current model size]
================================
Total params      : 2,540,077
--------------------------------
Total memory      : 17.49 MB
Total Flops       : 30.82 MFlops
Total Mem (Read)  : 14.3 MB
Total Mem (Write) : 12.87 MB
[Supermasks testing]
[Untrained loss : 99.5188]
[Starting training]
Epoch 0 	 93.291138 	 88.195847 	 92.181694
Epoch 10 	 68.123596 	 67.721840 	 71.069633
Epoch 20 	 65.173248 	 63.893112 	 69.334312
Epoch 30 	 63.035526 	 62.373119 	 64.794220
Epoch 40 	 61.154854 	 60.195641 	 63.843597
Epoch 50 	 56.984329 	 57.857140 	 61.847698
Epoch 60 	 54.230595 	 54.642735 	 61.964893
Epoch 70 	 50.334141 	 54.677593 	 58.992725
Epoch 80 	 46.043133 	 54.306351 	 59.855824
Epoch 90 	 41.641941 	 52.539227 	 57.313457
Epoch 100 	 39.154686 	 52.391209 	 58.869770
Epoch 110 	 38.243259 	 52.803059 	 57.846130
Epoch 120 	 36.080170 	 51.004730 	 57.928143
Epoch 130 	 36.758179 	 52.453796 	 56.417023
Epoch 140 	 35.760517 	 52.223289 	 57.156769
Epoch 150 	 35.610023 	 51.960487 	 56.234753
[Model stopped early]
Train loss       : 35.528694
Best valid loss  : 49.662884
Best test loss   : 58.295406
Pruning          : 0.05
0.001
0.001
[Current model size]
================================
Total params      : 2,523,994
--------------------------------
Total memory      : 17.09 MB
Total Flops       : 29.95 MFlops
Total Mem (Read)  : 13.88 MB
Total Mem (Write) : 12.51 MB
[Supermasks testing]
[Untrained loss : 99.4766]
[Starting training]
Epoch 0 	 93.394684 	 89.144157 	 92.711334
Epoch 10 	 69.177658 	 65.922134 	 69.390526
Epoch 20 	 64.519081 	 61.743633 	 66.182335
Epoch 30 	 59.709946 	 61.808266 	 65.308617
Epoch 40 	 56.332085 	 55.547848 	 60.214558
Epoch 50 	 52.331608 	 56.453762 	 59.175446
Epoch 60 	 48.942272 	 56.941032 	 59.206715
Epoch 70 	 43.718174 	 54.405151 	 57.305904
Epoch 80 	 41.774311 	 53.565903 	 56.381153
Epoch 90 	 37.779835 	 53.792618 	 55.644775
Epoch 100 	 35.987862 	 52.289192 	 55.269070
Epoch 110 	 35.780205 	 51.671959 	 55.262920
Epoch 120 	 33.987545 	 52.824955 	 55.317211
Epoch 130 	 33.538322 	 52.595619 	 55.297314
Epoch 140 	 33.545319 	 51.782539 	 55.417904
Epoch 150 	 33.191532 	 54.518188 	 55.636169
Epoch 160 	 32.833702 	 52.983643 	 54.812054
Epoch 170 	 33.474636 	 52.217445 	 54.814434
[Model stopped early]
Train loss       : 33.021412
Best valid loss  : 50.691925
Best test loss   : 55.221535
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 2,511,046
--------------------------------
Total memory      : 17.01 MB
Total Flops       : 29.94 MFlops
Total Mem (Read)  : 13.77 MB
Total Mem (Write) : 12.45 MB
[Supermasks testing]
[Untrained loss : 96.1045]
[Starting training]
Epoch 0 	 92.065163 	 89.580009 	 92.588676
Epoch 10 	 70.304840 	 70.378174 	 75.243057
Epoch 20 	 66.439522 	 65.758682 	 70.783714
Epoch 30 	 62.137520 	 63.515083 	 67.748177
Epoch 40 	 59.381275 	 59.687775 	 65.150887
Epoch 50 	 54.289646 	 59.814949 	 64.583366
Epoch 60 	 49.948612 	 56.119411 	 62.920918
Epoch 70 	 45.911091 	 54.434959 	 60.388325
Epoch 80 	 43.286945 	 52.145016 	 58.023670
Epoch 90 	 42.442043 	 52.555317 	 59.057049
Epoch 100 	 40.052383 	 52.413815 	 58.267506
Epoch 110 	 37.967972 	 52.878418 	 58.797024
Epoch 120 	 36.971813 	 53.443287 	 59.775063
Epoch 130 	 36.168270 	 52.826618 	 58.402462
Epoch 140 	 36.343784 	 51.329884 	 58.377655
[Model stopped early]
Train loss       : 35.937740
Best valid loss  : 50.673515
Best test loss   : 58.245583
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 2,502,534
--------------------------------
Total memory      : 16.95 MB
Total Flops       : 29.94 MFlops
Total Mem (Read)  : 13.71 MB
Total Mem (Write) : 12.42 MB
[Supermasks testing]
[Untrained loss : 292.6057]
[Starting training]
Epoch 0 	 92.889450 	 89.932793 	 92.994530
Epoch 10 	 67.430038 	 64.490891 	 68.950760
Epoch 20 	 64.102661 	 61.990547 	 66.468605
Epoch 30 	 62.439270 	 59.753155 	 64.821114
Epoch 40 	 60.032703 	 57.602043 	 62.595947
Epoch 50 	 57.639828 	 57.384838 	 61.070465
Epoch 60 	 55.705627 	 57.448044 	 59.617073
Epoch 70 	 53.810928 	 56.945961 	 61.507690
Epoch 80 	 50.242897 	 54.754204 	 59.195404
Epoch 90 	 46.841839 	 58.011185 	 61.485943
Epoch 100 	 44.646088 	 53.543468 	 58.733669
Epoch 110 	 43.327255 	 54.447304 	 59.030640
Epoch 120 	 41.284756 	 53.937984 	 59.711678
Epoch 130 	 40.392921 	 53.232586 	 58.742901
Epoch 140 	 39.627052 	 53.702770 	 58.374447
Epoch 150 	 38.887054 	 52.508289 	 58.396149
Epoch 160 	 38.638710 	 53.153812 	 58.433796
[Model stopped early]
Train loss       : 38.352348
Best valid loss  : 51.926853
Best test loss   : 58.599609
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 2,496,213
--------------------------------
Total memory      : 16.91 MB
Total Flops       : 29.93 MFlops
Total Mem (Read)  : 13.65 MB
Total Mem (Write) : 12.39 MB
[Supermasks testing]
[Untrained loss : 96.7642]
[Starting training]
Epoch 0 	 93.145874 	 91.071518 	 93.814133
Epoch 10 	 70.464699 	 67.269455 	 70.696510
Epoch 20 	 66.325111 	 65.104050 	 69.111534
Epoch 30 	 65.025215 	 64.994339 	 68.128906
Epoch 40 	 62.926739 	 61.017509 	 65.387657
Epoch 50 	 60.640282 	 60.728291 	 64.418968
Epoch 60 	 59.565903 	 58.669346 	 63.371956
Epoch 70 	 58.000950 	 57.941097 	 62.615101
Epoch 80 	 56.654896 	 60.019123 	 63.679836
Epoch 90 	 55.188766 	 57.656006 	 61.086208
Epoch 100 	 53.863823 	 56.891140 	 61.375423
Epoch 110 	 53.325188 	 55.550152 	 60.384693
Epoch 120 	 52.822937 	 55.350697 	 59.819843
Epoch 130 	 51.387062 	 55.279850 	 59.315197
Epoch 140 	 51.589924 	 55.143532 	 59.762371
Epoch 150 	 50.824707 	 55.144527 	 58.988857
[Model stopped early]
Train loss       : 51.214836
Best valid loss  : 54.228527
Best test loss   : 60.294239
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 2,492,029
--------------------------------
Total memory      : 16.89 MB
Total Flops       : 29.93 MFlops
Total Mem (Read)  : 13.62 MB
Total Mem (Write) : 12.37 MB
[Supermasks testing]
[Untrained loss : nan]
[Starting training]
Epoch 0 	 92.063179 	 90.505928 	 92.573257
Epoch 10 	 71.496269 	 72.996223 	 76.210983
Epoch 20 	 70.168724 	 67.356544 	 70.400909
Epoch 30 	 67.488388 	 65.922195 	 69.036095
Epoch 40 	 67.364319 	 64.916855 	 68.210114
Epoch 50 	 65.498665 	 63.716904 	 67.754189
Epoch 60 	 63.169918 	 61.980511 	 66.073151
Epoch 70 	 61.691589 	 61.746651 	 65.717216
Epoch 80 	 60.439144 	 59.267780 	 65.632103
Epoch 90 	 59.733780 	 60.570686 	 65.319107
Epoch 100 	 59.726654 	 60.398258 	 65.221695
Epoch 110 	 59.178436 	 60.699814 	 65.729942
[Model stopped early]
Train loss       : 59.796146
Best valid loss  : 59.267780
Best test loss   : 65.632103
Pruning          : 0.01
