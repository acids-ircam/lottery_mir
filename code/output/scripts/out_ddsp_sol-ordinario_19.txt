Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41281324.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, pillow-simd, future, torch, torchvision, tqdm, kiwisolver, python-dateutil, pyparsing, cycler, matplotlib, protobuf, termcolor, opt-einsum, absl-py, astor, h5py, keras-applications, wrapt, gast, keras-preprocessing, tensorflow-estimator, markdown, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, werkzeug, idna, urllib3, certifi, chardet, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, tensorboard, google-pasta, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281324.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-29 02:25:47.144919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 02:25:47.474774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is sol-ordinario_ddsp_cnn_xavier_trimming_batchnorm_rewind_global_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41281324.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 95.3163]
[Starting training]
Epoch 0 	 90.700623 	 85.141235 	 89.587334
Epoch 10 	 80.665962 	 79.698441 	 82.134735
Epoch 20 	 75.914185 	 78.440643 	 78.343468
Epoch 30 	 72.409142 	 69.867523 	 73.078789
Epoch 40 	 68.466988 	 66.945702 	 72.881256
Epoch 50 	 63.187065 	 66.073410 	 69.425499
Epoch 60 	 56.166550 	 55.005871 	 54.875893
Epoch 70 	 49.445477 	 49.664299 	 49.535522
Epoch 80 	 44.274982 	 42.100109 	 45.799969
Epoch 90 	 41.820988 	 40.282909 	 44.269566
Epoch 100 	 39.246738 	 37.886021 	 41.336357
Epoch 110 	 36.005161 	 36.048771 	 37.649086
Epoch 120 	 32.672157 	 35.380062 	 36.440357
Epoch 130 	 31.974751 	 33.289871 	 35.552624
Epoch 140 	 29.952368 	 31.684874 	 33.850502
Epoch 150 	 27.965933 	 30.713818 	 34.591713
Epoch 160 	 28.022768 	 29.262218 	 32.540554
Epoch 170 	 26.950583 	 30.370930 	 32.098484
Epoch 180 	 25.955952 	 30.032833 	 31.793409
Epoch 190 	 25.356697 	 29.208385 	 31.296976
Train loss       : 25.389839
Best valid loss  : 28.743450
Best test loss   : 30.758535
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 3,994,522
--------------------------------
Total memory      : 40.26 MB
Total Flops       : 622.57 MFlops
Total Mem (Read)  : 41.11 MB
Total Mem (Write) : 34.15 MB
[Supermasks testing]
[Untrained loss : 45.8869]
[Starting training]
Epoch 0 	 39.650208 	 41.739838 	 42.689312
Epoch 10 	 34.305786 	 34.881027 	 37.872753
Epoch 20 	 33.304581 	 33.696690 	 36.435722
Epoch 30 	 31.745348 	 35.462601 	 38.547039
Epoch 40 	 30.242533 	 31.601959 	 34.383785
Epoch 50 	 29.190165 	 30.654537 	 32.772549
Epoch 60 	 27.675861 	 29.680223 	 31.843374
Epoch 70 	 26.223017 	 29.035934 	 31.566656
Epoch 80 	 25.779694 	 29.019653 	 30.658802
Epoch 90 	 24.957109 	 27.635799 	 30.053354
Epoch 100 	 24.412626 	 27.726479 	 29.777613
Epoch 110 	 24.009335 	 27.266111 	 29.722273
Epoch 120 	 23.753128 	 27.561661 	 29.379593
Epoch 130 	 23.344040 	 27.561611 	 29.273726
Epoch 140 	 23.295732 	 26.864309 	 29.321035
[Model stopped early]
Train loss       : 23.356340
Best valid loss  : 26.810276
Best test loss   : 29.574036
Pruning          : 0.72
0.001
0.001
[Current model size]
================================
Total params      : 3,607,936
--------------------------------
Total memory      : 38.57 MB
Total Flops       : 618.76 MFlops
Total Mem (Read)  : 38.49 MB
Total Mem (Write) : 33.0 MB
[Supermasks testing]
[Untrained loss : 53.2778]
[Starting training]
Epoch 0 	 39.330330 	 37.735817 	 39.088432
Epoch 10 	 34.554493 	 34.492935 	 37.871883
Epoch 20 	 32.517735 	 34.216019 	 36.156227
Epoch 30 	 31.837595 	 32.781414 	 35.213940
Epoch 40 	 30.341965 	 32.936073 	 34.771587
Epoch 50 	 29.025053 	 30.824776 	 32.362537
Epoch 60 	 28.595963 	 30.191673 	 32.395164
Epoch 70 	 25.671082 	 28.594231 	 30.655104
Epoch 80 	 25.428661 	 28.566769 	 30.900373
Epoch 90 	 24.835196 	 28.044270 	 30.292999
Epoch 100 	 24.553932 	 28.060467 	 30.660669
Epoch 110 	 23.229563 	 27.095436 	 28.909927
Epoch 120 	 22.976776 	 27.263866 	 28.986139
Epoch 130 	 22.681334 	 26.796013 	 28.987030
Epoch 140 	 22.342424 	 26.555136 	 28.331863
Epoch 150 	 22.138985 	 26.737236 	 28.232748
Epoch 160 	 22.229538 	 26.725121 	 28.318033
Epoch 170 	 21.826311 	 26.393631 	 28.205145
Epoch 180 	 21.865705 	 26.469635 	 28.272308
Epoch 190 	 21.728111 	 26.630817 	 28.214300
Train loss       : 21.810934
Best valid loss  : 26.307022
Best test loss   : 28.274876
Pruning          : 0.52
0.001
0.001
[Current model size]
================================
Total params      : 3,279,514
--------------------------------
Total memory      : 36.11 MB
Total Flops       : 508.15 MFlops
Total Mem (Read)  : 35.14 MB
Total Mem (Write) : 30.9 MB
[Supermasks testing]
[Untrained loss : 66.1727]
[Starting training]
Epoch 0 	 42.640778 	 39.302731 	 42.202763
Epoch 10 	 34.205662 	 36.021152 	 39.898457
Epoch 20 	 31.909975 	 33.967854 	 35.425110
Epoch 30 	 31.662714 	 31.847809 	 35.141441
Epoch 40 	 30.325481 	 34.180084 	 36.003681
Epoch 50 	 29.221170 	 31.255991 	 32.761143
Epoch 60 	 29.041502 	 28.934549 	 31.739866
Epoch 70 	 27.402330 	 30.165615 	 32.131588
Epoch 80 	 24.787281 	 27.628666 	 30.235426
Epoch 90 	 24.206181 	 28.144098 	 29.952589
Epoch 100 	 23.749701 	 28.308516 	 29.967213
Epoch 110 	 23.429310 	 27.514299 	 29.323301
Epoch 120 	 23.370367 	 27.383228 	 29.398642
Epoch 130 	 22.650806 	 27.400177 	 28.905210
Epoch 140 	 21.908094 	 26.463848 	 28.309982
Epoch 150 	 21.839451 	 26.404352 	 28.292170
Epoch 160 	 21.307865 	 26.505842 	 28.258497
Epoch 170 	 21.654127 	 26.021725 	 28.289835
Epoch 180 	 21.181017 	 26.247597 	 28.223417
Epoch 190 	 21.171967 	 26.258984 	 28.022781
Train loss       : 20.948788
Best valid loss  : 25.594543
Best test loss   : 28.327677
Pruning          : 0.37
0.001
0.001
[Current model size]
================================
Total params      : 2,972,998
--------------------------------
Total memory      : 32.51 MB
Total Flops       : 320.12 MFlops
Total Mem (Read)  : 30.61 MB
Total Mem (Write) : 27.54 MB
[Supermasks testing]
[Untrained loss : 70.3722]
[Starting training]
Epoch 0 	 44.791298 	 40.150536 	 44.343395
Epoch 10 	 34.842918 	 35.605640 	 37.215656
Epoch 20 	 32.552238 	 33.616692 	 35.493130
Epoch 30 	 31.121502 	 32.228477 	 34.792217
Epoch 40 	 28.874033 	 31.242804 	 33.128975
Epoch 50 	 28.131775 	 30.216183 	 31.902319
Epoch 60 	 27.006887 	 29.448057 	 31.490833
Epoch 70 	 26.253241 	 30.118946 	 31.995453
Epoch 80 	 25.545998 	 28.994604 	 30.917261
Epoch 90 	 24.825163 	 27.634193 	 30.328352
Epoch 100 	 24.973721 	 28.565651 	 30.520578
Epoch 110 	 24.841129 	 28.740376 	 30.405592
Epoch 120 	 24.748203 	 28.830748 	 30.416447
[Model stopped early]
Train loss       : 24.783575
Best valid loss  : 27.634193
Best test loss   : 30.328352
Pruning          : 0.27
0.001
0.001
[Current model size]
================================
Total params      : 2,808,009
--------------------------------
Total memory      : 28.09 MB
Total Flops       : 229.28 MFlops
Total Mem (Read)  : 25.72 MB
Total Mem (Write) : 23.26 MB
[Supermasks testing]
[Untrained loss : 112.2087]
[Starting training]
Epoch 0 	 82.525330 	 72.657646 	 77.069832
Epoch 10 	 38.595158 	 39.231575 	 41.176216
Epoch 20 	 35.356426 	 37.612518 	 41.604229
Epoch 30 	 32.708931 	 33.724567 	 35.864716
Epoch 40 	 31.178242 	 31.439905 	 34.421646
Epoch 50 	 30.741934 	 31.293116 	 34.227531
Epoch 60 	 29.962881 	 33.010273 	 35.995235
Epoch 70 	 28.327492 	 31.739264 	 33.550945
Epoch 80 	 28.039682 	 30.698835 	 32.561989
Epoch 90 	 26.012718 	 29.390427 	 31.939741
Epoch 100 	 25.545694 	 29.407833 	 31.399115
Epoch 110 	 25.217085 	 28.664703 	 30.725895
Epoch 120 	 25.488722 	 29.511496 	 31.373793
Epoch 130 	 24.811527 	 28.438288 	 30.479887
Epoch 140 	 24.228224 	 27.818033 	 30.460646
Epoch 150 	 24.222612 	 28.598389 	 30.362747
Epoch 160 	 23.648920 	 28.389959 	 30.171057
Epoch 170 	 23.331129 	 27.686754 	 29.537485
Epoch 180 	 22.839554 	 27.228252 	 29.424896
Epoch 190 	 22.728434 	 27.043760 	 29.152761
[Model stopped early]
Train loss       : 22.585434
Best valid loss  : 26.930479
Best test loss   : 29.363203
Pruning          : 0.19
0.001
0.001
[Current model size]
================================
Total params      : 2,713,380
--------------------------------
Total memory      : 27.67 MB
Total Flops       : 229.25 MFlops
Total Mem (Read)  : 25.08 MB
Total Mem (Write) : 22.99 MB
[Supermasks testing]
[Untrained loss : 104.8145]
[Starting training]
Epoch 0 	 77.930359 	 63.443409 	 67.427208
Epoch 10 	 37.448151 	 37.946548 	 40.262188
Epoch 20 	 33.753681 	 34.992752 	 37.223064
Epoch 30 	 32.393509 	 33.072426 	 35.319881
Epoch 40 	 31.598749 	 33.436787 	 35.779942
Epoch 50 	 29.870838 	 32.054836 	 33.836514
Epoch 60 	 28.423796 	 30.940208 	 33.227230
Epoch 70 	 26.113117 	 28.770855 	 31.418308
Epoch 80 	 25.945135 	 29.904839 	 31.878052
Epoch 90 	 25.211931 	 28.587183 	 30.882971
Epoch 100 	 24.441990 	 28.825768 	 30.473082
Epoch 110 	 24.117668 	 28.112246 	 30.302664
Epoch 120 	 24.187109 	 28.092575 	 30.123398
Epoch 130 	 23.988180 	 28.346306 	 29.984098
Epoch 140 	 23.831026 	 27.948595 	 29.967691
Epoch 150 	 23.717676 	 27.562653 	 30.021769
Epoch 160 	 23.874290 	 27.848839 	 30.000647
Epoch 170 	 23.876480 	 27.915188 	 29.824432
Epoch 180 	 23.899385 	 27.867273 	 29.954716
Epoch 190 	 23.706696 	 27.911327 	 29.957712
[Model stopped early]
Train loss       : 23.712986
Best valid loss  : 27.317631
Best test loss   : 29.875172
Pruning          : 0.14
0.001
0.001
[Current model size]
================================
Total params      : 2,646,689
--------------------------------
Total memory      : 25.92 MB
Total Flops       : 149.89 MFlops
Total Mem (Read)  : 23.11 MB
Total Mem (Write) : 21.27 MB
[Supermasks testing]
[Untrained loss : 110.7793]
[Starting training]
Epoch 0 	 82.637947 	 69.005814 	 73.975647
Epoch 10 	 44.531021 	 42.566933 	 47.079334
Epoch 20 	 42.391720 	 41.262199 	 44.071117
Epoch 30 	 38.390491 	 39.145615 	 42.250828
Epoch 40 	 37.466316 	 38.201553 	 41.128719
Epoch 50 	 36.132038 	 36.551220 	 39.806961
Epoch 60 	 36.252041 	 35.758675 	 38.607342
Epoch 70 	 34.759937 	 35.960716 	 38.723686
Epoch 80 	 34.001347 	 35.079533 	 37.686825
Epoch 90 	 34.668945 	 35.029659 	 37.040783
Epoch 100 	 31.756659 	 33.648319 	 36.798176
Epoch 110 	 31.435026 	 33.401226 	 36.326717
Epoch 120 	 30.923058 	 32.593979 	 35.952671
Epoch 130 	 30.763292 	 32.002163 	 35.474915
Epoch 140 	 29.724783 	 32.655464 	 36.056221
Epoch 150 	 29.383350 	 31.544598 	 34.946041
Epoch 160 	 29.177277 	 32.738945 	 35.733425
Epoch 170 	 28.993860 	 32.297047 	 35.058071
Epoch 180 	 29.152157 	 31.204000 	 33.872925
Epoch 190 	 28.993137 	 31.699379 	 33.955921
Train loss       : 27.785852
Best valid loss  : 30.250599
Best test loss   : 33.483440
Pruning          : 0.10
0.001
0.001
[Current model size]
================================
Total params      : 2,595,314
--------------------------------
Total memory      : 25.59 MB
Total Flops       : 148.58 MFlops
Total Mem (Read)  : 22.67 MB
Total Mem (Write) : 21.03 MB
[Supermasks testing]
[Untrained loss : 156.3360]
[Starting training]
Epoch 0 	 85.144318 	 77.849731 	 82.238258
Epoch 10 	 44.885689 	 43.701431 	 1062.239990
Epoch 20 	 47.417133 	 43.537102 	 48.076477
Epoch 30 	 38.059101 	 38.980896 	 41.345676
Epoch 40 	 36.905098 	 37.257118 	 39.778252
Epoch 50 	 36.167477 	 37.382423 	 40.744648
Epoch 60 	 34.029324 	 34.974857 	 37.189957
Epoch 70 	 33.406208 	 34.577736 	 37.366135
Epoch 80 	 33.504932 	 34.987286 	 37.744183
Epoch 90 	 32.425018 	 34.660175 	 36.199276
Epoch 100 	 32.504620 	 34.468948 	 35.923340
Epoch 110 	 31.982901 	 33.911270 	 35.770622
Epoch 120 	 31.605518 	 33.789742 	 35.683041
Epoch 130 	 31.217793 	 33.829716 	 35.351055
Epoch 140 	 31.253181 	 33.900089 	 35.205910
Epoch 150 	 30.908812 	 33.396255 	 35.317944
[Model stopped early]
Train loss       : 31.087156
Best valid loss  : 32.691753
Best test loss   : 35.334743
Pruning          : 0.07
0.001
0.001
[Current model size]
================================
Total params      : 2,561,671
--------------------------------
Total memory      : 23.79 MB
Total Flops       : 113.5 MFlops
Total Mem (Read)  : 20.76 MB
Total Mem (Write) : 19.25 MB
[Supermasks testing]
[Untrained loss : 108.3188]
[Starting training]
Epoch 0 	 85.910622 	 80.866203 	 84.284920
Epoch 10 	 45.537914 	 45.192886 	 47.619530
Epoch 20 	 41.444092 	 42.573895 	 44.491528
Epoch 30 	 38.079689 	 38.560093 	 41.719563
Epoch 40 	 37.003899 	 37.461239 	 39.749737
Epoch 50 	 35.897377 	 38.775990 	 42.288822
Epoch 60 	 35.680706 	 35.765411 	 37.803505
Epoch 70 	 33.782688 	 35.677887 	 37.806927
Epoch 80 	 33.490376 	 33.295876 	 36.951900
Epoch 90 	 42.446095 	 36.667145 	 40.241848
Epoch 100 	 33.318401 	 34.933136 	 37.787022
Epoch 110 	 31.805553 	 33.244164 	 36.190163
Epoch 120 	 31.293152 	 33.405617 	 36.243328
Epoch 130 	 31.167192 	 32.841122 	 35.352364
Epoch 140 	 30.760145 	 33.414837 	 35.592682
Epoch 150 	 29.724857 	 31.963341 	 35.058609
Epoch 160 	 29.976046 	 32.283566 	 35.080479
Epoch 170 	 29.869213 	 31.681870 	 34.861629
Epoch 180 	 30.125355 	 31.900185 	 34.825813
Epoch 190 	 29.777657 	 32.151154 	 34.657951
Train loss       : 29.565060
Best valid loss  : 31.120153
Best test loss   : 34.815445
Pruning          : 0.05
0.001
0.001
[Current model size]
================================
Total params      : 2,535,841
--------------------------------
Total memory      : 21.98 MB
Total Flops       : 75.29 MFlops
Total Mem (Read)  : 18.86 MB
Total Mem (Write) : 17.44 MB
[Supermasks testing]
[Untrained loss : 109.0292]
[Starting training]
Epoch 0 	 87.691742 	 81.400795 	 85.330513
Epoch 10 	 49.755741 	 46.709766 	 52.032909
Epoch 20 	 46.274639 	 43.841938 	 46.361279
Epoch 30 	 40.390263 	 41.127857 	 43.781258
Epoch 40 	 45.463409 	 43.189171 	 45.345119
Epoch 50 	 41.359985 	 40.079201 	 41.592876
Epoch 60 	 39.347080 	 39.383583 	 41.171520
Epoch 70 	 39.201908 	 41.060162 	 43.393139
Epoch 80 	 37.287956 	 38.333530 	 40.041088
Epoch 90 	 38.057076 	 37.351883 	 40.068432
Epoch 100 	 36.622822 	 36.713158 	 39.008709
Epoch 110 	 35.931236 	 37.801586 	 39.340031
Epoch 120 	 35.226780 	 36.879356 	 38.226284
Epoch 130 	 34.267555 	 36.232719 	 37.638550
Epoch 140 	 33.586796 	 36.165302 	 37.487896
Epoch 150 	 34.122185 	 35.565430 	 37.456734
Epoch 160 	 32.949268 	 36.074211 	 37.568111
Epoch 170 	 32.869328 	 35.242722 	 37.187851
Epoch 180 	 32.454411 	 34.947578 	 37.362297
Epoch 190 	 32.704834 	 35.028908 	 37.037788
Train loss       : 32.379711
Best valid loss  : 34.068558
Best test loss   : 37.118633
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 2,521,606
--------------------------------
Total memory      : 20.54 MB
Total Flops       : 57.07 MFlops
Total Mem (Read)  : 17.37 MB
Total Mem (Write) : 16.01 MB
[Supermasks testing]
[Untrained loss : 112.1850]
[Starting training]
Epoch 0 	 89.951355 	 86.182030 	 89.102676
Epoch 10 	 58.873020 	 57.566963 	 62.839474
Epoch 20 	 55.925739 	 55.881546 	 58.642860
Epoch 30 	 53.774754 	 53.456318 	 57.302166
Epoch 40 	 52.514469 	 51.930717 	 56.245544
Epoch 50 	 51.881306 	 51.707558 	 54.937569
Epoch 60 	 51.257084 	 51.655331 	 55.119179
Epoch 70 	 49.804790 	 50.853523 	 53.930023
Epoch 80 	 48.678867 	 49.055576 	 52.529572
Epoch 90 	 48.368324 	 50.614105 	 52.952679
Epoch 100 	 47.308479 	 49.306450 	 51.387672
Epoch 110 	 46.528625 	 48.639000 	 51.856548
Epoch 120 	 46.217861 	 48.254578 	 51.535927
Epoch 130 	 46.144234 	 48.862740 	 51.221088
Epoch 140 	 46.198814 	 48.155968 	 51.016602
Epoch 150 	 45.894287 	 48.140892 	 51.159100
Epoch 160 	 46.219151 	 48.469131 	 50.822227
Epoch 170 	 45.674805 	 48.175415 	 50.962955
Epoch 180 	 45.524841 	 48.000629 	 50.996616
[Model stopped early]
Train loss       : 45.698528
Best valid loss  : 47.337879
Best test loss   : 51.017788
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 2,509,360
--------------------------------
Total memory      : 19.95 MB
Total Flops       : 45.69 MFlops
Total Mem (Read)  : 16.74 MB
Total Mem (Write) : 15.43 MB
[Supermasks testing]
[Untrained loss : 112.0278]
[Starting training]
Epoch 0 	 89.690575 	 85.454666 	 89.923332
Epoch 10 	 62.840851 	 57.854702 	 63.648445
Epoch 20 	 58.414185 	 54.377659 	 60.212177
Epoch 30 	 54.841198 	 53.831089 	 59.572323
Epoch 40 	 54.345291 	 51.522129 	 56.634968
Epoch 50 	 52.432415 	 52.053215 	 55.662048
Epoch 60 	 52.303883 	 51.839542 	 55.608173
Epoch 70 	 51.118187 	 50.602772 	 53.450542
Epoch 80 	 49.602638 	 50.575012 	 52.432747
Epoch 90 	 47.926346 	 47.665337 	 51.413593
Epoch 100 	 52.067833 	 51.927010 	 53.760006
Epoch 110 	 48.703922 	 49.772465 	 51.698292
Epoch 120 	 48.108734 	 49.604458 	 51.320984
[Model stopped early]
Train loss       : 48.154594
Best valid loss  : 47.665337
Best test loss   : 51.413593
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 2,506,207
--------------------------------
Total memory      : 19.18 MB
Total Flops       : 41.2 MFlops
Total Mem (Read)  : 15.96 MB
Total Mem (Write) : 14.66 MB
[Supermasks testing]
[Untrained loss : 112.1506]
[Starting training]
Epoch 0 	 90.780594 	 87.183266 	 90.205811
Epoch 10 	 60.905762 	 58.165882 	 62.655762
Epoch 20 	 57.474392 	 55.639412 	 60.160870
Epoch 30 	 55.721531 	 53.726322 	 58.381672
Epoch 40 	 53.917885 	 51.602467 	 56.993664
Epoch 50 	 53.842690 	 51.713886 	 56.616585
Epoch 60 	 51.851475 	 52.416122 	 55.650757
Epoch 70 	 51.576107 	 50.250248 	 55.626404
Epoch 80 	 50.051273 	 50.677361 	 54.532375
Epoch 90 	 49.303131 	 50.136303 	 52.880707
Epoch 100 	 48.590275 	 50.482838 	 52.895992
Epoch 110 	 47.984070 	 48.678963 	 52.134552
Epoch 120 	 47.042717 	 48.311111 	 51.491009
Epoch 130 	 46.617241 	 48.675179 	 51.183743
Epoch 140 	 45.779919 	 48.816875 	 51.146694
Epoch 150 	 45.694386 	 48.502872 	 51.132893
Epoch 160 	 45.152508 	 47.520798 	 50.476288
Epoch 170 	 45.322086 	 48.229008 	 50.586823
[Model stopped early]
Train loss       : 45.348011
Best valid loss  : 46.527470
Best test loss   : 51.037281
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 2,501,426
--------------------------------
Total memory      : 18.82 MB
Total Flops       : 38.44 MFlops
Total Mem (Read)  : 15.59 MB
Total Mem (Write) : 14.3 MB
[Supermasks testing]
[Untrained loss : 111.6822]
[Starting training]
Epoch 0 	 90.490219 	 85.675194 	 90.204185
Epoch 10 	 61.670494 	 58.108948 	 64.407394
Epoch 20 	 59.932198 	 57.065769 	 62.567005
Epoch 30 	 56.459030 	 54.329277 	 59.351925
Epoch 40 	 55.758209 	 54.100868 	 58.982399
Epoch 50 	 55.237789 	 54.183506 	 57.946838
Epoch 60 	 53.458694 	 51.192207 	 56.739796
Epoch 70 	 51.569122 	 50.924648 	 55.034840
Epoch 80 	 51.862858 	 51.842598 	 55.957916
Epoch 90 	 50.789642 	 48.995975 	 54.291302
Epoch 100 	 51.021275 	 51.266754 	 55.214687
Epoch 110 	 48.480625 	 49.604561 	 54.015308
Epoch 120 	 48.026657 	 49.770977 	 54.090168
Epoch 130 	 48.579498 	 48.907040 	 54.160351
Epoch 140 	 47.077969 	 48.487099 	 52.950966
Epoch 150 	 47.289986 	 48.369934 	 52.819588
Epoch 160 	 46.752823 	 48.963528 	 53.153656
Epoch 170 	 46.504448 	 48.425507 	 52.752472
[Model stopped early]
Train loss       : 46.675995
Best valid loss  : 47.190739
Best test loss   : 53.189991
Pruning          : 0.01
