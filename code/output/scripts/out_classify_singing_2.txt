Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41289062.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, six, pillow-simd, torchvision, tqdm, cycler, pyparsing, kiwisolver, python-dateutil, matplotlib, grpcio, opt-einsum, protobuf, absl-py, werkzeug, markdown, chardet, urllib3, certifi, idna, requests, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, wrapt, gast, astor, google-pasta, keras-preprocessing, termcolor, h5py, keras-applications, tensorflow-estimator, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41289062.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:59:22.772407: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:59:22.783054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is singing_classify_cnn_xavier_trimming_gradient_min_reinit_local_0.
*******
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.8765]
[Starting training]
Epoch 0 	 0.761489 	 0.683364 	 0.663971
Epoch 10 	 0.299977 	 0.372702 	 0.236949
Epoch 20 	 0.145680 	 0.241728 	 0.107537
Epoch 30 	 0.087201 	 0.188419 	 0.057904
Epoch 40 	 0.058594 	 0.169118 	 0.039522
Epoch 50 	 0.052045 	 0.162684 	 0.040074
Epoch 60 	 0.040441 	 0.163603 	 0.036397
Epoch 70 	 0.033318 	 0.157169 	 0.034191
Epoch 80 	 0.021369 	 0.155790 	 0.032169
Epoch 90 	 0.014246 	 0.154412 	 0.031434
Epoch 100 	 0.011029 	 0.153033 	 0.030882
Epoch 110 	 0.012753 	 0.150276 	 0.030331
Epoch 120 	 0.008157 	 0.151195 	 0.030515
Epoch 130 	 0.006089 	 0.152574 	 0.030699
[Model stopped early]
Train loss       : 0.005974
Best valid loss  : 0.147059
Best test loss   : 0.029779
Pruning          : 1.00
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 774,827
--------------------------------
Total memory      : 6.42 MB
Total Flops       : 398.98 MFlops
Total Mem (Read)  : 8.28 MB
Total Mem (Write) : 4.82 MB
[Supermasks testing]
[Untrained loss : 0.9550]
[Starting training]
Epoch 0 	 0.782514 	 0.716912 	 0.703768
Epoch 10 	 0.368796 	 0.411765 	 0.286397
Epoch 20 	 0.183249 	 0.267923 	 0.134191
Epoch 30 	 0.119141 	 0.199908 	 0.070588
Epoch 40 	 0.088925 	 0.190257 	 0.058732
Epoch 50 	 0.071691 	 0.177390 	 0.045956
Epoch 60 	 0.059283 	 0.173713 	 0.043107
Epoch 70 	 0.048483 	 0.168199 	 0.039338
Epoch 80 	 0.045611 	 0.163143 	 0.035662
Epoch 90 	 0.040901 	 0.171875 	 0.039706
Epoch 100 	 0.040556 	 0.159467 	 0.035110
Epoch 110 	 0.025735 	 0.157629 	 0.032904
Epoch 120 	 0.017233 	 0.153952 	 0.031250
[Model stopped early]
Train loss       : 0.017119
Best valid loss  : 0.153033
Best test loss   : 0.034007
Pruning          : 0.75
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 442,883
--------------------------------
Total memory      : 4.82 MB
Total Flops       : 243.8 MFlops
Total Mem (Read)  : 5.81 MB
Total Mem (Write) : 3.61 MB
[Supermasks testing]
[Untrained loss : 0.9871]
[Starting training]
Epoch 0 	 0.859605 	 0.750919 	 0.734467
Epoch 10 	 0.473116 	 0.511489 	 0.374449
Epoch 20 	 0.266774 	 0.327665 	 0.189154
Epoch 30 	 0.188534 	 0.255974 	 0.116360
Epoch 40 	 0.142808 	 0.214614 	 0.082353
Epoch 50 	 0.116039 	 0.198989 	 0.064614
Epoch 60 	 0.095358 	 0.189798 	 0.054320
Epoch 70 	 0.083984 	 0.177390 	 0.049724
Epoch 80 	 0.075253 	 0.176930 	 0.045404
Epoch 90 	 0.069278 	 0.178309 	 0.044853
Epoch 100 	 0.058594 	 0.173713 	 0.042647
Epoch 110 	 0.059972 	 0.162224 	 0.037868
Epoch 120 	 0.052275 	 0.166360 	 0.038787
Epoch 130 	 0.039982 	 0.163143 	 0.036949
Epoch 140 	 0.034467 	 0.153493 	 0.031801
Epoch 150 	 0.032973 	 0.158548 	 0.033824
Train loss       : 0.025161
Best valid loss  : 0.153493
Best test loss   : 0.031801
Pruning          : 0.56
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 254,405
--------------------------------
Total memory      : 3.61 MB
Total Flops       : 151.67 MFlops
Total Mem (Read)  : 4.19 MB
Total Mem (Write) : 2.71 MB
[Supermasks testing]
[Untrained loss : 0.7967]
[Starting training]
Epoch 0 	 0.792165 	 0.732996 	 0.728585
Epoch 10 	 0.523208 	 0.543658 	 0.450919
Epoch 20 	 0.336397 	 0.396140 	 0.260846
Epoch 30 	 0.246209 	 0.301930 	 0.167096
Epoch 40 	 0.196461 	 0.263327 	 0.131250
Epoch 50 	 0.164292 	 0.222886 	 0.093842
Epoch 60 	 0.137638 	 0.198989 	 0.074173
Epoch 70 	 0.123851 	 0.195772 	 0.071415
Epoch 80 	 0.118336 	 0.187040 	 0.061581
Epoch 90 	 0.098920 	 0.165441 	 0.047518
Epoch 100 	 0.098346 	 0.175092 	 0.052941
Epoch 110 	 0.086397 	 0.170956 	 0.047426
Epoch 120 	 0.072955 	 0.164982 	 0.043934
Epoch 130 	 0.064223 	 0.164062 	 0.041544
Epoch 140 	 0.062500 	 0.154412 	 0.036949
Epoch 150 	 0.052275 	 0.160846 	 0.038971
Train loss       : 0.051241
Best valid loss  : 0.152574
Best test loss   : 0.037684
Pruning          : 0.42
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 144,361
--------------------------------
Total memory      : 2.68 MB
Total Flops       : 94.38 MFlops
Total Mem (Read)  : 3.06 MB
Total Mem (Write) : 2.01 MB
[Supermasks testing]
[Untrained loss : 0.9636]
[Starting training]
Epoch 0 	 0.917624 	 0.854320 	 0.858088
Epoch 10 	 0.601103 	 0.565257 	 0.475368
Epoch 20 	 0.424288 	 0.443474 	 0.305974
Epoch 30 	 0.327436 	 0.376838 	 0.227941
Epoch 40 	 0.281020 	 0.327206 	 0.186213
Epoch 50 	 0.250115 	 0.287684 	 0.151471
Epoch 60 	 0.218176 	 0.266085 	 0.129412
Epoch 70 	 0.195542 	 0.259191 	 0.120221
Epoch 80 	 0.182560 	 0.230239 	 0.097243
Epoch 90 	 0.174632 	 0.230239 	 0.092096
Epoch 100 	 0.161880 	 0.219210 	 0.078585
Epoch 110 	 0.153608 	 0.215993 	 0.078309
Epoch 120 	 0.146140 	 0.211857 	 0.076838
Epoch 130 	 0.139821 	 0.206342 	 0.071599
Epoch 140 	 0.136259 	 0.193934 	 0.061949
Epoch 150 	 0.127642 	 0.188879 	 0.059467
Train loss       : 0.125000
Best valid loss  : 0.183364
Best test loss   : 0.055239
Pruning          : 0.32
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 84,015
--------------------------------
Total memory      : 2.01 MB
Total Flops       : 61.16 MFlops
Total Mem (Read)  : 2.33 MB
Total Mem (Write) : 1.51 MB
[Supermasks testing]
[Untrained loss : 0.8765]
[Starting training]
Epoch 0 	 0.856503 	 0.760570 	 0.750735
Epoch 10 	 0.621324 	 0.609375 	 0.541360
Epoch 20 	 0.499426 	 0.522978 	 0.416636
Epoch 30 	 0.423828 	 0.457261 	 0.322059
Epoch 40 	 0.373621 	 0.406710 	 0.272610
Epoch 50 	 0.338006 	 0.402574 	 0.270221
Epoch 60 	 0.314568 	 0.351562 	 0.215809
Epoch 70 	 0.295611 	 0.328125 	 0.195221
Epoch 80 	 0.271944 	 0.326746 	 0.187868
Epoch 90 	 0.270910 	 0.302390 	 0.159559
Epoch 100 	 0.257353 	 0.285386 	 0.150735
Epoch 110 	 0.243796 	 0.273438 	 0.134743
Epoch 120 	 0.234949 	 0.280790 	 0.140809
Epoch 130 	 0.218750 	 0.267004 	 0.124632
Epoch 140 	 0.212776 	 0.269761 	 0.125000
Epoch 150 	 0.205767 	 0.251379 	 0.108824
Train loss       : 0.198759
Best valid loss  : 0.250000
Best test loss   : 0.113787
Pruning          : 0.24
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 47,755
--------------------------------
Total memory      : 1.47 MB
Total Flops       : 39.21 MFlops
Total Mem (Read)  : 1.79 MB
Total Mem (Write) : 1.1 MB
[Supermasks testing]
[Untrained loss : 0.9607]
[Starting training]
Epoch 0 	 0.844210 	 0.818934 	 0.817279
Epoch 10 	 0.690487 	 0.675551 	 0.640441
Epoch 20 	 0.612362 	 0.619026 	 0.551103
Epoch 30 	 0.556296 	 0.570312 	 0.476654
Epoch 40 	 0.499081 	 0.518842 	 0.408180
Epoch 50 	 0.458065 	 0.496324 	 0.373346
Epoch 60 	 0.433019 	 0.468290 	 0.336949
Epoch 70 	 0.407514 	 0.456801 	 0.312132
Epoch 80 	 0.389821 	 0.432904 	 0.290993
Epoch 90 	 0.379481 	 0.410846 	 0.277298
Epoch 100 	 0.364200 	 0.410846 	 0.270221
Epoch 110 	 0.353745 	 0.404871 	 0.256893
Epoch 120 	 0.356043 	 0.395680 	 0.247610
Epoch 130 	 0.334674 	 0.389706 	 0.244118
Epoch 140 	 0.336052 	 0.371324 	 0.223346
Epoch 150 	 0.328010 	 0.372702 	 0.222151
Train loss       : 0.326861
Best valid loss  : 0.349724
Best test loss   : 0.205515
Pruning          : 0.18
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 27,239
--------------------------------
Total memory      : 1.07 MB
Total Flops       : 25.43 MFlops
Total Mem (Read)  : 1.41 MB
Total Mem (Write) : 822.46 KB
[Supermasks testing]
[Untrained loss : 0.9121]
[Starting training]
Epoch 0 	 0.911305 	 0.897059 	 0.892923
Epoch 10 	 0.708295 	 0.656250 	 0.638327
Epoch 20 	 0.661535 	 0.616268 	 0.576654
Epoch 30 	 0.607307 	 0.587316 	 0.519761
Epoch 40 	 0.557904 	 0.564798 	 0.468107
Epoch 50 	 0.514131 	 0.508732 	 0.399816
Epoch 60 	 0.475758 	 0.487132 	 0.375827
Epoch 70 	 0.450597 	 0.468750 	 0.348437
Epoch 80 	 0.431411 	 0.457261 	 0.342096
Epoch 90 	 0.414177 	 0.426471 	 0.305331
Epoch 100 	 0.409122 	 0.422335 	 0.296691
Epoch 110 	 0.396140 	 0.430607 	 0.306985
Epoch 120 	 0.384995 	 0.404412 	 0.275643
Epoch 130 	 0.386949 	 0.388327 	 0.263143
Epoch 140 	 0.373736 	 0.388327 	 0.264798
Epoch 150 	 0.367877 	 0.386029 	 0.261765
[Model stopped early]
Train loss       : 0.370290
Best valid loss  : 0.373621
Best test loss   : 0.255239
Pruning          : 0.13
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 16,485
--------------------------------
Total memory      : 0.80 MB
Total Flops       : 17.54 MFlops
Total Mem (Read)  : 1.17 MB
Total Mem (Write) : 616.86 KB
[Supermasks testing]
[Untrained loss : 0.8785]
[Starting training]
Epoch 0 	 0.942096 	 0.937500 	 0.932904
Epoch 10 	 0.724265 	 0.680147 	 0.666176
Epoch 20 	 0.684743 	 0.633732 	 0.601471
Epoch 30 	 0.642348 	 0.614430 	 0.563051
Epoch 40 	 0.610754 	 0.592371 	 0.524357
Epoch 50 	 0.584789 	 0.582261 	 0.513419
Epoch 60 	 0.571117 	 0.561121 	 0.471599
Epoch 70 	 0.553539 	 0.548713 	 0.446507
Epoch 80 	 0.544922 	 0.531710 	 0.429320
Epoch 90 	 0.525735 	 0.533088 	 0.427665
Epoch 100 	 0.514017 	 0.516544 	 0.415441
Epoch 110 	 0.516085 	 0.512408 	 0.402206
Epoch 120 	 0.511949 	 0.514706 	 0.401930
Epoch 130 	 0.493451 	 0.505055 	 0.397151
Epoch 140 	 0.497472 	 0.522059 	 0.406342
Epoch 150 	 0.489085 	 0.489890 	 0.379779
Train loss       : 0.480239
Best valid loss  : 0.479320
Best test loss   : 0.366176
Pruning          : 0.10
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 8,769
--------------------------------
Total memory      : 0.54 MB
Total Flops       : 10.66 MFlops
Total Mem (Read)  : 962.38 KB
Total Mem (Write) : 411.36 KB
[Supermasks testing]
[Untrained loss : 0.8763]
[Starting training]
Epoch 0 	 0.839269 	 0.826287 	 0.829596
Epoch 10 	 0.750000 	 0.698070 	 0.693015
Epoch 20 	 0.717256 	 0.684283 	 0.667647
Epoch 30 	 0.708065 	 0.676011 	 0.654779
Epoch 40 	 0.686351 	 0.674632 	 0.648805
Epoch 50 	 0.676356 	 0.656710 	 0.630974
Epoch 60 	 0.673598 	 0.663603 	 0.625092
Epoch 70 	 0.655331 	 0.653493 	 0.611213
Epoch 80 	 0.651999 	 0.649816 	 0.600460
Epoch 90 	 0.646255 	 0.641544 	 0.586581
Epoch 100 	 0.637293 	 0.639706 	 0.581158
Epoch 110 	 0.636604 	 0.642463 	 0.584191
Epoch 120 	 0.641774 	 0.636949 	 0.586213
Epoch 130 	 0.635110 	 0.630974 	 0.568107
Epoch 140 	 0.631664 	 0.630974 	 0.573621
Epoch 150 	 0.630285 	 0.627298 	 0.564798
Train loss       : 0.626723
Best valid loss  : 0.623162
Best test loss   : 0.562408
Pruning          : 0.08
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 5,489
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 846.73 KB
Total Mem (Write) : 308.53 KB
[Supermasks testing]
[Untrained loss : 0.9498]
[Starting training]
Epoch 0 	 0.775391 	 0.768842 	 0.757169
Epoch 10 	 0.740464 	 0.758732 	 0.743199
Epoch 20 	 0.726103 	 0.729779 	 0.711581
Epoch 30 	 0.708065 	 0.739430 	 0.717647
Epoch 40 	 0.697381 	 0.694393 	 0.666912
Epoch 50 	 0.698989 	 0.689338 	 0.655331
Epoch 60 	 0.683019 	 0.672335 	 0.633732
Epoch 70 	 0.671415 	 0.659926 	 0.615625
Epoch 80 	 0.676011 	 0.663143 	 0.609651
Epoch 90 	 0.664752 	 0.646140 	 0.597978
Epoch 100 	 0.660501 	 0.645680 	 0.596967
Epoch 110 	 0.654871 	 0.654871 	 0.595956
Epoch 120 	 0.648552 	 0.638787 	 0.584926
Epoch 130 	 0.645680 	 0.631434 	 0.572426
Epoch 140 	 0.651999 	 0.637408 	 0.581066
Epoch 150 	 0.640970 	 0.628676 	 0.567463
Train loss       : 0.641544
Best valid loss  : 0.623621
Best test loss   : 0.565625
Pruning          : 0.06
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 4,929
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 844.38 KB
Total Mem (Write) : 308.37 KB
[Supermasks testing]
[Untrained loss : 0.8763]
[Starting training]
Epoch 0 	 0.886489 	 0.876379 	 0.876287
Epoch 10 	 0.831916 	 0.850643 	 0.844853
Epoch 20 	 0.750460 	 0.698529 	 0.687316
Epoch 30 	 0.732307 	 0.686581 	 0.680331
Epoch 40 	 0.719554 	 0.675551 	 0.672059
Epoch 50 	 0.712316 	 0.671415 	 0.655882
Epoch 60 	 0.713465 	 0.668199 	 0.653677
Epoch 70 	 0.701517 	 0.658548 	 0.638971
Epoch 80 	 0.698529 	 0.658548 	 0.634559
Epoch 90 	 0.693934 	 0.654871 	 0.626103
Epoch 100 	 0.689223 	 0.648897 	 0.622427
Epoch 110 	 0.683364 	 0.646140 	 0.617096
Epoch 120 	 0.678768 	 0.646599 	 0.614706
Epoch 130 	 0.683824 	 0.647978 	 0.617463
Epoch 140 	 0.680032 	 0.636949 	 0.611397
Epoch 150 	 0.681756 	 0.640625 	 0.614522
Train loss       : 0.673943
Best valid loss  : 0.629596
Best test loss   : 0.601471
Pruning          : 0.04
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 4,589
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 842.93 KB
Total Mem (Write) : 308.25 KB
[Supermasks testing]
[Untrained loss : 0.7985]
[Starting training]
Epoch 0 	 0.809513 	 0.785846 	 0.795956
Epoch 10 	 0.804573 	 0.784467 	 0.794301
Epoch 20 	 0.749196 	 0.710478 	 0.706618
Epoch 30 	 0.723231 	 0.698070 	 0.689338
Epoch 40 	 0.718750 	 0.692096 	 0.670221
Epoch 50 	 0.707721 	 0.686121 	 0.660846
Epoch 60 	 0.700597 	 0.683364 	 0.658640
Epoch 70 	 0.684513 	 0.675092 	 0.651287
Epoch 80 	 0.679688 	 0.675551 	 0.644853
Epoch 90 	 0.677505 	 0.673254 	 0.638419
Epoch 100 	 0.678998 	 0.669577 	 0.639706
Epoch 110 	 0.667624 	 0.672335 	 0.639338
Epoch 120 	 0.677619 	 0.670496 	 0.633548
Epoch 130 	 0.669577 	 0.643382 	 0.607169
Epoch 140 	 0.678309 	 0.668658 	 0.633456
Epoch 150 	 0.671415 	 0.668199 	 0.634743
Train loss       : 0.674403
Best valid loss  : 0.643382
Best test loss   : 0.607169
Pruning          : 0.03
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 4,353
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 841.92 KB
Total Mem (Write) : 308.16 KB
[Supermasks testing]
[Untrained loss : 0.9158]
[Starting training]
Epoch 0 	 0.760915 	 0.767463 	 0.754412
Epoch 10 	 0.743107 	 0.758272 	 0.745221
Epoch 20 	 0.724150 	 0.744485 	 0.724908
Epoch 30 	 0.714959 	 0.727941 	 0.702665
Epoch 40 	 0.703814 	 0.717371 	 0.689062
Epoch 50 	 0.699219 	 0.689798 	 0.664430
Epoch 60 	 0.693819 	 0.713235 	 0.679044
Epoch 70 	 0.687500 	 0.670956 	 0.638143
Epoch 80 	 0.684168 	 0.653952 	 0.617739
Epoch 90 	 0.680262 	 0.653952 	 0.619761
Epoch 100 	 0.680032 	 0.654412 	 0.619026
Epoch 110 	 0.669347 	 0.649816 	 0.613327
Epoch 120 	 0.681411 	 0.652114 	 0.618015
Epoch 130 	 0.668428 	 0.648897 	 0.618015
Epoch 140 	 0.672909 	 0.640165 	 0.599357
[Model stopped early]
Train loss       : 0.665326
Best valid loss  : 0.634191
Best test loss   : 0.601011
Pruning          : 0.02
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 4,197
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 841.24 KB
Total Mem (Write) : 308.09 KB
[Supermasks testing]
[Untrained loss : 0.8763]
[Starting training]
Epoch 0 	 0.889591 	 0.878217 	 0.872702
Epoch 10 	 0.827206 	 0.844210 	 0.840901
Epoch 20 	 0.764131 	 0.694393 	 0.697794
Epoch 30 	 0.753447 	 0.681985 	 0.684375
Epoch 40 	 0.751264 	 0.675092 	 0.676103
Epoch 50 	 0.736443 	 0.674173 	 0.674265
Epoch 60 	 0.735064 	 0.673713 	 0.671140
Epoch 70 	 0.739890 	 0.676011 	 0.671324
[Model stopped early]
Train loss       : 0.739890
Best valid loss  : 0.670956
Best test loss   : 0.675368
Pruning          : 0.02
