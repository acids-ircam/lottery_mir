Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41281321.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, future, torch, six, torchvision, tqdm, cycler, pyparsing, kiwisolver, python-dateutil, matplotlib, opt-einsum, keras-preprocessing, grpcio, protobuf, termcolor, gast, google-pasta, tensorflow-estimator, h5py, keras-applications, astor, absl-py, markdown, werkzeug, urllib3, idna, certifi, chardet, requests, oauthlib, requests-oauthlib, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, google-auth-oauthlib, tensorboard, wrapt, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281321.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-29 02:25:22.842925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 02:25:23.174172: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is sol-ordinario_ddsp_cnn_xavier_trimming_information_rewind_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41281321.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 98.0601]
[Starting training]
Epoch 0 	 91.522079 	 85.299294 	 89.515648
Epoch 10 	 78.724464 	 81.727966 	 86.189835
Epoch 20 	 75.743530 	 75.502892 	 79.681740
Epoch 30 	 75.054474 	 75.071793 	 78.310577
Epoch 40 	 70.953026 	 72.949173 	 73.424431
Epoch 50 	 67.000740 	 70.066246 	 74.416374
Epoch 60 	 63.666210 	 67.778603 	 69.118454
Epoch 70 	 60.738651 	 61.075672 	 64.554832
Epoch 80 	 57.462555 	 57.805939 	 62.185932
Epoch 90 	 54.584221 	 52.470402 	 54.711590
Epoch 100 	 50.845432 	 54.333626 	 58.419872
Epoch 110 	 47.509342 	 48.616661 	 46.739403
Epoch 120 	 44.788082 	 48.677429 	 50.897484
Epoch 130 	 42.460865 	 45.514690 	 45.917542
Epoch 140 	 41.961292 	 42.882423 	 45.618252
Epoch 150 	 39.425426 	 37.310707 	 39.836700
Epoch 160 	 37.408329 	 42.252151 	 42.651951
Epoch 170 	 35.504974 	 34.958069 	 37.052532
Epoch 180 	 34.325668 	 34.159584 	 37.149361
Epoch 190 	 32.610210 	 33.794876 	 36.366169
Train loss       : 33.019283
Best valid loss  : 33.208492
Best test loss   : 35.917698
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,909,528
--------------------------------
Total memory      : 35.03 MB
Total Flops       : 326.2 MFlops
Total Mem (Read)  : 31.67 MB
Total Mem (Write) : 24.49 MB
[Supermasks testing]
[Untrained loss : 92.5506]
[Starting training]
Epoch 0 	 71.124359 	 62.383457 	 66.577072
Epoch 10 	 48.806824 	 46.783401 	 48.458324
Epoch 20 	 43.421371 	 45.810379 	 48.067291
Epoch 30 	 40.105839 	 37.635525 	 39.141731
Epoch 40 	 39.040722 	 38.771675 	 40.228382
Epoch 50 	 38.498772 	 36.187450 	 39.432823
Epoch 60 	 34.767025 	 33.202770 	 36.369854
Epoch 70 	 32.827091 	 35.219517 	 36.907532
Epoch 80 	 32.226154 	 32.297153 	 33.914150
Epoch 90 	 28.802181 	 30.666338 	 33.020565
Epoch 100 	 29.291615 	 30.002048 	 32.447052
Epoch 110 	 27.678352 	 30.448528 	 32.561901
Epoch 120 	 25.957956 	 29.280170 	 31.169703
Epoch 130 	 25.804193 	 28.114576 	 30.321690
Epoch 140 	 25.465710 	 28.767975 	 30.710779
Epoch 150 	 24.983244 	 27.503651 	 30.347696
Epoch 160 	 24.116825 	 27.586693 	 29.577988
Epoch 170 	 23.121393 	 26.714085 	 28.710205
Epoch 180 	 22.523542 	 26.998194 	 28.899839
Epoch 190 	 22.844078 	 26.865900 	 28.469946
Train loss       : 21.939112
Best valid loss  : 26.042721
Best test loss   : 28.568274
Pruning          : 0.72
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,898,752
--------------------------------
Total memory      : 29.65 MB
Total Flops       : 171.22 MFlops
Total Mem (Read)  : 22.92 MB
Total Mem (Write) : 17.34 MB
[Supermasks testing]
[Untrained loss : 91.9715]
[Starting training]
Epoch 0 	 74.409218 	 66.560844 	 68.139915
Epoch 10 	 55.689201 	 50.444881 	 53.013874
Epoch 20 	 47.169300 	 50.481079 	 53.140594
Epoch 30 	 40.561302 	 38.537418 	 40.591137
Epoch 40 	 40.623867 	 44.792332 	 47.984451
Epoch 50 	 36.757427 	 44.284920 	 48.973385
Epoch 60 	 32.183098 	 32.160851 	 34.812767
Epoch 70 	 31.340115 	 32.260159 	 34.723106
Epoch 80 	 28.563595 	 30.789011 	 32.262772
Epoch 90 	 26.770348 	 29.021593 	 31.178064
Epoch 100 	 26.302155 	 28.390047 	 30.892807
Epoch 110 	 26.257589 	 28.738609 	 30.859610
Epoch 120 	 25.709579 	 28.050121 	 30.062016
Epoch 130 	 25.245874 	 27.290855 	 30.092489
Epoch 140 	 25.029049 	 27.888981 	 29.974960
Epoch 150 	 24.255163 	 27.477262 	 29.331263
Epoch 160 	 24.063007 	 27.111855 	 29.356470
Epoch 170 	 23.752741 	 27.168869 	 29.310535
Epoch 180 	 23.617884 	 26.168121 	 29.177509
Epoch 190 	 23.514555 	 26.364296 	 28.849670
Train loss       : 23.368912
Best valid loss  : 26.168121
Best test loss   : 29.177509
Pruning          : 0.52
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,282,494
--------------------------------
Total memory      : 25.75 MB
Total Flops       : 89.46 MFlops
Total Mem (Read)  : 17.01 MB
Total Mem (Write) : 12.62 MB
[Supermasks testing]
[Untrained loss : 92.1490]
[Starting training]
Epoch 0 	 77.426231 	 68.807961 	 74.330635
Epoch 10 	 60.856403 	 58.158577 	 63.600094
Epoch 20 	 47.365902 	 44.133770 	 46.415066
Epoch 30 	 45.250759 	 41.219788 	 44.466591
Epoch 40 	 40.929119 	 45.589077 	 47.094204
Epoch 50 	 40.543602 	 39.971107 	 41.007671
Epoch 60 	 42.982185 	 37.359810 	 40.327065
Epoch 70 	 35.245255 	 35.645695 	 37.840370
Epoch 80 	 33.028519 	 32.968189 	 34.702766
Epoch 90 	 33.660572 	 36.625385 	 38.245712
Epoch 100 	 31.418764 	 32.640110 	 35.121407
Epoch 110 	 29.913517 	 32.380959 	 35.395016
Epoch 120 	 27.627880 	 29.764444 	 31.418716
Epoch 130 	 27.001335 	 29.300978 	 30.738478
Epoch 140 	 26.115257 	 28.787050 	 30.103638
Epoch 150 	 25.559986 	 28.064484 	 29.977644
Epoch 160 	 25.708900 	 28.242554 	 29.832916
Epoch 170 	 25.249342 	 28.406258 	 30.125504
Epoch 180 	 25.189022 	 27.522799 	 29.523682
Epoch 190 	 24.845892 	 28.048420 	 29.932867
Train loss       : 24.611662
Best valid loss  : 27.392519
Best test loss   : 29.521391
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 887,398
--------------------------------
Total memory      : 22.89 MB
Total Flops       : 46.15 MFlops
Total Mem (Read)  : 12.89 MB
Total Mem (Write) : 9.4 MB
[Supermasks testing]
[Untrained loss : 90.5925]
[Starting training]
Epoch 0 	 81.805801 	 69.542122 	 74.259254
Epoch 10 	 54.469284 	 61.232460 	 64.123795
Epoch 20 	 48.409611 	 46.405670 	 48.106827
Epoch 30 	 45.357800 	 39.564037 	 42.975124
Epoch 40 	 40.762051 	 39.393192 	 42.157368
Epoch 50 	 44.263523 	 43.837734 	 45.998409
Epoch 60 	 37.944359 	 37.484055 	 38.993019
Epoch 70 	 34.732685 	 34.075230 	 36.831635
Epoch 80 	 33.990509 	 41.236408 	 44.650684
Epoch 90 	 32.832615 	 32.607765 	 36.328796
Epoch 100 	 31.605177 	 34.949692 	 37.186512
Epoch 110 	 32.790138 	 34.004028 	 36.196178
Epoch 120 	 31.861107 	 50.229607 	 56.258343
Epoch 130 	 27.801289 	 30.892111 	 32.574390
Epoch 140 	 26.696224 	 29.556688 	 31.782276
Epoch 150 	 26.457331 	 28.779160 	 30.879658
Epoch 160 	 26.016493 	 29.171597 	 31.198576
Epoch 170 	 25.179821 	 27.806122 	 30.249159
Epoch 180 	 25.028128 	 27.959896 	 30.328888
Epoch 190 	 24.848448 	 27.964424 	 29.942657
Train loss       : 24.720959
Best valid loss  : 27.521645
Best test loss   : 29.884964
Pruning          : 0.27
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 630,951
--------------------------------
Total memory      : 20.84 MB
Total Flops       : 23.99 MFlops
Total Mem (Read)  : 10.04 MB
Total Mem (Write) : 7.21 MB
[Supermasks testing]
[Untrained loss : 89.3727]
[Starting training]
Epoch 0 	 81.917969 	 72.937004 	 77.355049
Epoch 10 	 59.790848 	 50.643166 	 53.813686
Epoch 20 	 54.656342 	 49.505005 	 51.752991
Epoch 30 	 46.437119 	 46.021400 	 48.061440
Epoch 40 	 47.450989 	 41.587860 	 43.827251
Epoch 50 	 42.137409 	 43.689819 	 46.409451
Epoch 60 	 40.246635 	 39.535809 	 41.559124
Epoch 70 	 37.742195 	 36.955318 	 39.035263
Epoch 80 	 37.010590 	 36.248005 	 37.980156
Epoch 90 	 35.773411 	 36.574303 	 38.257744
Epoch 100 	 34.676060 	 35.526394 	 37.067375
Epoch 110 	 33.712605 	 34.555630 	 35.691792
Epoch 120 	 33.518131 	 34.358322 	 35.364559
Epoch 130 	 32.990948 	 33.956905 	 35.938412
Epoch 140 	 33.055149 	 33.962669 	 35.049210
Epoch 150 	 32.554245 	 34.333542 	 35.039169
Epoch 160 	 32.627682 	 34.025818 	 35.135689
Epoch 170 	 32.364162 	 33.810078 	 34.999348
Epoch 180 	 32.263111 	 33.979443 	 34.934856
Epoch 190 	 32.423355 	 33.080982 	 35.093281
Train loss       : 32.741856
Best valid loss  : 32.863911
Best test loss   : 35.005505
Pruning          : 0.19
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 461,817
--------------------------------
Total memory      : 19.39 MB
Total Flops       : 12.83 MFlops
Total Mem (Read)  : 8.07 MB
Total Mem (Write) : 5.74 MB
[Supermasks testing]
[Untrained loss : 89.3785]
[Starting training]
Epoch 0 	 81.199341 	 72.870834 	 76.459839
Epoch 10 	 60.266609 	 53.006908 	 57.783581
Epoch 20 	 56.649452 	 48.917099 	 53.873096
Epoch 30 	 50.286621 	 48.219002 	 52.765575
Epoch 40 	 46.956924 	 47.272320 	 49.718914
Epoch 50 	 47.793095 	 50.394695 	 53.897316
Epoch 60 	 41.218781 	 39.770691 	 40.898075
Epoch 70 	 38.607536 	 40.859707 	 42.445583
Epoch 80 	 39.320660 	 41.554424 	 43.322174
Epoch 90 	 38.396759 	 40.356125 	 42.926563
Epoch 100 	 37.014717 	 37.524761 	 39.144989
Epoch 110 	 36.489891 	 35.906425 	 37.203873
Epoch 120 	 34.592449 	 34.960426 	 36.774334
Epoch 130 	 34.966427 	 36.036167 	 37.611431
Epoch 140 	 34.676437 	 33.945610 	 36.877609
Epoch 150 	 34.533390 	 35.270496 	 36.698921
Epoch 160 	 33.991905 	 34.851727 	 36.689541
Epoch 170 	 34.311817 	 34.732578 	 36.368320
[Model stopped early]
Train loss       : 34.103344
Best valid loss  : 33.945610
Best test loss   : 36.877609
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 348,567
--------------------------------
Total memory      : 18.35 MB
Total Flops       : 7.06 MFlops
Total Mem (Read)  : 6.7 MB
Total Mem (Write) : 4.71 MB
[Supermasks testing]
[Untrained loss : 89.1686]
[Starting training]
Epoch 0 	 85.808319 	 82.330307 	 85.713921
Epoch 10 	 68.980492 	 73.617302 	 77.163658
Epoch 20 	 60.522934 	 57.184917 	 61.114582
Epoch 30 	 57.034145 	 55.995995 	 58.974842
Epoch 40 	 54.064934 	 54.628090 	 55.054390
Epoch 50 	 47.557846 	 44.621910 	 46.219482
Epoch 60 	 52.849857 	 50.874699 	 54.122799
Epoch 70 	 45.260941 	 44.696461 	 45.633045
Epoch 80 	 43.832523 	 41.573914 	 44.741833
Epoch 90 	 43.278065 	 40.517399 	 42.940201
Epoch 100 	 42.538433 	 41.057560 	 44.206036
Epoch 110 	 39.370251 	 37.924541 	 39.955006
Epoch 120 	 40.471622 	 39.670403 	 41.980129
Epoch 130 	 38.816338 	 37.001247 	 39.377666
Epoch 140 	 38.229717 	 37.847103 	 41.010342
Epoch 150 	 38.223610 	 38.774269 	 41.394154
Epoch 160 	 37.059547 	 35.632320 	 38.290737
Epoch 170 	 35.803257 	 36.298126 	 38.875362
Epoch 180 	 36.853119 	 35.157593 	 38.226318
Epoch 190 	 35.350117 	 35.314400 	 38.239971
Train loss       : 35.342632
Best valid loss  : 34.821632
Best test loss   : 38.297169
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 272,819
--------------------------------
Total memory      : 17.56 MB
Total Flops       : 3.79 MFlops
Total Mem (Read)  : 5.68 MB
Total Mem (Write) : 3.94 MB
[Supermasks testing]
[Untrained loss : 89.4496]
[Starting training]
Epoch 0 	 86.280159 	 83.414566 	 86.927521
Epoch 10 	 66.194473 	 65.789696 	 69.088959
Epoch 20 	 62.722046 	 57.261230 	 60.714973
Epoch 30 	 58.249569 	 52.613140 	 56.156994
Epoch 40 	 55.672241 	 50.692646 	 53.611469
Epoch 50 	 50.939098 	 44.687458 	 48.159042
Epoch 60 	 49.914467 	 44.695465 	 46.504467
Epoch 70 	 45.029320 	 44.919350 	 47.907101
Epoch 80 	 45.712406 	 41.862309 	 43.466106
Epoch 90 	 45.698921 	 42.573174 	 43.706203
Epoch 100 	 43.473816 	 42.190517 	 43.815395
Epoch 110 	 43.368317 	 41.733566 	 43.099564
Epoch 120 	 42.208885 	 42.193352 	 43.401970
Epoch 130 	 41.638905 	 40.235889 	 42.287197
Epoch 140 	 41.114403 	 40.228306 	 41.952255
Epoch 150 	 40.445515 	 39.189751 	 41.187969
Epoch 160 	 40.487854 	 39.800957 	 41.188274
Epoch 170 	 40.770710 	 39.947372 	 41.332073
Epoch 180 	 40.464939 	 39.658993 	 41.147015
Epoch 190 	 40.706036 	 39.571320 	 41.119175
Train loss       : 40.485672
Best valid loss  : 39.109932
Best test loss   : 41.129623
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 220,983
--------------------------------
Total memory      : 17.11 MB
Total Flops       : 2.56 MFlops
Total Mem (Read)  : 5.08 MB
Total Mem (Write) : 3.52 MB
[Supermasks testing]
[Untrained loss : 90.6009]
[Starting training]
Epoch 0 	 86.758209 	 85.061691 	 88.419060
Epoch 10 	 70.998482 	 69.545158 	 72.557915
Epoch 20 	 66.947403 	 62.898335 	 67.302139
Epoch 30 	 62.302307 	 59.718712 	 65.484703
Epoch 40 	 63.212753 	 60.051472 	 64.252281
Epoch 50 	 60.608658 	 57.530960 	 62.783241
Epoch 60 	 57.387249 	 58.875607 	 62.941463
Epoch 70 	 53.334618 	 51.892891 	 55.878559
Epoch 80 	 52.368076 	 51.076450 	 55.387341
Epoch 90 	 51.690575 	 56.673489 	 57.305477
Epoch 100 	 51.611267 	 50.486008 	 53.513203
Epoch 110 	 50.349487 	 54.800957 	 56.645199
Epoch 120 	 46.576260 	 43.946819 	 47.595409
Epoch 130 	 46.222332 	 48.212536 	 51.811459
Epoch 140 	 44.428852 	 43.499104 	 46.314861
Epoch 150 	 43.456215 	 43.970768 	 47.486923
Epoch 160 	 43.563690 	 42.178219 	 44.996975
Epoch 170 	 42.954117 	 40.970299 	 44.369282
Epoch 180 	 42.133926 	 40.595383 	 44.039440
Epoch 190 	 41.586079 	 41.222176 	 44.196926
Train loss       : 41.482471
Best valid loss  : 40.402855
Best test loss   : 44.169811
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 185,398
--------------------------------
Total memory      : 16.72 MB
Total Flops       : 1.66 MFlops
Total Mem (Read)  : 4.58 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 91.3396]
[Starting training]
Epoch 0 	 87.162971 	 84.513290 	 88.481049
Epoch 10 	 70.533096 	 75.714676 	 79.479721
Epoch 20 	 65.868080 	 61.518543 	 65.459755
Epoch 30 	 62.379314 	 59.521225 	 63.628063
Epoch 40 	 61.274681 	 58.465515 	 62.390259
Epoch 50 	 60.092518 	 58.438389 	 62.026394
Epoch 60 	 59.376987 	 56.689648 	 59.604458
Epoch 70 	 56.789112 	 53.213669 	 57.310688
Epoch 80 	 55.542969 	 53.481663 	 57.010506
Epoch 90 	 55.690830 	 54.489872 	 57.846352
Epoch 100 	 55.622334 	 52.837196 	 56.118935
Epoch 110 	 55.178951 	 51.231579 	 54.824841
Epoch 120 	 52.586693 	 49.954483 	 52.387215
Epoch 130 	 50.326511 	 48.499729 	 50.788979
Epoch 140 	 51.700680 	 46.215706 	 48.615780
Epoch 150 	 48.578026 	 45.538815 	 47.614330
Epoch 160 	 49.131435 	 46.081299 	 47.689480
Epoch 170 	 47.049110 	 44.517349 	 46.617237
Epoch 180 	 46.826797 	 45.898449 	 47.470219
Epoch 190 	 47.277477 	 45.340103 	 46.472946
[Model stopped early]
Train loss       : 47.341591
Best valid loss  : 44.030666
Best test loss   : 46.885876
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 156,046
--------------------------------
Total memory      : 16.63 MB
Total Flops       : 1.63 MFlops
Total Mem (Read)  : 4.41 MB
Total Mem (Write) : 3.08 MB
[Supermasks testing]
[Untrained loss : 90.9934]
[Starting training]
Epoch 0 	 87.048813 	 85.184975 	 88.450768
Epoch 10 	 72.070839 	 67.117760 	 72.459518
Epoch 20 	 68.997124 	 63.938240 	 69.150230
Epoch 30 	 62.746109 	 56.792297 	 62.159039
Epoch 40 	 61.711887 	 55.323948 	 60.796291
Epoch 50 	 62.953571 	 60.381634 	 63.841030
Epoch 60 	 58.427170 	 59.781075 	 63.839123
Epoch 70 	 57.435883 	 55.067497 	 58.781887
Epoch 80 	 56.584599 	 53.561340 	 57.547527
Epoch 90 	 55.235912 	 54.137306 	 57.107449
Epoch 100 	 55.300243 	 53.475864 	 56.872364
Epoch 110 	 54.251331 	 53.305046 	 56.787220
Epoch 120 	 54.813950 	 53.058559 	 56.542416
Epoch 130 	 55.101265 	 53.169006 	 56.191067
Epoch 140 	 54.647228 	 52.287365 	 56.535595
[Model stopped early]
Train loss       : 54.013279
Best valid loss  : 51.733589
Best test loss   : 56.559307
Pruning          : 0.03
[Performing one full cumulative epoch]
Traceback (most recent call last):
  File "main.py", line 261, in <module>
    if (args.prune_selection in ['activation', 'information', 'info_target']):
  File "/scratch/esling/lottery/pruning.py", line 781, in reset
    replace_recurrent(m, l, m.unprune_idx[l], prev_kept)
  File "/scratch/esling/lottery/pruning.py", line 752, in replace_recurrent
    cur_ih = nn.Parameter(cur_ih[rep_id0])#torch.from_numpy(cur_ih[rep_id0]).to(self.args.device))
IndexError: too many indices for tensor of dimension 2
