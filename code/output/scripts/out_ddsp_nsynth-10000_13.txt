Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41146337.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, six, future, torch, torchvision, tqdm, cycler, kiwisolver, python-dateutil, pyparsing, matplotlib, google-pasta, h5py, keras-applications, opt-einsum, termcolor, astor, urllib3, chardet, certifi, idna, requests, werkzeug, markdown, absl-py, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, protobuf, tensorboard, wrapt, keras-preprocessing, gast, tensorflow-estimator, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146337.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-27 03:01:58.849105: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-27 03:01:58.859418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_ddsp_cnn_xavier_trimming_batchnorm_rewind_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41146337.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 93.5328]
[Starting training]
Epoch 0 	 76.721779 	 70.333092 	 72.535210
Epoch 10 	 77.023720 	 59.994106 	 62.246304
Epoch 20 	 52.507927 	 47.283772 	 48.153164
Epoch 30 	 48.780788 	 42.113213 	 43.596928
Epoch 40 	 43.605938 	 39.693977 	 41.311687
Epoch 50 	 40.977295 	 36.220142 	 37.990299
Epoch 60 	 38.496574 	 34.556820 	 36.440311
Epoch 70 	 35.479073 	 32.304638 	 34.160107
Epoch 80 	 33.471870 	 31.018959 	 33.050594
Epoch 90 	 32.085514 	 29.939541 	 32.079731
Epoch 100 	 31.079769 	 31.476257 	 33.090858
Epoch 110 	 29.735605 	 28.643549 	 30.629808
Epoch 120 	 30.417616 	 29.767296 	 31.785994
Epoch 130 	 27.972343 	 27.911539 	 29.694065
Epoch 140 	 27.169605 	 27.262516 	 29.162348
Epoch 150 	 27.121403 	 26.890112 	 29.000780
Epoch 160 	 25.190355 	 25.491035 	 27.496599
Epoch 170 	 24.845812 	 25.920124 	 27.639215
Epoch 180 	 24.712437 	 25.700985 	 27.524544
Epoch 190 	 24.410511 	 25.831385 	 27.649105
Train loss       : 24.169481
Best valid loss  : 24.950226
Best test loss   : 27.177801
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 3,792,824
--------------------------------
Total memory      : 35.14 MB
Total Flops       : 339.89 MFlops
Total Mem (Read)  : 35.15 MB
Total Mem (Write) : 28.95 MB
[Supermasks testing]
[Untrained loss : 61.4267]
[Starting training]
Epoch 0 	 33.345646 	 29.582153 	 31.457592
Epoch 10 	 29.658953 	 28.090069 	 29.983795
Epoch 20 	 28.470448 	 27.856871 	 29.801264
Epoch 30 	 27.960014 	 27.455353 	 29.234415
Epoch 40 	 27.330408 	 27.331364 	 28.886400
Epoch 50 	 26.836336 	 26.623070 	 28.524427
Epoch 60 	 26.511694 	 26.372913 	 28.199394
Epoch 70 	 25.725580 	 26.075665 	 27.874136
Epoch 80 	 24.667582 	 25.342670 	 27.215982
Epoch 90 	 24.275675 	 25.160725 	 26.871553
Epoch 100 	 24.049419 	 24.989153 	 26.890947
Epoch 110 	 23.262165 	 24.608349 	 26.351513
Epoch 120 	 23.160028 	 24.616329 	 26.420500
Epoch 130 	 22.875690 	 24.225403 	 26.181381
Epoch 140 	 22.773819 	 24.014570 	 26.196726
Epoch 150 	 22.692284 	 24.209652 	 26.262573
Epoch 160 	 22.557362 	 24.194344 	 26.064463
Epoch 170 	 22.520578 	 24.207287 	 26.080269
[Model stopped early]
Train loss       : 22.323360
Best valid loss  : 23.665552
Best test loss   : 26.163618
Pruning          : 0.72
0.001
0.001
[Current model size]
================================
Total params      : 3,316,816
--------------------------------
Total memory      : 29.84 MB
Total Flops       : 192.02 MFlops
Total Mem (Read)  : 28.52 MB
Total Mem (Write) : 24.13 MB
[Supermasks testing]
[Untrained loss : 72.1968]
[Starting training]
Epoch 0 	 35.367012 	 30.668818 	 32.894318
Epoch 10 	 29.897663 	 28.951698 	 30.765265
Epoch 20 	 31.294472 	 29.277569 	 31.142128
Epoch 30 	 28.917849 	 27.958414 	 29.766071
Epoch 40 	 28.266724 	 27.371569 	 29.066990
Epoch 50 	 28.430670 	 27.348669 	 29.210751
Epoch 60 	 27.916945 	 27.954298 	 29.771490
Epoch 70 	 26.946707 	 26.804447 	 28.747568
Epoch 80 	 26.744240 	 29.230350 	 30.980473
Epoch 90 	 25.908785 	 26.697935 	 28.265516
Epoch 100 	 25.583012 	 26.531290 	 28.264700
Epoch 110 	 25.390867 	 26.487169 	 28.246931
Epoch 120 	 25.210054 	 26.514566 	 28.080046
Epoch 130 	 25.109230 	 26.434366 	 28.147861
Epoch 140 	 25.071856 	 26.514563 	 28.189484
Epoch 150 	 24.969547 	 26.447598 	 28.011396
Epoch 160 	 24.876699 	 26.247244 	 28.067259
Epoch 170 	 24.794695 	 26.264566 	 28.017506
[Model stopped early]
Train loss       : 24.794695
Best valid loss  : 26.048609
Best test loss   : 28.081793
Pruning          : 0.52
0.001
0.001
[Current model size]
================================
Total params      : 3,028,378
--------------------------------
Total memory      : 26.00 MB
Total Flops       : 113.88 MFlops
Total Mem (Read)  : 23.92 MB
Total Mem (Write) : 20.63 MB
[Supermasks testing]
[Untrained loss : 75.1421]
[Starting training]
Epoch 0 	 39.443893 	 31.686869 	 33.807095
Epoch 10 	 31.051764 	 38.895565 	 39.402859
Epoch 20 	 29.204737 	 28.387175 	 30.343679
Epoch 30 	 27.997942 	 28.146608 	 29.564035
Epoch 40 	 27.929316 	 29.582678 	 31.155088
Epoch 50 	 26.992828 	 26.833052 	 28.620581
Epoch 60 	 26.100758 	 26.297703 	 28.032885
Epoch 70 	 25.935595 	 26.069283 	 27.834637
Epoch 80 	 25.718769 	 26.105837 	 27.692902
Epoch 90 	 25.391918 	 25.981188 	 27.702490
Epoch 100 	 24.640795 	 25.862902 	 27.581419
Epoch 110 	 24.384590 	 25.448172 	 27.186653
Epoch 120 	 23.801060 	 25.288130 	 27.078197
Epoch 130 	 23.659893 	 25.134380 	 26.923290
Epoch 140 	 23.361778 	 25.189701 	 26.836912
Epoch 150 	 23.250204 	 25.027660 	 26.807905
Epoch 160 	 23.192535 	 25.078667 	 26.755411
Epoch 170 	 23.227448 	 25.091459 	 26.795856
Epoch 180 	 23.162313 	 24.967268 	 26.754049
Epoch 190 	 23.076283 	 24.991190 	 26.713392
[Model stopped early]
Train loss       : 23.076283
Best valid loss  : 24.704210
Best test loss   : 26.767139
Pruning          : 0.37
0.001
0.001
[Current model size]
================================
Total params      : 2,844,854
--------------------------------
Total memory      : 23.18 MB
Total Flops       : 72.48 MFlops
Total Mem (Read)  : 20.64 MB
Total Mem (Write) : 18.05 MB
[Supermasks testing]
[Untrained loss : 89.2015]
[Starting training]
Epoch 0 	 42.920780 	 33.471329 	 35.601669
Epoch 10 	 31.439854 	 29.272163 	 31.234949
Epoch 20 	 29.556166 	 28.259983 	 30.367794
Epoch 30 	 28.740053 	 28.545242 	 30.233721
Epoch 40 	 28.969091 	 27.692060 	 29.168610
Epoch 50 	 27.071692 	 26.469751 	 28.387817
Epoch 60 	 26.696377 	 26.510695 	 28.437454
Epoch 70 	 25.479326 	 26.406921 	 28.340019
Epoch 80 	 25.195656 	 25.953072 	 27.612020
Epoch 90 	 24.505178 	 25.347815 	 27.190643
Epoch 100 	 24.221210 	 25.527925 	 27.215969
Epoch 110 	 23.866331 	 25.146294 	 27.089790
Epoch 120 	 23.775055 	 25.212219 	 26.972780
[Model stopped early]
Train loss       : 23.664446
Best valid loss  : 24.994715
Best test loss   : 27.162447
Pruning          : 0.27
0.001
0.001
[Current model size]
================================
Total params      : 2,726,701
--------------------------------
Total memory      : 21.15 MB
Total Flops       : 51.3 MFlops
Total Mem (Read)  : 18.35 MB
Total Mem (Write) : 16.21 MB
[Supermasks testing]
[Untrained loss : 98.6437]
[Starting training]
Epoch 0 	 46.480846 	 35.572487 	 37.795734
Epoch 10 	 31.927839 	 29.556978 	 31.654751
Epoch 20 	 30.962376 	 29.220018 	 31.183706
Epoch 30 	 29.053745 	 28.639456 	 30.432774
Epoch 40 	 28.433701 	 27.539511 	 29.147821
Epoch 50 	 27.820602 	 27.086344 	 28.970037
Epoch 60 	 27.301592 	 26.893393 	 28.493692
Epoch 70 	 27.269388 	 27.465420 	 29.209684
Epoch 80 	 25.699875 	 26.200500 	 27.822403
Epoch 90 	 25.338968 	 25.924477 	 27.622892
Epoch 100 	 25.342999 	 26.094721 	 27.907545
Epoch 110 	 24.321072 	 25.689924 	 27.303965
Epoch 120 	 24.184525 	 25.461258 	 27.131620
Epoch 130 	 24.127676 	 25.408617 	 27.288412
Epoch 140 	 23.855583 	 25.136747 	 26.988451
Epoch 150 	 23.724493 	 25.273714 	 26.945429
Epoch 160 	 23.577606 	 25.378635 	 26.997896
Epoch 170 	 23.542608 	 25.306921 	 27.009851
[Model stopped early]
Train loss       : 23.501907
Best valid loss  : 25.136747
Best test loss   : 26.988451
Pruning          : 0.19
0.001
0.001
[Current model size]
================================
Total params      : 2,649,351
--------------------------------
Total memory      : 19.73 MB
Total Flops       : 40.64 MFlops
Total Mem (Read)  : 16.76 MB
Total Mem (Write) : 14.91 MB
[Supermasks testing]
[Untrained loss : 90.4533]
[Starting training]
Epoch 0 	 54.922729 	 45.648064 	 46.520653
Epoch 10 	 35.685543 	 32.840580 	 34.662590
Epoch 20 	 33.612877 	 31.137035 	 33.387356
Epoch 30 	 31.940435 	 29.634529 	 31.916845
Epoch 40 	 33.101326 	 37.791107 	 38.443623
Epoch 50 	 29.679102 	 28.445608 	 30.242229
Epoch 60 	 29.477152 	 32.771381 	 35.180405
Epoch 70 	 28.201694 	 27.843351 	 29.819601
Epoch 80 	 28.128538 	 27.397749 	 29.183868
Epoch 90 	 26.659222 	 26.875252 	 28.417084
Epoch 100 	 26.273609 	 26.474955 	 28.266375
Epoch 110 	 25.440472 	 26.074793 	 27.902506
Epoch 120 	 25.308529 	 26.462181 	 28.050262
Epoch 130 	 24.931425 	 26.044613 	 27.757271
Epoch 140 	 24.775526 	 25.916477 	 27.649046
[Model stopped early]
Train loss       : 24.787691
Best valid loss  : 25.688034
Best test loss   : 27.923159
Pruning          : 0.14
0.001
0.001
[Current model size]
================================
Total params      : 2,597,821
--------------------------------
Total memory      : 18.71 MB
Total Flops       : 35.13 MFlops
Total Mem (Read)  : 15.63 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 86.2782]
[Starting training]
Epoch 0 	 58.150696 	 43.255806 	 45.278435
Epoch 10 	 38.345348 	 33.851116 	 35.925323
Epoch 20 	 36.187935 	 32.489304 	 34.723389
Epoch 30 	 35.216412 	 32.829506 	 34.501743
Epoch 40 	 33.050644 	 31.072237 	 33.165569
Epoch 50 	 32.052097 	 29.748140 	 31.959566
Epoch 60 	 31.391848 	 30.256130 	 31.923332
Epoch 70 	 31.227081 	 30.353886 	 32.246109
Epoch 80 	 30.085299 	 28.869955 	 30.535748
Epoch 90 	 28.575212 	 28.037325 	 29.916910
Epoch 100 	 28.132318 	 28.201782 	 29.713127
Epoch 110 	 27.323118 	 27.853476 	 29.418592
Epoch 120 	 26.868521 	 27.636118 	 29.240885
Epoch 130 	 26.691952 	 27.619244 	 29.101553
Epoch 140 	 26.461412 	 27.533129 	 29.085175
Epoch 150 	 26.382782 	 27.186560 	 29.010124
Epoch 160 	 26.400040 	 27.713512 	 29.033419
Epoch 170 	 26.309248 	 27.580984 	 29.109417
Epoch 180 	 26.258263 	 27.755173 	 29.172363
[Model stopped early]
Train loss       : 26.311592
Best valid loss  : 27.186560
Best test loss   : 29.010124
Pruning          : 0.10
0.001
0.001
[Current model size]
================================
Total params      : 2,563,373
--------------------------------
Total memory      : 17.92 MB
Total Flops       : 31.98 MFlops
Total Mem (Read)  : 14.78 MB
Total Mem (Write) : 13.26 MB
[Supermasks testing]
[Untrained loss : 106.8537]
[Starting training]
Epoch 0 	 64.734833 	 55.688343 	 56.502468
Epoch 10 	 41.398830 	 38.076229 	 40.187790
Epoch 20 	 37.930786 	 34.225563 	 36.114296
Epoch 30 	 36.345837 	 33.868816 	 35.914593
Epoch 40 	 34.536057 	 32.340725 	 34.336384
Epoch 50 	 33.709995 	 31.804668 	 33.945732
Epoch 60 	 33.214352 	 31.346453 	 33.267418
Epoch 70 	 32.653446 	 30.367140 	 32.132538
Epoch 80 	 32.370293 	 30.038601 	 32.006607
Epoch 90 	 30.468811 	 29.290010 	 31.135189
Epoch 100 	 29.936840 	 28.956034 	 30.875059
Epoch 110 	 29.646711 	 29.150427 	 30.703758
Epoch 120 	 28.749750 	 28.668831 	 30.436049
Epoch 130 	 28.649393 	 28.611713 	 30.363728
Epoch 140 	 28.387825 	 28.811071 	 30.369539
Epoch 150 	 27.940872 	 28.414997 	 29.959650
Epoch 160 	 27.815184 	 28.372362 	 30.067535
Epoch 170 	 27.680475 	 28.290394 	 29.863850
Epoch 180 	 27.452095 	 28.243685 	 29.836040
Epoch 190 	 27.355148 	 28.256458 	 29.768618
Train loss       : 27.306543
Best valid loss  : 28.027309
Best test loss   : 29.809855
Pruning          : 0.07
0.001
0.001
[Current model size]
================================
Total params      : 2,540,077
--------------------------------
Total memory      : 17.49 MB
Total Flops       : 30.82 MFlops
Total Mem (Read)  : 14.3 MB
Total Mem (Write) : 12.87 MB
[Supermasks testing]
[Untrained loss : 126.8423]
[Starting training]
Epoch 0 	 68.704590 	 58.914486 	 58.356815
Epoch 10 	 44.498425 	 42.676968 	 44.341717
Epoch 20 	 40.772678 	 36.134220 	 38.312382
Epoch 30 	 39.922348 	 37.411182 	 39.655842
Epoch 40 	 37.431965 	 36.017780 	 38.215744
Epoch 50 	 36.971062 	 36.222805 	 38.377396
Epoch 60 	 36.366917 	 35.450848 	 37.546810
Epoch 70 	 35.902653 	 32.557587 	 34.673637
Epoch 80 	 35.691463 	 34.592987 	 36.415230
Epoch 90 	 34.391079 	 33.121616 	 35.032959
Epoch 100 	 33.982563 	 31.975178 	 34.102909
Epoch 110 	 32.614346 	 31.242300 	 33.315002
Epoch 120 	 32.280724 	 31.381437 	 33.390537
Epoch 130 	 31.817440 	 30.772909 	 32.780796
Epoch 140 	 31.189358 	 30.361618 	 32.277962
Epoch 150 	 30.929119 	 30.302847 	 32.113411
Epoch 160 	 30.754347 	 30.509314 	 32.572220
Epoch 170 	 30.411789 	 30.315311 	 32.060432
Epoch 180 	 30.132483 	 29.865755 	 31.808418
Epoch 190 	 29.989941 	 29.970842 	 31.743536
Train loss       : 29.858801
Best valid loss  : 29.837336
Best test loss   : 31.756975
Pruning          : 0.05
0.001
0.001
[Current model size]
================================
Total params      : 2,523,994
--------------------------------
Total memory      : 17.09 MB
Total Flops       : 29.95 MFlops
Total Mem (Read)  : 13.88 MB
Total Mem (Write) : 12.51 MB
[Supermasks testing]
[Untrained loss : 136.1423]
[Starting training]
Epoch 0 	 68.050171 	 56.243786 	 57.199032
Epoch 10 	 44.495960 	 41.767788 	 43.797199
Epoch 20 	 42.513584 	 37.153824 	 39.444233
Epoch 30 	 40.050735 	 35.824322 	 37.618378
Epoch 40 	 38.958000 	 35.571449 	 37.743610
Epoch 50 	 37.854351 	 34.786457 	 36.808823
Epoch 60 	 35.675987 	 33.781616 	 35.852695
Epoch 70 	 35.511696 	 33.804405 	 35.461487
Epoch 80 	 34.571270 	 33.383392 	 35.110920
Epoch 90 	 34.138161 	 33.480045 	 35.175823
Epoch 100 	 33.643005 	 33.275246 	 34.897839
Epoch 110 	 33.995377 	 33.233788 	 35.056919
Epoch 120 	 32.618538 	 32.517784 	 34.325798
Epoch 130 	 31.932243 	 32.165829 	 34.103302
Epoch 140 	 31.897873 	 32.393757 	 34.240501
Epoch 150 	 31.435080 	 32.361149 	 34.085777
Epoch 160 	 31.196625 	 32.298164 	 34.044468
[Model stopped early]
Train loss       : 31.424574
Best valid loss  : 32.165829
Best test loss   : 34.103302
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 2,511,046
--------------------------------
Total memory      : 17.01 MB
Total Flops       : 29.94 MFlops
Total Mem (Read)  : 13.77 MB
Total Mem (Write) : 12.45 MB
[Supermasks testing]
[Untrained loss : 184.1087]
[Starting training]
Epoch 0 	 67.861313 	 57.017773 	 57.139561
Epoch 10 	 45.043846 	 41.029617 	 43.725815
Epoch 20 	 40.654610 	 37.679810 	 39.656796
Epoch 30 	 39.261646 	 38.454029 	 40.087254
Epoch 40 	 38.164265 	 34.867138 	 36.930794
Epoch 50 	 36.873920 	 34.819347 	 36.362736
Epoch 60 	 35.093906 	 33.298687 	 35.000126
Epoch 70 	 34.100288 	 32.658295 	 34.502331
Epoch 80 	 33.811554 	 32.661873 	 34.293919
Epoch 90 	 33.305351 	 32.576500 	 34.348911
Epoch 100 	 32.703861 	 32.770321 	 34.486012
Epoch 110 	 32.186184 	 32.123562 	 33.536030
Epoch 120 	 31.911146 	 31.854151 	 33.515484
Epoch 130 	 31.202179 	 31.664042 	 33.280399
Epoch 140 	 30.981819 	 31.326578 	 33.172272
Epoch 150 	 30.651236 	 31.242794 	 33.065746
Epoch 160 	 30.453350 	 31.417362 	 32.968517
Epoch 170 	 30.263853 	 31.208269 	 32.976990
Epoch 180 	 30.238741 	 31.261930 	 32.975281
Epoch 190 	 30.107182 	 31.273388 	 32.979530
Train loss       : 30.170731
Best valid loss  : 31.083372
Best test loss   : 32.909092
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 2,502,534
--------------------------------
Total memory      : 16.95 MB
Total Flops       : 29.94 MFlops
Total Mem (Read)  : 13.71 MB
Total Mem (Write) : 12.42 MB
[Supermasks testing]
[Untrained loss : 489.7032]
[Starting training]
Epoch 0 	 73.374153 	 65.588043 	 67.064529
Epoch 10 	 55.005230 	 53.402172 	 54.611816
Epoch 20 	 48.363411 	 43.616013 	 45.148163
Epoch 30 	 45.792019 	 40.216194 	 42.051418
Epoch 40 	 44.545654 	 38.643677 	 40.706841
Epoch 50 	 41.739693 	 38.645939 	 40.671642
Epoch 60 	 41.180988 	 37.690922 	 39.559273
Epoch 70 	 40.193344 	 35.770836 	 37.778664
Epoch 80 	 39.558628 	 36.507858 	 38.152546
Epoch 90 	 37.728508 	 35.549374 	 37.404999
Epoch 100 	 37.765972 	 35.231358 	 36.922348
Epoch 110 	 36.936794 	 34.991089 	 36.691620
Epoch 120 	 36.351231 	 34.602024 	 36.648685
Epoch 130 	 35.873100 	 34.948895 	 36.759663
Epoch 140 	 34.989288 	 34.583134 	 36.645714
Epoch 150 	 34.754181 	 34.702240 	 36.432610
Epoch 160 	 34.137081 	 34.473637 	 36.295769
Epoch 170 	 33.945126 	 34.777500 	 36.374138
Epoch 180 	 33.746368 	 34.504517 	 36.062805
Epoch 190 	 33.560028 	 34.463207 	 36.145821
Train loss       : 33.327793
Best valid loss  : 34.012005
Best test loss   : 36.037411
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 2,496,213
--------------------------------
Total memory      : 16.91 MB
Total Flops       : 29.93 MFlops
Total Mem (Read)  : 13.65 MB
Total Mem (Write) : 12.39 MB
[Supermasks testing]
[Untrained loss : 136.3307]
[Starting training]
Epoch 0 	 70.263420 	 57.580914 	 57.851166
Epoch 10 	 51.885231 	 48.713608 	 48.417316
Epoch 20 	 48.431175 	 45.264481 	 46.024914
Epoch 30 	 47.737255 	 45.059319 	 46.155930
Epoch 40 	 46.877922 	 43.653954 	 44.259228
Epoch 50 	 45.927620 	 45.563957 	 46.525265
Epoch 60 	 44.645428 	 42.810398 	 43.681404
Epoch 70 	 44.472042 	 42.368874 	 43.132797
Epoch 80 	 44.007763 	 41.427166 	 42.551815
Epoch 90 	 43.981663 	 42.961426 	 43.815311
Epoch 100 	 43.628868 	 41.305935 	 42.636631
Epoch 110 	 42.717125 	 40.590324 	 41.631264
Epoch 120 	 43.543613 	 41.393806 	 42.646294
Epoch 130 	 42.455799 	 40.531860 	 41.788074
Epoch 140 	 42.796135 	 40.818207 	 41.765129
Epoch 150 	 43.693966 	 41.036636 	 42.276974
[Model stopped early]
Train loss       : 42.290394
Best valid loss  : 39.832870
Best test loss   : 41.424728
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 2,492,029
--------------------------------
Total memory      : 16.89 MB
Total Flops       : 29.93 MFlops
Total Mem (Read)  : 13.62 MB
Total Mem (Write) : 12.37 MB
[Supermasks testing]
[Untrained loss : 159.6416]
[Starting training]
Epoch 0 	 78.402534 	 71.835854 	 73.994392
Epoch 10 	 54.690212 	 54.099354 	 53.320831
Epoch 20 	 51.247284 	 47.232708 	 46.858555
Epoch 30 	 48.903061 	 47.346066 	 48.032215
Epoch 40 	 48.427979 	 44.143368 	 44.826603
Epoch 50 	 47.289120 	 44.552807 	 45.688755
Epoch 60 	 45.420563 	 42.912575 	 43.975655
Epoch 70 	 47.920845 	 43.358437 	 44.228008
Epoch 80 	 44.373192 	 41.405914 	 42.923946
Epoch 90 	 43.985306 	 41.086891 	 42.177128
Epoch 100 	 43.293320 	 41.200718 	 42.209293
Epoch 110 	 45.281086 	 43.849640 	 44.157391
Epoch 120 	 43.486942 	 41.417446 	 42.384239
[Model stopped early]
Train loss       : 43.112007
Best valid loss  : 40.716259
Best test loss   : 42.405575
Pruning          : 0.01
