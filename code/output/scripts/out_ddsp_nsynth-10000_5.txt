Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41146328.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, six, future, torch, torchvision, tqdm, cycler, pyparsing, python-dateutil, kiwisolver, matplotlib, grpcio, gast, h5py, keras-applications, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, markdown, protobuf, werkzeug, absl-py, chardet, idna, certifi, urllib3, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, google-pasta, keras-preprocessing, opt-einsum, termcolor, astor, tensorflow-estimator, wrapt, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146328.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-27 03:01:59.098636: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-27 03:01:59.109163: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_ddsp_cnn_xavier_trimming_info_target_reinit_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41146328.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 92.5667]
[Starting training]
Epoch 0 	 75.796234 	 65.954811 	 67.684929
Epoch 10 	 65.688354 	 60.662014 	 62.373291
Epoch 20 	 56.089409 	 52.277893 	 52.303127
Epoch 30 	 47.674324 	 44.325588 	 45.556290
Epoch 40 	 42.411991 	 38.927319 	 40.823563
Epoch 50 	 38.955460 	 35.785732 	 37.560070
Epoch 60 	 35.201015 	 31.975430 	 33.669155
Epoch 70 	 33.120811 	 30.701199 	 32.674603
Epoch 80 	 33.312534 	 29.903400 	 31.849216
Epoch 90 	 33.035473 	 30.707029 	 32.670265
Epoch 100 	 29.282694 	 29.080662 	 30.654783
Epoch 110 	 28.414961 	 27.426668 	 29.309059
Epoch 120 	 27.833132 	 27.198950 	 28.944344
Epoch 130 	 26.035561 	 25.929842 	 27.731747
Epoch 140 	 26.888264 	 26.287779 	 28.056705
Epoch 150 	 25.531633 	 26.136808 	 27.715961
Epoch 160 	 25.363428 	 25.877277 	 27.567093
Epoch 170 	 25.019812 	 25.725216 	 27.403532
Epoch 180 	 24.831800 	 25.393440 	 27.082321
Epoch 190 	 24.670715 	 25.566109 	 27.262310
Train loss       : 24.289143
Best valid loss  : 25.021479
Best test loss   : 26.918854
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,909,528
--------------------------------
Total memory      : 35.03 MB
Total Flops       : 326.2 MFlops
Total Mem (Read)  : 31.67 MB
Total Mem (Write) : 24.49 MB
[Supermasks testing]
[Untrained loss : 80.3217]
[Starting training]
Epoch 0 	 69.783211 	 56.091072 	 56.122997
Epoch 10 	 43.690834 	 42.418823 	 44.018589
Epoch 20 	 36.711243 	 32.886662 	 34.825275
Epoch 30 	 34.466766 	 31.603985 	 33.694008
Epoch 40 	 32.834930 	 31.942808 	 33.845039
Epoch 50 	 31.557652 	 29.635208 	 31.459301
Epoch 60 	 30.875334 	 28.789268 	 30.662008
Epoch 70 	 29.433477 	 27.568127 	 29.257202
Epoch 80 	 28.690660 	 27.474442 	 29.105570
Epoch 90 	 27.827662 	 27.053131 	 28.638763
Epoch 100 	 27.084988 	 26.941416 	 28.593327
Epoch 110 	 28.227146 	 27.745716 	 29.416334
Epoch 120 	 26.603697 	 26.482927 	 28.041870
Epoch 130 	 26.208138 	 26.110477 	 27.661226
Epoch 140 	 25.631958 	 26.259335 	 27.827089
Epoch 150 	 25.259415 	 25.611307 	 27.159740
Epoch 160 	 24.531530 	 25.303284 	 26.808060
Epoch 170 	 24.213652 	 24.892525 	 26.545172
Epoch 180 	 24.071531 	 24.927637 	 26.575565
Epoch 190 	 23.952494 	 24.901276 	 26.803410
Train loss       : 23.843409
Best valid loss  : 24.606754
Best test loss   : 26.474771
Pruning          : 0.72
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,898,752
--------------------------------
Total memory      : 29.65 MB
Total Flops       : 171.22 MFlops
Total Mem (Read)  : 22.92 MB
Total Mem (Write) : 17.34 MB
[Supermasks testing]
[Untrained loss : 88.5503]
[Starting training]
Epoch 0 	 72.083740 	 57.210251 	 57.213787
Epoch 10 	 49.556252 	 43.871346 	 44.908428
Epoch 20 	 550825.125000 	 45.024044 	 46.605099
Epoch 30 	 44.245392 	 39.488155 	 41.187660
Epoch 40 	 42.528442 	 38.327332 	 40.193634
Epoch 50 	 40.535851 	 36.122120 	 38.333019
Epoch 60 	 37.501118 	 33.999405 	 36.188560
Epoch 70 	 36.955082 	 33.385319 	 35.524616
Epoch 80 	 36.182617 	 33.253357 	 35.208477
Epoch 90 	 36.035126 	 35.543667 	 37.064838
Epoch 100 	 35.691864 	 32.596191 	 34.814152
Epoch 110 	 33.625660 	 32.512661 	 35.068855
Epoch 120 	 32.721649 	 31.405708 	 33.568039
Epoch 130 	 32.195919 	 31.548351 	 33.787327
Epoch 140 	 31.317171 	 30.535892 	 32.830853
Epoch 150 	 30.627777 	 30.105444 	 32.336536
Epoch 160 	 29.490801 	 29.049492 	 31.359837
Epoch 170 	 29.050701 	 30.115971 	 32.281345
Epoch 180 	 28.566675 	 28.514063 	 30.779335
Epoch 190 	 29.204691 	 28.641169 	 30.872543
Train loss       : 27.637154
Best valid loss  : 27.863945
Best test loss   : 29.926779
Pruning          : 0.52
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,282,494
--------------------------------
Total memory      : 25.75 MB
Total Flops       : 89.46 MFlops
Total Mem (Read)  : 17.01 MB
Total Mem (Write) : 12.62 MB
[Supermasks testing]
[Untrained loss : 100.3276]
[Starting training]
Epoch 0 	 76.091072 	 59.588482 	 60.671402
Epoch 10 	 44.431408 	 39.784714 	 41.799648
Epoch 20 	 40.958160 	 36.521000 	 38.451717
Epoch 30 	 36.523132 	 34.662247 	 36.511391
Epoch 40 	 34.857975 	 33.143425 	 34.676987
Epoch 50 	 33.502300 	 31.747585 	 33.754669
Epoch 60 	 32.265873 	 30.502171 	 32.293411
Epoch 70 	 31.429611 	 29.879633 	 31.668282
Epoch 80 	 30.086794 	 29.007889 	 30.673323
Epoch 90 	 29.647406 	 28.563902 	 30.351944
Epoch 100 	 35.325760 	 29.587341 	 31.229034
Epoch 110 	 29.646526 	 37.526505 	 38.050678
Epoch 120 	 28.096449 	 28.242542 	 29.668051
Epoch 130 	 27.406223 	 27.203558 	 28.731035
Epoch 140 	 27.098953 	 26.991392 	 28.488863
Epoch 150 	 26.996706 	 26.816753 	 28.179937
Epoch 160 	 27.620844 	 26.802021 	 28.186567
Epoch 170 	 25.827051 	 26.301035 	 27.828312
Epoch 180 	 25.699165 	 25.813959 	 27.468582
Epoch 190 	 25.517635 	 25.901741 	 27.401022
Train loss       : 25.276260
Best valid loss  : 25.591833
Best test loss   : 27.542645
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 887,398
--------------------------------
Total memory      : 22.89 MB
Total Flops       : 46.15 MFlops
Total Mem (Read)  : 12.89 MB
Total Mem (Write) : 9.4 MB
[Supermasks testing]
[Untrained loss : 75.9665]
[Starting training]
Epoch 0 	 72.014130 	 57.935471 	 58.618107
Epoch 10 	 49.699627 	 45.480045 	 45.944859
Epoch 20 	 46.696877 	 44.756958 	 45.721935
Epoch 30 	 43.661362 	 41.241493 	 42.953560
Epoch 40 	 41.543900 	 36.740025 	 38.476494
Epoch 50 	 40.486820 	 37.756599 	 39.518646
Epoch 60 	 39.036842 	 35.078583 	 36.916706
Epoch 70 	 38.732758 	 34.347832 	 35.889740
Epoch 80 	 36.832748 	 38.987576 	 39.851219
Epoch 90 	 35.769180 	 36.855900 	 38.601231
Epoch 100 	 34.814381 	 32.528290 	 34.235600
Epoch 110 	 33.583202 	 31.682600 	 33.378708
Epoch 120 	 32.875458 	 31.079870 	 32.859783
Epoch 130 	 32.252411 	 30.602850 	 32.280937
Epoch 140 	 30.756491 	 30.999775 	 32.967598
Epoch 150 	 30.426546 	 29.073713 	 31.487034
Epoch 160 	 29.853527 	 30.211952 	 31.849096
Epoch 170 	 30.463247 	 29.917038 	 31.497833
Epoch 180 	 29.047850 	 28.912880 	 30.677584
Epoch 190 	 28.545124 	 28.433165 	 30.167410
Train loss       : 28.099718
Best valid loss  : 27.889557
Best test loss   : 29.681023
Pruning          : 0.27
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 630,951
--------------------------------
Total memory      : 20.84 MB
Total Flops       : 23.99 MFlops
Total Mem (Read)  : 10.04 MB
Total Mem (Write) : 7.21 MB
[Supermasks testing]
[Untrained loss : 92.0382]
[Starting training]
Epoch 0 	 75.642128 	 68.863281 	 68.359108
Epoch 10 	 47.027527 	 44.923931 	 44.975616
Epoch 20 	 43.948395 	 41.653687 	 42.292240
Epoch 30 	 41.763485 	 39.429317 	 40.694405
Epoch 40 	 40.003498 	 38.321899 	 39.841167
Epoch 50 	 39.141068 	 38.006931 	 39.596462
Epoch 60 	 37.334938 	 36.629372 	 37.753036
Epoch 70 	 37.018806 	 35.837410 	 37.246559
Epoch 80 	 35.731083 	 35.253555 	 36.381569
Epoch 90 	 34.506363 	 34.883953 	 35.971325
Epoch 100 	 34.239620 	 34.684017 	 35.908070
Epoch 110 	 34.462612 	 34.736420 	 35.726688
Epoch 120 	 33.998890 	 34.117237 	 35.463345
Epoch 130 	 32.689560 	 33.743198 	 35.056835
Epoch 140 	 32.929737 	 33.067787 	 34.675083
Epoch 150 	 32.286079 	 33.874977 	 35.039074
Epoch 160 	 30.973997 	 32.895390 	 34.228424
Epoch 170 	 30.360518 	 32.370388 	 33.524368
Epoch 180 	 30.086069 	 32.392700 	 33.512039
Epoch 190 	 29.930048 	 32.355682 	 33.322506
Train loss       : 29.861067
Best valid loss  : 32.102024
Best test loss   : 33.331577
Pruning          : 0.19
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 461,817
--------------------------------
Total memory      : 19.39 MB
Total Flops       : 12.83 MFlops
Total Mem (Read)  : 8.07 MB
Total Mem (Write) : 5.74 MB
[Supermasks testing]
[Untrained loss : 86.1386]
[Starting training]
Epoch 0 	 74.684113 	 61.193466 	 62.591461
Epoch 10 	 51.598328 	 48.871258 	 49.222065
Epoch 20 	 46.407696 	 42.662476 	 43.264393
Epoch 30 	 43.520439 	 41.569672 	 42.264351
Epoch 40 	 41.640030 	 40.185577 	 41.321205
Epoch 50 	 40.019806 	 39.240322 	 40.126690
Epoch 60 	 39.324726 	 39.000328 	 40.024467
Epoch 70 	 37.367920 	 37.742039 	 38.558598
Epoch 80 	 36.833000 	 37.373676 	 38.678535
Epoch 90 	 36.303783 	 36.477703 	 37.959282
Epoch 100 	 36.026947 	 36.813461 	 37.801216
Epoch 110 	 35.266609 	 36.998276 	 37.963089
Epoch 120 	 34.692760 	 35.774086 	 37.038956
Epoch 130 	 34.601963 	 36.088882 	 37.032959
Epoch 140 	 34.395386 	 36.110500 	 36.959896
Epoch 150 	 34.297520 	 36.192352 	 36.943832
[Model stopped early]
Train loss       : 34.199902
Best valid loss  : 35.651451
Best test loss   : 36.994690
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 348,567
--------------------------------
Total memory      : 18.35 MB
Total Flops       : 7.06 MFlops
Total Mem (Read)  : 6.7 MB
Total Mem (Write) : 4.71 MB
[Supermasks testing]
[Untrained loss : 89.5516]
[Starting training]
Epoch 0 	 77.086166 	 63.664944 	 65.398003
Epoch 10 	 51.255100 	 46.323948 	 46.581562
Epoch 20 	 47.293003 	 44.821033 	 45.534756
Epoch 30 	 45.130604 	 42.457657 	 43.426556
Epoch 40 	 43.067490 	 40.856125 	 41.729992
Epoch 50 	 42.309605 	 40.869835 	 41.411331
Epoch 60 	 41.375443 	 40.180485 	 40.297005
Epoch 70 	 40.033195 	 39.660873 	 40.315151
Epoch 80 	 39.340641 	 39.617069 	 39.840141
Epoch 90 	 39.020962 	 39.143898 	 39.283630
Epoch 100 	 38.574787 	 38.982121 	 39.234882
Epoch 110 	 38.567226 	 38.801743 	 38.983791
Epoch 120 	 38.034363 	 38.609142 	 38.667450
Epoch 130 	 37.609665 	 38.576530 	 39.001007
Epoch 140 	 37.282589 	 38.202648 	 38.398849
Epoch 150 	 37.283405 	 37.844570 	 38.249863
Epoch 160 	 37.080357 	 37.858433 	 38.413219
Epoch 170 	 36.821972 	 37.638451 	 38.028954
Epoch 180 	 36.620754 	 37.754826 	 38.401474
Epoch 190 	 36.511337 	 37.739883 	 37.993034
Train loss       : 36.256527
Best valid loss  : 37.245075
Best test loss   : 37.702507
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 272,819
--------------------------------
Total memory      : 17.56 MB
Total Flops       : 3.79 MFlops
Total Mem (Read)  : 5.68 MB
Total Mem (Write) : 3.94 MB
[Supermasks testing]
[Untrained loss : 98.0776]
[Starting training]
Epoch 0 	 79.817833 	 69.218330 	 71.407211
Epoch 10 	 51.371471 	 47.285866 	 47.061268
Epoch 20 	 48.607441 	 45.794697 	 45.934563
Epoch 30 	 47.127789 	 45.142365 	 45.539040
Epoch 40 	 46.460705 	 44.358990 	 44.450142
Epoch 50 	 45.482635 	 43.257675 	 43.076752
Epoch 60 	 44.968365 	 42.934113 	 43.017788
Epoch 70 	 44.699596 	 41.790630 	 42.844330
Epoch 80 	 43.842754 	 42.622910 	 42.605373
Epoch 90 	 42.473434 	 41.443916 	 41.925724
Epoch 100 	 42.500900 	 41.044357 	 41.929012
Epoch 110 	 42.108921 	 41.385052 	 41.428280
Epoch 120 	 41.924068 	 40.835094 	 41.569111
Epoch 130 	 41.233959 	 40.372650 	 41.206863
Epoch 140 	 41.010098 	 40.301678 	 41.399826
Epoch 150 	 40.861179 	 40.651806 	 41.412342
Epoch 160 	 40.671894 	 39.982063 	 41.144375
Epoch 170 	 40.853989 	 39.727341 	 41.333897
Epoch 180 	 40.536709 	 40.051579 	 41.318546
Epoch 190 	 40.421795 	 39.664776 	 41.150730
Train loss       : 40.248962
Best valid loss  : 39.664776
Best test loss   : 41.150730
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 220,983
--------------------------------
Total memory      : 17.11 MB
Total Flops       : 2.56 MFlops
Total Mem (Read)  : 5.08 MB
Total Mem (Write) : 3.52 MB
[Supermasks testing]
[Untrained loss : 83.5289]
[Starting training]
Epoch 0 	 77.611252 	 68.784721 	 71.210052
Epoch 10 	 54.763336 	 51.687035 	 50.992073
Epoch 20 	 50.405300 	 47.582916 	 47.131626
Epoch 30 	 48.550762 	 47.032990 	 46.360012
Epoch 40 	 47.771881 	 45.304836 	 45.354519
Epoch 50 	 46.964523 	 45.252960 	 44.937695
Epoch 60 	 46.367764 	 44.036182 	 44.452084
Epoch 70 	 45.859875 	 43.800953 	 43.880104
Epoch 80 	 45.229221 	 43.708473 	 44.336895
Epoch 90 	 45.164696 	 43.980583 	 44.668198
Epoch 100 	 44.808075 	 43.360493 	 43.573826
Epoch 110 	 44.329956 	 43.818909 	 44.602867
Epoch 120 	 43.568386 	 42.974457 	 43.258789
Epoch 130 	 42.758614 	 42.773911 	 42.737789
[Model stopped early]
Train loss       : 42.728012
Best valid loss  : 42.311207
Best test loss   : 43.185020
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 185,398
--------------------------------
Total memory      : 16.72 MB
Total Flops       : 1.66 MFlops
Total Mem (Read)  : 4.58 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 96.6152]
[Starting training]
Epoch 0 	 79.812500 	 68.891846 	 71.060265
Epoch 10 	 56.018032 	 53.318649 	 54.148861
Epoch 20 	 53.807236 	 50.732597 	 49.988068
Epoch 30 	 52.705727 	 50.261120 	 50.791859
Epoch 40 	 51.201641 	 48.602173 	 47.554108
Epoch 50 	 50.235420 	 46.467773 	 46.883987
Epoch 60 	 50.007866 	 45.920792 	 46.236248
Epoch 70 	 49.514626 	 45.688152 	 45.921219
Epoch 80 	 48.853436 	 45.840580 	 45.827541
Epoch 90 	 48.538528 	 45.696312 	 46.190395
Epoch 100 	 48.182873 	 45.293877 	 45.282589
Epoch 110 	 48.114738 	 44.806152 	 44.941078
Epoch 120 	 47.182281 	 44.295769 	 44.698597
Epoch 130 	 46.932423 	 43.969025 	 44.793434
Epoch 140 	 46.933231 	 44.322460 	 44.463436
Epoch 150 	 46.652378 	 44.305103 	 44.420113
Epoch 160 	 46.292950 	 43.638821 	 44.111259
Epoch 170 	 46.292381 	 44.234608 	 44.609043
Epoch 180 	 45.580708 	 44.038193 	 44.245205
Epoch 190 	 45.810570 	 43.814705 	 43.969196
[Model stopped early]
Train loss       : 45.713966
Best valid loss  : 43.255421
Best test loss   : 43.919395
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 156,046
--------------------------------
Total memory      : 16.63 MB
Total Flops       : 1.63 MFlops
Total Mem (Read)  : 4.41 MB
Total Mem (Write) : 3.08 MB
[Supermasks testing]
[Untrained loss : 80.9723]
[Starting training]
Epoch 0 	 77.265625 	 67.470573 	 69.226883
Epoch 10 	 57.535297 	 53.364506 	 53.509331
Epoch 20 	 55.294018 	 51.674007 	 51.448128
Epoch 30 	 54.073654 	 52.797874 	 54.010410
Epoch 40 	 51.868828 	 48.053432 	 48.137154
Epoch 50 	 51.303482 	 48.517220 	 48.995567
Epoch 60 	 50.438068 	 46.574944 	 46.505020
Epoch 70 	 49.712440 	 46.357243 	 46.776966
Epoch 80 	 49.510483 	 48.332268 	 47.700596
Epoch 90 	 49.209599 	 46.217663 	 46.301399
Epoch 100 	 48.612309 	 45.621647 	 45.530800
Epoch 110 	 48.609047 	 45.634785 	 45.522320
Epoch 120 	 48.135258 	 45.228638 	 45.237194
Epoch 130 	 47.964458 	 45.262531 	 45.333405
Epoch 140 	 48.125336 	 45.279606 	 45.523991
Epoch 150 	 48.004524 	 45.085567 	 45.249470
Epoch 160 	 47.810909 	 45.587818 	 45.316383
[Model stopped early]
Train loss       : 47.285328
Best valid loss  : 44.774017
Best test loss   : 45.264767
Pruning          : 0.03
[Performing one full cumulative epoch]
Traceback (most recent call last):
  File "main.py", line 261, in <module>
    model = pruning.reset(model)
  File "/scratch/esling/lottery/pruning.py", line 781, in reset
    replace_recurrent(m, l, m.unprune_idx[l], prev_kept)
  File "/scratch/esling/lottery/pruning.py", line 752, in replace_recurrent
    cur_ih = nn.Parameter(cur_ih[rep_id0])#torch.from_numpy(cur_ih[rep_id0]).to(self.args.device))
IndexError: too many indices for tensor of dimension 2
