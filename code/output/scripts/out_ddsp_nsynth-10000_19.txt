Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41146344.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, six, future, torch, torchvision, tqdm, kiwisolver, python-dateutil, pyparsing, cycler, matplotlib, tensorflow-estimator, astor, protobuf, h5py, keras-applications, grpcio, google-pasta, wrapt, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, werkzeug, oauthlib, chardet, idna, certifi, urllib3, requests, requests-oauthlib, google-auth-oauthlib, absl-py, markdown, tensorboard, termcolor, opt-einsum, gast, keras-preprocessing, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146344.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-27 03:03:16.174929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-27 03:03:16.551869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_ddsp_cnn_xavier_trimming_batchnorm_rewind_global_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41146344.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 85.4295]
[Starting training]
Epoch 0 	 76.181198 	 69.501717 	 71.342537
Epoch 10 	 62.476070 	 57.766754 	 58.145672
Epoch 20 	 50.427132 	 44.711327 	 46.325600
Epoch 30 	 46.224628 	 42.898468 	 43.759129
Epoch 40 	 41.675671 	 38.685791 	 40.673717
Epoch 50 	 38.592922 	 34.468079 	 36.608196
Epoch 60 	 34.717381 	 32.160084 	 34.122307
Epoch 70 	 32.693497 	 30.798716 	 32.644691
Epoch 80 	 30.959393 	 29.500927 	 31.327179
Epoch 90 	 29.616636 	 27.991266 	 29.928488
Epoch 100 	 28.464664 	 27.409033 	 29.117456
Epoch 110 	 32.132912 	 30.684471 	 32.896454
Epoch 120 	 28.393707 	 27.152018 	 28.907860
Epoch 130 	 28.032991 	 26.850399 	 28.678011
Epoch 140 	 27.046791 	 26.333363 	 28.195143
Epoch 150 	 26.641260 	 26.245678 	 28.048752
Epoch 160 	 26.312378 	 26.170902 	 27.816311
Epoch 170 	 25.807981 	 25.564077 	 27.397659
Epoch 180 	 25.537825 	 25.820356 	 27.689255
Epoch 190 	 24.759724 	 25.046167 	 26.995975
Train loss       : 24.603407
Best valid loss  : 24.697454
Best test loss   : 26.818497
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 3,868,092
--------------------------------
Total memory      : 40.26 MB
Total Flops       : 622.57 MFlops
Total Mem (Read)  : 40.63 MB
Total Mem (Write) : 34.15 MB
[Supermasks testing]
[Untrained loss : 47.7412]
[Starting training]
Epoch 0 	 29.058863 	 27.440083 	 29.093170
Epoch 10 	 27.882601 	 27.346167 	 29.237713
Epoch 20 	 26.643057 	 25.996548 	 27.963587
Epoch 30 	 31.128674 	 27.112207 	 28.665012
Epoch 40 	 25.722015 	 25.838175 	 27.630833
Epoch 50 	 26.209166 	 26.285454 	 27.911118
Epoch 60 	 24.970469 	 24.984526 	 26.789551
Epoch 70 	 24.517893 	 24.650455 	 26.306339
Epoch 80 	 23.955139 	 24.079967 	 25.925676
Epoch 90 	 23.121521 	 23.506086 	 25.464485
Epoch 100 	 22.987970 	 23.821350 	 25.519453
Epoch 110 	 22.270676 	 23.341022 	 25.007805
Epoch 120 	 22.275093 	 23.462769 	 25.008308
Epoch 130 	 22.125250 	 23.292511 	 25.040874
Epoch 140 	 21.988308 	 22.867102 	 24.830135
Epoch 150 	 21.960907 	 23.192549 	 24.779049
Epoch 160 	 21.696346 	 23.037823 	 24.686075
Epoch 170 	 21.640791 	 22.785877 	 24.681929
Epoch 180 	 21.468979 	 22.820097 	 24.623829
Epoch 190 	 21.468979 	 22.782408 	 24.587704
[Model stopped early]
Train loss       : 21.452085
Best valid loss  : 22.728029
Best test loss   : 24.689816
Pruning          : 0.72
0.001
0.001
[Current model size]
================================
Total params      : 3,509,653
--------------------------------
Total memory      : 38.66 MB
Total Flops       : 622.44 MFlops
Total Mem (Read)  : 38.21 MB
Total Mem (Write) : 33.09 MB
[Supermasks testing]
[Untrained loss : 350377.4688]
[Starting training]
Epoch 0 	 29.454706 	 27.846613 	 29.914637
Epoch 10 	 27.609837 	 28.180231 	 30.106134
Epoch 20 	 26.749918 	 25.995907 	 27.791664
Epoch 30 	 26.215878 	 27.213289 	 28.778030
Epoch 40 	 27133086.000000 	 30.320564 	 32.311684
Epoch 50 	 27.069281 	 26.307356 	 27.956345
Epoch 60 	 25.996861 	 26.388277 	 28.017300
Epoch 70 	 25.352810 	 25.379934 	 27.193398
Epoch 80 	 24.896185 	 25.220642 	 27.029467
Epoch 90 	 24.654390 	 25.019585 	 26.854506
Epoch 100 	 24.464775 	 24.832211 	 26.887953
Epoch 110 	 24.389698 	 24.841742 	 26.834436
Epoch 120 	 24.320633 	 24.803757 	 26.805450
Epoch 130 	 24.227528 	 24.889126 	 26.742208
Epoch 140 	 24.207293 	 24.794271 	 26.669365
Epoch 150 	 24.199018 	 24.654779 	 26.726879
Epoch 160 	 24.171709 	 24.587934 	 26.764944
Epoch 170 	 24.193823 	 24.769876 	 26.746149
Epoch 180 	 24.109892 	 24.888983 	 26.630859
Epoch 190 	 24.147436 	 24.817928 	 26.711269
Train loss       : 24.155039
Best valid loss  : 24.418131
Best test loss   : 26.684984
Pruning          : 0.52
0.001
0.001
[Current model size]
================================
Total params      : 3,281,948
--------------------------------
Total memory      : 37.51 MB
Total Flops       : 622.34 MFlops
Total Mem (Read)  : 36.58 MB
Total Mem (Write) : 32.32 MB
[Supermasks testing]
[Untrained loss : 72.6405]
[Starting training]
Epoch 0 	 29.751186 	 27.288498 	 29.110113
Epoch 10 	 28.365873 	 27.907930 	 29.551407
Epoch 20 	 26.571085 	 26.338057 	 27.920977
Epoch 30 	 26.059465 	 25.834265 	 27.643106
Epoch 40 	 25.689102 	 25.873665 	 27.512070
Epoch 50 	 25.290194 	 25.185863 	 27.045835
Epoch 60 	 24.976910 	 25.207550 	 26.876539
Epoch 70 	 24.094177 	 24.675144 	 26.350153
Epoch 80 	 23.907425 	 24.616854 	 26.416702
Epoch 90 	 23.745483 	 24.508007 	 26.219736
Epoch 100 	 23.518122 	 24.338297 	 26.058968
Epoch 110 	 23.329403 	 24.273951 	 25.908150
Epoch 120 	 23.260214 	 24.203228 	 25.812122
Epoch 130 	 22.869751 	 23.891113 	 25.706743
Epoch 140 	 22.648819 	 23.847750 	 25.567459
Epoch 150 	 22.468693 	 23.782114 	 25.534843
Epoch 160 	 22.536715 	 23.463305 	 25.500565
Epoch 170 	 22.520439 	 23.664282 	 25.476463
Epoch 180 	 22.436373 	 23.743151 	 25.453520
Epoch 190 	 22.441990 	 23.698538 	 25.440254
Train loss       : 22.430149
Best valid loss  : 23.416197
Best test loss   : 25.455702
Pruning          : 0.37
0.001
0.001
[Current model size]
================================
Total params      : 3,042,823
--------------------------------
Total memory      : 35.55 MB
Total Flops       : 517.24 MFlops
Total Mem (Read)  : 33.96 MB
Total Mem (Write) : 30.62 MB
[Supermasks testing]
[Untrained loss : 59.7379]
[Starting training]
Epoch 0 	 31.303854 	 28.015375 	 30.088402
Epoch 10 	 28.323748 	 27.425756 	 29.180214
Epoch 20 	 27.751596 	 28.426710 	 30.145529
Epoch 30 	 26.334249 	 25.817627 	 27.539129
Epoch 40 	 25.808689 	 25.990479 	 27.500877
Epoch 50 	 25.519802 	 25.379459 	 27.462776
Epoch 60 	 25.219351 	 25.272696 	 27.043243
Epoch 70 	 28.348873 	 27.750311 	 29.614264
Epoch 80 	 24.462473 	 24.626797 	 26.382196
Epoch 90 	 24.365955 	 25.393621 	 26.973768
Epoch 100 	 24.043633 	 24.263851 	 25.835354
Epoch 110 	 22.961906 	 23.483725 	 25.262440
Epoch 120 	 22.337021 	 23.419140 	 25.003859
Epoch 130 	 22.275469 	 23.712002 	 25.330666
Epoch 140 	 22.218664 	 23.281620 	 24.857040
Epoch 150 	 22.112797 	 23.039482 	 24.791706
Epoch 160 	 22.038208 	 23.275164 	 25.034990
Epoch 170 	 21.820755 	 22.992239 	 24.740248
Epoch 180 	 21.736208 	 23.031477 	 24.641356
Epoch 190 	 21.640106 	 23.083633 	 24.654764
Train loss       : 21.628719
Best valid loss  : 22.748953
Best test loss   : 24.621506
Pruning          : 0.27
0.001
0.001
[Current model size]
================================
Total params      : 2,844,961
--------------------------------
Total memory      : 32.83 MB
Total Flops       : 376.59 MFlops
Total Mem (Read)  : 30.66 MB
Total Mem (Write) : 28.07 MB
[Supermasks testing]
[Untrained loss : 73.2040]
[Starting training]
Epoch 0 	 34.922615 	 29.425589 	 31.118124
Epoch 10 	 27.759949 	 26.684515 	 28.483246
Epoch 20 	 27.015820 	 26.491314 	 28.112885
Epoch 30 	 550.065063 	 33.882931 	 40.311367
Epoch 40 	 38.268932 	 31.884163 	 33.620144
Epoch 50 	 505438.375000 	 30.415150 	 33.436134
[Model stopped early]
Train loss       : 28.507349
Best valid loss  : 25.794580
Best test loss   : 27.671467
Pruning          : 0.19
0.001
0.001
[Current model size]
================================
Total params      : 2,681,045
--------------------------------
Total memory      : 27.86 MB
Total Flops       : 125.53 MFlops
Total Mem (Read)  : 25.14 MB
Total Mem (Write) : 23.17 MB
[Supermasks testing]
[Untrained loss : 73.0497]
[Starting training]
Epoch 0 	 60.710899 	 33.298843 	 34.817020
Epoch 10 	 29.153324 	 36.183571 	 39.023136
Epoch 20 	 28.259132 	 27.010746 	 28.992645
Epoch 30 	 27.065565 	 27.631826 	 29.052982
Epoch 40 	 26.481569 	 26.000469 	 27.613962
Epoch 50 	 26.120817 	 25.744869 	 27.209917
Epoch 60 	 25.553373 	 25.418911 	 26.993275
Epoch 70 	 25.397730 	 25.314894 	 27.100925
Epoch 80 	 24.979719 	 24.925949 	 26.611662
Epoch 90 	 23.931610 	 24.336817 	 26.082180
Epoch 100 	 23.778807 	 24.299030 	 25.960417
Epoch 110 	 23.414370 	 24.136543 	 25.846275
Epoch 120 	 23.430590 	 24.234522 	 25.830462
Epoch 130 	 23.285452 	 24.054605 	 25.752882
Epoch 140 	 23.180130 	 24.108984 	 25.730772
[Model stopped early]
Train loss       : 23.152649
Best valid loss  : 23.891905
Best test loss   : 25.818136
Pruning          : 0.14
0.001
0.001
[Current model size]
================================
Total params      : 2,604,911
--------------------------------
Total memory      : 27.06 MB
Total Flops       : 114.6 MFlops
Total Mem (Read)  : 24.14 MB
Total Mem (Write) : 22.46 MB
[Supermasks testing]
[Untrained loss : 74.1253]
[Starting training]
Epoch 0 	 58.202831 	 45.370613 	 45.877571
Epoch 10 	 490.277069 	 37.439518 	 39.764271
Epoch 20 	 34.777119 	 31.336466 	 33.436169
Epoch 30 	 32.058056 	 29.901484 	 32.098869
Epoch 40 	 31.697632 	 29.883129 	 31.806736
Epoch 50 	 29.716726 	 28.795231 	 30.825802
Epoch 60 	 28.759096 	 28.100918 	 29.847925
Epoch 70 	 28.199444 	 26.870146 	 28.987234
Epoch 80 	 27.493294 	 26.847191 	 28.619871
Epoch 90 	 26.049610 	 25.653917 	 27.477011
Epoch 100 	 25.715357 	 25.718222 	 27.410137
Epoch 110 	 25.410551 	 25.457523 	 27.176559
Epoch 120 	 25.063135 	 25.247408 	 27.077900
Epoch 130 	 24.855354 	 25.266344 	 26.849636
Epoch 140 	 24.103689 	 24.870615 	 26.550522
Epoch 150 	 23.917845 	 24.398533 	 26.371634
Epoch 160 	 23.671156 	 24.445238 	 26.229191
Epoch 170 	 23.536692 	 24.413116 	 26.118557
Epoch 180 	 23.365185 	 24.286531 	 26.081438
Epoch 190 	 23.333439 	 24.407782 	 26.064434
Train loss       : 23.323286
Best valid loss  : 23.984020
Best test loss   : 26.014957
Pruning          : 0.10
0.001
0.001
[Current model size]
================================
Total params      : 2,557,164
--------------------------------
Total memory      : 25.60 MB
Total Flops       : 93.39 MFlops
Total Mem (Read)  : 22.56 MB
Total Mem (Write) : 21.06 MB
[Supermasks testing]
[Untrained loss : 76.7008]
[Starting training]
Epoch 0 	 56.723343 	 49.293667 	 49.791962
Epoch 10 	 36.295376 	 32.930389 	 34.836735
Epoch 20 	 33.071060 	 30.905788 	 32.984360
Epoch 30 	 31.899025 	 30.579363 	 32.442177
Epoch 40 	 30.842102 	 29.501099 	 31.001785
Epoch 50 	 29.825468 	 29.183834 	 30.733980
Epoch 60 	 29.110939 	 28.056219 	 29.874048
Epoch 70 	 28.412971 	 27.929150 	 29.698992
Epoch 80 	 27.709320 	 26.910675 	 28.555012
Epoch 90 	 27.383499 	 26.441221 	 28.192038
Epoch 100 	 27.554573 	 26.385696 	 28.342728
Epoch 110 	 26.703270 	 26.743298 	 28.490034
Epoch 120 	 26.370708 	 26.368774 	 28.005890
Epoch 130 	 26.095427 	 25.479460 	 27.495539
Epoch 140 	 25.924385 	 25.762707 	 27.540455
Epoch 150 	 25.000441 	 25.141605 	 26.941706
Epoch 160 	 24.785923 	 25.223307 	 26.856279
Epoch 170 	 24.189337 	 24.810024 	 26.556107
Epoch 180 	 24.205133 	 24.702732 	 26.460495
Epoch 190 	 24.000404 	 24.697098 	 26.346926
Train loss       : 23.823011
Best valid loss  : 24.317675
Best test loss   : 26.224888
Pruning          : 0.07
0.001
0.001
[Current model size]
================================
Total params      : 2,529,413
--------------------------------
Total memory      : 24.39 MB
Total Flops       : 67.88 MFlops
Total Mem (Read)  : 21.24 MB
Total Mem (Write) : 19.85 MB
[Supermasks testing]
[Untrained loss : 165.2187]
[Starting training]
Epoch 0 	 64.812325 	 52.119919 	 52.056366
Epoch 10 	 39.207863 	 36.271824 	 37.993359
Epoch 20 	 36.081814 	 34.357731 	 36.062889
Epoch 30 	 34.439610 	 33.270226 	 35.416668
Epoch 40 	 34.344025 	 31.423178 	 33.410004
Epoch 50 	 33.269497 	 32.021973 	 33.680767
Epoch 60 	 33.132797 	 31.369120 	 33.143517
Epoch 70 	 31.515158 	 29.901899 	 31.699844
Epoch 80 	 30.655308 	 29.810465 	 31.507154
Epoch 90 	 30.002558 	 28.906033 	 30.470564
Epoch 100 	 29.485767 	 28.810249 	 30.415670
Epoch 110 	 29.172628 	 28.403273 	 29.977137
Epoch 120 	 28.851990 	 28.161551 	 29.923965
Epoch 130 	 28.520975 	 28.085346 	 29.804947
Epoch 140 	 28.423077 	 29.461351 	 31.227392
Epoch 150 	 27.550051 	 27.208376 	 28.964357
Epoch 160 	 27.252428 	 26.936152 	 28.733601
Epoch 170 	 27.105444 	 26.935909 	 28.799114
Epoch 180 	 27.027428 	 26.807402 	 28.638874
Epoch 190 	 26.836170 	 26.631594 	 28.508614
Train loss       : 26.810980
Best valid loss  : 26.365257
Best test loss   : 28.529732
Pruning          : 0.05
0.001
0.001
[Current model size]
================================
Total params      : 2,518,692
--------------------------------
Total memory      : 22.82 MB
Total Flops       : 60.29 MFlops
Total Mem (Read)  : 19.65 MB
Total Mem (Write) : 18.3 MB
[Supermasks testing]
[Untrained loss : 160.9715]
[Starting training]
Epoch 0 	 62.217915 	 53.498711 	 53.605797
Epoch 10 	 39.349308 	 36.939102 	 38.318253
Epoch 20 	 36.401516 	 33.510605 	 35.325451
Epoch 30 	 34.794064 	 32.671566 	 34.510750
Epoch 40 	 32.968891 	 30.915461 	 32.957558
Epoch 50 	 32.741566 	 30.667484 	 32.819923
Epoch 60 	 31.794699 	 30.887625 	 32.555664
Epoch 70 	 31.498693 	 99161.359375 	 8441.625977
Epoch 80 	 30.483496 	 53441.039062 	 4249.543457
Epoch 90 	 29.989252 	 44847276.000000 	 3472464.250000
Epoch 100 	 29.521427 	 47504.214844 	 3899.292725
Epoch 110 	 29.347008 	 49.410767 	 32.547882
[Model stopped early]
Train loss       : 29.289860
Best valid loss  : 28.711050
Best test loss   : 48686.121094
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 2,506,360
--------------------------------
Total memory      : 21.37 MB
Total Flops       : 43.25 MFlops
Total Mem (Read)  : 18.16 MB
Total Mem (Write) : 16.86 MB
[Supermasks testing]
[Untrained loss : 391.6999]
[Starting training]
Epoch 0 	 68.627975 	 54.985043 	 54.712692
Epoch 10 	 43.438164 	 40.345329 	 42.440731
Epoch 20 	 39.263344 	 35.076492 	 37.342934
Epoch 30 	 37.253422 	 34.810230 	 36.407906
Epoch 40 	 35.588535 	 34.489056 	 36.188408
Epoch 50 	 34.351452 	 32.922428 	 34.955685
Epoch 60 	 33.690197 	 31.847862 	 33.705132
Epoch 70 	 33.368507 	 31.197411 	 33.560947
Epoch 80 	 33.100323 	 31.224369 	 33.261497
Epoch 90 	 32.332146 	 31.055544 	 32.688160
Epoch 100 	 32.152908 	 30.785294 	 32.702034
Epoch 110 	 32.006413 	 30.891569 	 32.634377
Epoch 120 	 31.793224 	 30.770893 	 32.440010
Epoch 130 	 31.672699 	 30.793728 	 32.477283
Epoch 140 	 31.529198 	 30.441210 	 32.206726
Epoch 150 	 31.436136 	 30.333132 	 32.136738
Epoch 160 	 31.218630 	 30.178482 	 32.034222
Epoch 170 	 31.136509 	 30.253059 	 32.082577
Epoch 180 	 31.086926 	 30.320650 	 32.048923
Epoch 190 	 31.025963 	 30.293152 	 31.984480
Train loss       : 30.907127
Best valid loss  : 29.726837
Best test loss   : 31.958862
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 2,501,953
--------------------------------
Total memory      : 19.98 MB
Total Flops       : 37.19 MFlops
Total Mem (Read)  : 16.75 MB
Total Mem (Write) : 15.47 MB
[Supermasks testing]
[Untrained loss : 200.4368]
[Starting training]
Epoch 0 	 69.418167 	 58.843643 	 58.983746
Epoch 10 	 48.191299 	 45.325546 	 45.500134
Epoch 20 	 44.454453 	 41.152897 	 42.137527
Epoch 30 	 43.741188 	 40.620705 	 41.931152
Epoch 40 	 42.271156 	 39.279224 	 41.051262
Epoch 50 	 43.523415 	 39.601070 	 41.338879
Epoch 60 	 39.918289 	 37.461319 	 38.862415
Epoch 70 	 39.562759 	 37.387825 	 38.903576
Epoch 80 	 41.967358 	 40.198399 	 41.635414
Epoch 90 	 40.133980 	 38.348312 	 39.944138
[Model stopped early]
Train loss       : 39.981274
Best valid loss  : 37.244198
Best test loss   : 39.064621
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 2,499,553
--------------------------------
Total memory      : 18.83 MB
Total Flops       : 34.84 MFlops
Total Mem (Read)  : 15.6 MB
Total Mem (Write) : 14.32 MB
[Supermasks testing]
[Untrained loss : 366.3103]
[Starting training]
Epoch 0 	 73.202576 	 61.195488 	 60.444843
Epoch 10 	 52.360416 	 47.985783 	 47.856789
Epoch 20 	 47.633282 	 47.300194 	 47.562305
Epoch 30 	 45.613014 	 42.745296 	 43.637829
Epoch 40 	 44.229988 	 42.207794 	 42.779774
Epoch 50 	 45.360867 	 41.721909 	 43.369461
Epoch 60 	 43.820839 	 40.689682 	 42.033119
Epoch 70 	 43.948063 	 41.273315 	 42.486835
Epoch 80 	 42.538902 	 40.760563 	 42.843227
Epoch 90 	 44.562908 	 41.535423 	 42.756668
Epoch 100 	 44.138649 	 41.983994 	 42.481613
Epoch 110 	 41.954159 	 40.109840 	 41.268002
[Model stopped early]
Train loss       : 42.261566
Best valid loss  : 38.922100
Best test loss   : 40.802002
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 2,497,879
--------------------------------
Total memory      : 18.22 MB
Total Flops       : 33.39 MFlops
Total Mem (Read)  : 14.97 MB
Total Mem (Write) : 13.7 MB
[Supermasks testing]
[Untrained loss : 365.2357]
[Starting training]
Epoch 0 	 77.356773 	 60.055134 	 61.036934
Epoch 10 	 49.093464 	 47.505764 	 48.802467
Epoch 20 	 46.797035 	 44.294563 	 45.093632
Epoch 30 	 46.120068 	 43.298145 	 43.875889
Epoch 40 	 44.338150 	 42.000069 	 42.762730
Epoch 50 	 44.423004 	 43.688251 	 44.358772
Epoch 60 	 43.110016 	 41.378822 	 42.778286
Epoch 70 	 44.387363 	 41.422497 	 42.610996
Epoch 80 	 50.483555 	 47.424316 	 47.778332
Epoch 90 	 42.576920 	 40.274448 	 41.528557
Epoch 100 	 44.000767 	 41.653042 	 42.514835
[Model stopped early]
Train loss       : 42.334209
Best valid loss  : 39.594872
Best test loss   : 41.000431
Pruning          : 0.01
