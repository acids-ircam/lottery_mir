Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288797.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, future, torch, six, torchvision, tqdm, cycler, pyparsing, kiwisolver, python-dateutil, matplotlib, h5py, keras-applications, keras-preprocessing, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, certifi, chardet, idna, urllib3, requests, requests-oauthlib, google-auth-oauthlib, protobuf, grpcio, werkzeug, markdown, absl-py, tensorboard, google-pasta, termcolor, opt-einsum, tensorflow-estimator, gast, astor, wrapt, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288797.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:52:35.036060: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:52:35.293689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_trimming_info_target_reinit_global_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288797.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7682]
[Starting training]
/localscratch/esling.41288797.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
Epoch 0 	 22.858688 	 0.600504 	 0.608066
Epoch 10 	 21.337446 	 0.504871 	 0.514992
Epoch 20 	 20.283001 	 0.382535 	 0.395805
Epoch 30 	 18.825491 	 0.245697 	 0.260144
Epoch 40 	 17.966593 	 0.186641 	 0.201488
Epoch 50 	 17.400105 	 0.165384 	 0.173129
Epoch 60 	 17.013350 	 0.139733 	 0.148983
Epoch 70 	 16.775906 	 0.133922 	 0.146244
Epoch 80 	 16.609390 	 0.131047 	 0.140603
Epoch 90 	 16.492987 	 0.131960 	 0.145523
Epoch 100 	 16.404030 	 0.131653 	 0.143744
Epoch 110 	 16.296396 	 0.130739 	 0.142246
Epoch 120 	 16.170139 	 0.127923 	 0.138835
Epoch 130 	 16.103230 	 0.125576 	 0.137716
Epoch 140 	 16.073345 	 0.123355 	 0.138001
Epoch 150 	 16.052572 	 0.125415 	 0.138411
Epoch 160 	 16.044130 	 0.128224 	 0.136667
Epoch 170 	 16.025894 	 0.125317 	 0.137551
Epoch 180 	 16.021900 	 0.124997 	 0.137436
Epoch 190 	 16.015472 	 0.124499 	 0.137253
Train loss       : 16.016129
Best valid loss  : 0.120785
Best test loss   : 0.137893
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,534,866
--------------------------------
Total memory      : 21.12 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 29.93 MB
Total Mem (Write) : 16.43 MB
[Supermasks testing]
[Untrained loss : 0.8293]
[Starting training]
Epoch 0 	 22.896439 	 0.620120 	 0.630380
Epoch 10 	 21.692457 	 0.544195 	 0.549941
Epoch 20 	 21.050014 	 0.465604 	 0.481364
Epoch 30 	 19.356371 	 0.286834 	 0.294536
Epoch 40 	 18.113571 	 0.203618 	 0.209141
/localscratch/esling.41288797.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 50 	 17.579782 	 0.160536 	 0.172438
Epoch 60 	 17.216747 	 0.147436 	 0.161363
Epoch 70 	 16.957386 	 0.132361 	 0.144445
Epoch 80 	 16.712820 	 0.124752 	 0.138447
Epoch 90 	 16.557783 	 0.128340 	 0.135661
Epoch 100 	 16.448635 	 0.128904 	 0.136033
Epoch 110 	 16.391083 	 0.121734 	 0.137231
Epoch 120 	 16.221905 	 0.121710 	 0.132096
Epoch 130 	 16.146303 	 0.122672 	 0.130432
[Model stopped early]
Train loss       : 16.138199
Best valid loss  : 0.117984
Best test loss   : 0.134869
Pruning          : 0.75
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,820,663
--------------------------------
Total memory      : 21.12 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 27.2 MB
Total Mem (Write) : 16.42 MB
[Supermasks testing]
[Untrained loss : 0.7694]
[Starting training]
Epoch 0 	 23.049734 	 0.640837 	 0.660810
Epoch 10 	 21.618490 	 0.528643 	 0.535600
Epoch 20 	 20.752146 	 0.415408 	 0.432780
Epoch 30 	 19.271011 	 0.274655 	 0.274413
Epoch 40 	 18.120529 	 0.180685 	 0.184563
Epoch 50 	 17.602608 	 0.162891 	 0.170975
Epoch 60 	 17.286964 	 0.138989 	 0.152989
Epoch 70 	 17.048990 	 0.141982 	 0.151703
Epoch 80 	 16.866024 	 0.138690 	 0.150421
Epoch 90 	 16.652182 	 0.133067 	 0.144024
Epoch 100 	 16.575491 	 0.137640 	 0.145238
Epoch 110 	 16.490191 	 0.134343 	 0.142428
Epoch 120 	 16.415447 	 0.134223 	 0.142621
Epoch 130 	 16.395670 	 0.131473 	 0.142337
[Model stopped early]
Train loss       : 16.389187
Best valid loss  : 0.128791
Best test loss   : 0.143519
Pruning          : 0.56
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,345,175
--------------------------------
Total memory      : 21.11 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 25.38 MB
Total Mem (Write) : 16.42 MB
[Supermasks testing]
[Untrained loss : 0.7613]
[Starting training]
Epoch 0 	 23.299006 	 0.704983 	 0.715767
Epoch 10 	 21.783628 	 0.544527 	 0.550724
Epoch 20 	 20.731543 	 0.427783 	 0.447490
Epoch 30 	 19.408253 	 0.280640 	 0.294451
Epoch 40 	 18.637993 	 0.208350 	 0.220812
Epoch 50 	 18.107880 	 0.181882 	 0.194915
Epoch 60 	 17.778763 	 0.163632 	 0.175454
Epoch 70 	 17.572771 	 0.159628 	 0.172292
Epoch 80 	 17.294836 	 0.149505 	 0.159209
Epoch 90 	 17.134048 	 0.150319 	 0.154142
Epoch 100 	 16.920279 	 0.139839 	 0.150478
Epoch 110 	 16.827963 	 0.139323 	 0.152942
Epoch 120 	 16.751358 	 0.137568 	 0.149455
Epoch 130 	 16.693253 	 0.137328 	 0.147394
Epoch 140 	 16.613201 	 0.135453 	 0.147521
Epoch 150 	 16.595762 	 0.136184 	 0.146906
Epoch 160 	 16.557295 	 0.135124 	 0.142855
Epoch 170 	 16.525028 	 0.134540 	 0.145048
Epoch 180 	 16.507704 	 0.134860 	 0.144198
Epoch 190 	 16.516354 	 0.132220 	 0.143015
Train loss       : 16.517849
Best valid loss  : 0.129739
Best test loss   : 0.144339
Pruning          : 0.42
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,032,308
--------------------------------
Total memory      : 21.11 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 24.18 MB
Total Mem (Write) : 16.42 MB
[Supermasks testing]
[Untrained loss : 0.8017]
[Starting training]
Epoch 0 	 23.391594 	 0.658255 	 0.666829
Epoch 10 	 21.964514 	 0.586290 	 0.587328
Epoch 20 	 21.848101 	 0.569543 	 0.572860
Epoch 30 	 21.294271 	 0.494262 	 0.506437
Epoch 40 	 20.553692 	 0.381167 	 0.392049
Epoch 50 	 20.221659 	 0.352451 	 0.367093
Epoch 60 	 19.703953 	 0.305228 	 0.322115
Epoch 70 	 19.400181 	 0.281901 	 0.300302
Epoch 80 	 19.208530 	 0.264235 	 0.282513
Epoch 90 	 18.946629 	 0.244701 	 0.259142
Epoch 100 	 18.795444 	 0.221008 	 0.241858
Epoch 110 	 18.550949 	 0.205462 	 0.219332
Epoch 120 	 18.439939 	 0.199749 	 0.217204
Epoch 130 	 18.272238 	 0.190136 	 0.209760
Epoch 140 	 18.153008 	 0.186057 	 0.203113
Epoch 150 	 18.074194 	 0.186995 	 0.204637
Epoch 160 	 18.010538 	 0.182591 	 0.200887
Epoch 170 	 17.980907 	 0.183505 	 0.200296
Epoch 180 	 17.967310 	 0.182206 	 0.201223
Epoch 190 	 17.915918 	 0.184463 	 0.198496
Train loss       : 17.873507
Best valid loss  : 0.178291
Best test loss   : 0.199077
Pruning          : 0.32
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,841,056
--------------------------------
Total memory      : 21.10 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 23.45 MB
Total Mem (Write) : 16.41 MB
[Supermasks testing]
[Untrained loss : 0.7235]
[Starting training]
Epoch 0 	 23.581213 	 0.659048 	 0.675794
Epoch 10 	 22.012714 	 0.580911 	 0.586483
Epoch 20 	 21.690348 	 0.534839 	 0.547259
Epoch 30 	 21.494251 	 0.499680 	 0.519627
Epoch 40 	 21.314203 	 0.483846 	 0.496213
Epoch 50 	 21.189234 	 0.461442 	 0.473890
Epoch 60 	 21.023273 	 0.446609 	 0.461698
Epoch 70 	 20.950678 	 0.430326 	 0.446915
Epoch 80 	 20.882284 	 0.423881 	 0.442446
Epoch 90 	 20.745031 	 0.424802 	 0.434016
Epoch 100 	 20.731554 	 0.426723 	 0.435248
Epoch 110 	 20.655735 	 0.416259 	 0.428286
Epoch 120 	 20.640165 	 0.414897 	 0.425510
Epoch 130 	 20.568874 	 0.415610 	 0.420917
Epoch 140 	 20.543499 	 0.401070 	 0.411058
Epoch 150 	 20.489237 	 0.397755 	 0.406102
Epoch 160 	 20.463902 	 0.393449 	 0.403467
Epoch 170 	 20.459871 	 0.393171 	 0.402771
Epoch 180 	 20.443594 	 0.389175 	 0.400374
Epoch 190 	 20.402943 	 0.392189 	 0.401697
Train loss       : 20.442074
Best valid loss  : 0.388638
Best test loss   : 0.398039
Pruning          : 0.24
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,376,472
--------------------------------
Total memory      : 17.70 MB
Total Flops       : 1.91 GFlops
Total Mem (Read)  : 19.03 MB
Total Mem (Write) : 13.77 MB
[Supermasks testing]
[Untrained loss : 0.7174]
[Starting training]
Epoch 0 	 23.901066 	 0.693879 	 0.707065
Epoch 10 	 21.985613 	 0.575077 	 0.580058
Epoch 20 	 21.694910 	 0.524357 	 0.536489
Epoch 30 	 21.467813 	 0.494952 	 0.515388
Epoch 40 	 21.308268 	 0.471688 	 0.490865
Epoch 50 	 21.112352 	 0.444520 	 0.458026
Epoch 60 	 21.043388 	 0.429617 	 0.445989
Epoch 70 	 20.927059 	 0.425350 	 0.435927
Epoch 80 	 20.861179 	 0.409116 	 0.427046
Epoch 90 	 20.757742 	 0.406277 	 0.424809
Epoch 100 	 20.717276 	 0.402761 	 0.418282
Epoch 110 	 20.637009 	 0.399179 	 0.414710
Epoch 120 	 20.606960 	 0.391255 	 0.411512
Epoch 130 	 20.569511 	 0.392946 	 0.406728
Epoch 140 	 20.503038 	 0.380771 	 0.395211
Epoch 150 	 20.437374 	 0.365557 	 0.382225
Epoch 160 	 20.389141 	 0.367491 	 0.382379
Epoch 170 	 20.364225 	 0.359050 	 0.380049
Epoch 180 	 20.330046 	 0.360055 	 0.378127
Epoch 190 	 20.283987 	 0.359273 	 0.374946
Train loss       : 20.226362
Best valid loss  : 0.350595
Best test loss   : 0.376886
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,376,472
--------------------------------
Total memory      : 17.70 MB
Total Flops       : 1.91 GFlops
Total Mem (Read)  : 19.03 MB
Total Mem (Write) : 13.77 MB
[Supermasks testing]
[Untrained loss : 0.7641]
[Starting training]
Epoch 0 	 23.761702 	 0.729710 	 0.746917
Epoch 10 	 21.988146 	 0.584537 	 0.584883
Epoch 20 	 21.768457 	 0.541662 	 0.555524
Epoch 30 	 21.511597 	 0.515118 	 0.524943
Epoch 40 	 21.361448 	 0.489974 	 0.503678
Epoch 50 	 21.225008 	 0.478111 	 0.478076
Epoch 60 	 21.065128 	 0.438612 	 0.448953
Epoch 70 	 20.887289 	 0.421280 	 0.430973
Epoch 80 	 20.827166 	 0.413715 	 0.423396
Epoch 90 	 20.758175 	 0.411607 	 0.420166
Epoch 100 	 20.681625 	 0.397277 	 0.411643
Epoch 110 	 20.565121 	 0.395010 	 0.402104
Epoch 120 	 20.563522 	 0.386083 	 0.395880
Epoch 130 	 20.561749 	 0.385088 	 0.391757
Epoch 140 	 20.492659 	 0.377488 	 0.385669
Epoch 150 	 20.457569 	 0.377720 	 0.389943
Epoch 160 	 20.438349 	 0.371827 	 0.387254
Epoch 170 	 20.383083 	 0.381553 	 0.386749
Epoch 180 	 20.363243 	 0.375756 	 0.383000
Epoch 190 	 20.329994 	 0.371055 	 0.385459
Train loss       : 20.301825
Best valid loss  : 0.364704
Best test loss   : 0.383539
Pruning          : 0.13
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,376,472
--------------------------------
Total memory      : 17.70 MB
Total Flops       : 1.91 GFlops
Total Mem (Read)  : 19.03 MB
Total Mem (Write) : 13.77 MB
[Supermasks testing]
[Untrained loss : 0.7806]
[Starting training]
Epoch 0 	 23.820889 	 0.772843 	 0.781933
Epoch 10 	 21.969954 	 0.568709 	 0.579382
Epoch 20 	 21.644436 	 0.520720 	 0.531758
Epoch 30 	 21.443222 	 0.489941 	 0.511470
Epoch 40 	 21.228600 	 0.456726 	 0.471039
Epoch 50 	 21.048376 	 0.430853 	 0.444596
Epoch 60 	 20.848923 	 0.417300 	 0.430878
Epoch 70 	 20.678749 	 0.384096 	 0.396486
Epoch 80 	 20.594913 	 0.372829 	 0.389077
Epoch 90 	 20.517317 	 0.367578 	 0.382490
Epoch 100 	 20.434971 	 0.359034 	 0.373426
Epoch 110 	 20.353989 	 0.354018 	 0.369282
Epoch 120 	 20.282312 	 0.347981 	 0.362571
Epoch 130 	 20.274136 	 0.348646 	 0.362377
Epoch 140 	 20.230661 	 0.344385 	 0.354934
Epoch 150 	 20.159126 	 0.331685 	 0.353285
Epoch 160 	 20.131451 	 0.332707 	 0.351611
Epoch 170 	 20.066998 	 0.329798 	 0.349955
Epoch 180 	 20.059195 	 0.333040 	 0.347682
Epoch 190 	 19.978085 	 0.331076 	 0.348738
Train loss       : 19.984276
Best valid loss  : 0.327690
Best test loss   : 0.344642
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,122,200
--------------------------------
Total memory      : 15.55 MB
Total Flops       : 1.5 GFlops
Total Mem (Read)  : 16.39 MB
Total Mem (Write) : 12.1 MB
[Supermasks testing]
[Untrained loss : 0.7302]
[Starting training]
Epoch 0 	 23.786455 	 0.715806 	 0.726886
Epoch 10 	 22.032473 	 0.584722 	 0.592615
Epoch 20 	 21.820856 	 0.546444 	 0.557572
Epoch 30 	 21.653322 	 0.520074 	 0.538261
Epoch 40 	 21.489126 	 0.506118 	 0.518830
Epoch 50 	 21.362976 	 0.492019 	 0.505792
Epoch 60 	 21.204214 	 0.466469 	 0.480172
Epoch 70 	 21.103323 	 0.456041 	 0.469696
Epoch 80 	 21.013065 	 0.443895 	 0.454444
Epoch 90 	 20.910488 	 0.434687 	 0.450006
Epoch 100 	 20.858364 	 0.425225 	 0.444975
Epoch 110 	 20.848030 	 0.425574 	 0.437799
Epoch 120 	 20.762329 	 0.417695 	 0.433447
Epoch 130 	 20.710873 	 0.415867 	 0.432645
Epoch 140 	 20.660904 	 0.418092 	 0.431592
Epoch 150 	 20.680252 	 0.416907 	 0.429506
Epoch 160 	 20.603706 	 0.417411 	 0.427129
Epoch 170 	 20.586283 	 0.410774 	 0.425431
Epoch 180 	 20.589725 	 0.408451 	 0.424870
Epoch 190 	 20.561686 	 0.410019 	 0.423692
Train loss       : 20.575050
Best valid loss  : 0.400971
Best test loss   : 0.421152
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 859,776
--------------------------------
Total memory      : 13.06 MB
Total Flops       : 1.08 GFlops
Total Mem (Read)  : 13.45 MB
Total Mem (Write) : 10.16 MB
[Supermasks testing]
[Untrained loss : 0.7801]
[Starting training]
Epoch 0 	 23.647287 	 0.733874 	 0.743173
Epoch 10 	 21.951666 	 0.566511 	 0.573210
Epoch 20 	 21.735310 	 0.539147 	 0.550676
Epoch 30 	 21.546024 	 0.511966 	 0.524288
Epoch 40 	 21.324503 	 0.482222 	 0.492157
Epoch 50 	 21.184811 	 0.472001 	 0.480685
slurmstepd: error: *** JOB 41288797 ON cdr348 CANCELLED AT 2020-04-29T16:49:02 DUE TO TIME LIMIT ***
