Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288814.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, pillow-simd, six, torchvision, tqdm, kiwisolver, python-dateutil, cycler, pyparsing, matplotlib, termcolor, protobuf, h5py, keras-applications, absl-py, grpcio, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, chardet, certifi, urllib3, idna, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, werkzeug, tensorboard, tensorflow-estimator, gast, keras-preprocessing, astor, wrapt, opt-einsum, google-pasta, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288814.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:52:46.477259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:52:46.490234: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_trimming_information_rewind_global_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288814.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7936]
[Starting training]
/localscratch/esling.41288814.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
Epoch 0 	 22.942362 	 0.646406 	 0.644564
Epoch 10 	 21.565235 	 0.550562 	 0.543098
Epoch 20 	 20.885105 	 0.477265 	 0.469571
Epoch 30 	 19.717148 	 0.335981 	 0.339324
Epoch 40 	 18.527645 	 0.253846 	 0.250430
Epoch 50 	 17.840691 	 0.205854 	 0.206267
Epoch 60 	 17.414724 	 0.178476 	 0.183123
Epoch 70 	 17.147457 	 0.163250 	 0.168018
Epoch 80 	 16.963980 	 0.159492 	 0.162173
Epoch 90 	 16.813484 	 0.158105 	 0.163586
Epoch 100 	 16.613886 	 0.155923 	 0.158063
Epoch 110 	 16.485044 	 0.147908 	 0.149868
Epoch 120 	 16.396494 	 0.152313 	 0.154680
Epoch 130 	 16.329660 	 0.153903 	 0.151576
Epoch 140 	 16.291264 	 0.153837 	 0.151001
[Model stopped early]
Train loss       : 16.256388
Best valid loss  : 0.147045
Best test loss   : 0.150613
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,551,116
--------------------------------
Total memory      : 21.12 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 29.99 MB
Total Mem (Write) : 16.43 MB
[Supermasks testing]
[Untrained loss : 0.2698]
[Starting training]
Epoch 0 	 17.750265 	 0.181776 	 0.185658
Epoch 10 	 17.138910 	 0.155015 	 0.161708
Epoch 20 	 16.842031 	 0.151549 	 0.152947
Epoch 30 	 16.664610 	 0.148885 	 0.148294
Epoch 40 	 16.548077 	 0.143850 	 0.147954
Epoch 50 	 16.483675 	 0.145410 	 0.146463
Epoch 60 	 16.418854 	 0.142159 	 0.145002
Epoch 70 	 16.400217 	 0.146551 	 0.146099
Epoch 80 	 16.372782 	 0.143616 	 0.144974
Epoch 90 	 16.359776 	 0.142285 	 0.144558
[Model stopped early]
Train loss       : 16.359776
Best valid loss  : 0.138898
Best test loss   : 0.144844
Pruning          : 0.75
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,829,680
--------------------------------
Total memory      : 21.12 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 27.23 MB
Total Mem (Write) : 16.42 MB
[Supermasks testing]
[Untrained loss : 0.3484]
[Starting training]
Epoch 0 	 18.354563 	 0.199468 	 0.202302
Epoch 10 	 17.375921 	 0.153323 	 0.158710
Epoch 20 	 17.159775 	 0.152214 	 0.152644
Epoch 30 	 16.886518 	 0.147958 	 0.150875
Epoch 40 	 16.771198 	 0.147455 	 0.148812
Epoch 50 	 16.683491 	 0.147118 	 0.149442
Epoch 60 	 16.635139 	 0.143080 	 0.145517
Epoch 70 	 16.513769 	 0.138765 	 0.146580
Epoch 80 	 16.465885 	 0.144739 	 0.146357
Epoch 90 	 16.444332 	 0.140596 	 0.144468
Epoch 100 	 16.401834 	 0.137906 	 0.143411
Epoch 110 	 16.396702 	 0.140233 	 0.143602
Epoch 120 	 16.404757 	 0.140325 	 0.143781
[Model stopped early]
Train loss       : 16.402712
Best valid loss  : 0.136856
Best test loss   : 0.145614
Pruning          : 0.56
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,366,000
--------------------------------
Total memory      : 21.11 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 25.46 MB
Total Mem (Write) : 16.42 MB
[Supermasks testing]
[Untrained loss : 0.4317]
[Starting training]
Epoch 0 	 19.226793 	 0.223865 	 0.225838
Epoch 10 	 17.808752 	 0.159263 	 0.161587
Epoch 20 	 17.503872 	 0.149290 	 0.157347
Epoch 30 	 17.370253 	 0.151071 	 0.159233
Epoch 40 	 17.134348 	 0.146844 	 0.150933
Epoch 50 	 17.033175 	 0.150593 	 0.151798
Epoch 60 	 16.929201 	 0.149868 	 0.149593
Epoch 70 	 16.831926 	 0.150599 	 0.149206
[Model stopped early]
Train loss       : 16.817062
Best valid loss  : 0.144143
Best test loss   : 0.148271
Pruning          : 0.42
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,050,524
--------------------------------
Total memory      : 21.11 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 24.25 MB
Total Mem (Write) : 16.42 MB
[Supermasks testing]
[Untrained loss : 0.6082]
[Starting training]
Epoch 0 	 20.423208 	 0.287565 	 0.285325
Epoch 10 	 18.503195 	 0.179121 	 0.185536
Epoch 20 	 18.181196 	 0.172488 	 0.175324
Epoch 30 	 18.026007 	 0.169192 	 0.171651
Epoch 40 	 17.854994 	 0.163612 	 0.171301
Epoch 50 	 17.786594 	 0.162565 	 0.167088
Epoch 60 	 17.682104 	 0.159755 	 0.161806
Epoch 70 	 17.623741 	 0.164196 	 0.163705
Epoch 80 	 17.439165 	 0.157172 	 0.162830
Epoch 90 	 17.448080 	 0.159195 	 0.162887
Epoch 100 	 17.298283 	 0.156883 	 0.159914
Epoch 110 	 17.264225 	 0.154491 	 0.159235
Epoch 120 	 17.215487 	 0.156854 	 0.158074
Epoch 130 	 17.188816 	 0.155127 	 0.160256
[Model stopped early]
Train loss       : 17.196590
Best valid loss  : 0.151066
Best test loss   : 0.159996
Pruning          : 0.32
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,839,550
--------------------------------
Total memory      : 21.10 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 23.44 MB
Total Mem (Write) : 16.41 MB
[Supermasks testing]
[Untrained loss : 0.7305]
[Starting training]
Epoch 0 	 22.218153 	 0.577958 	 0.563129
Epoch 10 	 20.783884 	 0.406806 	 0.414288
Epoch 20 	 20.535263 	 0.381730 	 0.377871
Epoch 30 	 20.377401 	 0.351277 	 0.361112
Epoch 40 	 20.239042 	 0.335059 	 0.342326
Epoch 50 	 20.117254 	 0.327815 	 0.335492
Epoch 60 	 20.058556 	 0.328698 	 0.335804
Epoch 70 	 19.963791 	 0.318572 	 0.328596
Epoch 80 	 19.883272 	 0.316667 	 0.320839
Epoch 90 	 19.878109 	 0.321799 	 0.325367
Epoch 100 	 19.790861 	 0.311681 	 0.317867
Epoch 110 	 19.807430 	 0.317042 	 0.325107
Epoch 120 	 19.765923 	 0.308465 	 0.315685
Epoch 130 	 19.760103 	 0.307865 	 0.316690
Epoch 140 	 19.756620 	 0.306277 	 0.314729
Epoch 150 	 19.674351 	 0.308226 	 0.310640
[Model stopped early]
Train loss       : 19.703335
Best valid loss  : 0.303389
Best test loss   : 0.312839
Pruning          : 0.24
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,839,550
--------------------------------
Total memory      : 21.10 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 23.44 MB
Total Mem (Write) : 16.41 MB
[Supermasks testing]
[Untrained loss : 0.7306]
[Starting training]
Epoch 0 	 22.213614 	 0.573482 	 0.569400
Epoch 10 	 20.824986 	 0.415623 	 0.421061
Epoch 20 	 20.523191 	 0.363279 	 0.365615
Epoch 30 	 20.351437 	 0.350292 	 0.348106
Epoch 40 	 20.186974 	 0.331450 	 0.332853
Epoch 50 	 20.114073 	 0.325619 	 0.324700
Epoch 60 	 20.063919 	 0.317192 	 0.325816
Epoch 70 	 20.000586 	 0.309356 	 0.318316
Epoch 80 	 19.970394 	 0.308689 	 0.321021
Epoch 90 	 19.882757 	 0.312142 	 0.321113
Epoch 100 	 19.814224 	 0.307699 	 0.312075
Epoch 110 	 19.802689 	 0.305319 	 0.313068
Epoch 120 	 19.745209 	 0.302120 	 0.306656
Epoch 130 	 19.715361 	 0.301752 	 0.313504
Epoch 140 	 19.740021 	 0.299715 	 0.308262
Epoch 150 	 19.722040 	 0.302379 	 0.310070
Epoch 160 	 19.642298 	 0.301407 	 0.306990
[Model stopped early]
Train loss       : 19.675451
Best valid loss  : 0.296335
Best test loss   : 0.308584
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,839,550
--------------------------------
Total memory      : 21.10 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 23.44 MB
Total Mem (Write) : 16.41 MB
[Supermasks testing]
[Untrained loss : 0.7284]
[Starting training]
Epoch 0 	 22.252754 	 0.582978 	 0.575570
Epoch 10 	 20.790329 	 0.426444 	 0.423134
Epoch 20 	 20.536661 	 0.367695 	 0.378039
Epoch 30 	 20.363821 	 0.346213 	 0.351116
Epoch 40 	 20.244946 	 0.335690 	 0.345925
Epoch 50 	 20.177219 	 0.327673 	 0.332248
Epoch 60 	 20.074345 	 0.319633 	 0.321912
Epoch 70 	 20.002665 	 0.312530 	 0.321160
Epoch 80 	 19.953169 	 0.320365 	 0.317579
Epoch 90 	 19.860458 	 0.306589 	 0.309729
Epoch 100 	 19.839596 	 0.308708 	 0.307745
Epoch 110 	 19.788225 	 0.303947 	 0.308764
Epoch 120 	 19.713684 	 0.302873 	 0.306780
Epoch 130 	 19.744381 	 0.307348 	 0.308426
Epoch 140 	 19.746311 	 0.300129 	 0.306193
Epoch 150 	 19.699564 	 0.303336 	 0.304261
Epoch 160 	 19.728170 	 0.303391 	 0.305318
Epoch 170 	 19.680593 	 0.299904 	 0.302973
[Model stopped early]
Train loss       : 19.715916
Best valid loss  : 0.296854
Best test loss   : 0.304509
Pruning          : 0.13
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,839,550
--------------------------------
Total memory      : 21.10 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 23.44 MB
Total Mem (Write) : 16.41 MB
[Supermasks testing]
[Untrained loss : 0.7291]
[Starting training]
Epoch 0 	 22.210720 	 0.566841 	 0.554846
Epoch 10 	 20.892551 	 0.430097 	 0.429906
Epoch 20 	 20.595335 	 0.374390 	 0.376893
Epoch 30 	 20.385561 	 0.353791 	 0.358582
Epoch 40 	 20.298611 	 0.358960 	 0.363127
Epoch 50 	 20.222530 	 0.344333 	 0.348628
Epoch 60 	 20.133730 	 0.342076 	 0.346510
Epoch 70 	 20.085281 	 0.343714 	 0.339099
Epoch 80 	 19.988602 	 0.336156 	 0.336564
Epoch 90 	 19.909134 	 0.333450 	 0.333831
Epoch 100 	 19.935997 	 0.335772 	 0.332429
Epoch 110 	 19.843143 	 0.335698 	 0.333624
Epoch 120 	 19.863882 	 0.329695 	 0.326385
Epoch 130 	 19.795261 	 0.322914 	 0.327383
Epoch 140 	 19.836361 	 0.329136 	 0.326868
Epoch 150 	 19.771017 	 0.327356 	 0.326069
Epoch 160 	 19.817743 	 0.326257 	 0.326868
Epoch 170 	 19.770411 	 0.328137 	 0.325993
Epoch 180 	 19.801081 	 0.326777 	 0.325796
[Model stopped early]
Train loss       : 19.801081
Best valid loss  : 0.321973
Best test loss   : 0.324462
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,839,550
--------------------------------
Total memory      : 21.10 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 23.44 MB
Total Mem (Write) : 16.41 MB
[Supermasks testing]
[Untrained loss : 0.7276]
[Starting training]
Epoch 0 	 22.219913 	 0.575271 	 0.568920
Epoch 10 	 20.804663 	 0.421580 	 0.423934
Epoch 20 	 20.500162 	 0.375201 	 0.380306
Epoch 30 	 20.409866 	 0.354472 	 0.358853
Epoch 40 	 20.227774 	 0.352693 	 0.358342
Epoch 50 	 20.162216 	 0.324444 	 0.334370
Epoch 60 	 20.011143 	 0.310733 	 0.330615
Epoch 70 	 19.994120 	 0.306998 	 0.325926
Epoch 80 	 19.988997 	 0.317271 	 0.335503
Epoch 90 	 19.872475 	 0.305695 	 0.324302
Epoch 100 	 19.905502 	 0.306029 	 0.320642
Epoch 110 	 19.776852 	 0.300886 	 0.314744
Epoch 120 	 19.745399 	 0.304021 	 0.314599
Epoch 130 	 19.729919 	 0.300757 	 0.311269
Epoch 140 	 19.716467 	 0.304173 	 0.309063
Epoch 150 	 19.687399 	 0.296247 	 0.310128
Epoch 160 	 19.652393 	 0.301867 	 0.312009
[Model stopped early]
Train loss       : 19.632137
Best valid loss  : 0.292905
Best test loss   : 0.308899
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,839,550
--------------------------------
Total memory      : 21.10 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 23.44 MB
Total Mem (Write) : 16.41 MB
[Supermasks testing]
[Untrained loss : 0.7302]
[Starting training]
Epoch 0 	 22.234976 	 0.580920 	 0.574503
Epoch 10 	 20.827631 	 0.411793 	 0.413509
Epoch 20 	 20.558346 	 0.373652 	 0.375838
Epoch 30 	 20.386015 	 0.361551 	 0.371070
Epoch 40 	 20.257839 	 0.351422 	 0.353798
Epoch 50 	 20.158096 	 0.340220 	 0.346911
Epoch 60 	 20.115360 	 0.337839 	 0.336885
Epoch 70 	 20.092642 	 0.334631 	 0.344354
Epoch 80 	 19.958359 	 0.325951 	 0.330271
Epoch 90 	 19.912273 	 0.312933 	 0.319917
Epoch 100 	 19.868820 	 0.315313 	 0.320402
Epoch 110 	 19.839710 	 0.314391 	 0.318375
Epoch 120 	 19.773172 	 0.311186 	 0.317669
Epoch 130 	 19.808439 	 0.309412 	 0.313930
Epoch 140 	 19.724115 	 0.311558 	 0.314046
Epoch 150 	 19.738853 	 0.307465 	 0.314763
Epoch 160 	 19.728800 	 0.306981 	 0.311471
Epoch 170 	 19.728333 	 0.305200 	 0.313038
[Model stopped early]
Train loss       : 19.756681
Best valid loss  : 0.303065
Best test loss   : 0.313565
Pruning          : 0.06
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,839,550
--------------------------------
Total memory      : 21.10 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 23.44 MB
Total Mem (Write) : 16.41 MB
[Supermasks testing]
[Untrained loss : 0.7280]
[Starting training]
Epoch 0 	 22.223181 	 0.570395 	 0.560905
Epoch 10 	 20.873117 	 0.427348 	 0.423452
Epoch 20 	 20.578054 	 0.386588 	 0.388302
Epoch 30 	 20.366499 	 0.350156 	 0.352220
Epoch 40 	 20.240072 	 0.339304 	 0.340880
Epoch 50 	 20.116848 	 0.327363 	 0.337594
Epoch 60 	 20.014963 	 0.322822 	 0.325672
Epoch 70 	 19.964842 	 0.308049 	 0.314229
Epoch 80 	 19.906101 	 0.310766 	 0.319044
Epoch 90 	 19.836473 	 0.304143 	 0.308361
Epoch 100 	 19.848454 	 0.297930 	 0.308662
Epoch 110 	 19.781246 	 0.301550 	 0.306829
[Model stopped early]
Train loss       : 19.822392
Best valid loss  : 0.296951
Best test loss   : 0.309728
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,839,550
--------------------------------
Total memory      : 21.10 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 23.44 MB
Total Mem (Write) : 16.41 MB
[Supermasks testing]
[Untrained loss : 0.7270]
[Starting training]
Epoch 0 	 22.239071 	 0.585242 	 0.572271
Epoch 10 	 20.796371 	 0.404566 	 0.411205
Epoch 20 	 20.548939 	 0.367071 	 0.372689
Epoch 30 	 20.408838 	 0.360798 	 0.365982
Epoch 40 	 20.231363 	 0.349926 	 0.347298
Epoch 50 	 20.180571 	 0.327916 	 0.330815
Epoch 60 	 20.061335 	 0.327740 	 0.331560
Epoch 70 	 20.000631 	 0.312713 	 0.318292
Epoch 80 	 19.950676 	 0.311814 	 0.315625
Epoch 90 	 19.919441 	 0.320102 	 0.319952
Epoch 100 	 19.836887 	 0.306277 	 0.310519
Epoch 110 	 19.783604 	 0.297458 	 0.305107
Epoch 120 	 19.728449 	 0.299475 	 0.307765
Epoch 130 	 19.762350 	 0.296595 	 0.304784
Epoch 140 	 19.699734 	 0.283593 	 0.299652
slurmstepd: error: *** JOB 41288814 ON cdr351 CANCELLED AT 2020-04-29T16:49:02 DUE TO TIME LIMIT ***
