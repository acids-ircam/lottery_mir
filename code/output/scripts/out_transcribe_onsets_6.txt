Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288789.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, six, pillow-simd, torchvision, tqdm, pyparsing, kiwisolver, cycler, python-dateutil, matplotlib, gast, tensorflow-estimator, google-pasta, grpcio, h5py, keras-applications, wrapt, absl-py, protobuf, opt-einsum, astor, markdown, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, urllib3, chardet, idna, certifi, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, tensorboard, termcolor, keras-preprocessing, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288789.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:52:09.602642: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:52:09.616513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_trimming_magnitude_reinit_global_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288789.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7326]
[Starting training]
/localscratch/esling.41288789.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
Epoch 0 	 22.687988 	 0.605172 	 0.605803
Epoch 10 	 21.315296 	 0.509598 	 0.513196
Epoch 20 	 19.907856 	 0.346418 	 0.345125
Epoch 30 	 18.323891 	 0.212043 	 0.210955
Epoch 40 	 17.628851 	 0.173938 	 0.175666
Epoch 50 	 17.228878 	 0.153715 	 0.156877
Epoch 60 	 16.916679 	 0.137796 	 0.154416
Epoch 70 	 16.733128 	 0.140204 	 0.145389
Epoch 80 	 16.525478 	 0.136039 	 0.145969
Epoch 90 	 16.327682 	 0.129330 	 0.138203
Epoch 100 	 16.250013 	 0.127225 	 0.138588
Epoch 110 	 16.182953 	 0.126863 	 0.137425
Epoch 120 	 16.148125 	 0.125733 	 0.133908
Epoch 130 	 16.117056 	 0.124339 	 0.137984
Epoch 140 	 16.091288 	 0.127584 	 0.136091
[Model stopped early]
Train loss       : 16.096506
Best valid loss  : 0.121925
Best test loss   : 0.136392
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 2,749,336
--------------------------------
Total memory      : 1.11 MB
Total Flops       : 8.96 MFlops
Total Mem (Read)  : 11.36 MB
Total Mem (Write) : 882.86 KB
[Supermasks testing]
[Untrained loss : 0.7406]
[Starting training]
Epoch 0 	 23.111935 	 0.728891 	 0.725106
Epoch 10 	 21.724167 	 0.556825 	 0.555151
Epoch 20 	 20.724850 	 0.408677 	 0.418542
Epoch 30 	 19.444265 	 0.291675 	 0.295512
Epoch 40 	 18.455349 	 0.209469 	 0.218904
Epoch 50 	 17.889759 	 0.183153 	 0.186154
Epoch 60 	 17.454369 	 0.162417 	 0.170303
Epoch 70 	 17.156265 	 0.145157 	 0.158294
Epoch 80 	 16.956381 	 0.143548 	 0.150279
Epoch 90 	 16.796343 	 0.142119 	 0.150659
Epoch 100 	 16.714928 	 0.138994 	 0.151171
Epoch 110 	 16.476950 	 0.135416 	 0.146861
Epoch 120 	 16.412607 	 0.138783 	 0.147141
Epoch 130 	 16.398577 	 0.137702 	 0.146062
Epoch 140 	 16.337011 	 0.133401 	 0.142930
Epoch 150 	 16.308231 	 0.136620 	 0.142442
Epoch 160 	 16.270988 	 0.131128 	 0.141742
Epoch 170 	 16.228153 	 0.133368 	 0.140875
[Model stopped early]
Train loss       : 16.230494
Best valid loss  : 0.129471
Best test loss   : 0.143966
Pruning          : 0.75
0.001
0.001
[Current model size]
================================
Total params      : 1,931,206
--------------------------------
Total memory      : 0.41 MB
Total Flops       : 3.32 MFlops
Total Mem (Read)  : 7.7 MB
Total Mem (Write) : 330.51 KB
[Supermasks testing]
[Untrained loss : 0.7451]
[Starting training]
Epoch 0 	 22.962048 	 0.650006 	 0.646072
Epoch 10 	 22.032780 	 0.594613 	 0.595768
Epoch 20 	 21.187603 	 0.468035 	 0.476207
Epoch 30 	 20.439285 	 0.385429 	 0.385655
Epoch 40 	 19.679958 	 0.311236 	 0.310289
Epoch 50 	 19.083473 	 0.260856 	 0.264063
Epoch 60 	 18.690556 	 0.235619 	 0.239482
Epoch 70 	 18.422049 	 0.218349 	 0.224347
Epoch 80 	 18.070688 	 0.204079 	 0.218547
Epoch 90 	 17.872227 	 0.198722 	 0.206609
Epoch 100 	 17.694613 	 0.191175 	 0.195104
Epoch 110 	 17.563900 	 0.186684 	 0.192617
Epoch 120 	 17.429312 	 0.178868 	 0.186273
Epoch 130 	 17.307455 	 0.171281 	 0.178470
Epoch 140 	 17.213404 	 0.163895 	 0.177709
Epoch 150 	 17.074963 	 0.165172 	 0.173064
Epoch 160 	 16.830267 	 0.151712 	 0.160340
Epoch 170 	 16.775724 	 0.148733 	 0.157191
Epoch 180 	 16.709831 	 0.147349 	 0.158448
Epoch 190 	 16.620163 	 0.142198 	 0.153130
Train loss       : 16.590488
Best valid loss  : 0.142198
Best test loss   : 0.153130
Pruning          : 0.56
0.001
0.001
[Current model size]
================================
Total params      : 1,384,468
--------------------------------
Total memory      : 0.41 MB
Total Flops       : 2.78 MFlops
Total Mem (Read)  : 5.61 MB
Total Mem (Write) : 326.34 KB
[Supermasks testing]
[Untrained loss : 0.7040]
[Starting training]
Epoch 0 	 23.323151 	 0.751065 	 0.750087
Epoch 10 	 22.357822 	 0.627897 	 0.628016
Epoch 20 	 21.402704 	 0.510563 	 0.512592
Epoch 30 	 20.616411 	 0.406476 	 0.417820
Epoch 40 	 20.134178 	 0.357820 	 0.360388
Epoch 50 	 19.897953 	 0.328332 	 0.333296
Epoch 60 	 19.504049 	 0.308923 	 0.315831
Epoch 70 	 19.212902 	 0.295905 	 0.305109
Epoch 80 	 18.993160 	 0.275722 	 0.279519
Epoch 90 	 18.771399 	 0.260547 	 0.263817
Epoch 100 	 18.510654 	 0.245462 	 0.248603
Epoch 110 	 18.261660 	 0.234866 	 0.241182
/localscratch/esling.41288789.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 120 	 18.049252 	 0.232638 	 0.239133
Epoch 130 	 17.830952 	 0.219977 	 0.228881
Epoch 140 	 17.715675 	 0.217583 	 0.223667
Epoch 150 	 17.632032 	 0.211051 	 0.219621
Epoch 160 	 17.530338 	 0.198989 	 0.212142
Epoch 170 	 17.443707 	 0.200114 	 0.207980
Epoch 180 	 17.302408 	 0.181393 	 0.194989
Epoch 190 	 17.257326 	 0.174110 	 0.189140
Train loss       : 17.180759
Best valid loss  : 0.172709
Best test loss   : 0.189543
Pruning          : 0.42
0.001
0.001
[Current model size]
================================
Total params      : 1,018,996
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 2.41 MFlops
Total Mem (Read)  : 4.21 MB
Total Mem (Write) : 323.23 KB
[Supermasks testing]
[Untrained loss : 0.7206]
[Starting training]
Epoch 0 	 23.240892 	 0.716665 	 0.715272
Epoch 10 	 22.048691 	 0.595942 	 0.595672
Epoch 20 	 21.324562 	 0.495515 	 0.505039
Epoch 30 	 20.437576 	 0.383941 	 0.378281
Epoch 40 	 19.504330 	 0.294094 	 0.290515
Epoch 50 	 18.916842 	 0.249605 	 0.246710
Epoch 60 	 18.491669 	 0.215714 	 0.220667
Epoch 70 	 18.098581 	 0.187505 	 0.190819
Epoch 80 	 17.847189 	 0.168333 	 0.175695
Epoch 90 	 17.640324 	 0.162669 	 0.172722
Epoch 100 	 17.541061 	 0.162249 	 0.171229
Epoch 110 	 17.439548 	 0.159468 	 0.169756
Epoch 120 	 17.295853 	 0.157994 	 0.165390
Epoch 130 	 17.202835 	 0.151469 	 0.159696
Epoch 140 	 17.127132 	 0.145416 	 0.157589
Epoch 150 	 17.051548 	 0.141768 	 0.155517
Epoch 160 	 16.922548 	 0.139797 	 0.153163
Epoch 170 	 16.851898 	 0.140974 	 0.150764
Epoch 180 	 16.733805 	 0.137629 	 0.148158
Epoch 190 	 16.681686 	 0.134857 	 0.149014
Train loss       : 16.693747
Best valid loss  : 0.134767
Best test loss   : 0.148842
Pruning          : 0.32
0.001
0.001
[Current model size]
================================
Total params      : 761,317
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 1.49 MFlops
Total Mem (Read)  : 3.12 MB
Total Mem (Write) : 211.49 KB
[Supermasks testing]
[Untrained loss : 0.7347]
[Starting training]
Epoch 0 	 23.484989 	 0.669087 	 0.669431
Epoch 10 	 21.827936 	 0.561397 	 0.567372
Epoch 20 	 21.446278 	 0.510661 	 0.521267
Epoch 30 	 20.947956 	 0.433120 	 0.440929
Epoch 40 	 20.324715 	 0.362309 	 0.373430
Epoch 50 	 19.868301 	 0.324179 	 0.333530
Epoch 60 	 19.479761 	 0.288018 	 0.296498
Epoch 70 	 19.153494 	 0.261007 	 0.269619
Epoch 80 	 18.837473 	 0.228920 	 0.235295
Epoch 90 	 18.633667 	 0.212389 	 0.218693
Epoch 100 	 18.466499 	 0.202398 	 0.210503
Epoch 110 	 18.271845 	 0.191733 	 0.198619
Epoch 120 	 18.115982 	 0.184550 	 0.189961
Epoch 130 	 17.976368 	 0.176843 	 0.186259
Epoch 140 	 17.894714 	 0.175169 	 0.183590
Epoch 150 	 17.776163 	 0.172760 	 0.180215
Epoch 160 	 17.746878 	 0.167776 	 0.178318
Epoch 170 	 17.589739 	 0.164960 	 0.174838
Epoch 180 	 17.478191 	 0.165021 	 0.172224
Epoch 190 	 17.438337 	 0.163334 	 0.173533
Train loss       : 17.362453
Best valid loss  : 0.157894
Best test loss   : 0.172212
Pruning          : 0.24
0.001
0.001
[Current model size]
================================
Total params      : 573,549
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 1.3 MFlops
Total Mem (Read)  : 2.4 MB
Total Mem (Write) : 209.69 KB
[Supermasks testing]
[Untrained loss : 0.7120]
[Starting training]
Epoch 0 	 23.363338 	 0.657510 	 0.658730
Epoch 10 	 21.843916 	 0.564760 	 0.570430
Epoch 20 	 21.419178 	 0.513082 	 0.516115
Epoch 30 	 20.590481 	 0.386226 	 0.397054
Epoch 40 	 20.188578 	 0.352572 	 0.359241
Epoch 50 	 19.735193 	 0.299274 	 0.314671
Epoch 60 	 19.249580 	 0.260141 	 0.270679
Epoch 70 	 18.994442 	 0.238909 	 0.249778
slurmstepd: error: _is_a_lwp: open() /proc/17991/status failed: No such file or directory
Epoch 80 	 18.765018 	 0.208894 	 0.223481
Epoch 90 	 18.530569 	 0.202942 	 0.212654
Epoch 100 	 18.371420 	 0.185337 	 0.197403
Epoch 110 	 18.239960 	 0.180051 	 0.194350
Epoch 120 	 18.105751 	 0.174099 	 0.189287
Epoch 130 	 18.011522 	 0.171764 	 0.180732
Epoch 140 	 17.907883 	 0.167540 	 0.181681
Epoch 150 	 17.759712 	 0.162519 	 0.178268
Epoch 160 	 17.718140 	 0.164286 	 0.180583
Epoch 170 	 17.670382 	 0.158237 	 0.177698
Epoch 180 	 17.591480 	 0.160576 	 0.175827
Epoch 190 	 17.547039 	 0.161053 	 0.175129
Train loss       : 17.523825
Best valid loss  : 0.156551
Best test loss   : 0.174455
Pruning          : 0.18
0.001
0.001
[Current model size]
================================
Total params      : 442,037
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 1.17 MFlops
Total Mem (Read)  : 1.9 MB
Total Mem (Write) : 208.36 KB
[Supermasks testing]
[Untrained loss : 0.7886]
[Starting training]
Epoch 0 	 23.326057 	 0.679446 	 0.677253
Epoch 10 	 21.868113 	 0.585205 	 0.585805
Epoch 20 	 21.545145 	 0.538971 	 0.545667
Epoch 30 	 20.966217 	 0.446129 	 0.446759
Epoch 40 	 20.233484 	 0.349025 	 0.358263
Epoch 50 	 19.990242 	 0.330663 	 0.336009
Epoch 60 	 19.738211 	 0.295677 	 0.302949
Epoch 70 	 19.490368 	 0.284376 	 0.288511
Epoch 80 	 19.272335 	 0.256694 	 0.257814
Epoch 90 	 19.015142 	 0.241752 	 0.239166
Epoch 100 	 18.868649 	 0.231742 	 0.235019
Epoch 110 	 18.704033 	 0.216391 	 0.216113
Epoch 120 	 18.568556 	 0.203799 	 0.210428
Epoch 130 	 18.442923 	 0.201057 	 0.207920
Epoch 140 	 18.379549 	 0.196395 	 0.196530
Epoch 150 	 18.345810 	 0.187627 	 0.189414
Epoch 160 	 18.206347 	 0.181834 	 0.190996
Epoch 170 	 18.151737 	 0.183097 	 0.186912
Epoch 180 	 18.033817 	 0.178210 	 0.183667
Epoch 190 	 17.981571 	 0.177306 	 0.182751
Train loss       : 17.899582
Best valid loss  : 0.171460
Best test loss   : 0.182853
Pruning          : 0.13
0.001
0.001
[Current model size]
================================
Total params      : 341,609
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 1.07 MFlops
Total Mem (Read)  : 1.52 MB
Total Mem (Write) : 207.36 KB
[Supermasks testing]
[Untrained loss : 0.8285]
[Starting training]
Epoch 0 	 23.522598 	 0.741234 	 0.732316
Epoch 10 	 22.022711 	 0.592536 	 0.595344
Epoch 20 	 21.896496 	 0.572866 	 0.576474
Epoch 30 	 21.440241 	 0.504726 	 0.514901
Epoch 40 	 21.020523 	 0.451448 	 0.458278
Epoch 50 	 20.672588 	 0.399737 	 0.410174
Epoch 60 	 20.445501 	 0.373098 	 0.382129
Epoch 70 	 20.241568 	 0.366032 	 0.368899
Epoch 80 	 20.076195 	 0.351556 	 0.357765
Epoch 90 	 19.920427 	 0.332868 	 0.345848
Epoch 100 	 19.787436 	 0.324693 	 0.337259
Epoch 110 	 19.671730 	 0.305338 	 0.321327
Epoch 120 	 19.494253 	 0.286164 	 0.299072
Epoch 130 	 19.389256 	 0.269003 	 0.285229
Epoch 140 	 19.240389 	 0.255681 	 0.271041
Epoch 150 	 19.123589 	 0.246557 	 0.261829
Epoch 160 	 19.070946 	 0.240724 	 0.254362
Epoch 170 	 18.923788 	 0.238011 	 0.247661
Epoch 180 	 18.878305 	 0.227852 	 0.237550
Epoch 190 	 18.778063 	 0.226903 	 0.241464
Train loss       : 18.767502
Best valid loss  : 0.218330
Best test loss   : 0.238130
Pruning          : 0.10
0.001
0.001
[Current model size]
================================
Total params      : 275,677
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 1.01 MFlops
Total Mem (Read)  : 1.27 MB
Total Mem (Write) : 206.61 KB
[Supermasks testing]
[Untrained loss : 0.7296]
[Starting training]
Epoch 0 	 23.702442 	 0.720624 	 0.715175
Epoch 10 	 21.951553 	 0.575374 	 0.575573
Epoch 20 	 21.597437 	 0.524420 	 0.534647
Epoch 30 	 21.225216 	 0.479883 	 0.485599
Epoch 40 	 20.853764 	 0.424553 	 0.425324
Epoch 50 	 20.616867 	 0.395086 	 0.401182
Epoch 60 	 20.440790 	 0.369062 	 0.375676
Epoch 70 	 20.300367 	 0.361631 	 0.367784
Epoch 80 	 20.174896 	 0.338030 	 0.344768
Epoch 90 	 20.021391 	 0.327197 	 0.329461
Epoch 100 	 19.961493 	 0.320144 	 0.322063
Epoch 110 	 19.928202 	 0.316186 	 0.315506
Epoch 120 	 19.820732 	 0.309291 	 0.318346
Epoch 130 	 19.709265 	 0.307390 	 0.311582
Epoch 140 	 19.621017 	 0.303309 	 0.305105
Epoch 150 	 19.597677 	 0.294558 	 0.294195
Epoch 160 	 19.495714 	 0.287341 	 0.294169
Epoch 170 	 19.475073 	 0.287938 	 0.286671
Epoch 180 	 19.453274 	 0.283109 	 0.285072
Epoch 190 	 19.353233 	 0.285850 	 0.285977
Train loss       : 19.297920
Best valid loss  : 0.274589
Best test loss   : 0.281130
Pruning          : 0.08
0.001
0.001
[Current model size]
================================
Total params      : 230,069
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 961.35 KFlops
Total Mem (Read)  : 1.09 MB
Total Mem (Write) : 206.07 KB
[Supermasks testing]
[Untrained loss : 0.7852]
[Starting training]
Epoch 0 	 23.763168 	 0.729346 	 0.724525
Epoch 10 	 21.980556 	 0.580805 	 0.584318
Epoch 20 	 21.644690 	 0.530196 	 0.539636
Epoch 30 	 21.475187 	 0.513607 	 0.519160
Epoch 40 	 21.309916 	 0.483310 	 0.483563
Epoch 50 	 21.113712 	 0.464698 	 0.467368
Epoch 60 	 20.996723 	 0.451165 	 0.457262
Epoch 70 	 20.866381 	 0.437818 	 0.434063
Epoch 80 	 20.786844 	 0.425893 	 0.424616
Epoch 90 	 20.656435 	 0.409862 	 0.414585
Epoch 100 	 20.606150 	 0.401866 	 0.405031
Epoch 110 	 20.538801 	 0.393465 	 0.401014
Epoch 120 	 20.441391 	 0.386789 	 0.392862
Epoch 130 	 20.416899 	 0.377109 	 0.387967
Epoch 140 	 20.359087 	 0.382362 	 0.380777
Epoch 150 	 20.336916 	 0.376651 	 0.377506
Epoch 160 	 20.234707 	 0.368545 	 0.370271
Epoch 170 	 20.158581 	 0.362963 	 0.370586
Epoch 180 	 20.135178 	 0.359476 	 0.363817
Epoch 190 	 20.115580 	 0.365823 	 0.366965
Train loss       : 20.046694
Best valid loss  : 0.350626
Best test loss   : 0.360473
Pruning          : 0.06
0.001
0.001
[Current model size]
================================
Total params      : 191,612
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 922.89 KFlops
Total Mem (Read)  : 966.23 KB
Total Mem (Write) : 205.63 KB
[Supermasks testing]
[Untrained loss : 0.8042]
[Starting training]
Epoch 0 	 23.584921 	 0.781955 	 0.779892
Epoch 10 	 22.289169 	 0.634170 	 0.632167
Epoch 20 	 22.131628 	 0.606342 	 0.609125
Epoch 30 	 22.088711 	 0.599045 	 0.601103
Epoch 40 	 22.061050 	 0.597452 	 0.602937
Epoch 50 	 22.039585 	 0.586717 	 0.594134
Epoch 60 	 21.968063 	 0.574564 	 0.584378
Epoch 70 	 21.891233 	 0.570319 	 0.574888
Epoch 80 	 21.872074 	 0.569100 	 0.576711
Epoch 90 	 21.847416 	 0.564264 	 0.574038
Epoch 100 	 21.823017 	 0.570400 	 0.576362
Epoch 110 	 21.791214 	 0.564259 	 0.573609
Epoch 120 	 21.771448 	 0.562844 	 0.572627
Epoch 130 	 21.761911 	 0.565237 	 0.575539
Epoch 140 	 21.786228 	 0.565000 	 0.573153
Epoch 150 	 21.809238 	 0.561278 	 0.566748
Epoch 160 	 21.764034 	 0.557345 	 0.566503
Epoch 170 	 21.755686 	 0.552579 	 0.565129
Epoch 180 	 21.765884 	 0.559605 	 0.565333
[Model stopped early]
Train loss       : 21.782284
Best valid loss  : 0.551255
Best test loss   : 0.564525
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 168,551
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 899.83 KFlops
Total Mem (Read)  : 875.83 KB
Total Mem (Write) : 205.32 KB
[Supermasks testing]
[Untrained loss : 0.7864]
[Starting training]
Epoch 0 	 23.799143 	 0.783644 	 0.779892
Epoch 10 	 22.385223 	 0.644983 	 0.646585
Epoch 20 	 22.266670 	 0.630969 	 0.629691
Epoch 30 	 22.185923 	 0.617543 	 0.620555
Epoch 40 	 22.089291 	 0.592457 	 0.602336
Epoch 50 	 22.077633 	 0.592676 	 0.599089
Epoch 60 	 22.052917 	 0.596461 	 0.601023
Epoch 70 	 21.990097 	 0.591750 	 0.597530
Epoch 80 	 22.000656 	 0.601201 	 0.602878
Epoch 90 	 22.006083 	 0.596752 	 0.595656
Epoch 100 	 21.974037 	 0.595209 	 0.599242
Epoch 110 	 21.957897 	 0.594002 	 0.599680
[Model stopped early]
Train loss       : 21.969795
Best valid loss  : 0.589430
Best test loss   : 0.594736
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 153,218
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 884.5 KFlops
Total Mem (Read)  : 815.71 KB
Total Mem (Write) : 205.09 KB
[Supermasks testing]
[Untrained loss : 0.7468]
[Starting training]
Epoch 0 	 23.842999 	 0.783655 	 0.779892
Epoch 10 	 22.420605 	 0.638185 	 0.635840
Epoch 20 	 22.308214 	 0.633665 	 0.628610
Epoch 30 	 22.231705 	 0.623407 	 0.622677
Epoch 40 	 22.210304 	 0.615517 	 0.617287
Epoch 50 	 22.190531 	 0.614486 	 0.614120
Epoch 60 	 22.161566 	 0.612739 	 0.609188
Epoch 70 	 22.163212 	 0.613832 	 0.613666
[Model stopped early]
Train loss       : 22.166975
Best valid loss  : 0.607641
Best test loss   : 0.614042
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 141,920
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 873.2 KFlops
Total Mem (Read)  : 771.41 KB
Total Mem (Write) : 204.93 KB
[Supermasks testing]
[Untrained loss : 0.7751]
[Starting training]
Epoch 0 	 23.891294 	 0.781758 	 0.779892
Epoch 10 	 22.456787 	 0.668696 	 0.664342
Epoch 20 	 22.328548 	 0.631031 	 0.629544
Epoch 30 	 22.300924 	 0.633590 	 0.631090
Epoch 40 	 22.261753 	 0.628099 	 0.624915
Epoch 50 	 22.211744 	 0.624242 	 0.622961
Epoch 60 	 22.235018 	 0.617315 	 0.614506
Epoch 70 	 22.180561 	 0.612127 	 0.610202
Epoch 80 	 22.177376 	 0.608912 	 0.604816
Epoch 90 	 22.163559 	 0.608234 	 0.607414
Epoch 100 	 22.143101 	 0.605498 	 0.604140
Epoch 110 	 22.143070 	 0.600323 	 0.602190
Epoch 120 	 22.121582 	 0.605112 	 0.603292
Epoch 130 	 22.130384 	 0.602597 	 0.600155
Epoch 140 	 22.108522 	 0.605489 	 0.599753
[Model stopped early]
Train loss       : 22.108522
Best valid loss  : 0.594387
Best test loss   : 0.601738
Pruning          : 0.02
