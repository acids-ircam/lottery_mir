Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41281304.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, six, pillow-simd, torchvision, tqdm, pyparsing, python-dateutil, cycler, kiwisolver, matplotlib, google-pasta, protobuf, absl-py, termcolor, keras-preprocessing, astor, opt-einsum, tensorflow-estimator, wrapt, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, certifi, urllib3, chardet, idna, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, grpcio, markdown, tensorboard, h5py, keras-applications, gast, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281304.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-29 01:59:01.155318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 01:59:01.166108: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is sol-ordinario_ddsp_cnn_xavier_trimming_gradient_min_reinit_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41281304.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 97.7907]
[Starting training]
Epoch 0 	 90.884087 	 85.761024 	 89.244377
Epoch 10 	 77.402328 	 80.839371 	 84.147064
Epoch 20 	 71.353004 	 68.839149 	 72.961800
Epoch 30 	 69.664558 	 63.791714 	 71.948372
Epoch 40 	 66.072266 	 63.250111 	 64.011497
Epoch 50 	 63.421150 	 59.187130 	 60.925270
Epoch 60 	 57.012901 	 58.481018 	 60.557430
Epoch 70 	 53.175953 	 48.777550 	 51.102566
Epoch 80 	 46.400082 	 45.654053 	 46.296394
Epoch 90 	 44.670612 	 46.003799 	 50.967712
Epoch 100 	 41.836300 	 39.198929 	 43.042770
Epoch 110 	 40.960499 	 39.920948 	 41.728157
Epoch 120 	 38.986023 	 39.100750 	 41.681225
Epoch 130 	 36.593285 	 37.558491 	 38.243900
Epoch 140 	 36.279312 	 35.709347 	 38.107483
Epoch 150 	 34.847057 	 35.781712 	 38.776543
Epoch 160 	 33.457531 	 35.150154 	 36.971371
Epoch 170 	 33.460327 	 33.753334 	 35.125732
Epoch 180 	 31.651749 	 33.425499 	 34.780571
Epoch 190 	 33.966324 	 34.460510 	 36.164730
Train loss       : 29.654465
Best valid loss  : 32.198372
Best test loss   : 33.921448
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,909,528
--------------------------------
Total memory      : 35.03 MB
Total Flops       : 326.2 MFlops
Total Mem (Read)  : 31.67 MB
Total Mem (Write) : 24.49 MB
[Supermasks testing]
[Untrained loss : 96.2364]
[Starting training]
Epoch 0 	 88.696228 	 85.695358 	 89.050529
Epoch 10 	 58.805603 	 50.417530 	 54.492413
Epoch 20 	 46.989338 	 42.278679 	 44.893211
Epoch 30 	 42.230366 	 38.937531 	 41.243221
Epoch 40 	 42.742153 	 44.108360 	 49.229267
Epoch 50 	 41.485058 	 38.815472 	 42.403503
Epoch 60 	 33.690876 	 34.403221 	 36.648415
Epoch 70 	 45.471924 	 48.327976 	 51.121498
Epoch 80 	 30.416672 	 31.742828 	 33.722454
Epoch 90 	 28.472477 	 30.067617 	 32.498394
Epoch 100 	 28.364202 	 28.804356 	 32.433346
Epoch 110 	 27.332060 	 29.758333 	 32.133389
Epoch 120 	 26.132963 	 28.576229 	 30.805902
Epoch 130 	 25.554535 	 28.226332 	 31.026320
Epoch 140 	 25.061117 	 27.804834 	 30.347483
Epoch 150 	 24.508863 	 27.086575 	 29.773869
Epoch 160 	 24.618137 	 27.596956 	 29.896305
Epoch 170 	 23.834990 	 26.910368 	 29.698303
Epoch 180 	 24.001787 	 27.475800 	 29.672440
Epoch 190 	 23.821493 	 27.007612 	 29.391571
Train loss       : 23.532656
Best valid loss  : 26.854614
Best test loss   : 29.413864
Pruning          : 0.72
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,898,752
--------------------------------
Total memory      : 29.65 MB
Total Flops       : 171.22 MFlops
Total Mem (Read)  : 22.92 MB
Total Mem (Write) : 17.34 MB
[Supermasks testing]
[Untrained loss : 96.8402]
[Starting training]
Epoch 0 	 90.806061 	 85.693230 	 88.903053
Epoch 10 	 61.659466 	 62.185768 	 66.407310
Epoch 20 	 53.948116 	 50.678802 	 54.138435
Epoch 30 	 45.782951 	 45.296543 	 49.657715
Epoch 40 	 41.340546 	 43.750462 	 47.124279
Epoch 50 	 37.376678 	 37.693180 	 41.050091
Epoch 60 	 32.615757 	 35.984997 	 39.447285
Epoch 70 	 31.380812 	 34.360668 	 37.622345
Epoch 80 	 30.369545 	 33.822250 	 36.812206
Epoch 90 	 29.501360 	 32.133404 	 35.451717
Epoch 100 	 29.640392 	 33.198853 	 36.109718
Epoch 110 	 26.923368 	 30.592777 	 33.051102
Epoch 120 	 26.163649 	 30.076252 	 33.172283
Epoch 130 	 26.384937 	 30.609509 	 33.387280
Epoch 140 	 25.123180 	 29.131943 	 31.409262
Epoch 150 	 24.753613 	 29.304615 	 31.081409
Epoch 160 	 24.623072 	 28.790091 	 31.084814
Epoch 170 	 24.394606 	 28.692223 	 30.736288
Epoch 180 	 24.235422 	 28.678757 	 30.579248
[Model stopped early]
Train loss       : 24.316141
Best valid loss  : 27.915062
Best test loss   : 31.170429
Pruning          : 0.52
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,282,494
--------------------------------
Total memory      : 25.75 MB
Total Flops       : 89.46 MFlops
Total Mem (Read)  : 17.01 MB
Total Mem (Write) : 12.62 MB
[Supermasks testing]
[Untrained loss : 95.8352]
[Starting training]
Epoch 0 	 91.370003 	 90.257660 	 94.042946
Epoch 10 	 62.575737 	 57.210686 	 62.823811
Epoch 20 	 52.050449 	 47.139648 	 51.927883
Epoch 30 	 51.127537 	 44.539375 	 49.513210
Epoch 40 	 40.433319 	 39.299171 	 45.119900
Epoch 50 	 37.148262 	 36.594612 	 41.632324
Epoch 60 	 34.816429 	 35.553673 	 41.073315
Epoch 70 	 33.674641 	 34.448925 	 39.346306
Epoch 80 	 32.872879 	 34.944771 	 37.628899
Epoch 90 	 31.916897 	 37.080669 	 41.473232
Epoch 100 	 31.755945 	 33.524559 	 36.570293
Epoch 110 	 29.143389 	 32.772171 	 35.762547
Epoch 120 	 28.497828 	 31.932140 	 35.584732
Epoch 130 	 27.373220 	 30.562675 	 33.662521
Epoch 140 	 27.120445 	 30.449907 	 33.957539
Epoch 150 	 26.526814 	 29.479101 	 32.916286
Epoch 160 	 25.668648 	 29.418381 	 31.750113
Epoch 170 	 25.377253 	 28.819160 	 32.210251
Epoch 180 	 25.225178 	 29.255610 	 31.785807
Epoch 190 	 25.357260 	 29.608023 	 32.555889
Train loss       : 24.668680
Best valid loss  : 27.920357
Best test loss   : 31.044697
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 887,398
--------------------------------
Total memory      : 22.89 MB
Total Flops       : 46.15 MFlops
Total Mem (Read)  : 12.89 MB
Total Mem (Write) : 9.4 MB
[Supermasks testing]
[Untrained loss : 95.7790]
[Starting training]
Epoch 0 	 90.995575 	 87.191719 	 89.984512
Epoch 10 	 64.207214 	 68.710907 	 72.015938
Epoch 20 	 52.208481 	 48.517040 	 52.420925
Epoch 30 	 44.714417 	 44.884613 	 48.278465
Epoch 40 	 41.129044 	 43.424889 	 47.668583
Epoch 50 	 38.428585 	 39.421520 	 44.224152
Epoch 60 	 35.775276 	 38.513054 	 43.158516
Epoch 70 	 33.740185 	 36.915199 	 41.036671
Epoch 80 	 33.424519 	 36.496033 	 40.456417
Epoch 90 	 32.819752 	 36.593746 	 41.327816
Epoch 100 	 31.461586 	 36.059685 	 40.133602
Epoch 110 	 31.537884 	 36.243511 	 41.103493
Epoch 120 	 30.782858 	 34.438000 	 39.650875
Epoch 130 	 30.480993 	 35.532013 	 39.737621
Epoch 140 	 30.178911 	 34.995113 	 39.364620
Epoch 150 	 30.058138 	 35.459873 	 39.678616
[Model stopped early]
Train loss       : 30.051489
Best valid loss  : 34.438000
Best test loss   : 39.650875
Pruning          : 0.27
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 630,951
--------------------------------
Total memory      : 20.84 MB
Total Flops       : 23.99 MFlops
Total Mem (Read)  : 10.04 MB
Total Mem (Write) : 7.21 MB
[Supermasks testing]
[Untrained loss : 108.8194]
[Starting training]
Epoch 0 	 92.519852 	 89.451080 	 92.726990
Epoch 10 	 65.342834 	 64.831566 	 68.368797
Epoch 20 	 59.665237 	 57.020153 	 63.383495
Epoch 30 	 55.037148 	 54.883011 	 59.294308
Epoch 40 	 51.825207 	 54.663731 	 60.062191
Epoch 50 	 44.720135 	 46.651173 	 50.202156
Epoch 60 	 50.974419 	 48.607635 	 52.516857
Epoch 70 	 40.367401 	 43.557472 	 49.134064
Epoch 80 	 42.469601 	 44.067749 	 50.517376
Epoch 90 	 36.562046 	 42.526665 	 46.898655
Epoch 100 	 35.810936 	 39.758076 	 46.035290
Epoch 110 	 35.542248 	 42.298004 	 45.368622
Epoch 120 	 34.993156 	 41.558674 	 44.945564
Epoch 130 	 34.782722 	 40.931290 	 44.286983
[Model stopped early]
Train loss       : 34.794579
Best valid loss  : 39.758076
Best test loss   : 46.035290
Pruning          : 0.19
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 461,817
--------------------------------
Total memory      : 19.39 MB
Total Flops       : 12.83 MFlops
Total Mem (Read)  : 8.07 MB
Total Mem (Write) : 5.74 MB
[Supermasks testing]
[Untrained loss : 104.4718]
[Starting training]
Epoch 0 	 97.159325 	 92.171448 	 94.238396
Epoch 10 	 66.410263 	 63.630379 	 68.612541
Epoch 20 	 60.958447 	 60.199444 	 66.433205
Epoch 30 	 58.282745 	 55.643749 	 61.058197
Epoch 40 	 55.627750 	 55.322197 	 60.428040
Epoch 50 	 51.514473 	 47.656044 	 53.795601
Epoch 60 	 47.154572 	 50.426365 	 54.817944
Epoch 70 	 44.275291 	 47.338078 	 52.991879
Epoch 80 	 42.499226 	 44.311371 	 47.627544
Epoch 90 	 43.262390 	 44.945866 	 50.020866
Epoch 100 	 38.911354 	 40.179447 	 44.627724
Epoch 110 	 37.990170 	 40.006035 	 44.126240
Epoch 120 	 37.008060 	 39.314266 	 42.990101
Epoch 130 	 36.216873 	 39.382507 	 43.480396
Epoch 140 	 35.773411 	 38.708683 	 42.483852
Epoch 150 	 34.579597 	 38.533539 	 41.866905
Epoch 160 	 34.428043 	 37.986694 	 40.628395
Epoch 170 	 34.506718 	 37.199684 	 41.638195
Epoch 180 	 33.943592 	 37.812412 	 40.963428
Epoch 190 	 34.190742 	 36.965012 	 41.059967
[Model stopped early]
Train loss       : 33.904121
Best valid loss  : 36.637379
Best test loss   : 41.166271
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 348,567
--------------------------------
Total memory      : 18.35 MB
Total Flops       : 7.06 MFlops
Total Mem (Read)  : 6.7 MB
Total Mem (Write) : 4.71 MB
[Supermasks testing]
[Untrained loss : 96.2782]
[Starting training]
Epoch 0 	 92.336395 	 89.980980 	 93.015099
Epoch 10 	 68.378914 	 65.218063 	 69.833992
Epoch 20 	 61.822620 	 60.172474 	 65.150360
Epoch 30 	 59.862110 	 57.509537 	 61.202694
Epoch 40 	 59.771381 	 55.189667 	 60.111328
Epoch 50 	 56.879353 	 54.167084 	 58.867687
Epoch 60 	 52.652645 	 52.404419 	 57.035877
Epoch 70 	 49.320045 	 49.958683 	 54.085247
Epoch 80 	 47.931305 	 47.719624 	 53.062641
Epoch 90 	 46.599949 	 46.624744 	 51.238735
Epoch 100 	 47.315712 	 46.972973 	 52.277950
Epoch 110 	 43.915874 	 46.859982 	 50.903625
Epoch 120 	 44.413612 	 47.029034 	 50.767620
Epoch 130 	 43.815395 	 45.523392 	 48.442696
Epoch 140 	 43.397827 	 45.987442 	 49.474590
Epoch 150 	 43.612030 	 48.241970 	 50.650246
Epoch 160 	 39.784389 	 43.643154 	 46.860786
Epoch 170 	 40.660629 	 45.305031 	 47.308083
Epoch 180 	 39.799248 	 43.307415 	 46.078331
Epoch 190 	 38.993660 	 43.851814 	 45.870796
[Model stopped early]
Train loss       : 38.845158
Best valid loss  : 42.876884
Best test loss   : 47.221157
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 272,819
--------------------------------
Total memory      : 17.56 MB
Total Flops       : 3.79 MFlops
Total Mem (Read)  : 5.68 MB
Total Mem (Write) : 3.94 MB
[Supermasks testing]
[Untrained loss : 95.3150]
[Starting training]
Epoch 0 	 92.330894 	 90.279381 	 92.689423
Epoch 10 	 66.978737 	 62.401131 	 67.450035
Epoch 20 	 62.960682 	 63.072323 	 68.085991
Epoch 30 	 60.932434 	 57.404324 	 62.665829
Epoch 40 	 59.936741 	 55.973785 	 61.690228
Epoch 50 	 55.835503 	 52.282589 	 56.542896
Epoch 60 	 52.916996 	 49.634579 	 54.044952
Epoch 70 	 52.272030 	 49.518936 	 54.148098
Epoch 80 	 49.021122 	 49.809959 	 53.758472
Epoch 90 	 46.944973 	 46.611095 	 49.792202
Epoch 100 	 46.231918 	 46.323139 	 50.566898
Epoch 110 	 46.720013 	 47.318073 	 49.909039
Epoch 120 	 45.323112 	 44.411217 	 48.353832
Epoch 130 	 44.131313 	 44.952431 	 47.385876
Epoch 140 	 43.684471 	 44.785820 	 47.354713
Epoch 150 	 44.051796 	 44.251953 	 47.856766
Epoch 160 	 43.114735 	 44.269234 	 47.361042
Epoch 170 	 43.065056 	 44.526421 	 47.574184
Epoch 180 	 42.840046 	 43.230145 	 47.147198
Epoch 190 	 42.317371 	 43.606480 	 47.223507
Train loss       : 42.751877
Best valid loss  : 43.230145
Best test loss   : 47.147198
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 220,983
--------------------------------
Total memory      : 17.11 MB
Total Flops       : 2.56 MFlops
Total Mem (Read)  : 5.08 MB
Total Mem (Write) : 3.52 MB
[Supermasks testing]
[Untrained loss : nan]
[Starting training]
Epoch 0 	 94.059998 	 91.636215 	 93.583496
Epoch 10 	 68.002380 	 66.603424 	 70.357437
Epoch 20 	 64.719902 	 61.076244 	 65.340607
Epoch 30 	 62.172039 	 59.219429 	 63.571411
Epoch 40 	 61.667225 	 57.193035 	 62.730587
Epoch 50 	 58.164909 	 57.932610 	 59.843334
Epoch 60 	 58.216232 	 55.662006 	 58.707546
Epoch 70 	 54.069214 	 54.439846 	 57.817230
Epoch 80 	 53.790398 	 53.752045 	 57.442760
Epoch 90 	 55.022881 	 53.767742 	 57.162117
Epoch 100 	 51.759876 	 52.579975 	 56.292004
Epoch 110 	 50.168938 	 52.384411 	 55.714764
Epoch 120 	 49.941101 	 51.711246 	 55.066750
Epoch 130 	 49.970913 	 52.195450 	 54.759705
Epoch 140 	 49.301598 	 50.904285 	 55.153664
Epoch 150 	 49.885544 	 51.205238 	 54.693642
Epoch 160 	 49.261311 	 51.339497 	 55.124630
Epoch 170 	 48.610634 	 52.329327 	 54.727650
Epoch 180 	 49.061234 	 51.694946 	 54.801697
Epoch 190 	 48.362244 	 51.601295 	 55.063099
Train loss       : 49.072876
Best valid loss  : 50.294739
Best test loss   : 54.664131
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 185,398
--------------------------------
Total memory      : 16.72 MB
Total Flops       : 1.66 MFlops
Total Mem (Read)  : 4.58 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 98.6300]
[Starting training]
Epoch 0 	 93.900330 	 88.279610 	 93.709869
Epoch 10 	 69.503105 	 70.610832 	 74.471184
Epoch 20 	 66.469826 	 63.742359 	 67.934967
Epoch 30 	 64.202766 	 61.384773 	 65.733757
Epoch 40 	 63.884338 	 60.932465 	 65.313461
Epoch 50 	 62.406872 	 60.013557 	 64.476532
Epoch 60 	 60.203907 	 60.549107 	 64.252831
Epoch 70 	 60.401585 	 58.577019 	 62.643188
Epoch 80 	 58.094730 	 57.042416 	 61.198708
Epoch 90 	 57.187614 	 55.995544 	 59.817425
Epoch 100 	 56.707596 	 56.488579 	 60.693245
Epoch 110 	 55.431110 	 55.254547 	 59.958263
Epoch 120 	 54.339691 	 54.673531 	 59.761574
Epoch 130 	 54.528038 	 54.709160 	 59.953247
Epoch 140 	 53.433167 	 53.935863 	 59.287258
Epoch 150 	 54.232178 	 53.784882 	 58.799160
Epoch 160 	 54.044689 	 53.629417 	 59.166279
Epoch 170 	 53.207375 	 53.768509 	 58.745434
Epoch 180 	 52.482170 	 53.515232 	 59.145691
Epoch 190 	 52.692638 	 53.769463 	 59.073830
Train loss       : 52.538490
Best valid loss  : 52.203125
Best test loss   : 58.499977
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 156,046
--------------------------------
Total memory      : 16.63 MB
Total Flops       : 1.63 MFlops
Total Mem (Read)  : 4.41 MB
Total Mem (Write) : 3.08 MB
[Supermasks testing]
[Untrained loss : 611.9980]
[Starting training]
Epoch 0 	 93.128220 	 89.028328 	 93.309937
Epoch 10 	 70.589500 	 71.110832 	 74.004967
Epoch 20 	 69.388657 	 67.565865 	 70.303947
Epoch 30 	 67.094627 	 66.079674 	 69.380913
Epoch 40 	 67.602402 	 66.671356 	 71.024544
Epoch 50 	 66.299477 	 65.219612 	 70.436729
Epoch 60 	 66.172226 	 63.059704 	 68.449089
Epoch 70 	 64.644844 	 65.686676 	 71.107712
Epoch 80 	 63.452763 	 63.014370 	 68.124214
Epoch 90 	 63.426975 	 62.558571 	 67.903572
Epoch 100 	 63.083622 	 62.696251 	 67.453255
Epoch 110 	 62.138214 	 62.463520 	 67.203163
Epoch 120 	 61.780773 	 62.911430 	 66.739510
Epoch 130 	 61.523605 	 61.328880 	 66.908661
Epoch 140 	 62.029938 	 62.560986 	 67.125053
Epoch 150 	 61.523926 	 62.965534 	 66.940849
[Model stopped early]
Train loss       : 61.007580
Best valid loss  : 60.557350
Best test loss   : 66.906975
Pruning          : 0.03
[Performing one full cumulative epoch]
Traceback (most recent call last):
  File "main.py", line 261, in <module>
    if (args.prune_selection in ['activation', 'information', 'info_target']):
  File "/scratch/esling/lottery/pruning.py", line 781, in reset
    replace_recurrent(m, l, m.unprune_idx[l], prev_kept)
  File "/scratch/esling/lottery/pruning.py", line 752, in replace_recurrent
    cur_ih = nn.Parameter(cur_ih[rep_id0])#torch.from_numpy(cur_ih[rep_id0]).to(self.args.device))
IndexError: too many indices for tensor of dimension 2
