Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40871919.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, future, torch, six, torchvision, tqdm, pyparsing, cycler, kiwisolver, python-dateutil, matplotlib, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, chardet, certifi, urllib3, idna, requests, requests-oauthlib, google-auth-oauthlib, absl-py, protobuf, werkzeug, grpcio, markdown, tensorboard, gast, astor, termcolor, wrapt, tensorflow-estimator, opt-einsum, h5py, keras-applications, keras-preprocessing, google-pasta, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871919.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-22 07:25:44.804643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-22 07:25:45.132061: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_sing_ae_cnn_xavier_trimming_information_reinit_global_0.
*******
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5697]
[Starting training]
Epoch 0 	 0.460182 	 0.409438 	 0.416268
Epoch 10 	 0.209034 	 0.202261 	 0.210500
Epoch 20 	 0.176606 	 0.175784 	 0.183353
Epoch 30 	 0.162649 	 0.163411 	 0.172046
Epoch 40 	 0.147818 	 0.157886 	 0.165489
Epoch 50 	 0.140957 	 0.153103 	 0.162473
Epoch 60 	 0.134278 	 0.144117 	 0.151691
Epoch 70 	 0.133083 	 0.143072 	 0.152372
Epoch 80 	 0.120223 	 0.131584 	 0.142350
Epoch 90 	 0.114815 	 0.131467 	 0.139765
Epoch 100 	 0.112888 	 0.130481 	 0.140360
Epoch 110 	 0.104315 	 0.123649 	 0.134904
Epoch 120 	 0.102959 	 0.123683 	 0.134570
Epoch 130 	 0.098704 	 0.122289 	 0.132288
Epoch 140 	 0.098001 	 0.122726 	 0.131969
Epoch 150 	 0.097212 	 0.120975 	 0.131584
Train loss       : 0.095040
Best valid loss  : 0.119020
Best test loss   : 0.130743
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 13,646,274
--------------------------------
Total memory      : 17.09 MB
Total Flops       : 3.02 GFlops
Total Mem (Read)  : 62.53 MB
Total Mem (Write) : 16.86 MB
[Supermasks testing]
[Untrained loss : 0.5729]
[Starting training]
Epoch 0 	 0.470052 	 0.419576 	 0.428926
Epoch 10 	 0.226969 	 0.217648 	 0.227540
Epoch 20 	 0.175439 	 0.171020 	 0.181781
Epoch 30 	 0.157231 	 0.163570 	 0.169463
Epoch 40 	 0.148322 	 0.153835 	 0.160881
Epoch 50 	 0.141606 	 0.150882 	 0.158418
Epoch 60 	 0.137861 	 0.147808 	 0.156290
Epoch 70 	 0.134192 	 0.141522 	 0.152103
Epoch 80 	 0.131130 	 0.141350 	 0.151085
Epoch 90 	 0.136645 	 0.140071 	 0.150723
Epoch 100 	 0.127990 	 0.140428 	 0.147511
Epoch 110 	 0.113870 	 0.127173 	 0.138958
Epoch 120 	 0.113174 	 0.126820 	 0.138967
Epoch 130 	 0.113266 	 0.128807 	 0.138608
Epoch 140 	 0.104829 	 0.123687 	 0.134179
Epoch 150 	 0.101597 	 0.123200 	 0.132801
Train loss       : 0.101060
Best valid loss  : 0.121275
Best test loss   : 0.132483
Pruning          : 0.78
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 8,922,728
--------------------------------
Total memory      : 12.15 MB
Total Flops       : 1.96 GFlops
Total Mem (Read)  : 41.58 MB
Total Mem (Write) : 11.92 MB
[Supermasks testing]
[Untrained loss : 0.5175]
[Starting training]
Epoch 0 	 0.455563 	 0.412616 	 0.424741
Epoch 10 	 0.225930 	 0.218399 	 0.229744
Epoch 20 	 0.180423 	 0.183696 	 0.192443
Epoch 30 	 0.162765 	 0.167024 	 0.173514
Epoch 40 	 0.152476 	 0.158231 	 0.167072
Epoch 50 	 0.145225 	 0.154247 	 0.161458
Epoch 60 	 0.138723 	 0.147954 	 0.155616
Epoch 70 	 0.135180 	 0.147536 	 0.153724
Epoch 80 	 0.136029 	 0.148427 	 0.154879
Epoch 90 	 0.131448 	 0.145861 	 0.152567
Epoch 100 	 0.129745 	 0.144515 	 0.151311
Epoch 110 	 0.115558 	 0.131170 	 0.140743
Epoch 120 	 0.114288 	 0.132729 	 0.140630
Epoch 130 	 0.107053 	 0.127026 	 0.136371
Epoch 140 	 0.106635 	 0.127769 	 0.136921
Epoch 150 	 0.105940 	 0.127608 	 0.135435
Train loss       : 0.102497
Best valid loss  : 0.125889
Best test loss   : 0.133521
Pruning          : 0.61
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,889,851
--------------------------------
Total memory      : 7.58 MB
Total Flops       : 801.22 MFlops
Total Mem (Read)  : 19.38 MB
Total Mem (Write) : 7.35 MB
[Supermasks testing]
[Untrained loss : 0.5166]
[Starting training]
Epoch 0 	 0.458232 	 0.416935 	 0.424808
Epoch 10 	 0.222045 	 0.213221 	 0.224275
Epoch 20 	 0.183912 	 0.181367 	 0.192168
Epoch 30 	 0.171290 	 0.172525 	 0.181631
Epoch 40 	 0.158644 	 0.167316 	 0.175543
Epoch 50 	 0.151479 	 0.159833 	 0.168040
Epoch 60 	 0.149615 	 0.153540 	 0.162772
Epoch 70 	 0.147647 	 0.151112 	 0.159644
Epoch 80 	 0.140146 	 0.150506 	 0.159214
Epoch 90 	 0.126077 	 0.138776 	 0.147675
Epoch 100 	 0.124689 	 0.139747 	 0.147303
Epoch 110 	 0.117161 	 0.133053 	 0.142513
Epoch 120 	 0.115886 	 0.134580 	 0.141791
Epoch 130 	 0.115076 	 0.133653 	 0.141849
Epoch 140 	 0.110833 	 0.130019 	 0.138733
Epoch 150 	 0.108801 	 0.128493 	 0.137888
Train loss       : 0.108357
Best valid loss  : 0.128154
Best test loss   : 0.137899
Pruning          : 0.47
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 996,347
--------------------------------
Total memory      : 3.46 MB
Total Flops       : 144.35 MFlops
Total Mem (Read)  : 5.33 MB
Total Mem (Write) : 3.23 MB
[Supermasks testing]
[Untrained loss : 0.5701]
[Starting training]
Epoch 0 	 0.470572 	 0.431143 	 0.438623
Epoch 10 	 0.246565 	 0.237772 	 0.249613
Epoch 20 	 0.198970 	 0.199631 	 0.210878
Epoch 30 	 0.181546 	 0.185530 	 0.192109
Epoch 40 	 0.172749 	 0.177781 	 0.183966
Epoch 50 	 0.164504 	 0.170127 	 0.176781
Epoch 60 	 0.159953 	 0.167401 	 0.174367
Epoch 70 	 0.156545 	 0.162871 	 0.170262
Epoch 80 	 0.151616 	 0.161184 	 0.170265
Epoch 90 	 0.138605 	 0.151421 	 0.159013
Epoch 100 	 0.137304 	 0.147362 	 0.157358
Epoch 110 	 0.135476 	 0.146136 	 0.156716
Epoch 120 	 0.133614 	 0.148324 	 0.156500
Epoch 130 	 0.132977 	 0.146148 	 0.156228
Epoch 140 	 0.132767 	 0.146661 	 0.155244
Epoch 150 	 0.124425 	 0.141815 	 0.150096
Train loss       : 0.124174
Best valid loss  : 0.138926
Best test loss   : 0.149767
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 757,219
--------------------------------
Total memory      : 2.71 MB
Total Flops       : 114.07 MFlops
Total Mem (Read)  : 4.21 MB
Total Mem (Write) : 2.47 MB
[Supermasks testing]
[Untrained loss : 0.5362]
[Starting training]
Epoch 0 	 0.480065 	 0.431737 	 0.442013
Epoch 10 	 0.262618 	 0.253076 	 0.266602
Epoch 20 	 0.212949 	 0.208332 	 0.218245
Epoch 30 	 0.188738 	 0.195618 	 0.201232
Epoch 40 	 0.177202 	 0.182953 	 0.187940
Epoch 50 	 0.167488 	 0.171681 	 0.179930
Epoch 60 	 0.161369 	 0.167517 	 0.173276
Epoch 70 	 0.156291 	 0.161763 	 0.167701
Epoch 80 	 0.151172 	 0.158585 	 0.165730
Epoch 90 	 0.148634 	 0.155720 	 0.162091
Epoch 100 	 0.136013 	 0.147605 	 0.155347
Epoch 110 	 0.133659 	 0.148394 	 0.154944
Epoch 120 	 0.132657 	 0.145707 	 0.154898
Epoch 130 	 0.130727 	 0.144153 	 0.152430
Epoch 140 	 0.126108 	 0.138872 	 0.147362
Epoch 150 	 0.123713 	 0.140965 	 0.147337
Train loss       : 0.120210
Best valid loss  : 0.136564
Best test loss   : 0.145455
Pruning          : 0.29
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 541,570
--------------------------------
Total memory      : 2.16 MB
Total Flops       : 72.49 MFlops
Total Mem (Read)  : 3.07 MB
Total Mem (Write) : 1.93 MB
[Supermasks testing]
[Untrained loss : 0.5690]
[Starting training]
Epoch 0 	 0.479613 	 0.434149 	 0.442391
Epoch 10 	 0.277726 	 0.268852 	 0.280218
Epoch 20 	 0.223393 	 0.222902 	 0.231830
Epoch 30 	 0.196911 	 0.204592 	 0.211979
Epoch 40 	 0.184293 	 0.189172 	 0.196320
Epoch 50 	 0.177763 	 0.182197 	 0.189563
Epoch 60 	 0.166889 	 0.173815 	 0.181213
Epoch 70 	 0.162431 	 0.169904 	 0.176871
Epoch 80 	 0.159005 	 0.164966 	 0.173113
Epoch 90 	 0.156460 	 0.161793 	 0.172164
Epoch 100 	 0.153313 	 0.163037 	 0.169948
Epoch 110 	 0.151109 	 0.160099 	 0.167480
Epoch 120 	 0.149976 	 0.157029 	 0.165891
Epoch 130 	 0.148108 	 0.155645 	 0.163311
Epoch 140 	 0.137133 	 0.147646 	 0.156274
Epoch 150 	 0.136836 	 0.149332 	 0.156566
Train loss       : 0.130835
Best valid loss  : 0.144170
Best test loss   : 0.153472
Pruning          : 0.23
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 365,615
--------------------------------
Total memory      : 1.74 MB
Total Flops       : 36.39 MFlops
Total Mem (Read)  : 2.11 MB
Total Mem (Write) : 1.5 MB
[Supermasks testing]
[Untrained loss : 0.5327]
[Starting training]
Epoch 0 	 0.485733 	 0.435505 	 0.443354
Epoch 10 	 0.308878 	 0.292894 	 0.305851
Epoch 20 	 0.245171 	 0.242402 	 0.250977
Epoch 30 	 0.220073 	 0.219193 	 0.230146
Epoch 40 	 0.208613 	 0.211034 	 0.219168
Epoch 50 	 0.199910 	 0.201823 	 0.211686
Epoch 60 	 0.192926 	 0.195476 	 0.203478
Epoch 70 	 0.187012 	 0.189933 	 0.199396
Epoch 80 	 0.183206 	 0.187877 	 0.197589
Epoch 90 	 0.179553 	 0.185743 	 0.193946
Epoch 100 	 0.175959 	 0.182269 	 0.189819
Epoch 110 	 0.174602 	 0.181129 	 0.188957
Epoch 120 	 0.174100 	 0.178060 	 0.185847
Epoch 130 	 0.169828 	 0.177618 	 0.184886
Epoch 140 	 0.166847 	 0.172257 	 0.181394
Epoch 150 	 0.165014 	 0.169775 	 0.178544
Train loss       : 0.163225
Best valid loss  : 0.169078
Best test loss   : 0.176396
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 247,057
--------------------------------
Total memory      : 1.41 MB
Total Flops       : 14.71 MFlops
Total Mem (Read)  : 1.47 MB
Total Mem (Write) : 1.17 MB
[Supermasks testing]
[Untrained loss : 0.5269]
[Starting training]
Epoch 0 	 0.502143 	 0.450564 	 0.455030
Epoch 10 	 0.345753 	 0.328874 	 0.341667
Epoch 20 	 0.297222 	 0.288883 	 0.299018
Epoch 30 	 0.270792 	 0.267064 	 0.276213
Epoch 40 	 0.255508 	 0.255386 	 0.264058
Epoch 50 	 0.247836 	 0.248370 	 0.260747
Epoch 60 	 0.242147 	 0.240248 	 0.250791
Epoch 70 	 0.238833 	 0.239419 	 0.247642
Epoch 80 	 0.229406 	 0.231508 	 0.240364
Epoch 90 	 0.226949 	 0.230871 	 0.239944
Epoch 100 	 0.221778 	 0.225954 	 0.235724
Epoch 110 	 0.218703 	 0.221723 	 0.233823
Epoch 120 	 0.218164 	 0.223251 	 0.233062
Epoch 130 	 0.216643 	 0.221148 	 0.232399
Epoch 140 	 0.215988 	 0.223074 	 0.231954
[Model stopped early]
Train loss       : 0.215988
Best valid loss  : 0.219018
Best test loss   : 0.233636
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 179,259
--------------------------------
Total memory      : 1.15 MB
Total Flops       : 10.37 MFlops
Total Mem (Read)  : 1.2 MB
Total Mem (Write) : 938.23 KB
[Supermasks testing]
[Untrained loss : 0.6350]
[Starting training]
Epoch 0 	 0.498037 	 0.446475 	 0.453126
Epoch 10 	 0.370377 	 0.357355 	 0.365865
Epoch 20 	 0.332151 	 0.323249 	 0.331099
Epoch 30 	 0.312541 	 0.305209 	 0.314272
Epoch 40 	 0.301415 	 0.295375 	 0.302889
Epoch 50 	 0.293727 	 0.290000 	 0.295148
Epoch 60 	 0.287785 	 0.283368 	 0.290417
Epoch 70 	 0.283293 	 0.279153 	 0.286029
Epoch 80 	 0.279251 	 0.273664 	 0.283999
Epoch 90 	 0.276503 	 0.274228 	 0.282446
Epoch 100 	 0.268308 	 0.264769 	 0.275007
Epoch 110 	 0.266721 	 0.268528 	 0.273991
Epoch 120 	 0.262268 	 0.263715 	 0.270468
Epoch 130 	 0.260864 	 0.262082 	 0.269584
Epoch 140 	 0.260384 	 0.263069 	 0.269277
Epoch 150 	 0.259903 	 0.258643 	 0.268425
Train loss       : 0.257058
Best valid loss  : 0.258322
Best test loss   : 0.269530
Pruning          : 0.11
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 145,683
--------------------------------
Total memory      : 0.95 MB
Total Flops       : 6.89 MFlops
Total Mem (Read)  : 983.07 KB
Total Mem (Write) : 732.71 KB
[Supermasks testing]
[Untrained loss : 0.6278]
[Starting training]
Epoch 0 	 0.492598 	 0.446924 	 0.452861
Epoch 10 	 0.402194 	 0.387737 	 0.396509
Epoch 20 	 0.361598 	 0.350328 	 0.360010
Epoch 30 	 0.339552 	 0.328726 	 0.338936
Epoch 40 	 0.325437 	 0.314073 	 0.325680
Epoch 50 	 0.315895 	 0.307296 	 0.316852
Epoch 60 	 0.309772 	 0.304322 	 0.312478
Epoch 70 	 0.305563 	 0.300807 	 0.308758
Epoch 80 	 0.302982 	 0.298476 	 0.306085
Epoch 90 	 0.300711 	 0.295311 	 0.303723
Epoch 100 	 0.297406 	 0.294115 	 0.302205
Epoch 110 	 0.295969 	 0.290187 	 0.300135
Epoch 120 	 0.289671 	 0.285823 	 0.294741
Epoch 130 	 0.288544 	 0.281498 	 0.295440
Epoch 140 	 0.287609 	 0.284916 	 0.293660
Epoch 150 	 0.286190 	 0.282193 	 0.293190
Train loss       : 0.282801
Best valid loss  : 0.278184
Best test loss   : 0.289647
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 107,998
--------------------------------
Total memory      : 0.79 MB
Total Flops       : 5.17 MFlops
Total Mem (Read)  : 824.59 KB
Total Mem (Write) : 573.08 KB
[Supermasks testing]
[Untrained loss : 0.8241]
[Starting training]
Epoch 0 	 0.499292 	 0.445825 	 0.454348
Epoch 10 	 0.414971 	 0.396153 	 0.410250
Epoch 20 	 0.404050 	 0.390996 	 0.400328
Epoch 30 	 0.397977 	 0.382128 	 0.394665
Epoch 40 	 0.391562 	 0.378307 	 0.390149
Epoch 50 	 0.387232 	 0.376920 	 0.386797
Epoch 60 	 0.384977 	 0.371857 	 0.384692
Epoch 70 	 0.382243 	 0.368335 	 0.382858
Epoch 80 	 0.379401 	 0.366263 	 0.380520
Epoch 90 	 0.378614 	 0.363735 	 0.380142
Epoch 100 	 0.376907 	 0.362063 	 0.378436
Epoch 110 	 0.376082 	 0.363525 	 0.377950
[Model stopped early]
Train loss       : 0.376078
Best valid loss  : 0.359901
Best test loss   : 0.379882
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 100,898
--------------------------------
Total memory      : 0.66 MB
Total Flops       : 3.43 MFlops
Total Mem (Read)  : 662.62 KB
Total Mem (Write) : 438.84 KB
[Supermasks testing]
[Untrained loss : 0.9052]
[Starting training]
Epoch 0 	 0.521137 	 0.484089 	 0.485519
Epoch 10 	 0.415948 	 0.397316 	 0.410692
Epoch 20 	 0.404466 	 0.392897 	 0.402113
Epoch 30 	 0.394103 	 0.381351 	 0.391968
Epoch 40 	 0.385763 	 0.372417 	 0.382281
Epoch 50 	 0.375607 	 0.368987 	 0.373881
Epoch 60 	 0.369839 	 0.359940 	 0.368641
Epoch 70 	 0.364579 	 0.350846 	 0.363156
Epoch 80 	 0.360543 	 0.351980 	 0.361209
Epoch 90 	 0.356531 	 0.347131 	 0.356514
Epoch 100 	 0.353248 	 0.339310 	 0.351678
Epoch 110 	 0.350190 	 0.336602 	 0.350545
Epoch 120 	 0.346860 	 0.336734 	 0.346238
Epoch 130 	 0.343048 	 0.333844 	 0.343687
Epoch 140 	 0.341520 	 0.330580 	 0.341639
Epoch 150 	 0.340049 	 0.332414 	 0.340817
Train loss       : 0.337815
Best valid loss  : 0.324489
Best test loss   : 0.338696
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 73,421
--------------------------------
Total memory      : 0.57 MB
Total Flops       : 2.96 MFlops
Total Mem (Read)  : 586.01 KB
Total Mem (Write) : 345.93 KB
[Supermasks testing]
[Untrained loss : 0.7791]
[Starting training]
Epoch 0 	 0.513736 	 0.460760 	 0.462090
Epoch 10 	 0.413713 	 0.400780 	 0.409700
Epoch 20 	 0.386511 	 0.373524 	 0.381421
Epoch 30 	 0.373341 	 0.359722 	 0.370710
Epoch 40 	 0.363778 	 0.353760 	 0.361596
Epoch 50 	 0.358689 	 0.346340 	 0.356945
Epoch 60 	 0.355113 	 0.342777 	 0.354708
Epoch 70 	 0.351632 	 0.343543 	 0.351928
Epoch 80 	 0.348860 	 0.340651 	 0.347929
Epoch 90 	 0.346549 	 0.336127 	 0.346969
Epoch 100 	 0.344321 	 0.339551 	 0.343608
Epoch 110 	 0.340027 	 0.333300 	 0.340494
Epoch 120 	 0.338878 	 0.332074 	 0.340257
Epoch 130 	 0.336912 	 0.329193 	 0.338780
Epoch 140 	 0.334540 	 0.326744 	 0.336367
Epoch 150 	 0.334198 	 0.326211 	 0.335845
Train loss       : 0.333254
Best valid loss  : 0.324212
Best test loss   : 0.334696
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 56,205
--------------------------------
Total memory      : 0.50 MB
Total Flops       : 2.51 MFlops
Total Mem (Read)  : 520.86 KB
Total Mem (Write) : 273.85 KB
[Supermasks testing]
[Untrained loss : 0.7173]
[Starting training]
Epoch 0 	 0.495326 	 0.443141 	 0.450156
Epoch 10 	 0.403920 	 0.391880 	 0.400280
Epoch 20 	 0.385478 	 0.370014 	 0.383228
Epoch 30 	 0.368178 	 0.356892 	 0.369701
Epoch 40 	 0.359988 	 0.349309 	 0.361743
Epoch 50 	 0.354422 	 0.343844 	 0.358496
Epoch 60 	 0.351817 	 0.341796 	 0.356206
Epoch 70 	 0.349082 	 0.338788 	 0.352885
Epoch 80 	 0.346988 	 0.335256 	 0.352381
Epoch 90 	 0.344738 	 0.334319 	 0.349502
Epoch 100 	 0.342630 	 0.332385 	 0.347489
Epoch 110 	 0.340232 	 0.325814 	 0.345192
Epoch 120 	 0.338284 	 0.327326 	 0.342935
Epoch 130 	 0.334515 	 0.324841 	 0.340853
Epoch 140 	 0.332111 	 0.323002 	 0.338321
Epoch 150 	 0.330629 	 0.321556 	 0.337628
Train loss       : 0.329418
Best valid loss  : 0.318733
Best test loss   : 0.337886
Pruning          : 0.03
