Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40871940.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, future, torch, six, torchvision, tqdm, pyparsing, cycler, python-dateutil, kiwisolver, matplotlib, google-pasta, absl-py, grpcio, tensorflow-estimator, opt-einsum, keras-preprocessing, gast, astor, urllib3, chardet, certifi, idna, requests, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, werkzeug, protobuf, tensorboard, h5py, keras-applications, termcolor, wrapt, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871940.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-22 07:38:13.566692: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-22 07:38:13.576698: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_sing_ae_cnn_xavier_masking_magnitude_rewind_local_0.
*******
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5683]
[Starting training]
Epoch 0 	 0.462170 	 0.426946 	 0.417244
Epoch 10 	 0.210867 	 0.213739 	 0.216406
Epoch 20 	 0.171668 	 0.179900 	 0.183124
Epoch 30 	 0.156838 	 0.174910 	 0.178599
Epoch 40 	 0.147795 	 0.159426 	 0.165199
Epoch 50 	 0.144089 	 0.157503 	 0.162104
Epoch 60 	 0.137009 	 0.153823 	 0.157000
Epoch 70 	 0.136969 	 0.153343 	 0.155576
Epoch 80 	 0.129579 	 0.148275 	 0.155023
Epoch 90 	 0.125461 	 0.147636 	 0.152370
Epoch 100 	 0.124046 	 0.146197 	 0.148871
Epoch 110 	 0.122858 	 0.143367 	 0.147238
Epoch 120 	 0.109607 	 0.135735 	 0.139321
Epoch 130 	 0.102593 	 0.131115 	 0.134192
Epoch 140 	 0.101429 	 0.129885 	 0.133361
Epoch 150 	 0.100583 	 0.129586 	 0.133500
Train loss       : 0.096862
Best valid loss  : 0.123441
Best test loss   : 0.131505
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1516]
[Starting training]
Epoch 0 	 0.133796 	 0.152214 	 0.153057
Epoch 10 	 0.129504 	 0.151245 	 0.151630
Epoch 20 	 0.125132 	 0.147998 	 0.149952
Epoch 30 	 0.123070 	 0.144750 	 0.148816
Epoch 40 	 0.109491 	 0.134095 	 0.137735
Epoch 50 	 0.109539 	 0.135644 	 0.138753
Epoch 60 	 0.107970 	 0.134323 	 0.137584
Epoch 70 	 0.100198 	 0.129943 	 0.133449
Epoch 80 	 0.100077 	 0.128884 	 0.133158
Epoch 90 	 0.095837 	 0.127229 	 0.131053
Epoch 100 	 0.094234 	 0.126169 	 0.130334
[Model stopped early]
Train loss       : 0.093938
Best valid loss  : 0.125040
Best test loss   : 0.133573
Pruning          : 0.70
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1519]
[Starting training]
Epoch 0 	 0.132965 	 0.148333 	 0.152001
Epoch 10 	 0.126879 	 0.146103 	 0.150703
Epoch 20 	 0.112348 	 0.136729 	 0.140235
Epoch 30 	 0.111296 	 0.135553 	 0.139515
Epoch 40 	 0.110240 	 0.134503 	 0.137966
Epoch 50 	 0.101955 	 0.129184 	 0.134103
Epoch 60 	 0.101394 	 0.131172 	 0.134142
Epoch 70 	 0.097293 	 0.128365 	 0.131746
Epoch 80 	 0.095259 	 0.126849 	 0.130871
[Model stopped early]
Train loss       : 0.095387
Best valid loss  : 0.125305
Best test loss   : 0.134086
Pruning          : 0.49
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1522]
[Starting training]
Epoch 0 	 0.133485 	 0.143617 	 0.149778
Epoch 10 	 0.126384 	 0.142632 	 0.147595
Epoch 20 	 0.126507 	 0.151467 	 0.153265
Epoch 30 	 0.111420 	 0.136493 	 0.140145
Epoch 40 	 0.110831 	 0.135748 	 0.138598
Epoch 50 	 0.102218 	 0.130553 	 0.133683
Epoch 60 	 0.098543 	 0.128889 	 0.132255
Epoch 70 	 0.098013 	 0.129016 	 0.131753
Epoch 80 	 0.097440 	 0.127283 	 0.131892
Epoch 90 	 0.096687 	 0.126940 	 0.131358
Epoch 100 	 0.096465 	 0.127732 	 0.131997
Epoch 110 	 0.095683 	 0.128411 	 0.131190
Epoch 120 	 0.093716 	 0.126441 	 0.130231
Epoch 130 	 0.092674 	 0.120290 	 0.129994
Epoch 140 	 0.092481 	 0.124959 	 0.129997
Epoch 150 	 0.092116 	 0.126280 	 0.129936
Train loss       : 0.091943
Best valid loss  : 0.120290
Best test loss   : 0.129994
Pruning          : 0.34
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1526]
[Starting training]
Epoch 0 	 0.133314 	 0.146476 	 0.151823
Epoch 10 	 0.126668 	 0.148118 	 0.149687
Epoch 20 	 0.124795 	 0.147851 	 0.150183
Epoch 30 	 0.111501 	 0.135683 	 0.139320
Epoch 40 	 0.110260 	 0.136113 	 0.138106
Epoch 50 	 0.104178 	 0.126343 	 0.134714
Epoch 60 	 0.101929 	 0.129980 	 0.134307
Epoch 70 	 0.101431 	 0.130006 	 0.133388
Epoch 80 	 0.097233 	 0.128668 	 0.131698
Epoch 90 	 0.095482 	 0.127026 	 0.130837
[Model stopped early]
Train loss       : 0.095482
Best valid loss  : 0.125460
Best test loss   : 0.133580
Pruning          : 0.24
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1533]
[Starting training]
Epoch 0 	 0.132438 	 0.147342 	 0.151482
Epoch 10 	 0.129196 	 0.149865 	 0.151416
Epoch 20 	 0.112253 	 0.136913 	 0.139273
Epoch 30 	 0.111210 	 0.137385 	 0.139171
Epoch 40 	 0.110198 	 0.136661 	 0.139504
Epoch 50 	 0.103626 	 0.130171 	 0.133552
Epoch 60 	 0.102030 	 0.129853 	 0.133648
Epoch 70 	 0.100248 	 0.129798 	 0.133770
Epoch 80 	 0.097127 	 0.128786 	 0.131377
Epoch 90 	 0.096082 	 0.127133 	 0.131064
Epoch 100 	 0.094289 	 0.126155 	 0.130478
Epoch 110 	 0.093193 	 0.126928 	 0.130034
[Model stopped early]
Train loss       : 0.093127
Best valid loss  : 0.124412
Best test loss   : 0.131119
Pruning          : 0.17
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1548]
[Starting training]
Epoch 0 	 0.133433 	 0.148904 	 0.151709
Epoch 10 	 0.126076 	 0.145094 	 0.149075
Epoch 20 	 0.111401 	 0.134461 	 0.139600
Epoch 30 	 0.109669 	 0.135432 	 0.138214
Epoch 40 	 0.107428 	 0.130168 	 0.137499
Epoch 50 	 0.107178 	 0.134547 	 0.137266
Epoch 60 	 0.100163 	 0.128898 	 0.133325
Epoch 70 	 0.099523 	 0.127664 	 0.132747
Epoch 80 	 0.095798 	 0.126473 	 0.130965
Epoch 90 	 0.095643 	 0.126811 	 0.130878
Epoch 100 	 0.093880 	 0.126678 	 0.130484
Epoch 110 	 0.093176 	 0.125349 	 0.130003
Epoch 120 	 0.092868 	 0.125874 	 0.129970
Epoch 130 	 0.092316 	 0.126494 	 0.129931
Epoch 140 	 0.092098 	 0.125101 	 0.129858
Epoch 150 	 0.092144 	 0.126371 	 0.129872
[Model stopped early]
Train loss       : 0.092075
Best valid loss  : 0.121650
Best test loss   : 0.129873
Pruning          : 0.12
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1630]
[Starting training]
Epoch 0 	 0.135428 	 0.149363 	 0.153120
Epoch 10 	 0.124030 	 0.144870 	 0.148554
Epoch 20 	 0.121167 	 0.145652 	 0.149146
Epoch 30 	 0.107681 	 0.133704 	 0.137434
Epoch 40 	 0.107970 	 0.136092 	 0.139675
Epoch 50 	 0.100301 	 0.128615 	 0.133443
Epoch 60 	 0.099729 	 0.129444 	 0.132649
Epoch 70 	 0.096469 	 0.124225 	 0.131586
Epoch 80 	 0.096209 	 0.127860 	 0.131490
Epoch 90 	 0.094548 	 0.125893 	 0.130589
Epoch 100 	 0.093929 	 0.124903 	 0.130370
Epoch 110 	 0.093498 	 0.126654 	 0.130241
Epoch 120 	 0.093393 	 0.126252 	 0.130234
[Model stopped early]
Train loss       : 0.093295
Best valid loss  : 0.121978
Best test loss   : 0.130320
Pruning          : 0.08
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1794]
[Starting training]
Epoch 0 	 0.139781 	 0.153145 	 0.156157
Epoch 10 	 0.124861 	 0.147401 	 0.151242
Epoch 20 	 0.122514 	 0.145808 	 0.147835
Epoch 30 	 0.120241 	 0.142168 	 0.145438
Epoch 40 	 0.117839 	 0.142541 	 0.146162
Epoch 50 	 0.117418 	 0.142396 	 0.145146
Epoch 60 	 0.116531 	 0.139810 	 0.142928
Epoch 70 	 0.116087 	 0.142431 	 0.146727
Epoch 80 	 0.117300 	 0.140072 	 0.145127
Epoch 90 	 0.104656 	 0.131241 	 0.135323
Epoch 100 	 0.099790 	 0.127723 	 0.132751
Epoch 110 	 0.099441 	 0.129824 	 0.132657
Epoch 120 	 0.099403 	 0.130257 	 0.132996
Epoch 130 	 0.099170 	 0.129446 	 0.132485
Epoch 140 	 0.096605 	 0.128129 	 0.130932
Epoch 150 	 0.096416 	 0.126269 	 0.130975
Train loss       : 0.096319
Best valid loss  : 0.123414
Best test loss   : 0.130767
Pruning          : 0.06
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.2235]
[Starting training]
Epoch 0 	 0.151003 	 0.155392 	 0.158609
Epoch 10 	 0.126161 	 0.145469 	 0.149481
Epoch 20 	 0.121586 	 0.142452 	 0.147612
Epoch 30 	 0.121439 	 0.144934 	 0.148218
Epoch 40 	 0.108907 	 0.134986 	 0.137831
Epoch 50 	 0.109120 	 0.135449 	 0.138904
Epoch 60 	 0.107752 	 0.135009 	 0.137524
Epoch 70 	 0.103227 	 0.131283 	 0.134719
Epoch 80 	 0.103012 	 0.129298 	 0.134592
Epoch 90 	 0.100500 	 0.129399 	 0.133514
Epoch 100 	 0.100310 	 0.128939 	 0.133319
Epoch 110 	 0.099260 	 0.129127 	 0.132677
Epoch 120 	 0.098689 	 0.129338 	 0.132610
[Model stopped early]
Train loss       : 0.098570
Best valid loss  : 0.124858
Best test loss   : 0.133105
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.2768]
[Starting training]
Epoch 0 	 0.166855 	 0.163455 	 0.168228
Epoch 10 	 0.129538 	 0.153244 	 0.155324
Epoch 20 	 0.119411 	 0.140103 	 0.144267
Epoch 30 	 0.115972 	 0.140397 	 0.143126
Epoch 40 	 0.115245 	 0.138365 	 0.143071
Epoch 50 	 0.110170 	 0.135160 	 0.139234
Epoch 60 	 0.109663 	 0.136743 	 0.139468
Epoch 70 	 0.107540 	 0.134673 	 0.137874
Epoch 80 	 0.107281 	 0.133604 	 0.137762
Epoch 90 	 0.106134 	 0.133892 	 0.137469
Epoch 100 	 0.105754 	 0.133403 	 0.137268
Epoch 110 	 0.105601 	 0.133900 	 0.137087
Epoch 120 	 0.105354 	 0.133589 	 0.137035
Epoch 130 	 0.105269 	 0.132313 	 0.136998
[Model stopped early]
Train loss       : 0.105291
Best valid loss  : 0.129957
Best test loss   : 0.137117
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.3451]
[Starting training]
Epoch 0 	 0.185941 	 0.176564 	 0.179403
Epoch 10 	 0.137147 	 0.153708 	 0.157492
Epoch 20 	 0.132279 	 0.151173 	 0.155981
Epoch 30 	 0.129436 	 0.149404 	 0.153146
Epoch 40 	 0.128634 	 0.149875 	 0.151911
Epoch 50 	 0.127895 	 0.147116 	 0.151423
Epoch 60 	 0.128417 	 0.147606 	 0.152645
Epoch 70 	 0.120445 	 0.142344 	 0.146705
Epoch 80 	 0.120216 	 0.142671 	 0.147136
Epoch 90 	 0.116368 	 0.139260 	 0.144317
Epoch 100 	 0.116131 	 0.140233 	 0.144655
Epoch 110 	 0.114588 	 0.138717 	 0.143390
Epoch 120 	 0.114497 	 0.139230 	 0.143360
Epoch 130 	 0.114417 	 0.140416 	 0.143251
Epoch 140 	 0.113303 	 0.138760 	 0.142828
Epoch 150 	 0.113112 	 0.138623 	 0.142815
Train loss       : 0.112636
Best valid loss  : 0.134931
Best test loss   : 0.142965
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.4213]
[Starting training]
Epoch 0 	 0.214365 	 0.191772 	 0.195870
Epoch 10 	 0.145387 	 0.160219 	 0.166124
Epoch 20 	 0.140520 	 0.160397 	 0.162392
Epoch 30 	 0.137485 	 0.159200 	 0.161612
Epoch 40 	 0.137052 	 0.155027 	 0.157880
Epoch 50 	 0.129139 	 0.150204 	 0.153775
Epoch 60 	 0.128785 	 0.150265 	 0.153918
Epoch 70 	 0.128420 	 0.149841 	 0.153850
Epoch 80 	 0.125012 	 0.148145 	 0.151366
Epoch 90 	 0.123244 	 0.143130 	 0.150446
Epoch 100 	 0.123347 	 0.147646 	 0.150352
Epoch 110 	 0.122470 	 0.146202 	 0.149870
Epoch 120 	 0.121917 	 0.145672 	 0.149775
[Model stopped early]
Train loss       : 0.121831
Best valid loss  : 0.142756
Best test loss   : 0.150443
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.4686]
[Starting training]
Epoch 0 	 0.249200 	 0.212468 	 0.216202
Epoch 10 	 0.154441 	 0.169804 	 0.174160
Epoch 20 	 0.148437 	 0.164615 	 0.168708
Epoch 30 	 0.144805 	 0.161695 	 0.166682
Epoch 40 	 0.144626 	 0.163607 	 0.166080
Epoch 50 	 0.143000 	 0.162099 	 0.167420
Epoch 60 	 0.142476 	 0.160888 	 0.165031
Epoch 70 	 0.141300 	 0.158787 	 0.165357
Epoch 80 	 0.141462 	 0.159021 	 0.164291
Epoch 90 	 0.136000 	 0.157149 	 0.160190
Epoch 100 	 0.135780 	 0.156267 	 0.160518
Epoch 110 	 0.133392 	 0.154110 	 0.158923
Epoch 120 	 0.133324 	 0.153751 	 0.159235
Epoch 130 	 0.132091 	 0.152239 	 0.158144
Epoch 140 	 0.131854 	 0.152013 	 0.158324
[Model stopped early]
Train loss       : 0.130916
Best valid loss  : 0.150663
Best test loss   : 0.159471
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5020]
[Starting training]
Epoch 0 	 0.301377 	 0.241366 	 0.245372
Epoch 10 	 0.171330 	 0.184503 	 0.188865
Epoch 20 	 0.162551 	 0.178186 	 0.183462
Epoch 30 	 0.159535 	 0.175303 	 0.179598
Epoch 40 	 0.157357 	 0.172253 	 0.177400
Epoch 50 	 0.155693 	 0.172786 	 0.177222
Epoch 60 	 0.155236 	 0.170856 	 0.177461
Epoch 70 	 0.150251 	 0.167569 	 0.173205
Epoch 80 	 0.150117 	 0.167922 	 0.172587
Epoch 90 	 0.149947 	 0.168486 	 0.172080
Epoch 100 	 0.149774 	 0.167140 	 0.172558
Epoch 110 	 0.147105 	 0.166617 	 0.171027
Epoch 120 	 0.146131 	 0.164622 	 0.170307
Epoch 130 	 0.145532 	 0.164709 	 0.169834
Epoch 140 	 0.145674 	 0.165223 	 0.170004
[Model stopped early]
Train loss       : 0.145173
Best valid loss  : 0.160427
Best test loss   : 0.170137
Pruning          : 0.01
