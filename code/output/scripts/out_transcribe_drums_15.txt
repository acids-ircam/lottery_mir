Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288855.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, six, pillow-simd, torchvision, tqdm, pyparsing, python-dateutil, kiwisolver, cycler, matplotlib, h5py, keras-applications, termcolor, opt-einsum, astor, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, markdown, protobuf, grpcio, certifi, chardet, idna, urllib3, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, werkzeug, tensorboard, keras-preprocessing, wrapt, tensorflow-estimator, gast, google-pasta, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288855.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 08:47:58.782215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 08:47:58.794608: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is drums_transcribe_cnn_xavier_trimming_activation_rewind_local_0.
*******
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
/localscratch/esling.41288855.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.8895]
[Starting training]
/localscratch/esling.41288855.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
/localscratch/esling.41288855.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 0 	 57.491589 	 0.878065 	 0.882632
Epoch 10 	 56.033138 	 0.858714 	 0.863213
Epoch 20 	 56.016014 	 0.856492 	 0.862308
Epoch 30 	 55.988682 	 0.858554 	 0.862101
Epoch 40 	 55.997089 	 0.853768 	 0.858722
Epoch 50 	 55.987350 	 0.855297 	 0.861141
Epoch 60 	 55.977581 	 0.854090 	 0.858967
[Model stopped early]
Train loss       : 55.967133
Best valid loss  : 0.851330
Best test loss   : 0.856782
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 5,425,453
--------------------------------
Total memory      : 15.86 MB
Total Flops       : 1.5 GFlops
Total Mem (Read)  : 33.07 MB
Total Mem (Write) : 12.34 MB
[Supermasks testing]
[Untrained loss : 0.8574]
[Starting training]
Epoch 0 	 55.960339 	 0.855881 	 0.859186
Epoch 10 	 55.580708 	 0.816335 	 0.820841
Epoch 20 	 55.585052 	 0.811545 	 0.812591
Epoch 30 	 55.452602 	 0.804175 	 0.809636
Epoch 40 	 55.422752 	 0.812263 	 0.814979
Epoch 50 	 55.348221 	 0.811883 	 0.814516
Epoch 60 	 55.332550 	 0.811001 	 0.813368
[Model stopped early]
Train loss       : 55.338490
Best valid loss  : 0.804175
Best test loss   : 0.809636
Pruning          : 0.75
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,016,941
--------------------------------
Total memory      : 11.90 MB
Total Flops       : 850.78 MFlops
Total Mem (Read)  : 24.62 MB
Total Mem (Write) : 9.26 MB
[Supermasks testing]
[Untrained loss : 0.8826]
[Starting training]
Epoch 0 	 55.958149 	 0.856702 	 0.860703
Epoch 10 	 55.639980 	 0.804835 	 0.812747
Epoch 20 	 55.538471 	 0.806169 	 0.812183
Epoch 30 	 55.430496 	 0.802187 	 0.807440
Epoch 40 	 55.396751 	 0.808812 	 0.809789
Epoch 50 	 55.371662 	 0.809538 	 0.810266
Epoch 60 	 55.343418 	 0.803634 	 0.804958
Epoch 70 	 55.308540 	 0.800226 	 0.802490
Epoch 80 	 55.327438 	 0.797481 	 0.802694
Epoch 90 	 55.297382 	 0.799412 	 0.802227
Epoch 100 	 55.278732 	 0.796058 	 0.801374
Epoch 110 	 55.255280 	 0.799463 	 0.802930
Epoch 120 	 55.262707 	 0.799059 	 0.802293
Epoch 130 	 55.280762 	 0.797641 	 0.801826
Epoch 140 	 55.253059 	 0.797670 	 0.799355
Epoch 150 	 55.260487 	 0.798917 	 0.800063
Epoch 160 	 55.266380 	 0.791972 	 0.799776
Epoch 170 	 55.259480 	 0.797925 	 0.800036
Epoch 180 	 55.254395 	 0.799438 	 0.801370
Epoch 190 	 55.256256 	 0.798222 	 0.800122
[Model stopped early]
Train loss       : 55.254417
Best valid loss  : 0.791972
Best test loss   : 0.799776
Pruning          : 0.56
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,084,541
--------------------------------
Total memory      : 8.93 MB
Total Flops       : 482.74 MFlops
Total Mem (Read)  : 18.75 MB
Total Mem (Write) : 6.95 MB
[Supermasks testing]
[Untrained loss : 0.8826]
[Starting training]
Epoch 0 	 55.969269 	 0.854772 	 0.862371
Epoch 10 	 55.533272 	 0.807854 	 0.808618
Epoch 20 	 55.483418 	 0.823189 	 0.822454
Epoch 30 	 55.326725 	 0.793168 	 0.798597
Epoch 40 	 55.266678 	 0.791860 	 0.794773
Epoch 50 	 55.226551 	 0.790392 	 0.792406
Epoch 60 	 55.216953 	 0.792244 	 0.794597
Epoch 70 	 55.133991 	 0.790976 	 0.793546
Epoch 80 	 55.137562 	 0.787224 	 0.790437
Epoch 90 	 55.144978 	 0.783043 	 0.785699
Epoch 100 	 55.126511 	 0.785103 	 0.785398
Epoch 110 	 55.090435 	 0.786713 	 0.789817
[Model stopped early]
Train loss       : 55.102993
Best valid loss  : 0.782557
Best test loss   : 0.788369
Pruning          : 0.42
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,454,982
--------------------------------
Total memory      : 6.70 MB
Total Flops       : 274.77 MFlops
Total Mem (Read)  : 14.61 MB
Total Mem (Write) : 5.21 MB
[Supermasks testing]
[Untrained loss : 0.8826]
[Starting training]
Epoch 0 	 55.982109 	 0.849577 	 0.857420
Epoch 10 	 55.514027 	 0.805025 	 0.803192
Epoch 20 	 55.406891 	 0.792453 	 0.796786
Epoch 30 	 55.325764 	 0.798273 	 0.798803
Epoch 40 	 55.221703 	 0.783294 	 0.787940
Epoch 50 	 55.170074 	 0.786925 	 0.784888
Epoch 60 	 55.114155 	 0.780045 	 0.786267
Epoch 70 	 54.958462 	 0.773086 	 0.778635
Epoch 80 	 54.837791 	 0.771442 	 0.774523
Epoch 90 	 54.741631 	 0.760239 	 0.770178
Epoch 100 	 54.689251 	 0.769817 	 0.771136
Epoch 110 	 54.671001 	 0.766611 	 0.769627
Epoch 120 	 54.615715 	 0.758835 	 0.763833
Epoch 130 	 54.604347 	 0.767744 	 0.765394
Epoch 140 	 54.615322 	 0.761035 	 0.762747
Epoch 150 	 54.583599 	 0.760414 	 0.764037
Epoch 160 	 54.568382 	 0.763471 	 0.763485
[Model stopped early]
Train loss       : 54.568470
Best valid loss  : 0.758046
Best test loss   : 0.765494
Pruning          : 0.32
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,019,736
--------------------------------
Total memory      : 4.97 MB
Total Flops       : 153.36 MFlops
Total Mem (Read)  : 11.61 MB
Total Mem (Write) : 3.87 MB
[Supermasks testing]
[Untrained loss : 0.8826]
[Starting training]
Epoch 0 	 55.981735 	 0.854151 	 0.863908
Epoch 10 	 55.376163 	 0.806984 	 0.810890
Epoch 20 	 55.231804 	 0.804247 	 0.813522
Epoch 30 	 55.100666 	 0.787311 	 0.788680
Epoch 40 	 55.052143 	 0.794208 	 0.798925
Epoch 50 	 54.937820 	 0.789191 	 0.798167
Epoch 60 	 54.899658 	 0.788385 	 0.792777
[Model stopped early]
Train loss       : 54.901623
Best valid loss  : 0.784473
Best test loss   : 0.790485
Pruning          : 0.24
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,717,038
--------------------------------
Total memory      : 3.73 MB
Total Flops       : 88.24 MFlops
Total Mem (Read)  : 9.49 MB
Total Mem (Write) : 2.9 MB
[Supermasks testing]
[Untrained loss : 0.8826]
[Starting training]
Epoch 0 	 55.984196 	 0.859013 	 0.863402
Epoch 10 	 55.290092 	 0.804462 	 0.811999
Epoch 20 	 55.157639 	 0.792731 	 0.797646
Epoch 30 	 55.064445 	 0.786606 	 0.790539
Epoch 40 	 54.964111 	 0.786399 	 0.791223
Epoch 50 	 54.952568 	 0.775119 	 0.778411
Epoch 60 	 54.893368 	 0.775772 	 0.780119
Epoch 70 	 54.817593 	 0.774349 	 0.779927
Epoch 80 	 54.755283 	 0.769581 	 0.773837
Epoch 90 	 54.742123 	 0.766617 	 0.770554
Epoch 100 	 54.724319 	 0.763729 	 0.770392
Epoch 110 	 54.724697 	 0.765516 	 0.770475
Epoch 120 	 54.689022 	 0.763591 	 0.767343
Epoch 130 	 54.692528 	 0.764029 	 0.765834
Epoch 140 	 54.692547 	 0.766774 	 0.768122
Epoch 150 	 54.670525 	 0.766488 	 0.769516
Epoch 160 	 54.675468 	 0.762665 	 0.765801
Epoch 170 	 54.697563 	 0.765574 	 0.766577
[Model stopped early]
Train loss       : 54.670353
Best valid loss  : 0.761896
Best test loss   : 0.767266
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,499,950
--------------------------------
Total memory      : 2.74 MB
Total Flops       : 49.12 MFlops
Total Mem (Read)  : 7.89 MB
Total Mem (Write) : 2.13 MB
[Supermasks testing]
[Untrained loss : 0.8826]
[Starting training]
Epoch 0 	 56.015461 	 0.854348 	 0.860895
Epoch 10 	 55.376266 	 0.806594 	 0.804861
Epoch 20 	 55.217865 	 0.807387 	 0.803973
Epoch 30 	 55.153687 	 0.801531 	 0.803887
Epoch 40 	 55.043880 	 0.790080 	 0.795436
Epoch 50 	 55.017334 	 0.782084 	 0.786719
Epoch 60 	 54.943977 	 0.776874 	 0.782201
Epoch 70 	 54.855225 	 0.772355 	 0.778865
Epoch 80 	 54.858120 	 0.770303 	 0.776682
Epoch 90 	 54.828579 	 0.769973 	 0.775224
Epoch 100 	 54.783012 	 0.771223 	 0.776541
Epoch 110 	 54.771942 	 0.771735 	 0.774165
Epoch 120 	 54.722771 	 0.767567 	 0.769351
Epoch 130 	 54.664742 	 0.760620 	 0.769555
Epoch 140 	 54.673672 	 0.761306 	 0.768314
Epoch 150 	 54.657104 	 0.764637 	 0.768008
Epoch 160 	 54.612404 	 0.762892 	 0.766000
Epoch 170 	 54.611317 	 0.761374 	 0.766266
Epoch 180 	 54.609627 	 0.759216 	 0.765075
Epoch 190 	 54.619221 	 0.762750 	 0.766278
Train loss       : 54.602283
Best valid loss  : 0.756043
Best test loss   : 0.764557
Pruning          : 0.13
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,346,677
--------------------------------
Total memory      : 2.00 MB
Total Flops       : 27.34 MFlops
Total Mem (Read)  : 6.73 MB
Total Mem (Write) : 1.56 MB
[Supermasks testing]
[Untrained loss : 0.8826]
[Starting training]
Epoch 0 	 56.025154 	 0.854375 	 0.858810
Epoch 10 	 55.479649 	 0.807940 	 0.806513
Epoch 20 	 55.243340 	 0.810615 	 0.809225
Epoch 30 	 55.138382 	 0.801056 	 0.804932
Epoch 40 	 55.030319 	 0.786639 	 0.795258
Epoch 50 	 54.981388 	 0.792975 	 0.797960
Epoch 60 	 54.910244 	 0.783379 	 0.787302
Epoch 70 	 54.878082 	 0.785350 	 0.786072
Epoch 80 	 54.779060 	 0.779632 	 0.778558
Epoch 90 	 54.730389 	 0.772176 	 0.771323
Epoch 100 	 54.669468 	 0.766720 	 0.768201
Epoch 110 	 54.615135 	 0.763686 	 0.767501
Epoch 120 	 54.548618 	 0.760486 	 0.758646
Epoch 130 	 54.538475 	 0.762172 	 0.761976
Epoch 140 	 54.503716 	 0.752696 	 0.755971
Epoch 150 	 54.456120 	 0.749966 	 0.746545
Epoch 160 	 54.417282 	 0.742005 	 0.740357
Epoch 170 	 54.362213 	 0.741431 	 0.740532
Epoch 180 	 54.358223 	 0.743356 	 0.741931
Epoch 190 	 54.327385 	 0.741002 	 0.740825
Train loss       : 54.312443
Best valid loss  : 0.739345
Best test loss   : 0.741607
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,234,585
--------------------------------
Total memory      : 1.50 MB
Total Flops       : 16.41 MFlops
Total Mem (Read)  : 5.92 MB
Total Mem (Write) : 1.17 MB
[Supermasks testing]
[Untrained loss : 0.8826]
[Starting training]
Epoch 0 	 56.080452 	 0.877978 	 0.882632
Epoch 10 	 55.533390 	 0.807900 	 0.810990
Epoch 20 	 55.473091 	 0.800938 	 0.804497
Epoch 30 	 55.276077 	 0.806366 	 0.807625
Epoch 40 	 55.150520 	 0.794807 	 0.801378
Epoch 50 	 55.133423 	 0.789612 	 0.798270
Epoch 60 	 55.046295 	 0.786650 	 0.787830
Epoch 70 	 55.025379 	 0.783968 	 0.786312
Epoch 80 	 55.002129 	 0.778809 	 0.782874
Epoch 90 	 54.948730 	 0.782588 	 0.782701
Epoch 100 	 54.930534 	 0.776263 	 0.781462
Epoch 110 	 54.915031 	 0.784459 	 0.784122
Epoch 120 	 54.917324 	 0.783512 	 0.784451
Epoch 130 	 54.917561 	 0.781674 	 0.782540
Epoch 140 	 54.909645 	 0.776643 	 0.780581
Epoch 150 	 54.903629 	 0.778176 	 0.781610
[Model stopped early]
Train loss       : 54.884487
Best valid loss  : 0.775198
Best test loss   : 0.781966
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,154,221
--------------------------------
Total memory      : 1.09 MB
Total Flops       : 9.52 MFlops
Total Mem (Read)  : 5.29 MB
Total Mem (Write) : 870.76 KB
[Supermasks testing]
[Untrained loss : 0.8826]
[Starting training]
Epoch 0 	 56.102348 	 0.873926 	 0.879122
Epoch 10 	 55.558338 	 0.815022 	 0.817908
Epoch 20 	 55.489571 	 0.807730 	 0.809836
Epoch 30 	 55.406040 	 0.800218 	 0.806741
Epoch 40 	 55.290085 	 0.808668 	 0.805779
Epoch 50 	 55.206566 	 0.797813 	 0.801717
Epoch 60 	 55.191616 	 0.803854 	 0.804491
Epoch 70 	 55.144699 	 0.797364 	 0.799000
Epoch 80 	 55.132534 	 0.798424 	 0.797340
Epoch 90 	 55.130592 	 0.796345 	 0.793593
Epoch 100 	 55.113113 	 0.794268 	 0.794562
Epoch 110 	 55.085793 	 0.793644 	 0.793037
Epoch 120 	 55.110432 	 0.792491 	 0.793084
[Model stopped early]
Train loss       : 55.082741
Best valid loss  : 0.789873
Best test loss   : 0.793282
Pruning          : 0.06
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,092,142
--------------------------------
Total memory      : 0.76 MB
Total Flops       : 5.44 MFlops
Total Mem (Read)  : 4.79 MB
Total Mem (Write) : 607.91 KB
[Supermasks testing]
[Untrained loss : 0.8826]
[Starting training]
Epoch 0 	 56.095966 	 0.876991 	 0.880827
Epoch 10 	 55.607056 	 0.818204 	 0.819620
Epoch 20 	 55.493195 	 0.801198 	 0.804659
Epoch 30 	 55.391056 	 0.796467 	 0.804278
Epoch 40 	 55.302765 	 0.800856 	 0.803241
Epoch 50 	 55.298271 	 0.804269 	 0.806908
Epoch 60 	 55.290096 	 0.802961 	 0.803626
[Model stopped early]
Train loss       : 55.259323
Best valid loss  : 0.796467
Best test loss   : 0.804278
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,047,388
--------------------------------
Total memory      : 0.51 MB
Total Flops       : 3.23 MFlops
Total Mem (Read)  : 4.43 MB
Total Mem (Write) : 410.77 KB
[Supermasks testing]
[Untrained loss : 0.8826]
[Starting training]
Epoch 0 	 56.108074 	 0.876985 	 0.881296
Epoch 10 	 55.704746 	 0.815549 	 0.821331
Epoch 20 	 55.574696 	 0.816171 	 0.817512
Epoch 30 	 55.553570 	 0.813439 	 0.819788
Epoch 40 	 55.539448 	 0.814504 	 0.813460
Epoch 50 	 55.507679 	 0.808963 	 0.809420
Epoch 60 	 55.472759 	 0.805358 	 0.807099
Epoch 70 	 55.453281 	 0.806122 	 0.808288
Epoch 80 	 55.418865 	 0.803488 	 0.803112
Epoch 90 	 55.410381 	 0.804202 	 0.805532
Epoch 100 	 55.395386 	 0.798760 	 0.802942
Epoch 110 	 55.414616 	 0.797857 	 0.801655
Epoch 120 	 55.372444 	 0.794713 	 0.801107
Epoch 130 	 55.380905 	 0.800430 	 0.801275
Epoch 140 	 55.360374 	 0.795651 	 0.798796
Epoch 150 	 55.365101 	 0.802001 	 0.801814
[Model stopped early]
Train loss       : 55.373547
Best valid loss  : 0.790475
Best test loss   : 0.798295
Pruning          : 0.03
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,015,320
--------------------------------
Total memory      : 0.35 MB
Total Flops       : 2.15 MFlops
Total Mem (Read)  : 4.18 MB
Total Mem (Write) : 279.34 KB
[Supermasks testing]
[Untrained loss : 0.8826]
[Starting training]
Epoch 0 	 56.118053 	 0.879319 	 0.882632
Epoch 10 	 56.000656 	 0.847674 	 0.852908
Epoch 20 	 55.899555 	 0.852966 	 0.858061
Epoch 30 	 55.817192 	 0.839429 	 0.847099
Epoch 40 	 55.724319 	 0.814411 	 0.822099
Epoch 50 	 55.681572 	 0.820477 	 0.827133
Epoch 60 	 55.657612 	 0.819430 	 0.821033
Epoch 70 	 55.623543 	 0.813754 	 0.822332
Epoch 80 	 55.580311 	 0.814853 	 0.819668
[Model stopped early]
Train loss       : 55.561977
Best valid loss  : 0.811826
Best test loss   : 0.821708
Pruning          : 0.02
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 991,581
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 1.72 MFlops
Total Mem (Read)  : 4.03 MB
Total Mem (Write) : 213.57 KB
[Supermasks testing]
[Untrained loss : 0.8826]
[Starting training]
Epoch 0 	 56.124653 	 0.878762 	 0.882632
Epoch 10 	 56.137321 	 0.878171 	 0.882632
Epoch 20 	 56.074776 	 0.878686 	 0.882632
Epoch 30 	 55.985859 	 0.859540 	 0.865677
Epoch 40 	 55.949982 	 0.855903 	 0.860198
Epoch 50 	 55.883720 	 0.857162 	 0.861756
Epoch 60 	 55.891945 	 0.852570 	 0.857148
Epoch 70 	 55.744354 	 0.827824 	 0.832863
Epoch 80 	 55.707268 	 0.821981 	 0.827303
Epoch 90 	 55.655598 	 0.816979 	 0.821211
Epoch 100 	 55.651665 	 0.815561 	 0.819860
Epoch 110 	 55.630493 	 0.817495 	 0.819405
Epoch 120 	 55.609573 	 0.811455 	 0.814449
Epoch 130 	 55.596306 	 0.813015 	 0.813881
Epoch 140 	 55.598492 	 0.816199 	 0.817302
Epoch 150 	 55.604057 	 0.813445 	 0.817893
[Model stopped early]
Train loss       : 55.604057
Best valid loss  : 0.807815
Best test loss   : 0.814996
Pruning          : 0.02
