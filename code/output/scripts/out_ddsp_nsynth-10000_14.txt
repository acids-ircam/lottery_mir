Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41146338.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, kiwisolver, pyparsing, python-dateutil, cycler, matplotlib, protobuf, astor, google-pasta, keras-preprocessing, grpcio, absl-py, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, urllib3, chardet, certifi, idna, requests, requests-oauthlib, google-auth-oauthlib, markdown, werkzeug, tensorboard, tensorflow-estimator, gast, termcolor, opt-einsum, wrapt, h5py, keras-applications, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146338.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-27 03:03:18.336220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-27 03:03:18.668858: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_ddsp_cnn_xavier_trimming_gradient_min_rewind_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41146338.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 81.7452]
[Starting training]
Epoch 0 	 77.125732 	 68.829315 	 71.214882
Epoch 10 	 65.303879 	 60.445217 	 61.413319
Epoch 20 	 56.499741 	 53.489410 	 54.627792
Epoch 30 	 50.159443 	 44.506306 	 45.766880
Epoch 40 	 44.688793 	 39.266636 	 42.020641
Epoch 50 	 42.161575 	 39.353012 	 41.288643
Epoch 60 	 40.906590 	 44.770985 	 114.334633
Epoch 70 	 36.817043 	 33.136959 	 35.122261
Epoch 80 	 33.847389 	 31.918732 	 34.103664
Epoch 90 	 35.220947 	 33.553680 	 35.186867
Epoch 100 	 31.310854 	 29.735123 	 31.772263
Epoch 110 	 30.484299 	 31.793430 	 33.535675
Epoch 120 	 28.210461 	 27.948324 	 29.847576
Epoch 130 	 27.590778 	 27.527071 	 29.164804
Epoch 140 	 27.066826 	 27.525436 	 29.739588
Epoch 150 	 26.614834 	 32.544312 	 34.535511
Epoch 160 	 26.065258 	 26.956495 	 28.780304
Epoch 170 	 25.169445 	 25.815798 	 27.712290
Epoch 180 	 25.624725 	 26.680965 	 28.514332
Epoch 190 	 24.596987 	 25.584322 	 27.174986
Train loss       : 24.362244
Best valid loss  : 25.045698
Best test loss   : 27.211910
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,909,528
--------------------------------
Total memory      : 35.03 MB
Total Flops       : 326.2 MFlops
Total Mem (Read)  : 31.67 MB
Total Mem (Write) : 24.49 MB
[Supermasks testing]
[Untrained loss : 256.8444]
[Starting training]
Epoch 0 	 64.160393 	 50.731819 	 50.754776
Epoch 10 	 37.868267 	 34.677624 	 36.387264
Epoch 20 	 33.674599 	 32.132549 	 34.012299
Epoch 30 	 31.548832 	 31.642344 	 33.434795
Epoch 40 	 30.096779 	 30.429193 	 31.919973
Epoch 50 	 28.923721 	 27.773039 	 29.281883
Epoch 60 	 28.460016 	 27.531265 	 28.931034
Epoch 70 	 27.737478 	 28.361839 	 29.851759
Epoch 80 	 29.906380 	 28.113277 	 30.040010
Epoch 90 	 26.475615 	 26.156029 	 27.659433
Epoch 100 	 25.994129 	 26.298838 	 28.053442
Epoch 110 	 25.618134 	 25.681595 	 27.481312
Epoch 120 	 25.234034 	 25.302847 	 26.942453
Epoch 130 	 24.840210 	 24.749651 	 26.701454
Epoch 140 	 24.591927 	 24.919466 	 26.606176
Epoch 150 	 23.982752 	 24.769770 	 26.467888
Epoch 160 	 23.609211 	 24.375103 	 25.954874
Epoch 170 	 23.505455 	 24.206831 	 25.883554
Epoch 180 	 23.297979 	 24.164768 	 25.836460
Epoch 190 	 23.238878 	 24.055344 	 25.811037
Train loss       : 23.178602
Best valid loss  : 23.572247
Best test loss   : 25.736992
Pruning          : 0.72
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,898,752
--------------------------------
Total memory      : 29.65 MB
Total Flops       : 171.22 MFlops
Total Mem (Read)  : 22.92 MB
Total Mem (Write) : 17.34 MB
[Supermasks testing]
[Untrained loss : 454.5916]
[Starting training]
Epoch 0 	 66.374908 	 53.733887 	 54.107685
Epoch 10 	 44.912884 	 41.733658 	 42.424503
Epoch 20 	 39.091263 	 37.865501 	 38.483829
Epoch 30 	 36.807182 	 36.223267 	 37.036942
Epoch 40 	 34.599129 	 34.128529 	 35.151215
Epoch 50 	 33.032669 	 33.272762 	 34.398781
Epoch 60 	 31.175020 	 33.207645 	 34.111214
Epoch 70 	 30.232056 	 31.780527 	 32.828022
Epoch 80 	 30.278418 	 32.002239 	 32.862858
Epoch 90 	 29.926016 	 32.180809 	 33.147518
Epoch 100 	 27.599253 	 31.061527 	 31.988632
Epoch 110 	 27.256609 	 31.631838 	 32.250729
Epoch 120 	 26.406536 	 30.533394 	 31.308952
Epoch 130 	 25.865385 	 30.378738 	 31.416121
Epoch 140 	 25.774014 	 30.351994 	 31.129551
Epoch 150 	 25.440989 	 30.343599 	 31.009632
Epoch 160 	 25.271341 	 30.097769 	 30.987101
Epoch 170 	 25.215588 	 30.056892 	 31.044853
Epoch 180 	 25.168333 	 30.212486 	 31.016287
[Model stopped early]
Train loss       : 25.082811
Best valid loss  : 29.871403
Best test loss   : 30.996578
Pruning          : 0.52
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,282,494
--------------------------------
Total memory      : 25.75 MB
Total Flops       : 89.46 MFlops
Total Mem (Read)  : 17.01 MB
Total Mem (Write) : 12.62 MB
[Supermasks testing]
[Untrained loss : 352.8743]
[Starting training]
Epoch 0 	 70.330078 	 55.039944 	 55.229359
Epoch 10 	 47.174492 	 42.103516 	 42.768700
Epoch 20 	 42.896698 	 39.393730 	 40.670341
Epoch 30 	 38.809517 	 37.276852 	 38.645432
Epoch 40 	 36.697559 	 37.284122 	 38.135990
Epoch 50 	 35.127754 	 35.210136 	 37.099316
Epoch 60 	 33.769386 	 34.491428 	 35.605087
Epoch 70 	 32.648327 	 33.832191 	 35.430233
Epoch 80 	 31.982286 	 33.599655 	 34.592377
Epoch 90 	 31.661818 	 34.374218 	 35.287636
Epoch 100 	 31.171146 	 33.102814 	 33.878681
Epoch 110 	 28.927637 	 31.870579 	 32.894306
Epoch 120 	 28.533821 	 31.842424 	 32.679062
Epoch 130 	 27.817659 	 31.551512 	 32.468166
Epoch 140 	 27.643650 	 31.236122 	 32.332703
Epoch 150 	 27.402016 	 31.575586 	 32.451370
Epoch 160 	 27.136236 	 31.296097 	 32.206985
Epoch 170 	 27.049200 	 31.023470 	 32.070461
Epoch 180 	 26.808323 	 31.186575 	 32.195404
Epoch 190 	 26.775160 	 31.098112 	 32.168625
Train loss       : 26.742970
Best valid loss  : 30.879589
Best test loss   : 32.164276
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 887,398
--------------------------------
Total memory      : 22.89 MB
Total Flops       : 46.15 MFlops
Total Mem (Read)  : 12.89 MB
Total Mem (Write) : 9.4 MB
[Supermasks testing]
[Untrained loss : 105.6974]
[Starting training]
Epoch 0 	 68.761681 	 54.218250 	 54.730629
Epoch 10 	 48.500690 	 44.029934 	 44.150234
Epoch 20 	 44.577915 	 42.020824 	 42.844753
Epoch 30 	 40.928169 	 39.509129 	 40.646595
Epoch 40 	 38.579105 	 38.124481 	 39.347057
Epoch 50 	 36.365559 	 36.234886 	 37.370354
Epoch 60 	 34.730785 	 35.546753 	 36.756638
Epoch 70 	 33.967190 	 35.099941 	 36.609676
Epoch 80 	 32.520935 	 33.708008 	 35.117592
Epoch 90 	 31.517382 	 33.376835 	 34.589600
Epoch 100 	 30.927258 	 33.844147 	 34.834911
Epoch 110 	 30.376238 	 32.728863 	 33.914562
Epoch 120 	 30.281603 	 32.447186 	 33.999378
Epoch 130 	 29.609222 	 32.096153 	 33.277554
Epoch 140 	 28.156271 	 31.869877 	 32.620804
Epoch 150 	 27.944212 	 32.513210 	 33.072609
Epoch 160 	 27.657839 	 31.488121 	 32.548805
Epoch 170 	 27.178850 	 31.449186 	 32.514500
Epoch 180 	 27.042194 	 31.413921 	 32.408482
Epoch 190 	 26.897326 	 31.249926 	 32.472210
Train loss       : 26.635670
Best valid loss  : 31.030161
Best test loss   : 32.285835
Pruning          : 0.27
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 630,951
--------------------------------
Total memory      : 20.84 MB
Total Flops       : 23.99 MFlops
Total Mem (Read)  : 10.04 MB
Total Mem (Write) : 7.21 MB
[Supermasks testing]
[Untrained loss : 122.1193]
[Starting training]
Epoch 0 	 69.221809 	 62.784702 	 62.072563
Epoch 10 	 49.851040 	 44.965332 	 45.306721
Epoch 20 	 46.206860 	 43.162552 	 43.708515
Epoch 30 	 42.498211 	 41.590160 	 42.689312
Epoch 40 	 40.519039 	 41.569271 	 42.666515
Epoch 50 	 38.694256 	 37.463623 	 38.245926
Epoch 60 	 37.085938 	 36.423138 	 37.239109
Epoch 70 	 35.655754 	 35.828503 	 37.076870
Epoch 80 	 34.623276 	 35.016499 	 35.912590
Epoch 90 	 34.579571 	 34.619526 	 35.595451
Epoch 100 	 33.233574 	 34.117622 	 35.034836
Epoch 110 	 32.674995 	 33.441887 	 34.416565
Epoch 120 	 31.933048 	 33.167439 	 34.710541
Epoch 130 	 30.797461 	 32.563484 	 33.683750
Epoch 140 	 30.295029 	 33.047646 	 34.228321
Epoch 150 	 29.714569 	 32.367928 	 33.595345
Epoch 160 	 29.306326 	 32.297684 	 33.367435
Epoch 170 	 29.335363 	 32.024952 	 33.360111
Epoch 180 	 29.092983 	 32.180122 	 33.280739
Epoch 190 	 28.964550 	 32.298836 	 33.332993
[Model stopped early]
Train loss       : 28.953321
Best valid loss  : 31.929132
Best test loss   : 33.353783
Pruning          : 0.19
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 461,817
--------------------------------
Total memory      : 19.39 MB
Total Flops       : 12.83 MFlops
Total Mem (Read)  : 8.07 MB
Total Mem (Write) : 5.74 MB
[Supermasks testing]
[Untrained loss : 98.6687]
[Starting training]
Epoch 0 	 71.103882 	 60.139591 	 60.670818
Epoch 10 	 51.930054 	 47.161896 	 47.110168
Epoch 20 	 48.295582 	 44.527336 	 45.329483
Epoch 30 	 45.668098 	 43.457413 	 44.038895
Epoch 40 	 44.741550 	 45.535595 	 45.895622
Epoch 50 	 42.688015 	 39.907532 	 41.183315
Epoch 60 	 42.325207 	 39.963760 	 40.695320
Epoch 70 	 40.342316 	 38.441280 	 39.626404
Epoch 80 	 39.817417 	 38.263706 	 39.557327
Epoch 90 	 38.922096 	 38.306000 	 39.237679
Epoch 100 	 37.385998 	 37.091286 	 38.073956
Epoch 110 	 36.779919 	 37.249634 	 38.265694
Epoch 120 	 36.535149 	 36.689648 	 37.771290
Epoch 130 	 35.632088 	 35.983070 	 37.316433
Epoch 140 	 35.466618 	 35.838036 	 37.246906
Epoch 150 	 35.230721 	 36.145786 	 37.347370
Epoch 160 	 35.262356 	 35.575268 	 37.119877
Epoch 170 	 34.935863 	 35.651409 	 37.176708
Epoch 180 	 34.672035 	 35.286957 	 36.842926
Epoch 190 	 34.509922 	 35.623657 	 37.174995
Train loss       : 34.410721
Best valid loss  : 35.089111
Best test loss   : 36.878548
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 348,567
--------------------------------
Total memory      : 18.35 MB
Total Flops       : 7.06 MFlops
Total Mem (Read)  : 6.7 MB
Total Mem (Write) : 4.71 MB
[Supermasks testing]
[Untrained loss : 98.6979]
[Starting training]
Epoch 0 	 74.315613 	 126.679382 	 109.257492
Epoch 10 	 55.139416 	 49.519680 	 49.867519
Epoch 20 	 48.392246 	 44.958641 	 45.009872
Epoch 30 	 45.379257 	 42.855526 	 43.562088
Epoch 40 	 43.577942 	 41.898384 	 42.992821
Epoch 50 	 41.716351 	 40.246223 	 41.425571
Epoch 60 	 40.984234 	 39.832623 	 40.772766
Epoch 70 	 40.419426 	 39.552139 	 40.958744
Epoch 80 	 39.460224 	 38.693089 	 39.488144
Epoch 90 	 39.022015 	 39.522884 	 39.775898
Epoch 100 	 37.696362 	 37.778767 	 38.617844
Epoch 110 	 36.672962 	 37.249699 	 37.946812
Epoch 120 	 36.617115 	 37.384319 	 37.973873
Epoch 130 	 36.131184 	 37.014374 	 37.575771
Epoch 140 	 36.131111 	 36.518066 	 37.493641
Epoch 150 	 35.894279 	 36.909840 	 37.598198
Epoch 160 	 35.696476 	 36.722923 	 37.380371
Epoch 170 	 35.700157 	 36.612705 	 37.337486
Epoch 180 	 35.602375 	 36.629631 	 37.274734
Epoch 190 	 35.600368 	 36.753174 	 37.253925
Train loss       : 35.620777
Best valid loss  : 36.336189
Best test loss   : 37.318264
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 272,819
--------------------------------
Total memory      : 17.56 MB
Total Flops       : 3.79 MFlops
Total Mem (Read)  : 5.68 MB
Total Mem (Write) : 3.94 MB
[Supermasks testing]
[Untrained loss : 98.6474]
[Starting training]
Epoch 0 	 73.599487 	 60.454712 	 61.353760
Epoch 10 	 55.303631 	 50.438137 	 50.237381
Epoch 20 	 53.537769 	 47.741421 	 48.208138
Epoch 30 	 49.383018 	 43.861305 	 44.596436
Epoch 40 	 47.858047 	 44.090431 	 44.651661
Epoch 50 	 47.083740 	 42.683186 	 43.281746
Epoch 60 	 46.227665 	 42.828373 	 43.404263
Epoch 70 	 45.487865 	 42.800777 	 42.925743
Epoch 80 	 45.280968 	 42.042400 	 42.562702
Epoch 90 	 44.989445 	 41.879604 	 42.475452
Epoch 100 	 44.713997 	 42.281273 	 42.845917
Epoch 110 	 44.465134 	 41.787262 	 42.398857
Epoch 120 	 44.216820 	 41.658512 	 42.463776
Epoch 130 	 44.316643 	 42.045067 	 42.725750
Epoch 140 	 44.183258 	 41.797585 	 42.526558
[Model stopped early]
Train loss       : 44.229721
Best valid loss  : 41.356030
Best test loss   : 42.392143
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 220,983
--------------------------------
Total memory      : 17.11 MB
Total Flops       : 2.56 MFlops
Total Mem (Read)  : 5.08 MB
Total Mem (Write) : 3.52 MB
[Supermasks testing]
[Untrained loss : 98.7398]
[Starting training]
Epoch 0 	 75.078789 	 64.931480 	 66.492325
Epoch 10 	 54.733215 	 51.448215 	 52.345329
Epoch 20 	 52.554768 	 49.002964 	 49.145275
Epoch 30 	 51.636814 	 49.277622 	 49.659908
Epoch 40 	 50.347298 	 47.416420 	 47.576336
Epoch 50 	 49.648228 	 45.383698 	 46.156395
Epoch 60 	 48.171665 	 45.463623 	 46.248161
Epoch 70 	 47.165154 	 44.097630 	 45.101307
Epoch 80 	 46.467350 	 43.484497 	 44.390339
Epoch 90 	 45.484314 	 44.360142 	 45.071419
Epoch 100 	 45.956081 	 43.268852 	 44.171967
Epoch 110 	 45.105179 	 42.276615 	 43.113636
Epoch 120 	 45.134792 	 41.840534 	 42.747166
Epoch 130 	 44.729416 	 41.887123 	 42.826229
Epoch 140 	 44.209206 	 41.796101 	 42.359486
Epoch 150 	 43.899258 	 41.585209 	 42.191631
Epoch 160 	 43.951996 	 41.616024 	 42.374958
Epoch 170 	 43.642643 	 41.459000 	 42.211029
[Model stopped early]
Train loss       : 43.731010
Best valid loss  : 41.170338
Best test loss   : 42.297768
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 185,398
--------------------------------
Total memory      : 16.72 MB
Total Flops       : 1.66 MFlops
Total Mem (Read)  : 4.58 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 98.4352]
[Starting training]
Epoch 0 	 75.666855 	 60.009586 	 60.448250
Epoch 10 	 58.296650 	 53.481377 	 53.308361
Epoch 20 	 55.490955 	 51.385349 	 51.904423
Epoch 30 	 54.565796 	 51.985497 	 52.907570
Epoch 40 	 53.059818 	 50.098221 	 51.098782
Epoch 50 	 52.561108 	 48.680096 	 49.596237
Epoch 60 	 52.653744 	 48.552338 	 49.312458
Epoch 70 	 51.449024 	 47.816463 	 48.593410
Epoch 80 	 50.897083 	 47.851223 	 48.549305
Epoch 90 	 51.226955 	 48.177326 	 49.099815
Epoch 100 	 50.579731 	 47.170631 	 48.174397
Epoch 110 	 50.347027 	 46.934196 	 47.825939
Epoch 120 	 49.876278 	 47.010426 	 47.712223
Epoch 130 	 49.673634 	 46.942558 	 47.616264
Epoch 140 	 49.792198 	 46.840225 	 47.606049
Epoch 150 	 49.874939 	 46.694954 	 47.588497
Epoch 160 	 49.773907 	 47.182102 	 47.767303
Epoch 170 	 49.835979 	 46.048084 	 47.547482
Epoch 180 	 49.788246 	 47.025066 	 47.725838
Epoch 190 	 49.362534 	 46.685417 	 47.475613
Train loss       : 49.624550
Best valid loss  : 46.048084
Best test loss   : 47.547482
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 156,046
--------------------------------
Total memory      : 16.63 MB
Total Flops       : 1.63 MFlops
Total Mem (Read)  : 4.41 MB
Total Mem (Write) : 3.08 MB
[Supermasks testing]
[Untrained loss : 98.4313]
[Starting training]
Epoch 0 	 76.709541 	 70.167526 	 71.807144
Epoch 10 	 58.064732 	 56.011467 	 56.851307
Epoch 20 	 55.511047 	 50.299656 	 50.855900
Epoch 30 	 54.492565 	 51.428402 	 52.215771
Epoch 40 	 53.494232 	 50.259422 	 50.445480
Epoch 50 	 51.502983 	 48.966488 	 49.470680
Epoch 60 	 51.251945 	 47.984653 	 48.574989
Epoch 70 	 51.013004 	 47.737125 	 47.994099
Epoch 80 	 50.283344 	 47.545116 	 47.721100
Epoch 90 	 50.076481 	 47.357952 	 47.317757
Epoch 100 	 49.951176 	 48.128555 	 48.400772
Epoch 110 	 49.746445 	 46.826622 	 47.039661
Epoch 120 	 49.749161 	 46.451321 	 46.979630
Epoch 130 	 49.590233 	 46.833099 	 46.851883
Epoch 140 	 49.442734 	 46.965805 	 46.874939
Epoch 150 	 49.592453 	 46.690510 	 46.819542
Epoch 160 	 49.669666 	 46.828934 	 46.946644
Epoch 170 	 49.570766 	 46.603706 	 46.796562
Epoch 180 	 49.323936 	 47.064602 	 47.186825
Epoch 190 	 49.401154 	 46.838013 	 46.976955
Train loss       : 49.362034
Best valid loss  : 46.220234
Best test loss   : 46.834389
Pruning          : 0.03
[Performing one full cumulative epoch]
Traceback (most recent call last):
  File "main.py", line 261, in <module>
    model = pruning.reset(model)
  File "/scratch/esling/lottery/pruning.py", line 781, in reset
    replace_recurrent(m, l, m.unprune_idx[l], prev_kept)
  File "/scratch/esling/lottery/pruning.py", line 752, in replace_recurrent
    cur_ih = nn.Parameter(cur_ih[rep_id0])#torch.from_numpy(cur_ih[rep_id0]).to(self.args.device))
IndexError: too many indices for tensor of dimension 2
