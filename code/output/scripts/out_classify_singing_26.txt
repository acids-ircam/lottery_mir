Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41289123.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, six, future, torch, torchvision, tqdm, cycler, python-dateutil, kiwisolver, pyparsing, matplotlib, absl-py, termcolor, tensorflow-estimator, astor, google-pasta, keras-preprocessing, protobuf, gast, wrapt, opt-einsum, grpcio, markdown, oauthlib, urllib3, certifi, chardet, idna, requests, requests-oauthlib, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, werkzeug, tensorboard, h5py, keras-applications, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41289123.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 05:00:30.615510: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 05:00:30.627792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is singing_classify_cnn_xavier_masking_magnitude_reinit_global_0.
*******
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9550]
[Starting training]
Epoch 0 	 0.790211 	 0.703585 	 0.695496
Epoch 10 	 0.311351 	 0.350184 	 0.225460
Epoch 20 	 0.145565 	 0.248162 	 0.101011
Epoch 30 	 0.086742 	 0.198070 	 0.060570
Epoch 40 	 0.058479 	 0.187960 	 0.046783
Epoch 50 	 0.053653 	 0.180607 	 0.041452
Epoch 60 	 0.040441 	 0.179228 	 0.039430
Epoch 70 	 0.033778 	 0.169577 	 0.035570
Epoch 80 	 0.028263 	 0.161765 	 0.034099
Epoch 90 	 0.029527 	 0.173254 	 0.035386
Epoch 100 	 0.014246 	 0.159007 	 0.032077
Epoch 110 	 0.012408 	 0.164522 	 0.033548
Epoch 120 	 0.009191 	 0.168199 	 0.033915
Epoch 130 	 0.006664 	 0.164062 	 0.033180
[Model stopped early]
Train loss       : 0.006778
Best valid loss  : 0.159007
Best test loss   : 0.032077
Pruning          : 1.00
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.7537]
[Starting training]
Epoch 0 	 0.781020 	 0.731158 	 0.717463
Epoch 10 	 0.366268 	 0.431985 	 0.278401
Epoch 20 	 0.182675 	 0.289982 	 0.133732
Epoch 30 	 0.108801 	 0.234375 	 0.076838
Epoch 40 	 0.084329 	 0.209099 	 0.060478
Epoch 50 	 0.064338 	 0.190257 	 0.048713
Epoch 60 	 0.040786 	 0.186581 	 0.042739
Epoch 70 	 0.028722 	 0.182904 	 0.038879
Epoch 80 	 0.029642 	 0.181066 	 0.038971
Epoch 90 	 0.021944 	 0.175551 	 0.036489
Epoch 100 	 0.022863 	 0.173254 	 0.035018
Epoch 110 	 0.014017 	 0.181066 	 0.036673
Epoch 120 	 0.014017 	 0.175551 	 0.036305
[Model stopped early]
Train loss       : 0.011949
Best valid loss  : 0.170037
Best test loss   : 0.034467
Pruning          : 0.70
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.8765]
[Starting training]
Epoch 0 	 0.822266 	 0.737132 	 0.742647
Epoch 10 	 0.395221 	 0.439338 	 0.307261
Epoch 20 	 0.205423 	 0.300092 	 0.139430
Epoch 30 	 0.123162 	 0.235754 	 0.081710
Epoch 40 	 0.094899 	 0.222886 	 0.063051
Epoch 50 	 0.072495 	 0.203125 	 0.053309
Epoch 60 	 0.064338 	 0.204963 	 0.053401
Epoch 70 	 0.055836 	 0.192096 	 0.044945
Epoch 80 	 0.034237 	 0.185662 	 0.039246
Epoch 90 	 0.026654 	 0.177849 	 0.037408
Epoch 100 	 0.028148 	 0.183364 	 0.038143
Epoch 110 	 0.017923 	 0.178309 	 0.036489
Epoch 120 	 0.015855 	 0.181066 	 0.036857
[Model stopped early]
Train loss       : 0.013902
Best valid loss  : 0.176471
Best test loss   : 0.036673
Pruning          : 0.49
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.798139 	 0.704504 	 0.689890
Epoch 10 	 0.356733 	 0.402114 	 0.273989
Epoch 20 	 0.187730 	 0.280331 	 0.135570
Epoch 30 	 0.116728 	 0.229779 	 0.080974
Epoch 40 	 0.093176 	 0.208180 	 0.063603
Epoch 50 	 0.070887 	 0.189798 	 0.052022
Epoch 60 	 0.061811 	 0.180607 	 0.046048
Epoch 70 	 0.051930 	 0.169577 	 0.039246
Epoch 80 	 0.049862 	 0.173254 	 0.038695
Epoch 90 	 0.043543 	 0.176930 	 0.039982
Epoch 100 	 0.025965 	 0.169577 	 0.035937
Epoch 110 	 0.027688 	 0.170956 	 0.035754
Epoch 120 	 0.021944 	 0.171875 	 0.034835
Epoch 130 	 0.015395 	 0.164982 	 0.034099
Epoch 140 	 0.015395 	 0.166360 	 0.033732
Epoch 150 	 0.013557 	 0.162224 	 0.032812
[Model stopped early]
Train loss       : 0.011949
Best valid loss  : 0.160386
Best test loss   : 0.032629
Pruning          : 0.34
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.7535]
[Starting training]
Epoch 0 	 0.806526 	 0.732077 	 0.733272
Epoch 10 	 0.396140 	 0.421415 	 0.299816
Epoch 20 	 0.220588 	 0.286765 	 0.149540
Epoch 30 	 0.140280 	 0.243107 	 0.090257
Epoch 40 	 0.103516 	 0.218750 	 0.070037
Epoch 50 	 0.086972 	 0.210478 	 0.064706
Epoch 60 	 0.080997 	 0.193934 	 0.053768
Epoch 70 	 0.058364 	 0.177849 	 0.041820
Epoch 80 	 0.061581 	 0.182445 	 0.042004
Epoch 90 	 0.056985 	 0.176930 	 0.040533
Epoch 100 	 0.047105 	 0.174632 	 0.037776
Epoch 110 	 0.035846 	 0.171415 	 0.037224
Epoch 120 	 0.028378 	 0.167279 	 0.035018
Epoch 130 	 0.027574 	 0.173713 	 0.036857
Epoch 140 	 0.024357 	 0.170496 	 0.034835
Epoch 150 	 0.020221 	 0.166360 	 0.034835
[Model stopped early]
Train loss       : 0.020221
Best valid loss  : 0.164522
Best test loss   : 0.034651
Pruning          : 0.24
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9871]
[Starting training]
Epoch 0 	 0.835018 	 0.720588 	 0.723989
Epoch 10 	 0.461282 	 0.477482 	 0.373621
Epoch 20 	 0.270221 	 0.339614 	 0.199081
Epoch 30 	 0.186926 	 0.281250 	 0.129044
Epoch 40 	 0.140510 	 0.232996 	 0.088511
Epoch 50 	 0.108915 	 0.209559 	 0.063603
Epoch 60 	 0.088465 	 0.189338 	 0.056434
Epoch 70 	 0.084674 	 0.183824 	 0.050000
Epoch 80 	 0.077321 	 0.176471 	 0.043750
Epoch 90 	 0.066291 	 0.183824 	 0.044026
Epoch 100 	 0.056756 	 0.177390 	 0.041176
Epoch 110 	 0.049173 	 0.161765 	 0.035570
Epoch 120 	 0.038488 	 0.166820 	 0.035846
Epoch 130 	 0.035731 	 0.162684 	 0.034467
Epoch 140 	 0.032054 	 0.168199 	 0.035202
[Model stopped early]
Train loss       : 0.028952
Best valid loss  : 0.160846
Best test loss   : 0.035478
Pruning          : 0.17
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.801126 	 0.728860 	 0.726471
Epoch 10 	 0.483341 	 0.507812 	 0.385754
Epoch 20 	 0.276195 	 0.337776 	 0.181710
Epoch 30 	 0.194738 	 0.260570 	 0.124357
Epoch 40 	 0.155790 	 0.237592 	 0.095496
Epoch 50 	 0.134076 	 0.217371 	 0.075368
Epoch 60 	 0.115464 	 0.204504 	 0.061397
Epoch 70 	 0.095588 	 0.197610 	 0.055147
Epoch 80 	 0.092371 	 0.193474 	 0.051654
Epoch 90 	 0.074449 	 0.188419 	 0.048897
Epoch 100 	 0.064683 	 0.186581 	 0.048805
Epoch 110 	 0.057904 	 0.184283 	 0.043566
Epoch 120 	 0.051356 	 0.176930 	 0.041268
Epoch 130 	 0.050092 	 0.179688 	 0.041452
Epoch 140 	 0.043084 	 0.178768 	 0.041360
Epoch 150 	 0.047105 	 0.176471 	 0.039798
Train loss       : 0.045726
Best valid loss  : 0.171875
Best test loss   : 0.039982
Pruning          : 0.12
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9871]
[Starting training]
Epoch 0 	 0.786190 	 0.750000 	 0.733456
Epoch 10 	 0.487247 	 0.508272 	 0.382629
Epoch 20 	 0.310087 	 0.379596 	 0.247243
Epoch 30 	 0.223690 	 0.296875 	 0.151563
Epoch 40 	 0.189568 	 0.290441 	 0.135662
Epoch 50 	 0.150965 	 0.244026 	 0.095772
Epoch 60 	 0.147978 	 0.229320 	 0.083824
Epoch 70 	 0.131664 	 0.220129 	 0.073346
Epoch 80 	 0.114085 	 0.207721 	 0.062868
Epoch 90 	 0.106733 	 0.212776 	 0.065165
Epoch 100 	 0.089614 	 0.195772 	 0.053217
Epoch 110 	 0.076172 	 0.193015 	 0.051930
Epoch 120 	 0.073644 	 0.183824 	 0.044669
Epoch 130 	 0.071691 	 0.182445 	 0.042923
Epoch 140 	 0.070312 	 0.180147 	 0.043107
Epoch 150 	 0.067900 	 0.180147 	 0.041912
Train loss       : 0.065028
Best valid loss  : 0.176471
Best test loss   : 0.040257
Pruning          : 0.08
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.774127 	 0.704963 	 0.697059
Epoch 10 	 0.518957 	 0.516085 	 0.408364
Epoch 20 	 0.344439 	 0.386029 	 0.239890
Epoch 30 	 0.268842 	 0.312500 	 0.176011
Epoch 40 	 0.230928 	 0.277114 	 0.137776
Epoch 50 	 0.193704 	 0.274816 	 0.120864
Epoch 60 	 0.176700 	 0.248162 	 0.103768
Epoch 70 	 0.162109 	 0.245404 	 0.091544
Epoch 80 	 0.146369 	 0.233915 	 0.081618
Epoch 90 	 0.137408 	 0.221967 	 0.072059
Epoch 100 	 0.117302 	 0.215533 	 0.066728
Epoch 110 	 0.108111 	 0.213695 	 0.061029
Epoch 120 	 0.102711 	 0.206342 	 0.057537
Epoch 130 	 0.102826 	 0.205423 	 0.057169
Epoch 140 	 0.101448 	 0.203125 	 0.058272
Epoch 150 	 0.098001 	 0.206801 	 0.060846
Train loss       : 0.091222
Best valid loss  : 0.193474
Best test loss   : 0.049081
Pruning          : 0.06
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.7537]
[Starting training]
Epoch 0 	 0.858111 	 0.831342 	 0.822610
Epoch 10 	 0.549288 	 0.556985 	 0.443015
Epoch 20 	 0.382008 	 0.416820 	 0.270496
Epoch 30 	 0.295611 	 0.353860 	 0.197151
Epoch 40 	 0.260570 	 0.306066 	 0.149540
Epoch 50 	 0.232537 	 0.298254 	 0.141268
Epoch 60 	 0.209674 	 0.283088 	 0.126379
Epoch 70 	 0.196806 	 0.250460 	 0.104044
Epoch 80 	 0.184168 	 0.254136 	 0.100000
Epoch 90 	 0.165671 	 0.235754 	 0.078676
Epoch 100 	 0.149816 	 0.231618 	 0.081801
Epoch 110 	 0.149127 	 0.224724 	 0.073162
Epoch 120 	 0.137063 	 0.220588 	 0.070680
Epoch 130 	 0.136834 	 0.221048 	 0.068934
Epoch 140 	 0.131204 	 0.208180 	 0.060570
Epoch 150 	 0.126034 	 0.213695 	 0.064430
Train loss       : 0.125000
Best valid loss  : 0.205882
Best test loss   : 0.059651
Pruning          : 0.04
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.8996]
[Starting training]
Epoch 0 	 0.838465 	 0.723805 	 0.721783
Epoch 10 	 0.565142 	 0.556985 	 0.471599
Epoch 20 	 0.437500 	 0.457721 	 0.335110
Epoch 30 	 0.356388 	 0.429228 	 0.283180
Epoch 40 	 0.310777 	 0.363971 	 0.221232
Epoch 50 	 0.287569 	 0.319393 	 0.177114
Epoch 60 	 0.260915 	 0.294118 	 0.146415
Epoch 70 	 0.253562 	 0.282629 	 0.140533
Epoch 80 	 0.223575 	 0.278493 	 0.134651
Epoch 90 	 0.225758 	 0.263787 	 0.119577
Epoch 100 	 0.206916 	 0.243566 	 0.101195
Epoch 110 	 0.209329 	 0.238971 	 0.099908
Epoch 120 	 0.200138 	 0.246783 	 0.099265
Epoch 130 	 0.187730 	 0.217371 	 0.080147
Epoch 140 	 0.189453 	 0.244485 	 0.093199
Epoch 150 	 0.172335 	 0.219210 	 0.080147
Train loss       : 0.159812
Best valid loss  : 0.210478
Best test loss   : 0.079044
Pruning          : 0.03
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9871]
[Starting training]
Epoch 0 	 0.849724 	 0.795956 	 0.794669
Epoch 10 	 0.591912 	 0.592371 	 0.521232
Epoch 20 	 0.458410 	 0.472886 	 0.355239
Epoch 30 	 0.391429 	 0.410846 	 0.271048
Epoch 40 	 0.346507 	 0.361213 	 0.220864
Epoch 50 	 0.324449 	 0.346507 	 0.201195
Epoch 60 	 0.297105 	 0.385570 	 0.229136
Epoch 70 	 0.281250 	 0.319853 	 0.166268
Epoch 80 	 0.275506 	 0.337316 	 0.177849
Epoch 90 	 0.253791 	 0.287684 	 0.137960
Epoch 100 	 0.246668 	 0.282169 	 0.126011
Epoch 110 	 0.235639 	 0.278493 	 0.125092
Epoch 120 	 0.223460 	 0.270221 	 0.116085
Epoch 130 	 0.215878 	 0.265165 	 0.113511
Epoch 140 	 0.200712 	 0.254136 	 0.105607
Epoch 150 	 0.204963 	 0.256893 	 0.107261
Train loss       : 0.203470
Best valid loss  : 0.240349
Best test loss   : 0.096048
Pruning          : 0.02
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.775620 	 0.762868 	 0.762684
Epoch 10 	 0.595933 	 0.640625 	 0.567463
Epoch 20 	 0.487707 	 0.521140 	 0.412040
Epoch 30 	 0.421071 	 0.447610 	 0.313235
Epoch 40 	 0.375919 	 0.408548 	 0.265257
Epoch 50 	 0.348346 	 0.393382 	 0.242555
Epoch 60 	 0.329159 	 0.370404 	 0.226379
Epoch 70 	 0.314913 	 0.357537 	 0.207077
Epoch 80 	 0.295267 	 0.344669 	 0.194026
Epoch 90 	 0.291131 	 0.348346 	 0.187224
Epoch 100 	 0.279527 	 0.353401 	 0.187040
Epoch 110 	 0.285501 	 0.312040 	 0.165533
Epoch 120 	 0.267808 	 0.314338 	 0.167739
Epoch 130 	 0.263557 	 0.335938 	 0.185938
Epoch 140 	 0.247358 	 0.299632 	 0.148805
Epoch 150 	 0.242877 	 0.288143 	 0.136489
Train loss       : 0.240924
Best valid loss  : 0.278033
Best test loss   : 0.131893
Pruning          : 0.01
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.8996]
[Starting training]
Epoch 0 	 0.832261 	 0.857996 	 0.848162
Epoch 10 	 0.616843 	 0.640625 	 0.578033
Epoch 20 	 0.512063 	 0.526195 	 0.428676
Epoch 30 	 0.455882 	 0.481158 	 0.335202
Epoch 40 	 0.412569 	 0.425092 	 0.279779
Epoch 50 	 0.390280 	 0.421875 	 0.268750
Epoch 60 	 0.376494 	 0.384191 	 0.235110
Epoch 70 	 0.356043 	 0.396599 	 0.247151
Epoch 80 	 0.343405 	 0.379596 	 0.220312
Epoch 90 	 0.318819 	 0.340533 	 0.195312
Epoch 100 	 0.323874 	 0.329963 	 0.179688
Epoch 110 	 0.315372 	 0.328585 	 0.175460
Epoch 120 	 0.311351 	 0.340533 	 0.181158
Epoch 130 	 0.303424 	 0.322151 	 0.164062
Epoch 140 	 0.300896 	 0.324908 	 0.170496
Epoch 150 	 0.295726 	 0.318474 	 0.162040
Train loss       : 0.291820
Best valid loss  : 0.311581
Best test loss   : 0.160386
Pruning          : 0.01
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.8996]
[Starting training]
Epoch 0 	 0.847771 	 0.788143 	 0.794853
Epoch 10 	 0.648438 	 0.674173 	 0.647426
Epoch 20 	 0.569164 	 0.644301 	 0.590441
Epoch 30 	 0.505630 	 0.605699 	 0.536121
Epoch 40 	 0.471737 	 0.521140 	 0.429596
Epoch 50 	 0.437500 	 0.498162 	 0.375092
Epoch 60 	 0.425666 	 0.452665 	 0.327849
Epoch 70 	 0.396140 	 0.430147 	 0.291268
Epoch 80 	 0.388327 	 0.429688 	 0.282077
Epoch 90 	 0.374081 	 0.399357 	 0.255055
Epoch 100 	 0.372128 	 0.391085 	 0.243015
Epoch 110 	 0.360754 	 0.378217 	 0.234467
Epoch 120 	 0.360064 	 0.376838 	 0.222335
Epoch 130 	 0.342256 	 0.366728 	 0.217923
Epoch 140 	 0.344324 	 0.366268 	 0.211765
Epoch 150 	 0.319623 	 0.351562 	 0.197243
Train loss       : 0.325597
Best valid loss  : 0.336857
Best test loss   : 0.184099
Pruning          : 0.01
