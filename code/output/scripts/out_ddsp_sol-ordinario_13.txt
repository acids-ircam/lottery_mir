Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41281318.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, six, future, torch, torchvision, tqdm, kiwisolver, python-dateutil, pyparsing, cycler, matplotlib, h5py, keras-applications, google-pasta, wrapt, werkzeug, markdown, absl-py, grpcio, certifi, urllib3, chardet, idna, requests, protobuf, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, gast, tensorflow-estimator, opt-einsum, astor, keras-preprocessing, termcolor, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281318.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-29 02:25:22.842928: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 02:25:23.174154: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is sol-ordinario_ddsp_cnn_xavier_trimming_batchnorm_rewind_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41281318.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 110.6186]
[Starting training]
Epoch 0 	 93.411469 	 90.168175 	 93.265671
Epoch 10 	 81.232361 	 77.576401 	 80.952324
Epoch 20 	 78.462677 	 78.022369 	 83.125298
Epoch 30 	 73.640182 	 74.561241 	 80.301758
Epoch 40 	 72.070198 	 68.969620 	 73.275246
Epoch 50 	 67.413895 	 65.082909 	 69.256264
Epoch 60 	 65.110779 	 62.156425 	 64.942612
Epoch 70 	 59.076572 	 65.742638 	 66.066917
Epoch 80 	 52.113625 	 53.586735 	 56.363277
Epoch 90 	 57.916496 	 65.634796 	 68.550941
Epoch 100 	 50.992653 	 54.506535 	 57.149818
Epoch 110 	 48.618229 	 52.675514 	 55.517708
[Model stopped early]
Train loss       : 47.500256
Best valid loss  : 48.237034
Best test loss   : 51.350067
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 3,792,824
--------------------------------
Total memory      : 35.14 MB
Total Flops       : 339.89 MFlops
Total Mem (Read)  : 35.15 MB
Total Mem (Write) : 28.95 MB
[Supermasks testing]
[Untrained loss : 96.4112]
[Starting training]
Epoch 0 	 55.999516 	 53.480431 	 56.554382
Epoch 10 	 43.346901 	 41.322689 	 42.549263
Epoch 20 	 39.910828 	 39.356663 	 41.364071
Epoch 30 	 39.147800 	 36.986401 	 37.795998
Epoch 40 	 35.603901 	 37.562263 	 39.262981
Epoch 50 	 34.852863 	 34.304745 	 36.166195
Epoch 60 	 31.673630 	 33.258820 	 34.641975
Epoch 70 	 30.900543 	 32.891338 	 34.308670
Epoch 80 	 30.210957 	 32.368031 	 33.499279
Epoch 90 	 28.172955 	 31.006201 	 32.028519
Epoch 100 	 26.836231 	 29.049625 	 31.544699
Epoch 110 	 26.100683 	 28.972246 	 30.556641
Epoch 120 	 25.544559 	 28.434357 	 30.449503
Epoch 130 	 25.343981 	 28.834921 	 30.489710
Epoch 140 	 24.248344 	 27.880323 	 29.564234
Epoch 150 	 24.146715 	 27.423241 	 29.397877
Epoch 160 	 23.672865 	 27.539795 	 29.253819
Epoch 170 	 23.703609 	 27.852255 	 29.422155
Epoch 180 	 23.458254 	 27.575521 	 28.996855
Epoch 190 	 23.084169 	 27.300941 	 29.039997
Train loss       : 22.947950
Best valid loss  : 26.554804
Best test loss   : 29.033209
Pruning          : 0.72
0.001
0.001
[Current model size]
================================
Total params      : 3,316,816
--------------------------------
Total memory      : 29.84 MB
Total Flops       : 192.02 MFlops
Total Mem (Read)  : 28.52 MB
Total Mem (Write) : 24.13 MB
[Supermasks testing]
[Untrained loss : 70.7803]
[Starting training]
Epoch 0 	 63.555969 	 55.816525 	 57.575478
Epoch 10 	 44.933163 	 45.654545 	 47.233459
Epoch 20 	 39.722015 	 40.498756 	 41.541954
Epoch 30 	 38.759201 	 36.437008 	 38.001816
Epoch 40 	 34.849758 	 34.911854 	 35.947041
Epoch 50 	 32.458031 	 32.424908 	 34.412628
Epoch 60 	 31.899075 	 32.653442 	 32.980862
Epoch 70 	 29.360176 	 31.252342 	 32.079506
Epoch 80 	 28.719612 	 31.643732 	 32.415173
Epoch 90 	 27.544176 	 30.319193 	 31.039932
Epoch 100 	 27.314589 	 29.487244 	 31.092726
Epoch 110 	 27.147163 	 30.279213 	 31.189520
Epoch 120 	 26.813234 	 30.179335 	 31.071609
Epoch 130 	 26.218155 	 29.667877 	 30.511637
Epoch 140 	 25.989790 	 29.718803 	 30.316822
Epoch 150 	 26.069304 	 29.547482 	 30.362656
Epoch 160 	 25.643566 	 29.802500 	 30.276798
Epoch 170 	 25.648018 	 29.526953 	 30.360830
Epoch 180 	 25.752169 	 29.129599 	 30.371267
Epoch 190 	 25.783628 	 29.252151 	 30.263865
[Model stopped early]
Train loss       : 25.680847
Best valid loss  : 28.518396
Best test loss   : 30.387640
Pruning          : 0.52
0.001
0.001
[Current model size]
================================
Total params      : 3,028,378
--------------------------------
Total memory      : 26.00 MB
Total Flops       : 113.88 MFlops
Total Mem (Read)  : 23.92 MB
Total Mem (Write) : 20.63 MB
[Supermasks testing]
[Untrained loss : 78.6281]
[Starting training]
Epoch 0 	 63.489502 	 52.518791 	 57.892445
Epoch 10 	 43.954384 	 40.547024 	 41.590832
Epoch 20 	 41.126156 	 38.495770 	 39.804222
Epoch 30 	 36.375324 	 37.628597 	 38.257145
Epoch 40 	 34.463207 	 34.370495 	 37.452740
Epoch 50 	 32.136570 	 33.361443 	 34.963867
Epoch 60 	 32.808571 	 34.468880 	 34.792114
Epoch 70 	 30.374329 	 33.076065 	 34.173077
Epoch 80 	 28.866497 	 31.413101 	 32.373329
Epoch 90 	 28.634846 	 30.724398 	 31.941381
Epoch 100 	 27.597994 	 30.522535 	 31.174179
Epoch 110 	 27.542393 	 30.709938 	 31.902792
Epoch 120 	 27.335487 	 29.909119 	 31.539286
Epoch 130 	 26.807062 	 29.239780 	 30.916925
Epoch 140 	 25.937672 	 29.285221 	 30.261316
Epoch 150 	 25.315905 	 29.203932 	 29.915211
Epoch 160 	 24.547409 	 27.672592 	 29.560257
Epoch 170 	 24.219553 	 27.812496 	 29.586737
Epoch 180 	 23.882351 	 27.120983 	 29.142759
Epoch 190 	 23.622223 	 27.982903 	 29.019262
Train loss       : 23.703671
Best valid loss  : 26.889072
Best test loss   : 29.036163
Pruning          : 0.37
0.001
0.001
[Current model size]
================================
Total params      : 2,844,854
--------------------------------
Total memory      : 23.18 MB
Total Flops       : 72.48 MFlops
Total Mem (Read)  : 20.64 MB
Total Mem (Write) : 18.05 MB
[Supermasks testing]
[Untrained loss : 96.9983]
[Starting training]
Epoch 0 	 73.861488 	 56.865669 	 62.083393
Epoch 10 	 42.551842 	 42.379539 	 43.774609
Epoch 20 	 37.134541 	 35.326500 	 37.853859
Epoch 30 	 34.833065 	 34.294449 	 35.773296
Epoch 40 	 37.590939 	 38.385170 	 40.237499
Epoch 50 	 32.826988 	 33.001255 	 34.660740
Epoch 60 	 30.687725 	 31.107752 	 32.932625
Epoch 70 	 29.241489 	 31.379744 	 32.472095
Epoch 80 	 28.932329 	 31.426516 	 32.739906
Epoch 90 	 28.107496 	 30.003325 	 31.581635
Epoch 100 	 27.742558 	 30.465466 	 31.393042
Epoch 110 	 27.013224 	 30.155388 	 31.096827
Epoch 120 	 26.934383 	 29.228491 	 31.168940
Epoch 130 	 26.705807 	 28.547537 	 30.870855
Epoch 140 	 26.642677 	 29.525707 	 30.829697
Epoch 150 	 26.540918 	 29.333212 	 30.752686
Epoch 160 	 26.505760 	 29.526377 	 30.839844
[Model stopped early]
Train loss       : 26.257826
Best valid loss  : 28.547537
Best test loss   : 30.870855
Pruning          : 0.27
0.001
0.001
[Current model size]
================================
Total params      : 2,726,701
--------------------------------
Total memory      : 21.15 MB
Total Flops       : 51.3 MFlops
Total Mem (Read)  : 18.35 MB
Total Mem (Write) : 16.21 MB
[Supermasks testing]
[Untrained loss : 158.8524]
[Starting training]
Epoch 0 	 85.054565 	 73.658005 	 77.135948
Epoch 10 	 45.798138 	 43.034763 	 44.755985
Epoch 20 	 40.397366 	 37.850563 	 39.095074
Epoch 30 	 36.693821 	 37.010712 	 37.784222
Epoch 40 	 35.222336 	 34.366150 	 35.789852
Epoch 50 	 32.511787 	 32.026501 	 34.022896
Epoch 60 	 30.670580 	 31.826761 	 33.095417
Epoch 70 	 30.554445 	 31.253347 	 33.301628
Epoch 80 	 30.103872 	 31.809723 	 33.576595
Epoch 90 	 29.324034 	 31.161285 	 32.756954
Epoch 100 	 28.946129 	 31.386608 	 32.176880
Epoch 110 	 28.127657 	 30.613598 	 31.678223
Epoch 120 	 27.410511 	 30.635405 	 31.515869
Epoch 130 	 27.094788 	 30.434523 	 31.393621
Epoch 140 	 26.959007 	 30.162342 	 31.197123
Epoch 150 	 26.596869 	 29.544369 	 31.038599
[Model stopped early]
Train loss       : 26.531471
Best valid loss  : 29.021818
Best test loss   : 31.658804
Pruning          : 0.19
0.001
0.001
[Current model size]
================================
Total params      : 2,649,351
--------------------------------
Total memory      : 19.73 MB
Total Flops       : 40.64 MFlops
Total Mem (Read)  : 16.76 MB
Total Mem (Write) : 14.91 MB
[Supermasks testing]
[Untrained loss : 167.8355]
[Starting training]
Epoch 0 	 88.609253 	 76.827148 	 79.913521
Epoch 10 	 47.075008 	 45.815025 	 46.464855
Epoch 20 	 40.992580 	 39.595119 	 40.879314
Epoch 30 	 38.479145 	 37.295116 	 38.462891
Epoch 40 	 35.022594 	 35.448296 	 37.296429
Epoch 50 	 34.772659 	 34.429226 	 35.399086
Epoch 60 	 33.332623 	 33.451687 	 36.211712
Epoch 70 	 32.306866 	 33.290279 	 34.552502
Epoch 80 	 31.800886 	 32.848835 	 33.765320
Epoch 90 	 29.533831 	 32.073982 	 32.812756
Epoch 100 	 28.678648 	 30.651676 	 32.726166
Epoch 110 	 28.745199 	 31.505325 	 32.837685
Epoch 120 	 27.795883 	 31.284943 	 31.898087
Epoch 130 	 27.349958 	 30.934019 	 31.657566
[Model stopped early]
Train loss       : 27.341175
Best valid loss  : 30.309219
Best test loss   : 32.530148
Pruning          : 0.14
0.001
0.001
[Current model size]
================================
Total params      : 2,597,821
--------------------------------
Total memory      : 18.71 MB
Total Flops       : 35.13 MFlops
Total Mem (Read)  : 15.63 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 151.5295]
[Starting training]
Epoch 0 	 89.510567 	 81.716866 	 85.214531
Epoch 10 	 48.977203 	 43.815670 	 44.953789
Epoch 20 	 43.039673 	 43.349663 	 44.183964
Epoch 30 	 40.519794 	 40.300896 	 40.877869
Epoch 40 	 38.836826 	 37.627213 	 39.779343
Epoch 50 	 37.531410 	 39.499462 	 40.125881
Epoch 60 	 37.145794 	 34.552635 	 37.280453
Epoch 70 	 35.359360 	 34.898323 	 37.156780
Epoch 80 	 32.678123 	 33.387779 	 35.012161
Epoch 90 	 31.924114 	 33.294468 	 34.623577
Epoch 100 	 31.177816 	 32.328884 	 33.608959
Epoch 110 	 30.242823 	 32.612007 	 33.767941
Epoch 120 	 30.199926 	 31.989536 	 33.626354
Epoch 130 	 30.106504 	 32.131283 	 33.448872
Epoch 140 	 29.215204 	 32.101254 	 33.158382
Epoch 150 	 29.203247 	 31.406321 	 32.976070
[Model stopped early]
Train loss       : 29.420891
Best valid loss  : 31.214682
Best test loss   : 33.544842
Pruning          : 0.10
0.001
0.001
[Current model size]
================================
Total params      : 2,563,373
--------------------------------
Total memory      : 17.92 MB
Total Flops       : 31.98 MFlops
Total Mem (Read)  : 14.78 MB
Total Mem (Write) : 13.26 MB
[Supermasks testing]
[Untrained loss : 156.7780]
[Starting training]
Epoch 0 	 89.990616 	 81.395134 	 84.147888
Epoch 10 	 56.984127 	 50.523613 	 54.694042
Epoch 20 	 45.165157 	 41.907619 	 43.491600
Epoch 30 	 44.393639 	 42.173275 	 43.913296
Epoch 40 	 40.790379 	 40.386829 	 41.323708
Epoch 50 	 40.299049 	 42.608276 	 44.299179
Epoch 60 	 38.409218 	 37.428463 	 39.234291
Epoch 70 	 36.740864 	 36.424107 	 37.941181
Epoch 80 	 35.830822 	 36.272770 	 37.791237
Epoch 90 	 34.463551 	 35.547478 	 37.058460
Epoch 100 	 33.757240 	 35.405354 	 36.480267
Epoch 110 	 33.177361 	 34.223957 	 36.181759
Epoch 120 	 32.741974 	 34.206600 	 35.959114
Epoch 130 	 32.272667 	 34.467087 	 35.663063
Epoch 140 	 32.595661 	 34.441425 	 35.641525
Epoch 150 	 32.013088 	 34.131050 	 35.448425
Epoch 160 	 32.084373 	 34.067047 	 35.199139
Epoch 170 	 31.923178 	 33.934746 	 35.560444
Epoch 180 	 32.210003 	 34.139496 	 35.443806
Epoch 190 	 31.796795 	 33.744797 	 35.384003
Train loss       : 31.698895
Best valid loss  : 33.101292
Best test loss   : 35.267200
Pruning          : 0.07
0.001
0.001
[Current model size]
================================
Total params      : 2,540,077
--------------------------------
Total memory      : 17.49 MB
Total Flops       : 30.82 MFlops
Total Mem (Read)  : 14.3 MB
Total Mem (Write) : 12.87 MB
[Supermasks testing]
[Untrained loss : 163.5493]
[Starting training]
Epoch 0 	 88.690880 	 73.247826 	 77.205383
Epoch 10 	 57.142273 	 54.312531 	 57.682411
Epoch 20 	 51.482079 	 45.022541 	 47.391800
Epoch 30 	 46.684147 	 45.751446 	 48.470440
Epoch 40 	 44.381451 	 41.318428 	 42.330486
Epoch 50 	 43.977276 	 41.182350 	 42.735592
Epoch 60 	 42.750885 	 41.001846 	 42.236988
Epoch 70 	 40.364033 	 40.867008 	 42.044849
Epoch 80 	 39.743729 	 40.584530 	 42.805946
Epoch 90 	 38.838234 	 42.429901 	 44.121311
Epoch 100 	 39.451149 	 41.975937 	 43.570435
Epoch 110 	 38.069317 	 37.345238 	 39.106091
Epoch 120 	 35.809532 	 36.235840 	 38.076447
Epoch 130 	 36.131931 	 36.064663 	 37.910336
Epoch 140 	 35.101780 	 34.794106 	 37.614910
Epoch 150 	 34.458252 	 35.880299 	 37.025158
Epoch 160 	 34.220116 	 34.901405 	 37.186428
Epoch 170 	 33.086258 	 35.337090 	 36.754749
Epoch 180 	 32.788406 	 35.115215 	 36.643063
Epoch 190 	 32.473263 	 35.191620 	 36.633526
Train loss       : 32.474861
Best valid loss  : 34.306892
Best test loss   : 36.845863
Pruning          : 0.05
0.001
0.001
[Current model size]
================================
Total params      : 2,523,994
--------------------------------
Total memory      : 17.09 MB
Total Flops       : 29.95 MFlops
Total Mem (Read)  : 13.88 MB
Total Mem (Write) : 12.51 MB
[Supermasks testing]
[Untrained loss : 165.1556]
[Starting training]
Epoch 0 	 94.069374 	 85.188622 	 90.894554
Epoch 10 	 61.594265 	 62.969990 	 67.054527
Epoch 20 	 59.525330 	 56.920322 	 60.990993
Epoch 30 	 52.075790 	 49.837231 	 51.219696
Epoch 40 	 51.561481 	 47.851768 	 49.685009
Epoch 50 	 46.280788 	 45.544167 	 47.031895
Epoch 60 	 44.588768 	 43.099545 	 42.298473
Epoch 70 	 44.054306 	 42.657066 	 43.013721
Epoch 80 	 43.110985 	 41.090309 	 41.399372
Epoch 90 	 42.069050 	 42.312275 	 42.236000
Epoch 100 	 41.567223 	 39.842346 	 40.741684
Epoch 110 	 40.221905 	 39.553844 	 40.487812
Epoch 120 	 39.636414 	 39.964695 	 40.198273
Epoch 130 	 39.805164 	 39.598053 	 40.054188
Epoch 140 	 39.513298 	 39.711990 	 40.279385
Epoch 150 	 39.741245 	 39.531738 	 40.050682
Epoch 160 	 39.385387 	 39.240005 	 40.030407
Epoch 170 	 39.112083 	 39.000984 	 40.162205
Epoch 180 	 40.417076 	 39.485092 	 39.959522
[Model stopped early]
Train loss       : 39.552628
Best valid loss  : 37.844028
Best test loss   : 40.143257
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 2,511,046
--------------------------------
Total memory      : 17.01 MB
Total Flops       : 29.94 MFlops
Total Mem (Read)  : 13.77 MB
Total Mem (Write) : 12.45 MB
[Supermasks testing]
[Untrained loss : 137.4882]
[Starting training]
Epoch 0 	 91.500900 	 86.728798 	 90.668373
Epoch 10 	 61.689850 	 62.618793 	 65.878197
Epoch 20 	 58.644978 	 54.317070 	 58.626705
Epoch 30 	 57.907780 	 56.031818 	 59.151344
Epoch 40 	 55.819496 	 52.667629 	 57.135395
Epoch 50 	 54.965405 	 54.050549 	 56.912365
Epoch 60 	 53.396847 	 53.465427 	 56.583485
Epoch 70 	 49.923714 	 48.938469 	 50.888836
Epoch 80 	 48.636059 	 46.186878 	 49.407352
Epoch 90 	 44.608372 	 44.955921 	 46.094032
Epoch 100 	 45.047604 	 43.229984 	 45.103676
Epoch 110 	 43.330296 	 45.671749 	 47.489838
Epoch 120 	 42.814064 	 41.728882 	 43.564495
Epoch 130 	 41.509098 	 39.823490 	 42.344391
Epoch 140 	 38.822609 	 40.031815 	 41.666546
Epoch 150 	 38.175049 	 39.254612 	 41.342152
Epoch 160 	 38.308300 	 38.983746 	 41.429852
Epoch 170 	 37.101620 	 38.669884 	 40.723442
Epoch 180 	 37.605606 	 38.549072 	 40.511547
Epoch 190 	 37.360836 	 38.242096 	 40.452740
Train loss       : 37.030689
Best valid loss  : 37.150040
Best test loss   : 40.195049
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 2,502,534
--------------------------------
Total memory      : 16.95 MB
Total Flops       : 29.94 MFlops
Total Mem (Read)  : 13.71 MB
Total Mem (Write) : 12.42 MB
[Supermasks testing]
[Untrained loss : 128.0945]
[Starting training]
Epoch 0 	 90.766914 	 87.985954 	 90.624474
Epoch 10 	 64.699730 	 69.433884 	 73.098373
Epoch 20 	 57.817394 	 53.190815 	 57.815731
Epoch 30 	 57.277306 	 53.042931 	 57.383240
Epoch 40 	 56.047039 	 51.650208 	 54.865875
Epoch 50 	 53.430843 	 50.906635 	 54.804867
Epoch 60 	 52.011005 	 48.130703 	 52.381348
Epoch 70 	 49.464680 	 46.987953 	 49.570602
Epoch 80 	 47.907406 	 45.806324 	 47.791168
Epoch 90 	 46.595078 	 44.134979 	 45.681355
Epoch 100 	 46.822617 	 43.655197 	 45.082878
Epoch 110 	 43.712646 	 43.936268 	 44.994900
Epoch 120 	 42.667603 	 42.840519 	 43.744350
Epoch 130 	 43.306381 	 41.952541 	 42.856770
Epoch 140 	 42.742161 	 40.892643 	 42.546589
Epoch 150 	 42.836708 	 40.414497 	 42.533108
Epoch 160 	 41.610077 	 41.527187 	 42.840115
Epoch 170 	 40.355801 	 40.427273 	 41.873207
Epoch 180 	 40.859650 	 40.679821 	 42.582378
Epoch 190 	 40.391109 	 40.239967 	 41.556030
Train loss       : 40.127720
Best valid loss  : 39.377762
Best test loss   : 41.552738
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 2,496,213
--------------------------------
Total memory      : 16.91 MB
Total Flops       : 29.93 MFlops
Total Mem (Read)  : 13.65 MB
Total Mem (Write) : 12.39 MB
[Supermasks testing]
[Untrained loss : 121.5443]
[Starting training]
Epoch 0 	 89.879120 	 86.546104 	 90.360542
Epoch 10 	 65.645355 	 58.794415 	 62.513058
Epoch 20 	 60.196899 	 55.230854 	 60.698605
Epoch 30 	 62.157318 	 54.445126 	 59.276031
Epoch 40 	 56.820194 	 50.702782 	 55.985065
Epoch 50 	 52.749290 	 50.584702 	 54.446224
Epoch 60 	 48.373188 	 49.220829 	 51.322327
Epoch 70 	 46.638226 	 44.771362 	 47.782803
Epoch 80 	 44.144848 	 42.370544 	 44.708889
Epoch 90 	 43.685284 	 43.361752 	 45.597576
Epoch 100 	 42.608257 	 41.401367 	 44.020870
Epoch 110 	 42.569233 	 41.552197 	 44.237495
Epoch 120 	 41.071938 	 41.695259 	 43.101635
Epoch 130 	 39.944756 	 41.026230 	 42.696712
Epoch 140 	 39.442547 	 40.218739 	 42.878735
Epoch 150 	 39.425186 	 40.426346 	 42.737545
Epoch 160 	 39.565578 	 40.515507 	 42.252804
Epoch 170 	 39.292160 	 40.751999 	 42.317841
Epoch 180 	 38.883343 	 39.563717 	 42.389271
Epoch 190 	 39.126999 	 40.097607 	 42.350719
Train loss       : 39.078747
Best valid loss  : 39.423550
Best test loss   : 42.418831
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 2,492,029
--------------------------------
Total memory      : 16.89 MB
Total Flops       : 29.93 MFlops
Total Mem (Read)  : 13.62 MB
Total Mem (Write) : 12.37 MB
[Supermasks testing]
[Untrained loss : 122.6671]
[Starting training]
Epoch 0 	 90.364059 	 87.041969 	 90.285233
Epoch 10 	 66.763573 	 59.169083 	 65.106865
Epoch 20 	 63.966103 	 57.262905 	 62.045013
Epoch 30 	 61.061413 	 56.071491 	 60.234875
Epoch 40 	 61.158821 	 60.770294 	 64.349968
Epoch 50 	 58.629353 	 54.722244 	 59.486408
Epoch 60 	 58.509331 	 53.941521 	 58.887196
Epoch 70 	 56.890800 	 52.905609 	 58.506977
Epoch 80 	 56.951794 	 54.027779 	 58.206665
Epoch 90 	 55.504166 	 53.735703 	 58.052509
Epoch 100 	 55.755062 	 53.100613 	 57.998138
Epoch 110 	 55.498562 	 53.722176 	 58.097546
Epoch 120 	 56.393803 	 54.168953 	 57.996529
Epoch 130 	 55.555073 	 52.934303 	 57.803036
Epoch 140 	 55.597794 	 52.936768 	 57.746948
Epoch 150 	 55.906174 	 53.994183 	 57.529797
Epoch 160 	 55.628319 	 53.622028 	 57.684242
Epoch 170 	 55.750072 	 53.498985 	 57.614033
[Model stopped early]
Train loss       : 56.033768
Best valid loss  : 52.251709
Best test loss   : 57.476124
Pruning          : 0.01
