Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41289096.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, cycler, python-dateutil, kiwisolver, pyparsing, matplotlib, gast, termcolor, protobuf, astor, keras-preprocessing, h5py, keras-applications, tensorflow-estimator, markdown, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, chardet, urllib3, certifi, idna, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, grpcio, absl-py, tensorboard, google-pasta, opt-einsum, wrapt, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41289096.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 05:01:04.090483: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 05:01:04.417261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is singing_classify_cnn_xavier_trimming_magnitude_rewind_global_0.
*******
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.808019 	 0.718290 	 0.706618
Epoch 10 	 0.315602 	 0.375000 	 0.249357
Epoch 20 	 0.135800 	 0.245404 	 0.108364
Epoch 30 	 0.090303 	 0.186581 	 0.062132
Epoch 40 	 0.061121 	 0.163143 	 0.045221
Epoch 50 	 0.056066 	 0.167739 	 0.044301
Epoch 60 	 0.038373 	 0.158088 	 0.034375
Epoch 70 	 0.039062 	 0.156250 	 0.033456
Epoch 80 	 0.031365 	 0.156250 	 0.033456
Epoch 90 	 0.029412 	 0.149357 	 0.031985
Epoch 100 	 0.026195 	 0.153952 	 0.031066
Epoch 110 	 0.015395 	 0.145680 	 0.029228
Epoch 120 	 0.008502 	 0.144761 	 0.029044
Epoch 130 	 0.007812 	 0.147978 	 0.029596
Epoch 140 	 0.007583 	 0.143842 	 0.029044
[Model stopped early]
Train loss       : 0.007812
Best valid loss  : 0.142463
Best test loss   : 0.028676
Pruning          : 1.00
0.0001
0.0001
[Current model size]
================================
Total params      : 1,207,031
--------------------------------
Total memory      : 8.56 MB
Total Flops       : 663.22 MFlops
Total Mem (Read)  : 11.53 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.3071]
[Starting training]
Epoch 0 	 0.060547 	 0.166360 	 0.036765
Epoch 10 	 0.044922 	 0.159467 	 0.036213
Epoch 20 	 0.029297 	 0.151195 	 0.030882
Epoch 30 	 0.021944 	 0.150276 	 0.030699
Epoch 40 	 0.020565 	 0.152574 	 0.031066
Epoch 50 	 0.016429 	 0.146599 	 0.029412
Epoch 60 	 0.011719 	 0.146599 	 0.029412
Epoch 70 	 0.010800 	 0.147059 	 0.029412
Epoch 80 	 0.010915 	 0.149816 	 0.029963
[Model stopped early]
Train loss       : 0.007812
Best valid loss  : 0.142463
Best test loss   : 0.028676
Pruning          : 0.75
0.0001
0.0001
[Current model size]
================================
Total params      : 1,096,123
--------------------------------
Total memory      : 8.56 MB
Total Flops       : 663.1 MFlops
Total Mem (Read)  : 11.1 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.7537]
[Starting training]
Epoch 0 	 0.440947 	 0.294118 	 0.178585
Epoch 10 	 0.183479 	 0.197151 	 0.065993
Epoch 20 	 0.141659 	 0.176011 	 0.049081
Epoch 30 	 0.111558 	 0.168199 	 0.043934
Epoch 40 	 0.104779 	 0.175551 	 0.047794
Epoch 50 	 0.084099 	 0.170956 	 0.039890
Epoch 60 	 0.063074 	 0.163603 	 0.036765
Epoch 70 	 0.067096 	 0.165441 	 0.038235
Epoch 80 	 0.055836 	 0.165441 	 0.037500
Epoch 90 	 0.054458 	 0.161765 	 0.034559
[Model stopped early]
Train loss       : 0.051011
Best valid loss  : 0.155331
Best test loss   : 0.033640
Pruning          : 0.56
0.0001
0.0001
[Current model size]
================================
Total params      : 1,091,587
--------------------------------
Total memory      : 8.56 MB
Total Flops       : 663.1 MFlops
Total Mem (Read)  : 11.09 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.7537]
[Starting training]
Epoch 0 	 0.441291 	 0.308824 	 0.189246
Epoch 10 	 0.178653 	 0.198070 	 0.069210
Epoch 20 	 0.136949 	 0.187500 	 0.052757
Epoch 30 	 0.122817 	 0.167279 	 0.044485
Epoch 40 	 0.105699 	 0.166820 	 0.039890
Epoch 50 	 0.074334 	 0.155331 	 0.036397
Epoch 60 	 0.071232 	 0.156710 	 0.035110
Epoch 70 	 0.061926 	 0.154871 	 0.034191
Epoch 80 	 0.056411 	 0.158088 	 0.034191
Epoch 90 	 0.053768 	 0.157169 	 0.034926
Epoch 100 	 0.053539 	 0.153952 	 0.032721
Epoch 110 	 0.050551 	 0.151654 	 0.031618
Epoch 120 	 0.051356 	 0.149816 	 0.031434
[Model stopped early]
Train loss       : 0.048024
Best valid loss  : 0.149357
Best test loss   : 0.031250
Pruning          : 0.42
0.0001
0.0001
[Current model size]
================================
Total params      : 1,088,171
--------------------------------
Total memory      : 8.55 MB
Total Flops       : 663.1 MFlops
Total Mem (Read)  : 11.07 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.7537]
[Starting training]
Epoch 0 	 0.480239 	 0.366728 	 0.265533
Epoch 10 	 0.184053 	 0.196232 	 0.072702
Epoch 20 	 0.139936 	 0.179688 	 0.054596
Epoch 30 	 0.120749 	 0.166820 	 0.043382
Epoch 40 	 0.103860 	 0.168658 	 0.041728
Epoch 50 	 0.093520 	 0.166360 	 0.038419
Epoch 60 	 0.081687 	 0.159926 	 0.036213
Epoch 70 	 0.069853 	 0.161765 	 0.036581
Epoch 80 	 0.060662 	 0.158088 	 0.033640
Epoch 90 	 0.054802 	 0.164522 	 0.036029
Epoch 100 	 0.049632 	 0.161305 	 0.034375
Epoch 110 	 0.048139 	 0.158088 	 0.032721
Epoch 120 	 0.045611 	 0.160386 	 0.033456
[Model stopped early]
Train loss       : 0.049517
Best valid loss  : 0.152574
Best test loss   : 0.031434
Pruning          : 0.32
0.0001
0.0001
[Current model size]
================================
Total params      : 1,085,623
--------------------------------
Total memory      : 8.55 MB
Total Flops       : 663.09 MFlops
Total Mem (Read)  : 11.06 MB
Total Mem (Write) : 6.41 MB
[Supermasks testing]
[Untrained loss : 0.7119]
[Starting training]
Epoch 0 	 0.580997 	 0.431526 	 0.322426
Epoch 10 	 0.196461 	 0.211857 	 0.083732
Epoch 20 	 0.147863 	 0.189798 	 0.061305
Epoch 30 	 0.118451 	 0.176011 	 0.048346
Epoch 40 	 0.113281 	 0.173713 	 0.047243
Epoch 50 	 0.094669 	 0.164522 	 0.039154
Epoch 60 	 0.078814 	 0.158088 	 0.036213
Epoch 70 	 0.073185 	 0.160846 	 0.035110
Epoch 80 	 0.065257 	 0.159926 	 0.034926
Epoch 90 	 0.057215 	 0.152574 	 0.032904
Epoch 100 	 0.058479 	 0.156250 	 0.033640
Epoch 110 	 0.054228 	 0.154412 	 0.033456
[Model stopped early]
Train loss       : 0.053079
Best valid loss  : 0.152114
Best test loss   : 0.032904
Pruning          : 0.24
0.0001
0.0001
[Current model size]
================================
Total params      : 1,083,639
--------------------------------
Total memory      : 8.55 MB
Total Flops       : 663.09 MFlops
Total Mem (Read)  : 11.05 MB
Total Mem (Write) : 6.41 MB
[Supermasks testing]
[Untrained loss : 0.8783]
[Starting training]
Epoch 0 	 0.675551 	 0.625460 	 0.578401
Epoch 10 	 0.239430 	 0.238511 	 0.124357
Epoch 20 	 0.176585 	 0.192555 	 0.072335
Epoch 30 	 0.148208 	 0.176471 	 0.058915
Epoch 40 	 0.137638 	 0.171875 	 0.046140
Epoch 50 	 0.118107 	 0.159926 	 0.041728
Epoch 60 	 0.112132 	 0.156250 	 0.039522
Epoch 70 	 0.086857 	 0.151654 	 0.034926
Epoch 80 	 0.083869 	 0.155331 	 0.036397
Epoch 90 	 0.078585 	 0.150276 	 0.034743
Epoch 100 	 0.073644 	 0.151195 	 0.034191
Epoch 110 	 0.070657 	 0.154871 	 0.034007
Epoch 120 	 0.067670 	 0.149816 	 0.033272
Epoch 130 	 0.066751 	 0.152114 	 0.034191
Epoch 140 	 0.061581 	 0.153493 	 0.034007
Epoch 150 	 0.061351 	 0.151195 	 0.033088
[Model stopped early]
Train loss       : 0.064568
Best valid loss  : 0.147518
Best test loss   : 0.032353
Pruning          : 0.18
0.0001
0.0001
[Current model size]
================================
Total params      : 1,082,119
--------------------------------
Total memory      : 8.55 MB
Total Flops       : 663.09 MFlops
Total Mem (Read)  : 11.05 MB
Total Mem (Write) : 6.41 MB
[Supermasks testing]
[Untrained loss : 0.9463]
[Starting training]
Epoch 0 	 0.897978 	 0.875000 	 0.866360
Epoch 10 	 0.557560 	 0.491728 	 0.414614
Epoch 20 	 0.512753 	 0.447610 	 0.370588
Epoch 30 	 0.492188 	 0.420956 	 0.353493
Epoch 40 	 0.480469 	 0.370864 	 0.292647
Epoch 50 	 0.464384 	 0.343290 	 0.249081
Epoch 60 	 0.456112 	 0.329504 	 0.233272
Epoch 70 	 0.442785 	 0.316636 	 0.208088
Epoch 80 	 0.436006 	 0.310662 	 0.209559
Epoch 90 	 0.429458 	 0.283548 	 0.182904
Epoch 100 	 0.414867 	 0.290441 	 0.183088
Epoch 110 	 0.409697 	 0.287224 	 0.179963
Epoch 120 	 0.395680 	 0.278033 	 0.173713
Epoch 130 	 0.401999 	 0.284007 	 0.175368
Epoch 140 	 0.405331 	 0.271140 	 0.157721
Epoch 150 	 0.396369 	 0.258732 	 0.152574
Train loss       : 0.400850
Best valid loss  : 0.257353
Best test loss   : 0.145404
Pruning          : 0.13
0.0001
0.0001
[Current model size]
================================
Total params      : 745,669
--------------------------------
Total memory      : 6.84 MB
Total Flops       : 469.56 MFlops
Total Mem (Read)  : 8.48 MB
Total Mem (Write) : 5.13 MB
[Supermasks testing]
[Untrained loss : 0.9421]
[Starting training]
Epoch 0 	 0.943359 	 0.958180 	 0.953493
Epoch 10 	 0.830193 	 0.905331 	 0.888235
Epoch 20 	 0.745864 	 0.693015 	 0.648897
Epoch 30 	 0.582146 	 0.515165 	 0.462500
Epoch 40 	 0.553653 	 0.508272 	 0.457353
Epoch 50 	 0.547449 	 0.504136 	 0.461949
Epoch 60 	 0.539522 	 0.492647 	 0.447243
Epoch 70 	 0.534352 	 0.493566 	 0.440625
Epoch 80 	 0.530905 	 0.495404 	 0.440625
Epoch 90 	 0.530101 	 0.486673 	 0.426103
Epoch 100 	 0.512983 	 0.476103 	 0.416544
Epoch 110 	 0.514706 	 0.473346 	 0.415257
Epoch 120 	 0.511374 	 0.470588 	 0.408824
Epoch 130 	 0.509536 	 0.472886 	 0.409743
Epoch 140 	 0.505285 	 0.468750 	 0.407904
Epoch 150 	 0.503906 	 0.465533 	 0.402757
Train loss       : 0.509651
Best valid loss  : 0.461397
Best test loss   : 0.396507
Pruning          : 0.10
0.0001
0.0001
[Current model size]
================================
Total params      : 449,405
--------------------------------
Total memory      : 5.12 MB
Total Flops       : 296.27 MFlops
Total Mem (Read)  : 6.06 MB
Total Mem (Write) : 3.84 MB
[Supermasks testing]
[Untrained loss : 0.9421]
[Starting training]
Epoch 0 	 0.948874 	 0.955423 	 0.951287
Epoch 10 	 0.828699 	 0.872243 	 0.859007
Epoch 20 	 0.699219 	 0.622702 	 0.578493
Epoch 30 	 0.583180 	 0.534926 	 0.488235
Epoch 40 	 0.556296 	 0.521140 	 0.468934
Epoch 50 	 0.556870 	 0.497702 	 0.449081
Epoch 60 	 0.539062 	 0.508272 	 0.453493
Epoch 70 	 0.542165 	 0.494945 	 0.440074
Epoch 80 	 0.538833 	 0.486213 	 0.432353
Epoch 90 	 0.528722 	 0.479320 	 0.426103
Epoch 100 	 0.525276 	 0.476103 	 0.420404
Epoch 110 	 0.519187 	 0.483456 	 0.420588
Epoch 120 	 0.517693 	 0.479320 	 0.417279
Epoch 130 	 0.513557 	 0.471507 	 0.407353
Epoch 140 	 0.518382 	 0.471507 	 0.403860
Epoch 150 	 0.515625 	 0.474265 	 0.411397
Train loss       : 0.505630
Best valid loss  : 0.461857
Best test loss   : 0.400368
Pruning          : 0.08
0.0001
0.0001
[Current model size]
================================
Total params      : 272,455
--------------------------------
Total memory      : 3.68 MB
Total Flops       : 186.31 MFlops
Total Mem (Read)  : 4.31 MB
Total Mem (Write) : 2.76 MB
[Supermasks testing]
[Untrained loss : 0.9421]
[Starting training]
Epoch 0 	 0.948644 	 0.959099 	 0.953860
Epoch 10 	 0.830767 	 0.917739 	 0.906250
Epoch 20 	 0.715074 	 0.672335 	 0.625368
Epoch 30 	 0.592831 	 0.528493 	 0.474816
Epoch 40 	 0.568359 	 0.534926 	 0.485110
Epoch 50 	 0.558134 	 0.501838 	 0.451103
Epoch 60 	 0.552390 	 0.502757 	 0.447426
Epoch 70 	 0.537454 	 0.508732 	 0.449816
Epoch 80 	 0.535271 	 0.497702 	 0.443566
Epoch 90 	 0.536650 	 0.496324 	 0.444853
Epoch 100 	 0.534007 	 0.489430 	 0.439338
Epoch 110 	 0.530561 	 0.489430 	 0.437132
Epoch 120 	 0.529412 	 0.488971 	 0.435662
Epoch 130 	 0.527918 	 0.492647 	 0.437500
[Model stopped early]
Train loss       : 0.527918
Best valid loss  : 0.488971
Best test loss   : 0.438603
Pruning          : 0.06
0.0001
0.0001
[Current model size]
================================
Total params      : 168,989
--------------------------------
Total memory      : 2.87 MB
Total Flops       : 123.36 MFlops
Total Mem (Read)  : 3.3 MB
Total Mem (Write) : 2.15 MB
[Supermasks testing]
[Untrained loss : 0.9421]
[Starting training]
Epoch 0 	 0.948070 	 0.958180 	 0.953493
Epoch 10 	 0.844210 	 0.917279 	 0.912868
Epoch 20 	 0.710478 	 0.652114 	 0.609191
Epoch 30 	 0.602711 	 0.547335 	 0.496691
Epoch 40 	 0.578814 	 0.516544 	 0.466728
Epoch 50 	 0.575597 	 0.514706 	 0.459743
Epoch 60 	 0.557790 	 0.509651 	 0.456618
Epoch 70 	 0.554228 	 0.502298 	 0.449816
Epoch 80 	 0.544003 	 0.500000 	 0.448529
Epoch 90 	 0.545496 	 0.499081 	 0.447243
Epoch 100 	 0.544922 	 0.499540 	 0.447426
Epoch 110 	 0.535156 	 0.498621 	 0.447978
Epoch 120 	 0.538603 	 0.492647 	 0.444301
Epoch 130 	 0.530905 	 0.490809 	 0.440625
Epoch 140 	 0.528837 	 0.494026 	 0.442463
Epoch 150 	 0.522059 	 0.489430 	 0.440074
[Model stopped early]
Train loss       : 0.527918
Best valid loss  : 0.488971
Best test loss   : 0.442463
Pruning          : 0.04
0.0001
0.0001
[Current model size]
================================
Total params      : 103,679
--------------------------------
Total memory      : 1.96 MB
Total Flops       : 76.9 MFlops
Total Mem (Read)  : 2.37 MB
Total Mem (Write) : 1.47 MB
[Supermasks testing]
[Untrained loss : 0.9421]
[Starting training]
Epoch 0 	 0.946232 	 0.959559 	 0.954596
Epoch 10 	 0.855699 	 0.869945 	 0.866912
Epoch 20 	 0.710593 	 0.658548 	 0.606066
Epoch 30 	 0.622358 	 0.563419 	 0.512316
Epoch 40 	 0.593176 	 0.538143 	 0.490993
Epoch 50 	 0.585823 	 0.524816 	 0.476103
Epoch 60 	 0.573874 	 0.512408 	 0.467647
Epoch 70 	 0.569738 	 0.517004 	 0.463235
Epoch 80 	 0.563189 	 0.511029 	 0.457537
Epoch 90 	 0.555951 	 0.515165 	 0.458824
Epoch 100 	 0.555722 	 0.505515 	 0.452574
Epoch 110 	 0.552849 	 0.500460 	 0.449632
Epoch 120 	 0.550551 	 0.501838 	 0.450735
Epoch 130 	 0.550092 	 0.495404 	 0.447978
Epoch 140 	 0.549517 	 0.496324 	 0.448713
Epoch 150 	 0.551241 	 0.496783 	 0.447794
Train loss       : 0.543888
Best valid loss  : 0.493566
Best test loss   : 0.447243
Pruning          : 0.03
0.0001
0.0001
[Current model size]
================================
Total params      : 64,484
--------------------------------
Total memory      : 1.54 MB
Total Flops       : 51.95 MFlops
Total Mem (Read)  : 1.9 MB
Total Mem (Write) : 1.15 MB
[Supermasks testing]
[Untrained loss : 0.9419]
[Starting training]
Epoch 0 	 0.947840 	 0.958640 	 0.954228
Epoch 10 	 0.853745 	 0.853860 	 0.850919
Epoch 20 	 0.708640 	 0.684283 	 0.626471
Epoch 30 	 0.665326 	 0.568474 	 0.507537
Epoch 40 	 0.619370 	 0.531250 	 0.477941
Epoch 50 	 0.604205 	 0.521140 	 0.468382
Epoch 60 	 0.589040 	 0.520221 	 0.465441
Epoch 70 	 0.584214 	 0.515625 	 0.461949
Epoch 80 	 0.578470 	 0.522518 	 0.467831
Epoch 90 	 0.579504 	 0.508272 	 0.455699
Epoch 100 	 0.570083 	 0.514246 	 0.455147
Epoch 110 	 0.561121 	 0.511029 	 0.452941
[Model stopped early]
Train loss       : 0.565602
Best valid loss  : 0.503217
Best test loss   : 0.456066
Pruning          : 0.02
0.0001
0.0001
[Current model size]
================================
Total params      : 43,474
--------------------------------
Total memory      : 1.26 MB
Total Flops       : 37.89 MFlops
Total Mem (Read)  : 1.61 MB
Total Mem (Write) : 964.66 KB
[Supermasks testing]
[Untrained loss : 0.9419]
[Starting training]
Epoch 0 	 0.949449 	 0.958180 	 0.954779
Epoch 10 	 0.845014 	 0.858456 	 0.864522
Epoch 20 	 0.726103 	 0.714154 	 0.659375
Epoch 30 	 0.679688 	 0.606618 	 0.544302
Epoch 40 	 0.635110 	 0.557904 	 0.500368
Epoch 50 	 0.622243 	 0.539522 	 0.486765
Epoch 60 	 0.613511 	 0.551930 	 0.494669
Epoch 70 	 0.600873 	 0.535386 	 0.480147
Epoch 80 	 0.597197 	 0.532169 	 0.477022
Epoch 90 	 0.601677 	 0.528952 	 0.470404
Epoch 100 	 0.592946 	 0.521140 	 0.465993
Epoch 110 	 0.592601 	 0.524357 	 0.468015
Epoch 120 	 0.592027 	 0.522518 	 0.466176
Epoch 130 	 0.591682 	 0.524816 	 0.468934
Epoch 140 	 0.586282 	 0.520680 	 0.465625
Epoch 150 	 0.593290 	 0.521599 	 0.465257
[Model stopped early]
Train loss       : 0.588235
Best valid loss  : 0.516085
Best test loss   : 0.463235
Pruning          : 0.02
