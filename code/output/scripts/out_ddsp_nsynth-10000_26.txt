Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40977519.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, six, future, torch, torchvision, tqdm, cycler, kiwisolver, python-dateutil, pyparsing, matplotlib, google-pasta, grpcio, wrapt, termcolor, astor, h5py, keras-applications, werkzeug, absl-py, idna, chardet, urllib3, certifi, requests, protobuf, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, opt-einsum, tensorflow-estimator, gast, keras-preprocessing, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40977519.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-24 12:15:51.144848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-24 12:15:51.494935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_ddsp_cnn_xavier_masking_magnitude_reinit_global_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.40977519.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 96.4235]
[Starting training]
Epoch 0 	 77.302872 	 71.212128 	 73.344704
Epoch 10 	 62.928619 	 57.642521 	 59.937218
Epoch 20 	 47.986515 	 42.821865 	 43.904781
Epoch 30 	 43.167225 	 37.322437 	 39.389347
Epoch 40 	 39.862469 	 35.865173 	 37.610653
Epoch 50 	 38.227562 	 33.646721 	 35.826881
Epoch 60 	 34.348495 	 31.160221 	 33.493713
Epoch 70 	 35.092888 	 32.126305 	 34.120148
Epoch 80 	 31.768389 	 29.945564 	 32.147129
Epoch 90 	 30.249886 	 28.721849 	 30.787849
Epoch 100 	 29.021908 	 27.886757 	 29.703506
Epoch 110 	 28.515882 	 28.192255 	 29.768852
Epoch 120 	 27.493870 	 27.591103 	 29.307144
Epoch 130 	 26.400377 	 26.282213 	 28.132877
Epoch 140 	 26.014484 	 26.442911 	 28.052069
Epoch 150 	 25.820583 	 26.229116 	 27.930531
Epoch 160 	 25.510723 	 25.757687 	 27.574772
Epoch 170 	 25.183020 	 25.905949 	 27.603622
Epoch 180 	 19657.523438 	 25.648392 	 27.305820
Epoch 190 	 24.548388 	 25.586067 	 27.159727
Train loss       : 24.320877
Best valid loss  : 25.136385
Best test loss   : 27.206802
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 86.5898]
[Starting training]
Epoch 0 	 70.670876 	 56.300571 	 56.641216
Epoch 10 	 52.517551 	 46.405712 	 46.765408
Epoch 20 	 49.341419 	 44.009666 	 45.289654
Epoch 30 	 46.494461 	 46.469090 	 46.813732
Epoch 40 	 44.321732 	 40.596989 	 41.709892
Epoch 50 	 46.420044 	 48.893921 	 49.967209
Epoch 60 	 42.531910 	 47.246265 	 48.376373
Epoch 70 	 40.267757 	 43.221642 	 44.417225
[Model stopped early]
Train loss       : 39.975025
Best valid loss  : 39.334915
Best test loss   : 40.813587
Pruning          : 0.70
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 83.0823]
[Starting training]
Epoch 0 	 68.618279 	 54.966084 	 55.334560
Epoch 10 	 44.386131 	 41.244396 	 43.003277
Epoch 20 	 38.460758 	 35.464500 	 37.231548
Epoch 30 	 34.726528 	 34.017803 	 35.685711
Epoch 40 	 33.267902 	 31.178211 	 32.797504
Epoch 50 	 32.041149 	 30.511414 	 32.304844
Epoch 60 	 30.398333 	 28.570467 	 30.511526
Epoch 70 	 29.286060 	 29.361574 	 30.784855
Epoch 80 	 28.591347 	 29.495312 	 31.197159
Epoch 90 	 30.755301 	 28.846706 	 30.744617
Epoch 100 	 28.722630 	 27.723284 	 29.560591
Epoch 110 	 27.506519 	 27.220995 	 29.030300
Epoch 120 	 27.222221 	 27.196190 	 29.247467
Epoch 130 	 26.804142 	 26.857742 	 28.650972
Epoch 140 	 26.270260 	 26.966913 	 28.595074
Epoch 150 	 26.173365 	 26.627163 	 28.430820
Epoch 160 	 26.033298 	 26.773869 	 28.639898
Epoch 170 	 25.714502 	 26.423208 	 28.234053
Epoch 180 	 25.631002 	 26.307764 	 28.220409
Epoch 190 	 25.428486 	 25.874781 	 28.195633
Train loss       : 25.295464
Best valid loss  : 25.874781
Best test loss   : 28.195633
Pruning          : 0.49
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 85.0925]
[Starting training]
Epoch 0 	 71.188721 	 55.461979 	 55.397892
Epoch 10 	 52.702999 	 50.253872 	 51.208015
Epoch 20 	 46.774212 	 44.510891 	 45.983467
Epoch 30 	 43.312996 	 38.004585 	 40.900936
Epoch 40 	 41.731518 	 35.639465 	 37.727768
Epoch 50 	 39.553089 	 35.250050 	 37.069782
Epoch 60 	 36.960480 	 33.131901 	 35.215366
Epoch 70 	 34.330669 	 31.980894 	 33.660957
Epoch 80 	 37.514809 	 34.189903 	 35.888756
Epoch 90 	 35.251339 	 32.017281 	 34.197403
[Model stopped early]
Train loss       : 35.346039
Best valid loss  : 31.138256
Best test loss   : 33.341232
Pruning          : 0.34
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 96.9217]
[Starting training]
Epoch 0 	 72.506287 	 251.449799 	 79.277596
Epoch 10 	 49.206497 	 43.249733 	 43.843487
Epoch 20 	 43.552303 	 39.437298 	 41.232372
Epoch 30 	 40.592155 	 37.627438 	 39.686443
Epoch 40 	 39.050224 	 34.890530 	 36.890705
Epoch 50 	 36.724995 	 34.267498 	 35.860664
Epoch 60 	 34.286777 	 32.645695 	 34.163403
Epoch 70 	 32.327969 	 117.117073 	 39.012054
Epoch 80 	 31.032433 	 29.719864 	 31.476559
Epoch 90 	 29.850309 	 29.011080 	 30.543484
Epoch 100 	 30.044786 	 28.952246 	 30.542158
Epoch 110 	 27.728376 	 27.862894 	 29.717659
Epoch 120 	 27.866243 	 27.310507 	 29.173597
Epoch 130 	 26.882883 	 26.919794 	 28.712635
Epoch 140 	 26.479141 	 26.425171 	 27.969130
Epoch 150 	 26.517456 	 27.336901 	 28.963762
Epoch 160 	 25.241762 	 25.313101 	 27.007053
Epoch 170 	 25.307674 	 25.683310 	 27.207823
Epoch 180 	 23.963509 	 25.325407 	 26.893644
Epoch 190 	 23.725567 	 25.185205 	 26.831474
Train loss       : 23.554749
Best valid loss  : 24.553267
Best test loss   : 26.464128
Pruning          : 0.24
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 84.6787]
[Starting training]
Epoch 0 	 1112.563721 	 63.255276 	 63.571815
Epoch 10 	 47.238033 	 42.726006 	 44.392788
Epoch 20 	 40.568687 	 36.079956 	 37.734489
Epoch 30 	 39.539864 	 35.022213 	 36.780338
Epoch 40 	 36.781273 	 34.361469 	 35.806770
Epoch 50 	 33.961033 	 31.120516 	 33.007046
Epoch 60 	 31.834896 	 30.299253 	 32.202183
Epoch 70 	 30.380775 	 28.848576 	 31.100668
Epoch 80 	 33.128445 	 30.286417 	 32.313362
Epoch 90 	 29.447092 	 28.037773 	 29.924196
Epoch 100 	 28.406668 	 27.988321 	 29.619638
Epoch 110 	 27.426058 	 27.589346 	 29.267960
Epoch 120 	 26.960758 	 27.104712 	 28.689983
Epoch 130 	 26.494228 	 26.668554 	 28.271700
Epoch 140 	 26.138735 	 26.508518 	 28.101347
Epoch 150 	 25.592260 	 26.080196 	 27.675543
Epoch 160 	 25.334625 	 26.760832 	 28.378874
Epoch 170 	 24.981739 	 25.563126 	 27.361830
Epoch 180 	 24.689100 	 25.777678 	 27.397684
Epoch 190 	 24.816105 	 20641020.000000 	 243138016.000000
Train loss       : 26.104151
Best valid loss  : 25.238960
Best test loss   : 27.243168
Pruning          : 0.17
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 82.0030]
[Starting training]
Epoch 0 	 72.049484 	 56.589306 	 57.170406
Epoch 10 	 44.894363 	 43.149918 	 43.893032
Epoch 20 	 36.656677 	 36.342823 	 37.992176
Epoch 30 	 33.458450 	 30.542334 	 32.438675
Epoch 40 	 30.933798 	 29.870426 	 31.699295
Epoch 50 	 29.622400 	 28.005409 	 29.737179
Epoch 60 	 29.324121 	 27.442842 	 29.412592
Epoch 70 	 27.920288 	 26.858702 	 28.741957
Epoch 80 	 27.289333 	 27.181660 	 28.761564
Epoch 90 	 26.940220 	 26.397118 	 28.194252
Epoch 100 	 26.565247 	 26.556648 	 28.245615
Epoch 110 	 26.333168 	 26.027374 	 27.490524
Epoch 120 	 25.062342 	 25.515705 	 27.087572
Epoch 130 	 24.761965 	 25.297195 	 27.008467
Epoch 140 	 24.612362 	 25.255266 	 26.848011
Epoch 150 	 24.519327 	 25.298462 	 26.795044
Epoch 160 	 24.342392 	 25.103743 	 26.697102
Epoch 170 	 24.276403 	 24.904764 	 26.587772
Epoch 180 	 24.173811 	 24.984684 	 26.591022
[Model stopped early]
Train loss       : 24.186113
Best valid loss  : 24.785971
Best test loss   : 26.632437
Pruning          : 0.12
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 82.1449]
[Starting training]
Epoch 0 	 71.179390 	 55.584660 	 55.455376
Epoch 10 	 38.113052 	 34.840675 	 36.833015
Epoch 20 	 33.615059 	 31.896004 	 33.617569
Epoch 30 	 31.151104 	 29.271746 	 30.966654
Epoch 40 	 30.514843 	 28.822931 	 30.594202
Epoch 50 	 28.983772 	 28.225964 	 29.898817
Epoch 60 	 28.355593 	 26.876680 	 28.739508
Epoch 70 	 28.990208 	 27.974028 	 29.515106
Epoch 80 	 27.320021 	 26.385199 	 28.138773
Epoch 90 	 26.937111 	 25.881451 	 27.863132
Epoch 100 	 26.521513 	 26.084774 	 27.905628
Epoch 110 	 26.279894 	 25.606930 	 27.303091
Epoch 120 	 25.882757 	 25.611719 	 27.259035
Epoch 130 	 25.628698 	 25.652332 	 27.576683
Epoch 140 	 25.407536 	 25.328501 	 27.189730
Epoch 150 	 25.097799 	 25.112370 	 26.877609
Epoch 160 	 24.462755 	 24.523226 	 26.318348
Epoch 170 	 24.108217 	 24.376043 	 26.152937
Epoch 180 	 24.037529 	 23.976494 	 26.128202
Epoch 190 	 23.912426 	 24.196609 	 26.152187
Train loss       : 23.820278
Best valid loss  : 23.976494
Best test loss   : 26.128202
Pruning          : 0.08
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 78.7155]
[Starting training]
Epoch 0 	 74.051384 	 63.204163 	 64.777962
Epoch 10 	 39.347572 	 35.808678 	 37.760429
Epoch 20 	 38.507664 	 36.463978 	 38.182625
Epoch 30 	 77.943817 	 36.038616 	 38.759510
Epoch 40 	 33.000435 	 30.116573 	 32.352032
Epoch 50 	 31.681320 	 29.800833 	 31.807322
Epoch 60 	 30.516895 	 28.909498 	 30.964418
Epoch 70 	 29.100080 	 27.757851 	 29.389883
Epoch 80 	 28.795509 	 27.007637 	 28.900515
Epoch 90 	 27.945442 	 26.903910 	 28.704632
Epoch 100 	 27.971581 	 26.955927 	 28.932825
Epoch 110 	 27.162231 	 26.353563 	 28.084087
Epoch 120 	 26.575590 	 25.988468 	 27.595673
Epoch 130 	 26.497095 	 25.645857 	 27.369314
Epoch 140 	 25.081852 	 25.175137 	 26.797956
Epoch 150 	 25.224970 	 25.076305 	 26.645010
Epoch 160 	 25.021269 	 24.796577 	 26.484608
Epoch 170 	 24.488468 	 24.594152 	 26.344744
Epoch 180 	 24.394440 	 24.547853 	 26.153236
Epoch 190 	 24.210722 	 24.633772 	 26.276371
Train loss       : 23.897503
Best valid loss  : 24.167381
Best test loss   : 25.931240
Pruning          : 0.06
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 88.6492]
[Starting training]
Epoch 0 	 73.338837 	 58.788376 	 59.153408
Epoch 10 	 39.636036 	 36.913582 	 39.061947
Epoch 20 	 35.185001 	 250.590134 	 43.377785
Epoch 30 	 92.134315 	 34.104927 	 36.054653
Epoch 40 	 33.277157 	 31.632256 	 33.517483
Epoch 50 	 31.653975 	 29.108601 	 30.960426
Epoch 60 	 30.372770 	 28.488497 	 30.412743
Epoch 70 	 30.409348 	 28.978821 	 30.457869
Epoch 80 	 29.030655 	 27.529207 	 29.286547
Epoch 90 	 28.660469 	 27.479588 	 29.749472
Epoch 100 	 28.210602 	 27.015413 	 29.032377
Epoch 110 	 27.644257 	 26.430408 	 28.347708
Epoch 120 	 27.412807 	 26.567377 	 28.422148
Epoch 130 	 27.023634 	 26.145170 	 27.998720
Epoch 140 	 26.613602 	 25.987974 	 28.088411
Epoch 150 	 25.970396 	 25.464554 	 27.255487
Epoch 160 	 25.928062 	 25.201008 	 27.085962
Epoch 170 	 25.548374 	 25.011995 	 26.947800
Epoch 180 	 25.288757 	 24.871605 	 26.683405
Epoch 190 	 25.162573 	 24.859451 	 26.684290
Train loss       : 25.120649
Best valid loss  : 24.730167
Best test loss   : 26.603731
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 94.1512]
[Starting training]
Epoch 0 	 75.336273 	 60.320789 	 60.091816
Epoch 10 	 46.200424 	 42.802929 	 43.103664
Epoch 20 	 37.074310 	 34.419537 	 3561.543701
Epoch 30 	 34.112923 	 31.856457 	 33.442585
Epoch 40 	 32.896351 	 30.799334 	 32.695328
Epoch 50 	 31.174896 	 29.444460 	 31.173000
Epoch 60 	 30.080284 	 28.243330 	 30.179377
Epoch 70 	 29.411360 	 28.813822 	 30.663218
Epoch 80 	 29.282932 	 27.996986 	 29.622402
Epoch 90 	 28.039793 	 27.042168 	 28.741184
Epoch 100 	 27.568228 	 27.226887 	 29.242155
Epoch 110 	 27.333170 	 26.754297 	 28.597458
Epoch 120 	 26.796829 	 26.092806 	 27.804277
Epoch 130 	 26.767115 	 26.069538 	 27.768423
Epoch 140 	 25.759672 	 26.134035 	 27.865618
Epoch 150 	 25.620907 	 25.663866 	 27.189020
Epoch 160 	 25.428537 	 25.387814 	 27.166517
Epoch 170 	 25.445826 	 25.620543 	 27.325352
Epoch 180 	 25.351131 	 25.474028 	 27.213430
Epoch 190 	 24.865681 	 25.343168 	 27.019039
Train loss       : 24.807611
Best valid loss  : 24.924950
Best test loss   : 26.848490
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 85.0934]
[Starting training]
Epoch 0 	 75.382553 	 73.574562 	 60.723827
Epoch 10 	 45.581863 	 44.390999 	 44.017483
Epoch 20 	 39.467163 	 39.129524 	 40.057373
Epoch 30 	 35.664921 	 33.223118 	 34.972080
Epoch 40 	 33.059715 	 31.259388 	 32.992004
Epoch 50 	 31.981030 	 31.210436 	 33.133995
Epoch 60 	 31.481699 	 29.164362 	 31.412012
Epoch 70 	 30.239735 	 28.822823 	 30.604156
Epoch 80 	 29.558615 	 29.067348 	 30.946680
Epoch 90 	 28.694227 	 27.557421 	 29.135073
Epoch 100 	 28.163914 	 27.594479 	 29.239653
Epoch 110 	 27.555296 	 26.904846 	 28.538254
Epoch 120 	 27.230518 	 27.070293 	 28.534420
Epoch 130 	 27.065144 	 26.642611 	 28.271261
Epoch 140 	 27.066442 	 26.771486 	 28.303713
Epoch 150 	 26.954609 	 26.488171 	 28.179537
Epoch 160 	 26.915071 	 26.392954 	 28.078693
Epoch 170 	 26.758350 	 26.396523 	 28.070974
Epoch 180 	 26.673870 	 26.373091 	 28.065914
Epoch 190 	 26.559383 	 26.253914 	 28.042030
Train loss       : 26.541405
Best valid loss  : 25.977278
Best test loss   : 28.003412
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 84.0294]
[Starting training]
Epoch 0 	 75.901466 	 62.435894 	 64.040344
Epoch 10 	 48.544338 	 47.573845 	 46.372753
Epoch 20 	 43.990822 	 42.526848 	 42.639854
Epoch 30 	 40.910694 	 40.004391 	 40.956989
Epoch 40 	 39.011196 	 39.613251 	 63.713230
Epoch 50 	 41.002892 	 40.206936 	 41.133369
Epoch 60 	 38.419708 	 38.796532 	 39.646870
Epoch 70 	 36.732620 	 38.094196 	 38.786488
Epoch 80 	 36.119209 	 37.792450 	 38.931618
Epoch 90 	 35.514679 	 37.002243 	 37.890408
Epoch 100 	 34.770592 	 36.648342 	 37.441208
Epoch 110 	 34.235313 	 35.858944 	 36.989239
Epoch 120 	 33.751850 	 35.927444 	 36.926182
Epoch 130 	 33.289261 	 35.475887 	 36.658882
Epoch 140 	 32.989040 	 35.320850 	 36.471287
Epoch 150 	 32.703163 	 35.010994 	 36.337185
Epoch 160 	 32.507515 	 35.152191 	 36.245903
Epoch 170 	 32.432011 	 34.989594 	 36.108627
Epoch 180 	 32.397652 	 34.983322 	 36.010338
Epoch 190 	 32.336143 	 34.760002 	 36.007805
Train loss       : 32.272587
Best valid loss  : 34.493237
Best test loss   : 36.001968
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 87.3413]
[Starting training]
Epoch 0 	 77.728729 	 70.970009 	 72.730911
Epoch 10 	 47.811428 	 45.590225 	 45.229847
Epoch 20 	 44.347404 	 43.513313 	 43.546059
Epoch 30 	 46.995880 	 46.126125 	 45.999931
Epoch 40 	 44.569199 	 42.944397 	 43.765034
Epoch 50 	 42.540718 	 42.175701 	 43.219734
Epoch 60 	 41.545200 	 41.244759 	 42.454739
Epoch 70 	 40.360584 	 40.795330 	 41.942642
Epoch 80 	 39.706390 	 40.688816 	 41.610268
Epoch 90 	 38.732719 	 39.954815 	 40.632629
Epoch 100 	 38.131248 	 39.370762 	 41.143345
Epoch 110 	 37.483471 	 39.333405 	 40.184113
Epoch 120 	 36.511345 	 38.480003 	 39.422634
Epoch 130 	 36.173405 	 37.653992 	 38.641960
Epoch 140 	 35.540840 	 37.196205 	 38.295757
Epoch 150 	 34.959259 	 36.761131 	 38.067184
Epoch 160 	 34.259808 	 36.494297 	 37.600456
Epoch 170 	 34.060200 	 36.138298 	 37.142941
Epoch 180 	 33.668777 	 36.638950 	 37.489826
Epoch 190 	 32.786095 	 35.835529 	 36.748619
Train loss       : 32.621559
Best valid loss  : 35.389526
Best test loss   : 36.692829
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 86.7097]
[Starting training]
Epoch 0 	 78.172279 	 70.438843 	 72.632446
Epoch 10 	 48.207161 	 46.624142 	 46.552071
Epoch 20 	 45.732914 	 44.269768 	 44.334900
Epoch 30 	 42.786583 	 42.544685 	 43.747498
Epoch 40 	 40.781345 	 41.454700 	 42.062668
Epoch 50 	 38.869999 	 41.480225 	 41.917706
Epoch 60 	 41.777752 	 41.423649 	 41.608974
Epoch 70 	 37.109535 	 39.261368 	 39.676941
Epoch 80 	 36.279312 	 39.167660 	 39.266060
Epoch 90 	 35.395874 	 38.719067 	 39.027748
Epoch 100 	 35.096287 	 38.844524 	 39.075668
Epoch 110 	 34.808098 	 38.520966 	 38.838375
Epoch 120 	 34.424213 	 37.950520 	 38.616390
Epoch 130 	 34.245449 	 38.200020 	 38.761944
Epoch 140 	 34.197723 	 38.183578 	 38.539661
Epoch 150 	 34.002205 	 38.085648 	 38.420033
Epoch 160 	 34.018917 	 38.068375 	 38.472801
Epoch 170 	 33.888657 	 38.166023 	 38.487000
Epoch 180 	 33.892845 	 37.853558 	 38.418537
Epoch 190 	 33.858101 	 38.136547 	 38.446659
Train loss       : 33.822777
Best valid loss  : 37.628609
Best test loss   : 38.533848
Pruning          : 0.01
