Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40871927.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, six, future, torch, torchvision, tqdm, cycler, pyparsing, python-dateutil, kiwisolver, matplotlib, google-pasta, wrapt, protobuf, opt-einsum, h5py, keras-applications, grpcio, astor, werkzeug, urllib3, chardet, certifi, idna, requests, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, tensorboard, gast, tensorflow-estimator, keras-preprocessing, termcolor, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871927.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-22 07:33:12.326387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-22 07:33:12.336948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_sing_ae_cnn_xavier_trimming_information_rewind_local_0.
*******
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5244]
[Starting training]
Epoch 0 	 0.462431 	 0.433368 	 0.430660
Epoch 10 	 0.205494 	 0.205361 	 0.201282
Epoch 20 	 0.172836 	 0.180111 	 0.174109
Epoch 30 	 0.158126 	 0.167467 	 0.163054
Epoch 40 	 0.147794 	 0.158532 	 0.154097
Epoch 50 	 0.143920 	 0.161427 	 0.154947
Epoch 60 	 0.136905 	 0.149007 	 0.144829
Epoch 70 	 0.131071 	 0.150223 	 0.144942
Epoch 80 	 0.129655 	 0.145988 	 0.140660
Epoch 90 	 0.127008 	 0.147983 	 0.142176
Epoch 100 	 0.112851 	 0.134949 	 0.129481
Epoch 110 	 0.111447 	 0.134855 	 0.129676
Epoch 120 	 0.102606 	 0.129353 	 0.123886
Epoch 130 	 0.101978 	 0.130067 	 0.122603
Epoch 140 	 0.097883 	 0.125783 	 0.120538
Epoch 150 	 0.097207 	 0.126048 	 0.120573
Train loss       : 0.095140
Best valid loss  : 0.122893
Best test loss   : 0.120497
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 13,660,755
--------------------------------
Total memory      : 18.12 MB
Total Flops       : 2.95 GFlops
Total Mem (Read)  : 62.53 MB
Total Mem (Write) : 17.89 MB
[Supermasks testing]
[Untrained loss : 0.1443]
[Starting training]
Epoch 0 	 0.133181 	 0.146240 	 0.139820
Epoch 10 	 0.126616 	 0.146815 	 0.140715
Epoch 20 	 0.124225 	 0.146290 	 0.139139
Epoch 30 	 0.110241 	 0.134842 	 0.127437
Epoch 40 	 0.109984 	 0.133144 	 0.127858
Epoch 50 	 0.108397 	 0.131970 	 0.126082
Epoch 60 	 0.106470 	 0.129764 	 0.125428
Epoch 70 	 0.099292 	 0.122765 	 0.121195
Epoch 80 	 0.098299 	 0.127559 	 0.121155
Epoch 90 	 0.095211 	 0.123605 	 0.118844
Epoch 100 	 0.093264 	 0.122930 	 0.117985
Epoch 110 	 0.092150 	 0.122942 	 0.117683
[Model stopped early]
Train loss       : 0.092182
Best valid loss  : 0.119400
Best test loss   : 0.119021
Pruning          : 0.78
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 8,896,787
--------------------------------
Total memory      : 14.18 MB
Total Flops       : 1.87 GFlops
Total Mem (Read)  : 42.12 MB
Total Mem (Write) : 13.95 MB
[Supermasks testing]
[Untrained loss : 0.1440]
[Starting training]
Epoch 0 	 0.132914 	 0.147956 	 0.142947
Epoch 10 	 0.126344 	 0.146416 	 0.139343
Epoch 20 	 0.125164 	 0.143928 	 0.137471
Epoch 30 	 0.123327 	 0.141379 	 0.136064
Epoch 40 	 0.119686 	 0.144269 	 0.136431
Epoch 50 	 0.119837 	 0.141028 	 0.136083
Epoch 60 	 0.116960 	 0.133724 	 0.133109
Epoch 70 	 0.116805 	 0.135882 	 0.130831
Epoch 80 	 0.104883 	 0.129252 	 0.123606
Epoch 90 	 0.098198 	 0.121014 	 0.119925
Epoch 100 	 0.097501 	 0.123777 	 0.120536
Epoch 110 	 0.094042 	 0.121663 	 0.118119
Epoch 120 	 0.092307 	 0.121096 	 0.117169
Epoch 130 	 0.091290 	 0.121534 	 0.116890
Epoch 140 	 0.091231 	 0.121399 	 0.116793
Epoch 150 	 0.091023 	 0.121417 	 0.116556
Train loss       : 0.090564
Best valid loss  : 0.119064
Best test loss   : 0.116764
Pruning          : 0.61
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 5,874,567
--------------------------------
Total memory      : 11.12 MB
Total Flops       : 1.2 GFlops
Total Mem (Read)  : 28.85 MB
Total Mem (Write) : 10.88 MB
[Supermasks testing]
[Untrained loss : 0.1442]
[Starting training]
Epoch 0 	 0.132933 	 0.148810 	 0.142919
Epoch 10 	 0.125688 	 0.146284 	 0.141794
Epoch 20 	 0.123824 	 0.143386 	 0.138303
Epoch 30 	 0.110290 	 0.134688 	 0.128384
Epoch 40 	 0.108704 	 0.131584 	 0.126679
Epoch 50 	 0.108180 	 0.132546 	 0.126341
Epoch 60 	 0.108097 	 0.132765 	 0.126438
Epoch 70 	 0.099253 	 0.125507 	 0.120955
Epoch 80 	 0.098988 	 0.124767 	 0.120613
Epoch 90 	 0.098183 	 0.126228 	 0.121010
Epoch 100 	 0.094323 	 0.122248 	 0.118626
Epoch 110 	 0.093844 	 0.123316 	 0.118162
Epoch 120 	 0.091756 	 0.121535 	 0.117543
Epoch 130 	 0.090893 	 0.122204 	 0.117215
Epoch 140 	 0.090402 	 0.121887 	 0.117056
Epoch 150 	 0.090319 	 0.122220 	 0.117106
Train loss       : 0.090186
Best valid loss  : 0.119218
Best test loss   : 0.117075
Pruning          : 0.47
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,931,875
--------------------------------
Total memory      : 8.72 MB
Total Flops       : 774.71 MFlops
Total Mem (Read)  : 20.07 MB
Total Mem (Write) : 8.49 MB
[Supermasks testing]
[Untrained loss : 0.1455]
[Starting training]
Epoch 0 	 0.132732 	 0.146986 	 0.140208
Epoch 10 	 0.129631 	 0.148719 	 0.141260
Epoch 20 	 0.124718 	 0.143353 	 0.138302
Epoch 30 	 0.110156 	 0.133615 	 0.127075
Epoch 40 	 0.103168 	 0.128946 	 0.122506
Epoch 50 	 0.102276 	 0.127509 	 0.122445
Epoch 60 	 0.098242 	 0.123728 	 0.120114
Epoch 70 	 0.097565 	 0.125962 	 0.120045
Epoch 80 	 0.095656 	 0.124032 	 0.119326
Epoch 90 	 0.094476 	 0.125516 	 0.118958
Epoch 100 	 0.094286 	 0.124175 	 0.118975
Epoch 110 	 0.094239 	 0.124175 	 0.118888
Epoch 120 	 0.093702 	 0.124471 	 0.118790
Epoch 130 	 0.093363 	 0.123652 	 0.118703
Epoch 140 	 0.093250 	 0.120194 	 0.118702
Epoch 150 	 0.093425 	 0.123297 	 0.118644
Train loss       : 0.093062
Best valid loss  : 0.119891
Best test loss   : 0.118617
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,670,357
--------------------------------
Total memory      : 6.85 MB
Total Flops       : 507.44 MFlops
Total Mem (Read)  : 14.2 MB
Total Mem (Write) : 6.62 MB
[Supermasks testing]
[Untrained loss : 0.1750]
[Starting training]
Epoch 0 	 0.134932 	 0.150296 	 0.143049
Epoch 10 	 0.127280 	 0.147598 	 0.140884
Epoch 20 	 0.124280 	 0.139066 	 0.136864
Epoch 30 	 0.122450 	 0.140220 	 0.135273
Epoch 40 	 0.109176 	 0.132979 	 0.126262
Epoch 50 	 0.108069 	 0.132256 	 0.126620
Epoch 60 	 0.107254 	 0.132133 	 0.126749
Epoch 70 	 0.099589 	 0.125836 	 0.121076
Epoch 80 	 0.099046 	 0.122872 	 0.121316
Epoch 90 	 0.098352 	 0.126471 	 0.120542
Epoch 100 	 0.094724 	 0.123489 	 0.118928
Epoch 110 	 0.092961 	 0.122824 	 0.117843
Epoch 120 	 0.092553 	 0.123070 	 0.117732
Epoch 130 	 0.091829 	 0.123720 	 0.117439
Epoch 140 	 0.091242 	 0.122714 	 0.117482
[Model stopped early]
Train loss       : 0.091209
Best valid loss  : 0.119017
Best test loss   : 0.117802
Pruning          : 0.29
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,842,777
--------------------------------
Total memory      : 5.40 MB
Total Flops       : 337.15 MFlops
Total Mem (Read)  : 10.22 MB
Total Mem (Write) : 5.16 MB
[Supermasks testing]
[Untrained loss : 0.2723]
[Starting training]
Epoch 0 	 0.139708 	 0.143368 	 0.140620
Epoch 10 	 0.127879 	 0.145597 	 0.140696
Epoch 20 	 0.113191 	 0.135780 	 0.129952
Epoch 30 	 0.105615 	 0.130422 	 0.125027
Epoch 40 	 0.104094 	 0.130323 	 0.124568
Epoch 50 	 0.099914 	 0.128178 	 0.121790
Epoch 60 	 0.099277 	 0.126946 	 0.121089
Epoch 70 	 0.098554 	 0.127148 	 0.121022
Epoch 80 	 0.098303 	 0.125540 	 0.121034
Epoch 90 	 0.097500 	 0.126467 	 0.120973
Epoch 100 	 0.097023 	 0.127433 	 0.120531
Epoch 110 	 0.096679 	 0.124655 	 0.120584
Epoch 120 	 0.094641 	 0.125486 	 0.119577
Epoch 130 	 0.093783 	 0.124916 	 0.118766
[Model stopped early]
Train loss       : 0.093645
Best valid loss  : 0.121202
Best test loss   : 0.120381
Pruning          : 0.23
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,288,101
--------------------------------
Total memory      : 4.26 MB
Total Flops       : 226.68 MFlops
Total Mem (Read)  : 7.45 MB
Total Mem (Write) : 4.02 MB
[Supermasks testing]
[Untrained loss : 0.4084]
[Starting training]
Epoch 0 	 0.171314 	 0.156409 	 0.149987
Epoch 10 	 0.132280 	 0.150795 	 0.143664
Epoch 20 	 0.114531 	 0.135760 	 0.130169
Epoch 30 	 0.112906 	 0.136709 	 0.130572
Epoch 40 	 0.111619 	 0.135367 	 0.129734
Epoch 50 	 0.110008 	 0.136942 	 0.129605
Epoch 60 	 0.102389 	 0.128056 	 0.122769
Epoch 70 	 0.101964 	 0.128344 	 0.122655
Epoch 80 	 0.098406 	 0.126482 	 0.120714
Epoch 90 	 0.098112 	 0.126497 	 0.120418
Epoch 100 	 0.096226 	 0.124580 	 0.119770
Epoch 110 	 0.096023 	 0.124291 	 0.119671
Epoch 120 	 0.095004 	 0.123907 	 0.119308
Epoch 130 	 0.094771 	 0.124439 	 0.119134
[Model stopped early]
Train loss       : 0.094712
Best valid loss  : 0.122177
Best test loss   : 0.119572
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 915,695
--------------------------------
Total memory      : 3.37 MB
Total Flops       : 155.1 MFlops
Total Mem (Read)  : 5.53 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 0.5050]
[Starting training]
Epoch 0 	 0.220320 	 0.181235 	 0.172459
Epoch 10 	 0.135067 	 0.153533 	 0.146243
Epoch 20 	 0.131902 	 0.152648 	 0.145892
Epoch 30 	 0.128375 	 0.145533 	 0.138794
Epoch 40 	 0.125073 	 0.145685 	 0.141449
Epoch 50 	 0.123257 	 0.141146 	 0.136805
Epoch 60 	 0.127264 	 0.144003 	 0.138974
Epoch 70 	 0.122486 	 0.140749 	 0.135196
Epoch 80 	 0.120315 	 0.140046 	 0.136158
Epoch 90 	 0.108103 	 0.130228 	 0.126181
Epoch 100 	 0.108796 	 0.133364 	 0.126960
Epoch 110 	 0.102313 	 0.128552 	 0.122864
Epoch 120 	 0.101806 	 0.128858 	 0.122672
Epoch 130 	 0.098686 	 0.124175 	 0.120630
Epoch 140 	 0.097272 	 0.122331 	 0.120102
Epoch 150 	 0.096962 	 0.124405 	 0.120208
Train loss       : 0.096478
Best valid loss  : 0.121193
Best test loss   : 0.119922
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 658,395
--------------------------------
Total memory      : 2.68 MB
Total Flops       : 107.48 MFlops
Total Mem (Read)  : 4.15 MB
Total Mem (Write) : 2.44 MB
[Supermasks testing]
[Untrained loss : 0.5376]
[Starting training]
Epoch 0 	 0.252201 	 0.200924 	 0.193284
Epoch 10 	 0.140975 	 0.156685 	 0.150118
Epoch 20 	 0.137082 	 0.152197 	 0.145612
Epoch 30 	 0.131303 	 0.147772 	 0.141718
Epoch 40 	 0.128950 	 0.147254 	 0.142211
Epoch 50 	 0.128288 	 0.147136 	 0.140916
Epoch 60 	 0.115073 	 0.136334 	 0.131691
Epoch 70 	 0.109217 	 0.132603 	 0.126831
Epoch 80 	 0.108913 	 0.131734 	 0.127497
Epoch 90 	 0.105441 	 0.130373 	 0.125186
Epoch 100 	 0.104163 	 0.130135 	 0.124483
Epoch 110 	 0.103343 	 0.127789 	 0.124199
[Model stopped early]
Train loss       : 0.103287
Best valid loss  : 0.126330
Best test loss   : 0.125346
Pruning          : 0.11
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 480,315
--------------------------------
Total memory      : 2.14 MB
Total Flops       : 75.77 MFlops
Total Mem (Read)  : 3.17 MB
Total Mem (Write) : 1.91 MB
[Supermasks testing]
[Untrained loss : 0.5281]
[Starting training]
Epoch 0 	 0.297954 	 0.231418 	 0.225260
Epoch 10 	 0.148704 	 0.159355 	 0.156236
Epoch 20 	 0.140367 	 0.155894 	 0.149812
Epoch 30 	 0.135870 	 0.147957 	 0.143949
Epoch 40 	 0.133763 	 0.148058 	 0.144209
Epoch 50 	 0.129287 	 0.145883 	 0.141766
Epoch 60 	 0.118846 	 0.136846 	 0.133659
Epoch 70 	 0.118652 	 0.136968 	 0.132934
Epoch 80 	 0.111982 	 0.131982 	 0.127448
Epoch 90 	 0.109320 	 0.130811 	 0.126423
Epoch 100 	 0.109081 	 0.131058 	 0.126110
Epoch 110 	 0.108614 	 0.129195 	 0.126494
Epoch 120 	 0.107203 	 0.129177 	 0.125328
Epoch 130 	 0.107160 	 0.125757 	 0.125275
Epoch 140 	 0.106740 	 0.129554 	 0.124925
Epoch 150 	 0.106076 	 0.127774 	 0.124670
Train loss       : 0.105827
Best valid loss  : 0.125757
Best test loss   : 0.125275
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 352,407
--------------------------------
Total memory      : 1.71 MB
Total Flops       : 53.87 MFlops
Total Mem (Read)  : 2.44 MB
Total Mem (Write) : 1.48 MB
[Supermasks testing]
[Untrained loss : 0.5400]
[Starting training]
Epoch 0 	 0.384627 	 0.311932 	 0.306184
Epoch 10 	 0.179206 	 0.184117 	 0.179468
Epoch 20 	 0.163074 	 0.165139 	 0.164643
Epoch 30 	 0.155882 	 0.165922 	 0.160360
Epoch 40 	 0.150046 	 0.158812 	 0.154371
Epoch 50 	 0.147424 	 0.161771 	 0.156371
Epoch 60 	 0.145805 	 0.159459 	 0.152562
Epoch 70 	 0.135554 	 0.149713 	 0.145075
Epoch 80 	 0.131040 	 0.147876 	 0.141517
Epoch 90 	 0.130671 	 0.147422 	 0.140914
Epoch 100 	 0.128075 	 0.146185 	 0.139368
Epoch 110 	 0.127806 	 0.144819 	 0.139297
Epoch 120 	 0.126444 	 0.143506 	 0.138804
Epoch 130 	 0.126129 	 0.144551 	 0.138467
Epoch 140 	 0.126202 	 0.139091 	 0.138508
Epoch 150 	 0.125839 	 0.143007 	 0.138255
Train loss       : 0.125327
Best valid loss  : 0.138236
Best test loss   : 0.138265
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 262,565
--------------------------------
Total memory      : 1.39 MB
Total Flops       : 39.05 MFlops
Total Mem (Read)  : 1.91 MB
Total Mem (Write) : 1.16 MB
[Supermasks testing]
[Untrained loss : 0.5453]
[Starting training]
Epoch 0 	 0.424160 	 0.357548 	 0.355005
Epoch 10 	 0.210205 	 0.212089 	 0.209174
Epoch 20 	 0.183990 	 0.187787 	 0.187045
Epoch 30 	 0.173817 	 0.177298 	 0.176128
Epoch 40 	 0.167910 	 0.174742 	 0.169698
Epoch 50 	 0.162335 	 0.170007 	 0.164967
Epoch 60 	 0.159093 	 0.170115 	 0.165805
Epoch 70 	 0.157024 	 0.166349 	 0.161785
Epoch 80 	 0.149658 	 0.158711 	 0.153479
Epoch 90 	 0.145595 	 0.157649 	 0.153442
Epoch 100 	 0.142473 	 0.153769 	 0.149936
Epoch 110 	 0.141096 	 0.154021 	 0.149766
Epoch 120 	 0.140746 	 0.153464 	 0.149171
Epoch 130 	 0.138137 	 0.149458 	 0.147728
Epoch 140 	 0.137745 	 0.151464 	 0.147532
Epoch 150 	 0.136638 	 0.151332 	 0.147113
Train loss       : 0.136664
Best valid loss  : 0.147629
Best test loss   : 0.147129
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 196,095
--------------------------------
Total memory      : 1.13 MB
Total Flops       : 28.47 MFlops
Total Mem (Read)  : 1.51 MB
Total Mem (Write) : 920.85 KB
[Supermasks testing]
[Untrained loss : 0.5648]
[Starting training]
Epoch 0 	 0.440905 	 0.388384 	 0.390064
Epoch 10 	 0.241208 	 0.238390 	 0.239280
Epoch 20 	 0.207747 	 0.211985 	 0.212149
Epoch 30 	 0.194126 	 0.199154 	 0.198702
Epoch 40 	 0.186675 	 0.190709 	 0.189479
Epoch 50 	 0.180122 	 0.183615 	 0.184091
Epoch 60 	 0.175922 	 0.181848 	 0.180542
Epoch 70 	 0.173549 	 0.180080 	 0.177056
Epoch 80 	 0.170098 	 0.178656 	 0.175129
Epoch 90 	 0.168495 	 0.174498 	 0.173130
Epoch 100 	 0.165655 	 0.172907 	 0.171320
Epoch 110 	 0.159632 	 0.168636 	 0.165069
Epoch 120 	 0.156885 	 0.167886 	 0.164052
Epoch 130 	 0.152982 	 0.162302 	 0.161442
Epoch 140 	 0.150764 	 0.163458 	 0.160235
Epoch 150 	 0.149693 	 0.159414 	 0.159682
Train loss       : 0.149218
Best valid loss  : 0.159414
Best test loss   : 0.159682
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 147,027
--------------------------------
Total memory      : 0.93 MB
Total Flops       : 20.91 MFlops
Total Mem (Read)  : 1.21 MB
Total Mem (Write) : 714.55 KB
[Supermasks testing]
[Untrained loss : 0.5495]
[Starting training]
Epoch 0 	 0.464802 	 0.433741 	 0.429320
Epoch 10 	 0.288958 	 0.288566 	 0.288376
Epoch 20 	 0.249443 	 0.256774 	 0.256402
Epoch 30 	 0.232132 	 0.238674 	 0.238105
Epoch 40 	 0.218856 	 0.222542 	 0.225000
Epoch 50 	 0.210495 	 0.216573 	 0.216375
Epoch 60 	 0.204781 	 0.209946 	 0.208874
Epoch 70 	 0.199931 	 0.203777 	 0.204657
Epoch 80 	 0.196514 	 0.201962 	 0.201043
Epoch 90 	 0.193357 	 0.197811 	 0.197591
Epoch 100 	 0.191364 	 0.196361 	 0.195098
Epoch 110 	 0.184303 	 0.190717 	 0.190145
Epoch 120 	 0.180934 	 0.187427 	 0.187896
Epoch 130 	 0.179553 	 0.187122 	 0.186401
Epoch 140 	 0.178615 	 0.188192 	 0.185995
Epoch 150 	 0.177764 	 0.186814 	 0.185745
Train loss       : 0.177727
Best valid loss  : 0.184052
Best test loss   : 0.185741
Pruning          : 0.03
