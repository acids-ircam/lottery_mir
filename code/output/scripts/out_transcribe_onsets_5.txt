Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288787.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, future, torch, six, torchvision, tqdm, kiwisolver, python-dateutil, cycler, pyparsing, matplotlib, protobuf, tensorflow-estimator, termcolor, keras-preprocessing, grpcio, astor, absl-py, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, idna, certifi, chardet, urllib3, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, markdown, tensorboard, opt-einsum, gast, wrapt, google-pasta, h5py, keras-applications, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288787.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:52:11.324227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:52:11.336626: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_trimming_info_target_reinit_local_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288787.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7279]
[Starting training]
/localscratch/esling.41288787.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
Epoch 0 	 22.629162 	 0.568392 	 0.568223
Epoch 10 	 21.042431 	 0.483218 	 0.486945
Epoch 20 	 19.580475 	 0.317563 	 0.317842
Epoch 30 	 18.369404 	 0.228025 	 0.223074
Epoch 40 	 17.627762 	 0.180762 	 0.183763
Epoch 50 	 17.167820 	 0.158197 	 0.160731
Epoch 60 	 16.826933 	 0.149211 	 0.154829
Epoch 70 	 16.664263 	 0.144706 	 0.148473
Epoch 80 	 16.523672 	 0.147998 	 0.151720
Epoch 90 	 16.434641 	 0.142893 	 0.149095
Epoch 100 	 16.331453 	 0.144060 	 0.147221
Epoch 110 	 16.194643 	 0.140246 	 0.142861
Epoch 120 	 16.107075 	 0.135556 	 0.143264
Epoch 130 	 16.067709 	 0.136353 	 0.142168
Epoch 140 	 16.054590 	 0.137018 	 0.142215
Epoch 150 	 16.040882 	 0.136383 	 0.142556
Epoch 160 	 16.035238 	 0.139247 	 0.143379
Epoch 170 	 16.028158 	 0.140281 	 0.142921
[Model stopped early]
Train loss       : 16.028158
Best valid loss  : 0.132814
Best test loss   : 0.141985
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,031,141
--------------------------------
Total memory      : 15.85 MB
Total Flops       : 1.5 GFlops
Total Mem (Read)  : 23.9 MB
Total Mem (Write) : 12.33 MB
[Supermasks testing]
[Untrained loss : 0.7391]
[Starting training]
Epoch 0 	 22.866428 	 0.619102 	 0.611652
Epoch 10 	 21.315683 	 0.506853 	 0.514358
Epoch 20 	 19.755791 	 0.324016 	 0.329980
Epoch 30 	 18.399387 	 0.220869 	 0.222102
Epoch 40 	 17.574055 	 0.169797 	 0.169053
Epoch 50 	 17.114586 	 0.145960 	 0.146175
Epoch 60 	 16.895145 	 0.142801 	 0.145698
Epoch 70 	 16.701164 	 0.143910 	 0.144834
/localscratch/esling.41288787.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 80 	 16.437286 	 0.135893 	 0.137500
Epoch 90 	 16.362625 	 0.133760 	 0.135761
Epoch 100 	 16.310144 	 0.132956 	 0.134912
Epoch 110 	 16.235481 	 0.136676 	 0.136710
Epoch 120 	 16.183796 	 0.131318 	 0.132374
Epoch 130 	 16.132589 	 0.128242 	 0.130332
Epoch 140 	 16.107782 	 0.126526 	 0.131853
Epoch 150 	 16.082735 	 0.125886 	 0.131453
[Model stopped early]
Train loss       : 16.088663
Best valid loss  : 0.125591
Best test loss   : 0.130237
Pruning          : 0.75
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,015,845
--------------------------------
Total memory      : 11.89 MB
Total Flops       : 848.79 MFlops
Total Mem (Read)  : 16.95 MB
Total Mem (Write) : 9.25 MB
[Supermasks testing]
[Untrained loss : 0.7593]
[Starting training]
Epoch 0 	 22.892643 	 0.592981 	 0.594250
Epoch 10 	 21.556553 	 0.549656 	 0.539839
Epoch 20 	 20.877645 	 0.455332 	 0.456010
Epoch 30 	 19.252529 	 0.280150 	 0.280285
Epoch 40 	 18.109560 	 0.180467 	 0.189936
Epoch 50 	 17.507938 	 0.154983 	 0.164607
Epoch 60 	 17.131998 	 0.155695 	 0.163861
Epoch 70 	 16.888992 	 0.138760 	 0.149763
Epoch 80 	 16.680099 	 0.137956 	 0.141028
Epoch 90 	 16.534500 	 0.138048 	 0.142547
Epoch 100 	 16.476755 	 0.136467 	 0.144595
Epoch 110 	 16.299173 	 0.138964 	 0.142534
Epoch 120 	 16.225286 	 0.136274 	 0.140489
Epoch 130 	 16.176870 	 0.136735 	 0.139557
Epoch 140 	 16.162313 	 0.137227 	 0.140992
Epoch 150 	 16.130278 	 0.134998 	 0.142529
Epoch 160 	 16.125290 	 0.135607 	 0.140900
[Model stopped early]
Train loss       : 16.132807
Best valid loss  : 0.131538
Best test loss   : 0.141667
Pruning          : 0.56
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,378,357
--------------------------------
Total memory      : 8.92 MB
Total Flops       : 481.03 MFlops
Total Mem (Read)  : 12.21 MB
Total Mem (Write) : 6.94 MB
[Supermasks testing]
[Untrained loss : 0.7693]
[Starting training]
Epoch 0 	 23.132069 	 0.617725 	 0.619761
Epoch 10 	 21.652775 	 0.555467 	 0.549685
Epoch 20 	 20.826981 	 0.447136 	 0.446008
Epoch 30 	 19.074184 	 0.261728 	 0.265300
Epoch 40 	 18.227432 	 0.197166 	 0.195692
Epoch 50 	 17.696075 	 0.163099 	 0.168783
Epoch 60 	 17.308237 	 0.153122 	 0.157336
Epoch 70 	 17.064701 	 0.141217 	 0.142361
Epoch 80 	 16.865551 	 0.141498 	 0.146811
Epoch 90 	 16.649689 	 0.136542 	 0.139113
Epoch 100 	 16.544184 	 0.135652 	 0.139106
Epoch 110 	 16.461546 	 0.136184 	 0.139380
Epoch 120 	 16.443558 	 0.134843 	 0.141521
Epoch 130 	 16.376535 	 0.134734 	 0.139035
Epoch 140 	 16.316380 	 0.133478 	 0.138363
Epoch 150 	 16.306717 	 0.133781 	 0.139274
Epoch 160 	 16.304787 	 0.135628 	 0.139122
[Model stopped early]
Train loss       : 16.307913
Best valid loss  : 0.130426
Best test loss   : 0.139578
Pruning          : 0.42
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 969,982
--------------------------------
Total memory      : 6.69 MB
Total Flops       : 273.29 MFlops
Total Mem (Read)  : 8.92 MB
Total Mem (Write) : 5.21 MB
[Supermasks testing]
[Untrained loss : 0.7509]
[Starting training]
Epoch 0 	 23.140568 	 0.635728 	 0.636836
Epoch 10 	 21.739441 	 0.565206 	 0.563575
Epoch 20 	 20.959110 	 0.472302 	 0.469425
Epoch 30 	 19.651415 	 0.304407 	 0.302460
Epoch 40 	 18.787725 	 0.236737 	 0.234301
Epoch 50 	 18.292110 	 0.201038 	 0.204420
Epoch 60 	 17.903316 	 0.179558 	 0.185218
Epoch 70 	 17.584587 	 0.155834 	 0.161712
Epoch 80 	 17.363550 	 0.151887 	 0.156981
Epoch 90 	 17.183701 	 0.146009 	 0.152072
Epoch 100 	 17.076803 	 0.144908 	 0.154884
Epoch 110 	 16.832458 	 0.142929 	 0.146776
Epoch 120 	 16.696880 	 0.143034 	 0.145382
Epoch 130 	 16.670046 	 0.141151 	 0.147129
Epoch 140 	 16.602047 	 0.140965 	 0.145574
Epoch 150 	 16.599342 	 0.141496 	 0.144921
Epoch 160 	 16.562080 	 0.142570 	 0.144387
Epoch 170 	 16.552677 	 0.140079 	 0.142869
Epoch 180 	 16.546921 	 0.140308 	 0.145100
Epoch 190 	 16.585817 	 0.145138 	 0.144291
[Model stopped early]
Train loss       : 16.549845
Best valid loss  : 0.137164
Best test loss   : 0.144293
Pruning          : 0.32
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 700,624
--------------------------------
Total memory      : 4.96 MB
Total Flops       : 152.05 MFlops
Total Mem (Read)  : 6.54 MB
Total Mem (Write) : 3.86 MB
[Supermasks testing]
[Untrained loss : 0.7277]
[Starting training]
Epoch 0 	 23.243168 	 0.667875 	 0.667026
Epoch 10 	 21.649523 	 0.554902 	 0.552216
Epoch 20 	 21.388700 	 0.522214 	 0.524115
Epoch 30 	 20.269880 	 0.374564 	 0.374110
Epoch 40 	 19.513809 	 0.309820 	 0.322081
Epoch 50 	 18.880138 	 0.238374 	 0.244262
Epoch 60 	 18.408421 	 0.200300 	 0.207688
Epoch 70 	 18.047316 	 0.185638 	 0.193614
Epoch 80 	 17.847752 	 0.171704 	 0.183061
Epoch 90 	 17.658533 	 0.169933 	 0.177054
Epoch 100 	 17.514074 	 0.156914 	 0.160630
Epoch 110 	 17.368786 	 0.154043 	 0.159087
Epoch 120 	 17.242193 	 0.156727 	 0.160793
Epoch 130 	 17.144030 	 0.151041 	 0.152241
Epoch 140 	 17.050430 	 0.151084 	 0.148733
Epoch 150 	 16.912460 	 0.142211 	 0.150137
Epoch 160 	 16.844271 	 0.143828 	 0.148894
Epoch 170 	 16.797874 	 0.138097 	 0.145663
Epoch 180 	 16.773916 	 0.140608 	 0.146580
Epoch 190 	 16.695633 	 0.137630 	 0.143739
Train loss       : 16.679111
Best valid loss  : 0.137630
Best test loss   : 0.143739
Pruning          : 0.24
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 522,854
--------------------------------
Total memory      : 3.72 MB
Total Flops       : 87.05 MFlops
Total Mem (Read)  : 4.9 MB
Total Mem (Write) : 2.9 MB
[Supermasks testing]
[Untrained loss : 0.8027]
[Starting training]
Epoch 0 	 23.347979 	 0.660684 	 0.664533
Epoch 10 	 21.918991 	 0.595219 	 0.591300
Epoch 20 	 21.584787 	 0.536252 	 0.536684
Epoch 30 	 20.934793 	 0.454958 	 0.459691
Epoch 40 	 20.361023 	 0.400693 	 0.400042
Epoch 50 	 19.639862 	 0.297503 	 0.303684
Epoch 60 	 19.041128 	 0.262722 	 0.270603
Epoch 70 	 18.661371 	 0.226812 	 0.227278
Epoch 80 	 18.335423 	 0.198521 	 0.201572
Epoch 90 	 18.082592 	 0.183430 	 0.188831
Epoch 100 	 17.949543 	 0.181214 	 0.184766
Epoch 110 	 17.739393 	 0.175987 	 0.178854
Epoch 120 	 17.680435 	 0.173027 	 0.171783
Epoch 130 	 17.517109 	 0.167070 	 0.168871
Epoch 140 	 17.481035 	 0.173234 	 0.169808
Epoch 150 	 17.459915 	 0.164988 	 0.167019
Epoch 160 	 17.414742 	 0.163312 	 0.165706
Epoch 170 	 17.354319 	 0.163135 	 0.163143
Epoch 180 	 17.342123 	 0.160814 	 0.165786
Epoch 190 	 17.329903 	 0.161310 	 0.164264
Train loss       : 17.290741
Best valid loss  : 0.158641
Best test loss   : 0.163150
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 399,974
--------------------------------
Total memory      : 2.73 MB
Total Flops       : 48.02 MFlops
Total Mem (Read)  : 3.66 MB
Total Mem (Write) : 2.13 MB
[Supermasks testing]
[Untrained loss : 0.7300]
[Starting training]
Epoch 0 	 23.324211 	 0.655462 	 0.661395
Epoch 10 	 21.807631 	 0.570098 	 0.568131
Epoch 20 	 21.285440 	 0.506179 	 0.506893
Epoch 30 	 20.528116 	 0.386052 	 0.399805
Epoch 40 	 19.976822 	 0.331721 	 0.335558
Epoch 50 	 19.570509 	 0.293169 	 0.295750
Epoch 60 	 19.317734 	 0.274402 	 0.275839
Epoch 70 	 19.081760 	 0.261814 	 0.260289
Epoch 80 	 18.906698 	 0.238637 	 0.238105
Epoch 90 	 18.773336 	 0.227787 	 0.226582
Epoch 100 	 18.583622 	 0.219636 	 0.218655
Epoch 110 	 18.475576 	 0.212511 	 0.214522
Epoch 120 	 18.324936 	 0.205961 	 0.204285
Epoch 130 	 18.215900 	 0.205282 	 0.200189
Epoch 140 	 18.178541 	 0.204211 	 0.202050
Epoch 150 	 18.050251 	 0.200051 	 0.196990
Epoch 160 	 17.962997 	 0.193951 	 0.192147
Epoch 170 	 17.926311 	 0.192635 	 0.192473
Epoch 180 	 17.888582 	 0.192646 	 0.191171
Epoch 190 	 17.849613 	 0.192241 	 0.191202
Train loss       : 17.864395
Best valid loss  : 0.188525
Best test loss   : 0.191413
Pruning          : 0.13
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 316,333
--------------------------------
Total memory      : 1.99 MB
Total Flops       : 26.32 MFlops
Total Mem (Read)  : 2.77 MB
Total Mem (Write) : 1.55 MB
[Supermasks testing]
[Untrained loss : 0.7751]
[Starting training]
Epoch 0 	 23.563944 	 0.705049 	 0.709990
Epoch 10 	 22.226786 	 0.630356 	 0.628165
Epoch 20 	 21.979336 	 0.589083 	 0.590697
Epoch 30 	 21.370920 	 0.514364 	 0.518508
Epoch 40 	 20.754967 	 0.420891 	 0.429524
Epoch 50 	 20.453857 	 0.389922 	 0.392833
Epoch 60 	 20.215408 	 0.342883 	 0.354847
Epoch 70 	 19.948521 	 0.332205 	 0.335793
Epoch 80 	 19.752995 	 0.317012 	 0.318660
Epoch 90 	 19.568853 	 0.307713 	 0.307935
Epoch 100 	 19.441782 	 0.295106 	 0.291713
Epoch 110 	 19.343740 	 0.290928 	 0.288107
Epoch 120 	 19.214090 	 0.281088 	 0.280636
Epoch 130 	 19.116453 	 0.263086 	 0.258431
Epoch 140 	 19.037018 	 0.255081 	 0.252792
Epoch 150 	 18.907101 	 0.245118 	 0.244716
Epoch 160 	 18.844976 	 0.252412 	 0.247227
Epoch 170 	 18.720840 	 0.246371 	 0.244072
Epoch 180 	 18.649088 	 0.251269 	 0.243309
[Model stopped early]
Train loss       : 18.636183
Best valid loss  : 0.245118
Best test loss   : 0.244716
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 257,489
--------------------------------
Total memory      : 1.49 MB
Total Flops       : 15.44 MFlops
Total Mem (Read)  : 2.16 MB
Total Mem (Write) : 1.16 MB
[Supermasks testing]
[Untrained loss : 0.6998]
[Starting training]
Epoch 0 	 23.633022 	 0.717147 	 0.717663
Epoch 10 	 21.972717 	 0.591519 	 0.593333
Epoch 20 	 21.722580 	 0.566133 	 0.568123
Epoch 30 	 21.245464 	 0.475835 	 0.488225
Epoch 40 	 20.769974 	 0.417220 	 0.420595
Epoch 50 	 20.520121 	 0.385014 	 0.386662
Epoch 60 	 20.298420 	 0.358075 	 0.366110
Epoch 70 	 20.135157 	 0.335102 	 0.342049
Epoch 80 	 20.033829 	 0.335193 	 0.330161
Epoch 90 	 19.914883 	 0.324085 	 0.324765
Epoch 100 	 19.783136 	 0.313266 	 0.310560
Epoch 110 	 19.739014 	 0.300713 	 0.303485
Epoch 120 	 19.640684 	 0.299279 	 0.300995
Epoch 130 	 19.585039 	 0.296350 	 0.300737
Epoch 140 	 19.545921 	 0.292382 	 0.294438
Epoch 150 	 19.397112 	 0.296166 	 0.291254
Epoch 160 	 19.339249 	 0.293387 	 0.290599
Epoch 170 	 19.328548 	 0.287625 	 0.287565
Epoch 180 	 19.231564 	 0.283747 	 0.284363
Epoch 190 	 19.227678 	 0.283474 	 0.285143
Train loss       : 19.179682
Best valid loss  : 0.280658
Best test loss   : 0.283508
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 216,037
--------------------------------
Total memory      : 1.08 MB
Total Flops       : 8.58 MFlops
Total Mem (Read)  : 1.68 MB
Total Mem (Write) : 861.98 KB
[Supermasks testing]
[Untrained loss : 0.7405]
[Starting training]
Epoch 0 	 23.781754 	 0.777722 	 0.778308
Epoch 10 	 22.025259 	 0.595397 	 0.596583
Epoch 20 	 21.793900 	 0.563750 	 0.564859
Epoch 30 	 21.480949 	 0.522327 	 0.528182
Epoch 40 	 21.218792 	 0.495750 	 0.501845
Epoch 50 	 20.989145 	 0.456154 	 0.456085
Epoch 60 	 20.813377 	 0.431844 	 0.434919
Epoch 70 	 20.667599 	 0.417021 	 0.411229
Epoch 80 	 20.539953 	 0.396824 	 0.395992
Epoch 90 	 20.404673 	 0.373889 	 0.368372
Epoch 100 	 20.308594 	 0.360442 	 0.365281
Epoch 110 	 20.207346 	 0.351924 	 0.355584
Epoch 120 	 20.158958 	 0.339367 	 0.341952
Epoch 130 	 20.102823 	 0.343415 	 0.342190
Epoch 140 	 19.986881 	 0.343293 	 0.341128
Epoch 150 	 19.927866 	 0.336541 	 0.336754
Epoch 160 	 19.938408 	 0.336136 	 0.338179
Epoch 170 	 19.899269 	 0.335819 	 0.334532
Epoch 180 	 19.853323 	 0.336908 	 0.336491
Epoch 190 	 19.852785 	 0.331281 	 0.336526
Train loss       : 19.845503
Best valid loss  : 0.328555
Best test loss   : 0.334980
Pruning          : 0.06
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 184,678
--------------------------------
Total memory      : 0.75 MB
Total Flops       : 4.54 MFlops
Total Mem (Read)  : 1.3 MB
Total Mem (Write) : 599.12 KB
[Supermasks testing]
[Untrained loss : 0.7470]
[Starting training]
Epoch 0 	 23.574486 	 0.713605 	 0.719620
Epoch 10 	 22.041706 	 0.595724 	 0.595151
Epoch 20 	 21.814877 	 0.560882 	 0.558861
Epoch 30 	 21.544439 	 0.540889 	 0.542657
Epoch 40 	 21.405155 	 0.512509 	 0.514605
Epoch 50 	 21.316687 	 0.500275 	 0.496432
Epoch 60 	 21.151417 	 0.462920 	 0.468835
Epoch 70 	 21.017929 	 0.445386 	 0.450378
Epoch 80 	 20.933361 	 0.429432 	 0.438057
Epoch 90 	 20.803614 	 0.413630 	 0.415495
Epoch 100 	 20.754585 	 0.412991 	 0.416207
Epoch 110 	 20.683935 	 0.410011 	 0.409964
Epoch 120 	 20.653574 	 0.413685 	 0.415092
Epoch 130 	 20.623091 	 0.407373 	 0.404567
Epoch 140 	 20.598927 	 0.404594 	 0.402116
Epoch 150 	 20.578318 	 0.402788 	 0.401692
Epoch 160 	 20.540226 	 0.401378 	 0.399052
Epoch 170 	 20.551767 	 0.398467 	 0.397758
Epoch 180 	 20.540327 	 0.403563 	 0.400465
Epoch 190 	 20.521227 	 0.404694 	 0.398881
Train loss       : 20.538387
Best valid loss  : 0.394515
Best test loss   : 0.399135
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 162,452
--------------------------------
Total memory      : 0.50 MB
Total Flops       : 2.34 MFlops
Total Mem (Read)  : 1.02 MB
Total Mem (Write) : 401.99 KB
[Supermasks testing]
[Untrained loss : 0.7424]
[Starting training]
Epoch 0 	 23.722731 	 0.761441 	 0.761845
Epoch 10 	 22.078665 	 0.605897 	 0.602844
Epoch 20 	 21.925766 	 0.568832 	 0.567790
Epoch 30 	 21.808159 	 0.564520 	 0.563195
Epoch 40 	 21.732340 	 0.555722 	 0.557381
Epoch 50 	 21.570068 	 0.528180 	 0.526013
Epoch 60 	 21.465336 	 0.512622 	 0.513857
Epoch 70 	 21.361546 	 0.503597 	 0.497550
Epoch 80 	 21.248308 	 0.480743 	 0.483555
Epoch 90 	 21.212936 	 0.474139 	 0.475382
Epoch 100 	 21.122885 	 0.465113 	 0.470783
Epoch 110 	 21.051540 	 0.447973 	 0.454815
Epoch 120 	 21.052114 	 0.445362 	 0.450564
Epoch 130 	 21.009903 	 0.446228 	 0.451310
slurmstepd: error: *** JOB 41288787 ON cdr345 CANCELLED AT 2020-04-29T16:49:02 DUE TO TIME LIMIT ***
