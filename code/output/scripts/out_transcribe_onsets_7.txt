Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288792.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, pillow-simd, six, torchvision, tqdm, cycler, pyparsing, python-dateutil, kiwisolver, matplotlib, gast, h5py, keras-applications, keras-preprocessing, absl-py, termcolor, opt-einsum, wrapt, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, protobuf, urllib3, chardet, certifi, idna, requests, grpcio, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, werkzeug, tensorboard, tensorflow-estimator, google-pasta, astor, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288792.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:52:09.602579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:52:09.616481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_trimming_batchnorm_reinit_global_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288792.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7653]
[Starting training]
Epoch 0 	 22.896164 	 0.652856 	 0.658661
Epoch 10 	 22.158098 	 0.583006 	 0.593221
Epoch 20 	 22.074579 	 0.574799 	 0.590689
Epoch 30 	 21.940945 	 0.574153 	 0.583116
Epoch 40 	 21.556087 	 0.539295 	 0.554292
Epoch 50 	 20.121761 	 0.382835 	 0.390714
Epoch 60 	 18.210327 	 0.210955 	 0.215093
Epoch 70 	 17.323162 	 0.159409 	 0.164852
Epoch 80 	 16.973759 	 0.147224 	 0.157130
Epoch 90 	 16.773491 	 0.144212 	 0.149792
Epoch 100 	 16.630304 	 0.148115 	 0.150845
Epoch 110 	 16.543240 	 0.146371 	 0.150969
Epoch 120 	 16.389463 	 0.146875 	 0.147639
Epoch 130 	 16.308210 	 0.136305 	 0.138916
Epoch 140 	 16.232960 	 0.137713 	 0.137730
Epoch 150 	 16.180309 	 0.135223 	 0.137971
Epoch 160 	 16.159134 	 0.137295 	 0.136495
Epoch 170 	 16.130270 	 0.135691 	 0.136488
Epoch 180 	 16.118605 	 0.136602 	 0.135904
[Model stopped early]
Train loss       : 16.123989
Best valid loss  : 0.133809
Best test loss   : 0.135590
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 2,612,607
--------------------------------
Total memory      : 5.45 MB
Total Flops       : 23.64 MFlops
Total Mem (Read)  : 14.21 MB
Total Mem (Write) : 4.24 MB
[Supermasks testing]
[Untrained loss : 0.7125]
[Starting training]
Epoch 0 	 23.186201 	 0.720184 	 0.723684
Epoch 10 	 22.022640 	 0.584892 	 0.593220
Epoch 20 	 21.911734 	 0.558604 	 0.570869
Epoch 30 	 21.603691 	 0.505828 	 0.521201
Epoch 40 	 20.668520 	 0.399076 	 0.414972
Epoch 50 	 20.082458 	 0.337627 	 0.355370
Epoch 60 	 19.704817 	 0.317935 	 0.335491
Epoch 70 	 19.388577 	 0.286904 	 0.295768
Epoch 80 	 18.984558 	 0.264899 	 0.268185
Epoch 90 	 18.642221 	 0.241702 	 0.243011
Epoch 100 	 18.348345 	 0.233194 	 0.236216
Epoch 110 	 18.165382 	 0.215971 	 0.222074
Epoch 120 	 17.955814 	 0.214016 	 0.216917
Epoch 130 	 17.750843 	 0.209926 	 0.214447
Epoch 140 	 17.638163 	 0.203090 	 0.208310
Epoch 150 	 17.509727 	 0.201692 	 0.208618
Epoch 160 	 17.429445 	 0.198769 	 0.205616
Epoch 170 	 17.365368 	 0.204178 	 0.204050
Epoch 180 	 17.275118 	 0.201008 	 0.201796
Epoch 190 	 17.261862 	 0.197027 	 0.201247
Train loss       : 17.222601
Best valid loss  : 0.194927
Best test loss   : 0.201532
Pruning          : 0.75
0.001
0.001
[Current model size]
================================
Total params      : 1,868,122
--------------------------------
Total memory      : 2.86 MB
Total Flops       : 12.75 MFlops
Total Mem (Read)  : 9.36 MB
Total Mem (Write) : 2.22 MB
[Supermasks testing]
[Untrained loss : 0.7646]
[Starting training]
Epoch 0 	 23.123480 	 0.621781 	 0.633387
Epoch 10 	 21.660536 	 0.523290 	 0.537560
Epoch 20 	 20.899448 	 0.434933 	 0.451388
Epoch 30 	 19.854719 	 0.310713 	 0.331748
Epoch 40 	 19.005262 	 0.249746 	 0.255976
Epoch 50 	 18.390995 	 0.207963 	 0.213317
Epoch 60 	 17.972431 	 0.184592 	 0.194174
Epoch 70 	 17.694777 	 0.177145 	 0.182986
Epoch 80 	 17.446993 	 0.165228 	 0.170025
Epoch 90 	 17.210772 	 0.158536 	 0.153804
Epoch 100 	 17.003365 	 0.151452 	 0.147187
Epoch 110 	 16.894037 	 0.145921 	 0.147932
Epoch 120 	 16.691952 	 0.144380 	 0.140810
Epoch 130 	 16.635046 	 0.140626 	 0.143240
Epoch 140 	 16.531519 	 0.139680 	 0.140036
Epoch 150 	 16.467617 	 0.142285 	 0.140563
Epoch 160 	 16.460266 	 0.139358 	 0.139456
Epoch 170 	 16.432135 	 0.137654 	 0.138153
[Model stopped early]
Train loss       : 16.432135
Best valid loss  : 0.134129
Best test loss   : 0.139237
Pruning          : 0.56
0.001
0.001
[Current model size]
================================
Total params      : 1,377,055
--------------------------------
Total memory      : 0.60 MB
Total Flops       : 3.4 MFlops
Total Mem (Read)  : 5.73 MB
Total Mem (Write) : 479.41 KB
[Supermasks testing]
[Untrained loss : 0.7359]
[Starting training]
Epoch 0 	 23.329126 	 0.692309 	 0.696290
Epoch 10 	 21.929752 	 0.558142 	 0.571457
Epoch 20 	 21.577652 	 0.517468 	 0.530109
Epoch 30 	 20.674255 	 0.393452 	 0.404752
Epoch 40 	 19.800764 	 0.322090 	 0.328964
Epoch 50 	 19.081547 	 0.260503 	 0.261838
Epoch 60 	 18.561199 	 0.233928 	 0.230587
Epoch 70 	 18.137270 	 0.200807 	 0.202779
Epoch 80 	 17.887615 	 0.191216 	 0.198785
Epoch 90 	 17.691984 	 0.183012 	 0.188338
Epoch 100 	 17.519310 	 0.177136 	 0.181320
Epoch 110 	 17.317142 	 0.166631 	 0.173553
Epoch 120 	 17.217440 	 0.164918 	 0.167681
Epoch 130 	 17.083000 	 0.166857 	 0.167628
Epoch 140 	 16.970533 	 0.159859 	 0.165014
Epoch 150 	 16.927921 	 0.161570 	 0.160697
Epoch 160 	 16.788397 	 0.151576 	 0.156877
Epoch 170 	 16.704260 	 0.154745 	 0.155034
Epoch 180 	 16.629353 	 0.148557 	 0.152498
Epoch 190 	 16.567278 	 0.149785 	 0.152632
Train loss       : 16.531076
Best valid loss  : 0.145266
Best test loss   : 0.150364
Pruning          : 0.42
0.001
0.001
[Current model size]
================================
Total params      : 1,008,194
--------------------------------
Total memory      : 0.43 MB
Total Flops       : 2.39 MFlops
Total Mem (Read)  : 4.19 MB
Total Mem (Write) : 345.03 KB
[Supermasks testing]
[Untrained loss : 0.7789]
[Starting training]
Epoch 0 	 23.295856 	 0.654130 	 0.657184
Epoch 10 	 22.018620 	 0.560251 	 0.573575
Epoch 20 	 21.459427 	 0.496920 	 0.513667
Epoch 30 	 20.466885 	 0.377477 	 0.391092
Epoch 40 	 19.606695 	 0.298448 	 0.308842
Epoch 50 	 18.996979 	 0.246825 	 0.263682
Epoch 60 	 18.496368 	 0.203323 	 0.214609
Epoch 70 	 18.225998 	 0.192935 	 0.206605
Epoch 80 	 17.973829 	 0.184555 	 0.194193
Epoch 90 	 17.799896 	 0.173070 	 0.183302
Epoch 100 	 17.671967 	 0.169041 	 0.180234
Epoch 110 	 17.564442 	 0.161394 	 0.167921
Epoch 120 	 17.451996 	 0.162493 	 0.168438
Epoch 130 	 17.346905 	 0.160720 	 0.167106
Epoch 140 	 17.180260 	 0.149457 	 0.155997
Epoch 150 	 17.133617 	 0.148219 	 0.153735
Epoch 160 	 17.002964 	 0.146047 	 0.149588
Epoch 170 	 16.943132 	 0.143911 	 0.150287
Epoch 180 	 16.849831 	 0.147006 	 0.146503
Epoch 190 	 16.829050 	 0.147279 	 0.145661
Train loss       : 16.808420
Best valid loss  : 0.142728
Best test loss   : 0.145717
Pruning          : 0.32
0.001
0.001
[Current model size]
================================
Total params      : 753,540
--------------------------------
Total memory      : 0.32 MB
Total Flops       : 1.7 MFlops
Total Mem (Read)  : 3.14 MB
Total Mem (Write) : 255.17 KB
[Supermasks testing]
[Untrained loss : 0.7237]
[Starting training]
Epoch 0 	 23.538853 	 0.699234 	 0.707117
Epoch 10 	 21.923691 	 0.552154 	 0.565477
Epoch 20 	 21.522257 	 0.494164 	 0.512521
Epoch 30 	 20.347553 	 0.348256 	 0.361922
Epoch 40 	 19.698029 	 0.285056 	 0.288825
Epoch 50 	 19.302431 	 0.251407 	 0.256829
Epoch 60 	 18.921726 	 0.223823 	 0.224456
Epoch 70 	 18.534298 	 0.205285 	 0.207441
Epoch 80 	 18.329380 	 0.194208 	 0.198429
Epoch 90 	 18.106985 	 0.187556 	 0.184410
Epoch 100 	 17.958282 	 0.185308 	 0.181123
Epoch 110 	 17.842329 	 0.178745 	 0.174019
Epoch 120 	 17.739309 	 0.174888 	 0.171090
Epoch 130 	 17.541914 	 0.170201 	 0.170339
Epoch 140 	 17.482548 	 0.173074 	 0.166634
Epoch 150 	 17.454699 	 0.169897 	 0.164550
Epoch 160 	 17.369699 	 0.167087 	 0.164257
[Model stopped early]
Train loss       : 17.391794
Best valid loss  : 0.164343
Best test loss   : 0.165702
Pruning          : 0.24
0.001
0.001
[Current model size]
================================
Total params      : 574,400
--------------------------------
Total memory      : 0.32 MB
Total Flops       : 1.52 MFlops
Total Mem (Read)  : 2.45 MB
Total Mem (Write) : 253.38 KB
[Supermasks testing]
[Untrained loss : 0.7461]
[Starting training]
Epoch 0 	 23.420298 	 0.668694 	 0.671281
Epoch 10 	 21.899035 	 0.541816 	 0.560202
Epoch 20 	 21.372881 	 0.474915 	 0.491032
Epoch 30 	 20.449326 	 0.368305 	 0.382612
Epoch 40 	 20.033804 	 0.329022 	 0.344255
Epoch 50 	 19.670038 	 0.297930 	 0.306567
Epoch 60 	 19.467850 	 0.262350 	 0.279450
Epoch 70 	 19.196800 	 0.237623 	 0.255753
Epoch 80 	 18.932119 	 0.217409 	 0.236628
Epoch 90 	 18.799959 	 0.217135 	 0.229872
Epoch 100 	 18.650261 	 0.206249 	 0.223512
Epoch 110 	 18.409815 	 0.202027 	 0.210842
Epoch 120 	 18.264872 	 0.197409 	 0.207591
Epoch 130 	 18.216249 	 0.191760 	 0.200396
Epoch 140 	 18.113672 	 0.190704 	 0.202878
Epoch 150 	 18.076456 	 0.188267 	 0.198924
Epoch 160 	 18.047054 	 0.184854 	 0.197965
Epoch 170 	 17.980070 	 0.181781 	 0.193526
Epoch 180 	 17.934036 	 0.181325 	 0.192444
Epoch 190 	 17.963139 	 0.184144 	 0.191211
Train loss       : 17.899584
Best valid loss  : 0.176234
Best test loss   : 0.193427
Pruning          : 0.18
0.001
0.001
[Current model size]
================================
Total params      : 448,872
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 1.18 MFlops
Total Mem (Read)  : 1.93 MB
Total Mem (Write) : 208.3 KB
[Supermasks testing]
[Untrained loss : 0.7813]
[Starting training]
Epoch 0 	 23.555243 	 0.713892 	 0.718848
Epoch 10 	 22.119408 	 0.580266 	 0.593867
Epoch 20 	 21.820011 	 0.533363 	 0.554211
Epoch 30 	 21.555271 	 0.497847 	 0.516386
Epoch 40 	 21.113033 	 0.440459 	 0.461866
Epoch 50 	 20.598795 	 0.379477 	 0.383427
Epoch 60 	 20.417749 	 0.359288 	 0.367488
Epoch 70 	 20.217766 	 0.331656 	 0.343508
Epoch 80 	 20.148891 	 0.332507 	 0.340505
Epoch 90 	 20.002365 	 0.321036 	 0.331584
Epoch 100 	 19.949425 	 0.311513 	 0.323742
Epoch 110 	 19.807871 	 0.308060 	 0.317843
Epoch 120 	 19.727606 	 0.297901 	 0.312882
Epoch 130 	 19.630463 	 0.288240 	 0.305761
Epoch 140 	 19.530685 	 0.279311 	 0.290681
Epoch 150 	 19.467501 	 0.282922 	 0.292878
Epoch 160 	 19.391788 	 0.274775 	 0.283280
Epoch 170 	 19.291874 	 0.261019 	 0.270947
Epoch 180 	 19.177397 	 0.241489 	 0.252342
Epoch 190 	 19.158224 	 0.239611 	 0.249166
Train loss       : 19.009060
Best valid loss  : 0.234167
Best test loss   : 0.248369
Pruning          : 0.13
0.001
0.001
[Current model size]
================================
Total params      : 355,937
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 1.09 MFlops
Total Mem (Read)  : 1.57 MB
Total Mem (Write) : 207.32 KB
[Supermasks testing]
[Untrained loss : 0.7332]
[Starting training]
Epoch 0 	 23.488575 	 0.647518 	 0.660509
Epoch 10 	 21.955215 	 0.549623 	 0.569616
Epoch 20 	 21.620787 	 0.512046 	 0.530516
Epoch 30 	 21.108149 	 0.444240 	 0.466950
Epoch 40 	 20.715378 	 0.386853 	 0.407971
Epoch 50 	 20.407654 	 0.365584 	 0.376441
Epoch 60 	 20.228622 	 0.337710 	 0.346030
Epoch 70 	 20.004808 	 0.304872 	 0.318716
Epoch 80 	 19.871237 	 0.298542 	 0.307551
Epoch 90 	 19.734158 	 0.287950 	 0.300326
Epoch 100 	 19.605282 	 0.280970 	 0.290032
Epoch 110 	 19.483988 	 0.267781 	 0.282828
Epoch 120 	 19.419428 	 0.257030 	 0.270473
Epoch 130 	 19.307760 	 0.250144 	 0.259242
Epoch 140 	 19.205574 	 0.245575 	 0.258739
Epoch 150 	 19.163572 	 0.232416 	 0.242944
Epoch 160 	 19.111071 	 0.227738 	 0.237339
Epoch 170 	 19.034616 	 0.227096 	 0.235491
Epoch 180 	 18.957491 	 0.219258 	 0.226070
Epoch 190 	 18.924986 	 0.210154 	 0.221967
Train loss       : 18.799601
Best valid loss  : 0.210154
Best test loss   : 0.221967
Pruning          : 0.10
0.001
0.001
[Current model size]
================================
Total params      : 288,827
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 1.02 MFlops
Total Mem (Read)  : 1.32 MB
Total Mem (Write) : 206.58 KB
[Supermasks testing]
[Untrained loss : 0.7485]
[Starting training]
Epoch 0 	 23.405136 	 0.730299 	 0.733951
Epoch 10 	 22.140215 	 0.578930 	 0.593349
Epoch 20 	 21.834625 	 0.526263 	 0.545068
Epoch 30 	 21.395897 	 0.478872 	 0.500667
Epoch 40 	 21.028479 	 0.435028 	 0.452036
Epoch 50 	 20.869635 	 0.424847 	 0.438667
Epoch 60 	 20.685577 	 0.404946 	 0.420582
Epoch 70 	 20.459858 	 0.367609 	 0.381794
Epoch 80 	 20.342936 	 0.342458 	 0.357867
Epoch 90 	 20.175779 	 0.319177 	 0.336615
Epoch 100 	 20.067257 	 0.314799 	 0.326524
Epoch 110 	 19.967037 	 0.310406 	 0.323032
Epoch 120 	 19.894495 	 0.310231 	 0.321045
Epoch 130 	 19.778528 	 0.297339 	 0.313445
Epoch 140 	 19.736813 	 0.302582 	 0.310412
Epoch 150 	 19.694645 	 0.299536 	 0.307695
Epoch 160 	 19.662655 	 0.290619 	 0.306854
Epoch 170 	 19.652111 	 0.296074 	 0.307503
Epoch 180 	 19.554611 	 0.293261 	 0.306780
Epoch 190 	 19.596365 	 0.291081 	 0.306800
[Model stopped early]
Train loss       : 19.558193
Best valid loss  : 0.290619
Best test loss   : 0.306854
Pruning          : 0.08
0.001
0.001
[Current model size]
================================
Total params      : 240,745
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 972.03 KFlops
Total Mem (Read)  : 1.13 MB
Total Mem (Write) : 206.04 KB
[Supermasks testing]
[Untrained loss : 0.7943]
[Starting training]
Epoch 0 	 23.482021 	 0.732656 	 0.729798
Epoch 10 	 22.118160 	 0.576920 	 0.592799
Epoch 20 	 21.933084 	 0.546020 	 0.564098
Epoch 30 	 21.553864 	 0.483393 	 0.512561
Epoch 40 	 21.171432 	 0.439784 	 0.466125
Epoch 50 	 20.904648 	 0.409867 	 0.425578
Epoch 60 	 20.700867 	 0.391505 	 0.409652
Epoch 70 	 20.626291 	 0.374915 	 0.386562
Epoch 80 	 20.511316 	 0.370570 	 0.376745
Epoch 90 	 20.431732 	 0.348593 	 0.363354
Epoch 100 	 20.372925 	 0.342337 	 0.357875
Epoch 110 	 20.310690 	 0.357498 	 0.366270
Epoch 120 	 20.242468 	 0.339665 	 0.358807
Epoch 130 	 20.134319 	 0.332448 	 0.349085
Epoch 140 	 20.103580 	 0.329624 	 0.343207
Epoch 150 	 20.055359 	 0.325655 	 0.340752
Epoch 160 	 20.069771 	 0.317656 	 0.334249
Epoch 170 	 19.975212 	 0.317938 	 0.333930
Epoch 180 	 19.900045 	 0.313608 	 0.330087
Epoch 190 	 19.860027 	 0.311482 	 0.319541
Train loss       : 19.868137
Best valid loss  : 0.301548
Best test loss   : 0.318192
Pruning          : 0.06
0.001
0.001
[Current model size]
================================
Total params      : 206,060
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 937.34 KFlops
Total Mem (Read)  : 1022.66 KB
Total Mem (Write) : 205.63 KB
[Supermasks testing]
[Untrained loss : 0.7745]
[Starting training]
Epoch 0 	 23.835234 	 0.706722 	 0.707600
Epoch 10 	 22.141727 	 0.579826 	 0.593862
Epoch 20 	 22.039707 	 0.562430 	 0.578863
Epoch 30 	 21.696371 	 0.519256 	 0.541004
Epoch 40 	 21.502283 	 0.484360 	 0.512526
Epoch 50 	 21.318777 	 0.451677 	 0.472216
Epoch 60 	 21.155239 	 0.437665 	 0.461236
Epoch 70 	 20.993322 	 0.427335 	 0.444018
Epoch 80 	 20.929329 	 0.421197 	 0.435570
Epoch 90 	 20.869003 	 0.417212 	 0.430297
Epoch 100 	 20.831987 	 0.407576 	 0.415934
Epoch 110 	 20.768869 	 0.397941 	 0.413593
Epoch 120 	 20.669727 	 0.387306 	 0.406376
Epoch 130 	 20.626064 	 0.381558 	 0.390840
Epoch 140 	 20.595226 	 0.372466 	 0.388923
Epoch 150 	 20.559500 	 0.383591 	 0.388230
Epoch 160 	 20.531742 	 0.376627 	 0.384432
Epoch 170 	 20.419476 	 0.365401 	 0.374200
Epoch 180 	 20.406937 	 0.362288 	 0.372641
Epoch 190 	 20.364697 	 0.359559 	 0.368369
Train loss       : 20.303106
Best valid loss  : 0.354499
Best test loss   : 0.365333
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 176,584
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 907.87 KFlops
Total Mem (Read)  : 907.2 KB
Total Mem (Write) : 205.3 KB
[Supermasks testing]
[Untrained loss : 0.7333]
[Starting training]
Epoch 0 	 23.556391 	 0.724491 	 0.726057
Epoch 10 	 22.142071 	 0.575346 	 0.588928
Epoch 20 	 21.934855 	 0.541589 	 0.562445
Epoch 30 	 21.746292 	 0.515888 	 0.532265
Epoch 40 	 21.617250 	 0.504526 	 0.520500
Epoch 50 	 21.475426 	 0.477299 	 0.492684
Epoch 60 	 21.408255 	 0.471796 	 0.481731
Epoch 70 	 21.295042 	 0.468129 	 0.479838
Epoch 80 	 21.264095 	 0.458594 	 0.473516
Epoch 90 	 21.181051 	 0.451273 	 0.457698
Epoch 100 	 21.121500 	 0.441180 	 0.447973
Epoch 110 	 21.024734 	 0.435485 	 0.435894
Epoch 120 	 20.957890 	 0.417025 	 0.422837
Epoch 130 	 20.912411 	 0.420882 	 0.429086
Epoch 140 	 20.841217 	 0.420251 	 0.425627
Epoch 150 	 20.806238 	 0.409629 	 0.419357
Epoch 160 	 20.863409 	 0.410953 	 0.418430
Epoch 170 	 20.744669 	 0.408519 	 0.415866
Epoch 180 	 20.754684 	 0.403660 	 0.416955
Epoch 190 	 20.751398 	 0.401355 	 0.410478
Train loss       : 20.740421
Best valid loss  : 0.398922
Best test loss   : 0.409740
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 155,857
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 887.14 KFlops
Total Mem (Read)  : 825.98 KB
Total Mem (Write) : 205.06 KB
[Supermasks testing]
[Untrained loss : 0.7132]
[Starting training]
Epoch 0 	 23.761942 	 0.785164 	 0.779595
Epoch 10 	 22.188101 	 0.578550 	 0.592157
Epoch 20 	 22.007721 	 0.547411 	 0.567667
Epoch 30 	 21.843637 	 0.520408 	 0.542797
Epoch 40 	 21.717373 	 0.504749 	 0.532260
Epoch 50 	 21.591848 	 0.494651 	 0.522570
Epoch 60 	 21.568314 	 0.491591 	 0.520794
Epoch 70 	 21.480659 	 0.488903 	 0.512018
Epoch 80 	 21.466629 	 0.482136 	 0.506660
Epoch 90 	 21.406002 	 0.485817 	 0.504714
Epoch 100 	 21.368362 	 0.476948 	 0.497946
Epoch 110 	 21.335201 	 0.474001 	 0.498709
Epoch 120 	 21.281042 	 0.471723 	 0.490925
Epoch 130 	 21.254435 	 0.475727 	 0.490967
Epoch 140 	 21.270334 	 0.476158 	 0.492664
Epoch 150 	 21.193233 	 0.470321 	 0.490497
Epoch 160 	 21.193893 	 0.469344 	 0.491863
[Model stopped early]
Train loss       : 21.193893
Best valid loss  : 0.464800
Best test loss   : 0.491498
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 141,741
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 873.02 KFlops
Total Mem (Read)  : 770.66 KB
Total Mem (Write) : 204.87 KB
[Supermasks testing]
[Untrained loss : 0.7468]
[Starting training]
Epoch 0 	 23.804676 	 0.783432 	 0.780063
Epoch 10 	 22.266962 	 0.585026 	 0.601438
Epoch 20 	 22.108101 	 0.569826 	 0.581819
Epoch 30 	 22.000906 	 0.540473 	 0.560436
Epoch 40 	 21.845642 	 0.521497 	 0.544548
Epoch 50 	 21.779562 	 0.513736 	 0.529882
Epoch 60 	 21.732794 	 0.518699 	 0.535972
Epoch 70 	 21.705904 	 0.510288 	 0.526323
Epoch 80 	 21.662733 	 0.503195 	 0.525209
Epoch 90 	 21.648233 	 0.502350 	 0.523454
Epoch 100 	 21.592327 	 0.496382 	 0.519195
Epoch 110 	 21.568928 	 0.494150 	 0.519381
Epoch 120 	 21.541815 	 0.492479 	 0.514507
Epoch 130 	 21.532412 	 0.489719 	 0.514449
Epoch 140 	 21.533348 	 0.486087 	 0.507713
Epoch 150 	 21.498814 	 0.491272 	 0.511494
Epoch 160 	 21.511271 	 0.489410 	 0.511020
[Model stopped early]
Train loss       : 21.500650
Best valid loss  : 0.483388
Best test loss   : 0.509766
Pruning          : 0.02
