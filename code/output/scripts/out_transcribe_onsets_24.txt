Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288816.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, pillow-simd, future, torch, torchvision, tqdm, pyparsing, cycler, kiwisolver, python-dateutil, matplotlib, gast, protobuf, werkzeug, grpcio, absl-py, certifi, idna, urllib3, chardet, requests, oauthlib, requests-oauthlib, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, google-auth-oauthlib, markdown, tensorboard, opt-einsum, wrapt, google-pasta, h5py, keras-applications, keras-preprocessing, tensorflow-estimator, astor, termcolor, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288816.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:52:46.477184: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:52:46.490233: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_masking_magnitude_reinit_local_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288816.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7398]
[Starting training]
/localscratch/esling.41288816.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
Epoch 0 	 22.851978 	 0.610880 	 0.600708
Epoch 10 	 21.872450 	 0.579065 	 0.574545
Epoch 20 	 21.638977 	 0.543696 	 0.548524
Epoch 30 	 20.321421 	 0.386856 	 0.389396
Epoch 40 	 19.513920 	 0.329661 	 0.326163
Epoch 50 	 18.763273 	 0.250452 	 0.243218
Epoch 60 	 18.154486 	 0.219386 	 0.213416
Epoch 70 	 17.794855 	 0.195162 	 0.192644
Epoch 80 	 17.355221 	 0.158386 	 0.150213
Epoch 90 	 17.115652 	 0.147083 	 0.148429
Epoch 100 	 16.960043 	 0.149472 	 0.145429
Epoch 110 	 16.797819 	 0.145175 	 0.143353
Epoch 120 	 16.614550 	 0.142958 	 0.136961
Epoch 130 	 16.518826 	 0.140631 	 0.135521
Epoch 140 	 16.446573 	 0.140121 	 0.131504
Epoch 150 	 16.378593 	 0.136913 	 0.129243
Epoch 160 	 16.351871 	 0.135978 	 0.130959
Epoch 170 	 16.306116 	 0.137999 	 0.129950
Epoch 180 	 16.290167 	 0.134096 	 0.130446
Epoch 190 	 16.275852 	 0.136666 	 0.130145
Train loss       : 16.268702
Best valid loss  : 0.128993
Best test loss   : 0.129290
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7865]
[Starting training]
Epoch 0 	 22.725590 	 0.618436 	 0.606152
Epoch 10 	 21.278339 	 0.485879 	 0.482157
Epoch 20 	 20.368465 	 0.388615 	 0.384812
Epoch 30 	 18.891546 	 0.259962 	 0.245813
/localscratch/esling.41288816.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 40 	 17.976273 	 0.204773 	 0.195919
Epoch 50 	 17.355913 	 0.157179 	 0.153380
Epoch 60 	 17.007286 	 0.147791 	 0.149654
Epoch 70 	 16.799669 	 0.142211 	 0.140908
Epoch 80 	 16.602655 	 0.133701 	 0.127308
Epoch 90 	 16.459677 	 0.130058 	 0.122175
Epoch 100 	 16.272497 	 0.128918 	 0.119473
Epoch 110 	 16.201500 	 0.124894 	 0.119542
Epoch 120 	 16.150673 	 0.124659 	 0.121435
Epoch 130 	 16.099588 	 0.126666 	 0.118482
Epoch 140 	 16.067616 	 0.127088 	 0.117156
[Model stopped early]
Train loss       : 16.059465
Best valid loss  : 0.123476
Best test loss   : 0.120116
Pruning          : 0.70
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7899]
[Starting training]
Epoch 0 	 22.905750 	 0.697383 	 0.695621
Epoch 10 	 21.753359 	 0.547394 	 0.544619
Epoch 20 	 21.274843 	 0.482053 	 0.481361
Epoch 30 	 19.755684 	 0.319813 	 0.311821
Epoch 40 	 18.338675 	 0.214188 	 0.204988
Epoch 50 	 17.607588 	 0.151920 	 0.147393
Epoch 60 	 17.197336 	 0.143115 	 0.134912
Epoch 70 	 16.920961 	 0.128416 	 0.124192
Epoch 80 	 16.727074 	 0.132563 	 0.124226
Epoch 90 	 16.475317 	 0.130442 	 0.122112
Epoch 100 	 16.414345 	 0.128694 	 0.121322
Epoch 110 	 16.331347 	 0.127290 	 0.120628
Epoch 120 	 16.263920 	 0.129085 	 0.118200
Epoch 130 	 16.210855 	 0.128368 	 0.118167
Epoch 140 	 16.194613 	 0.128273 	 0.117053
Epoch 150 	 16.190592 	 0.125776 	 0.118683
Epoch 160 	 16.163589 	 0.127064 	 0.117602
Epoch 170 	 16.147453 	 0.125419 	 0.118324
Epoch 180 	 16.134121 	 0.125422 	 0.119095
Epoch 190 	 16.116343 	 0.127194 	 0.119129
[Model stopped early]
Train loss       : 16.123783
Best valid loss  : 0.123042
Best test loss   : 0.118109
Pruning          : 0.49
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7403]
[Starting training]
Epoch 0 	 22.694138 	 0.624155 	 0.613934
Epoch 10 	 21.656467 	 0.544681 	 0.539384
Epoch 20 	 20.711918 	 0.398653 	 0.395203
Epoch 30 	 18.787554 	 0.234734 	 0.228144
Epoch 40 	 17.763109 	 0.165296 	 0.158806
Epoch 50 	 17.247501 	 0.141184 	 0.136364
Epoch 60 	 16.961157 	 0.134529 	 0.126755
Epoch 70 	 16.817225 	 0.130287 	 0.126265
Epoch 80 	 16.693375 	 0.128988 	 0.125153
Epoch 90 	 16.543140 	 0.128484 	 0.122488
Epoch 100 	 16.370705 	 0.128639 	 0.117909
Epoch 110 	 16.261080 	 0.124744 	 0.116134
Epoch 120 	 16.210184 	 0.120480 	 0.114463
Epoch 130 	 16.176441 	 0.122800 	 0.114998
Epoch 140 	 16.176025 	 0.121830 	 0.113647
Epoch 150 	 16.157282 	 0.123394 	 0.113466
Epoch 160 	 16.149021 	 0.121316 	 0.113230
Epoch 170 	 16.148611 	 0.122419 	 0.112251
Epoch 180 	 16.124447 	 0.121531 	 0.113354
Epoch 190 	 16.143745 	 0.119597 	 0.113327
[Model stopped early]
Train loss       : 16.134604
Best valid loss  : 0.117532
Best test loss   : 0.113677
Pruning          : 0.34
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7347]
[Starting training]
Epoch 0 	 22.632616 	 0.591216 	 0.584514
Epoch 10 	 21.498085 	 0.531139 	 0.527222
Epoch 20 	 20.528814 	 0.397919 	 0.393768
Epoch 30 	 18.681601 	 0.230575 	 0.224144
Epoch 40 	 17.761087 	 0.173215 	 0.166310
Epoch 50 	 17.374561 	 0.160494 	 0.153173
Epoch 60 	 17.102074 	 0.147785 	 0.143638
Epoch 70 	 16.933475 	 0.143719 	 0.139905
Epoch 80 	 16.653837 	 0.134403 	 0.129445
Epoch 90 	 16.569706 	 0.133416 	 0.128775
Epoch 100 	 16.496555 	 0.133161 	 0.125149
Epoch 110 	 16.415627 	 0.131644 	 0.124798
Epoch 120 	 16.383492 	 0.133059 	 0.126155
Epoch 130 	 16.341274 	 0.131781 	 0.123862
Epoch 140 	 16.317514 	 0.128869 	 0.124002
Epoch 150 	 16.296045 	 0.129819 	 0.124374
Epoch 160 	 16.282402 	 0.130747 	 0.123153
Epoch 170 	 16.266855 	 0.130482 	 0.122739
Epoch 180 	 16.264200 	 0.130027 	 0.123466
Epoch 190 	 16.250673 	 0.129941 	 0.123103
[Model stopped early]
Train loss       : 16.240652
Best valid loss  : 0.127356
Best test loss   : 0.124471
Pruning          : 0.24
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7361]
[Starting training]
Epoch 0 	 22.763727 	 0.633691 	 0.627052
Epoch 10 	 21.782843 	 0.572623 	 0.564553
Epoch 20 	 20.635292 	 0.429787 	 0.427774
Epoch 30 	 18.833609 	 0.230944 	 0.229438
Epoch 40 	 17.836269 	 0.161032 	 0.161585
Epoch 50 	 17.383684 	 0.144037 	 0.141372
Epoch 60 	 17.103436 	 0.140158 	 0.131981
Epoch 70 	 16.931570 	 0.139142 	 0.129246
Epoch 80 	 16.809269 	 0.136013 	 0.127908
Epoch 90 	 16.710821 	 0.131167 	 0.126176
Epoch 100 	 16.639410 	 0.133614 	 0.123908
Epoch 110 	 16.565006 	 0.129815 	 0.124915
Epoch 120 	 16.507193 	 0.131694 	 0.122909
Epoch 130 	 16.472958 	 0.127571 	 0.124710
Epoch 140 	 16.320400 	 0.126087 	 0.120690
Epoch 150 	 16.271172 	 0.127089 	 0.121502
Epoch 160 	 16.253922 	 0.128900 	 0.117946
Epoch 170 	 16.177399 	 0.125179 	 0.118413
Epoch 180 	 16.157108 	 0.125559 	 0.116454
[Model stopped early]
Train loss       : 16.145210
Best valid loss  : 0.122325
Best test loss   : 0.119203
Pruning          : 0.17
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7636]
[Starting training]
Epoch 0 	 22.910391 	 0.653995 	 0.646713
Epoch 10 	 21.566940 	 0.532475 	 0.533697
Epoch 20 	 20.551765 	 0.403254 	 0.406125
Epoch 30 	 18.810776 	 0.235596 	 0.226231
Epoch 40 	 17.857611 	 0.166542 	 0.160446
Epoch 50 	 17.423435 	 0.155245 	 0.150182
Epoch 60 	 17.217190 	 0.137326 	 0.135061
Epoch 70 	 17.013325 	 0.135264 	 0.129974
Epoch 80 	 16.909187 	 0.128418 	 0.123895
Epoch 90 	 16.814285 	 0.130591 	 0.122520
Epoch 100 	 16.616604 	 0.126204 	 0.123270
Epoch 110 	 16.561977 	 0.127684 	 0.121960
Epoch 120 	 16.510187 	 0.128193 	 0.119415
Epoch 130 	 16.440159 	 0.128627 	 0.120407
Epoch 140 	 16.404547 	 0.127019 	 0.119902
Epoch 150 	 16.378778 	 0.124344 	 0.119219
Epoch 160 	 16.378304 	 0.125361 	 0.120631
Epoch 170 	 16.375860 	 0.126416 	 0.120235
Epoch 180 	 16.362091 	 0.126805 	 0.118950
Epoch 190 	 16.355753 	 0.125493 	 0.118363
Train loss       : 16.365116
Best valid loss  : 0.122641
Best test loss   : 0.118145
Pruning          : 0.12
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7598]
[Starting training]
Epoch 0 	 22.916584 	 0.623346 	 0.615128
Epoch 10 	 21.626312 	 0.541013 	 0.531514
Epoch 20 	 21.106024 	 0.476337 	 0.459280
Epoch 30 	 19.126780 	 0.248366 	 0.249796
Epoch 40 	 17.995625 	 0.164985 	 0.161276
Epoch 50 	 17.590841 	 0.151443 	 0.142694
Epoch 60 	 17.317633 	 0.145661 	 0.133745
Epoch 70 	 17.158112 	 0.147051 	 0.135152
Epoch 80 	 17.036930 	 0.139422 	 0.128599
Epoch 90 	 16.944263 	 0.130900 	 0.126230
Epoch 100 	 16.766148 	 0.133887 	 0.123550
Epoch 110 	 16.689787 	 0.130921 	 0.125290
Epoch 120 	 16.642391 	 0.132339 	 0.124168
Epoch 130 	 16.605946 	 0.129994 	 0.123240
Epoch 140 	 16.544897 	 0.126001 	 0.121652
Epoch 150 	 16.498772 	 0.127241 	 0.122529
Epoch 160 	 16.457127 	 0.130027 	 0.121188
Epoch 170 	 16.435469 	 0.129075 	 0.121719
Epoch 180 	 16.407795 	 0.123766 	 0.120142
Epoch 190 	 16.412638 	 0.126896 	 0.119785
Train loss       : 16.417059
Best valid loss  : 0.123362
Best test loss   : 0.120905
Pruning          : 0.08
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7660]
[Starting training]
Epoch 0 	 22.996161 	 0.665462 	 0.651943
Epoch 10 	 21.581947 	 0.543083 	 0.534196
Epoch 20 	 20.753395 	 0.422587 	 0.418213
Epoch 30 	 18.788261 	 0.214116 	 0.211353
Epoch 40 	 17.832499 	 0.154704 	 0.146988
Epoch 50 	 17.449120 	 0.145377 	 0.137357
Epoch 60 	 17.266373 	 0.135660 	 0.129113
Epoch 70 	 17.124039 	 0.133178 	 0.124855
Epoch 80 	 17.000492 	 0.127713 	 0.119885
Epoch 90 	 16.826168 	 0.129712 	 0.121893
Epoch 100 	 16.788486 	 0.126148 	 0.120343
Epoch 110 	 16.696201 	 0.126774 	 0.118104
Epoch 120 	 16.667397 	 0.124213 	 0.119252
Epoch 130 	 16.589115 	 0.122946 	 0.118005
Epoch 140 	 16.570658 	 0.123636 	 0.117595
Epoch 150 	 16.548454 	 0.122603 	 0.117789
Epoch 160 	 16.552948 	 0.123097 	 0.119269
Epoch 170 	 16.552689 	 0.125984 	 0.118034
Epoch 180 	 16.554413 	 0.125815 	 0.118435
Epoch 190 	 16.540268 	 0.125883 	 0.117007
Train loss       : 16.522648
Best valid loss  : 0.120786
Best test loss   : 0.119121
Pruning          : 0.06
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7180]
[Starting training]
Epoch 0 	 23.153381 	 0.672538 	 0.669615
Epoch 10 	 21.881449 	 0.569020 	 0.565287
Epoch 20 	 21.646202 	 0.553360 	 0.548318
Epoch 30 	 20.561180 	 0.395209 	 0.395496
Epoch 40 	 19.272774 	 0.262752 	 0.260223
Epoch 50 	 18.505676 	 0.200305 	 0.198624
Epoch 60 	 18.062668 	 0.183260 	 0.173394
Epoch 70 	 17.868601 	 0.170914 	 0.168790
Epoch 80 	 17.714405 	 0.158585 	 0.155909
Epoch 90 	 17.505249 	 0.146009 	 0.138963
Epoch 100 	 17.385494 	 0.135433 	 0.133909
Epoch 110 	 17.270725 	 0.141553 	 0.131385
Epoch 120 	 17.162722 	 0.135553 	 0.129806
Epoch 130 	 17.108496 	 0.135300 	 0.129334
Epoch 140 	 17.059191 	 0.132458 	 0.127962
Epoch 150 	 17.021818 	 0.130146 	 0.121936
Epoch 160 	 16.894039 	 0.132454 	 0.121519
Epoch 170 	 16.819418 	 0.128174 	 0.119299
Epoch 180 	 16.772436 	 0.127022 	 0.117802
Epoch 190 	 16.770582 	 0.127754 	 0.117879
Train loss       : 16.733160
Best valid loss  : 0.124108
Best test loss   : 0.117563
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
[Untrained loss : 0.7747]
[Starting training]
Epoch 0 	 23.046896 	 0.645290 	 0.639742
Epoch 10 	 21.633785 	 0.540651 	 0.534633
Epoch 20 	 21.032461 	 0.463801 	 0.458110
Epoch 30 	 19.611708 	 0.286536 	 0.278866
Epoch 40 	 18.513199 	 0.203813 	 0.196378
slurmstepd: error: *** JOB 41288816 ON cdr351 CANCELLED AT 2020-04-29T16:49:02 DUE TO TIME LIMIT ***
