Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41289073.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, pillow-simd, future, torch, torchvision, tqdm, kiwisolver, python-dateutil, cycler, pyparsing, matplotlib, protobuf, tensorflow-estimator, absl-py, termcolor, gast, wrapt, h5py, keras-applications, astor, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, oauthlib, urllib3, idna, chardet, certifi, requests, requests-oauthlib, google-auth-oauthlib, grpcio, werkzeug, markdown, tensorboard, keras-preprocessing, google-pasta, opt-einsum, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41289073.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 05:00:22.805231: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 05:00:23.133289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is singing_classify_cnn_xavier_trimming_gradient_min_reinit_global_0.
*******
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9550]
[Starting training]
Epoch 0 	 0.776999 	 0.709559 	 0.696140
Epoch 10 	 0.294118 	 0.384651 	 0.228860
Epoch 20 	 0.136604 	 0.234375 	 0.089154
Epoch 30 	 0.089269 	 0.187040 	 0.057721
Epoch 40 	 0.070657 	 0.169118 	 0.044210
Epoch 50 	 0.050781 	 0.163603 	 0.037776
Epoch 60 	 0.039522 	 0.166360 	 0.036857
Epoch 70 	 0.033088 	 0.153952 	 0.032261
Epoch 80 	 0.028837 	 0.155790 	 0.031893
Epoch 90 	 0.014017 	 0.155790 	 0.031526
Epoch 100 	 0.012868 	 0.157169 	 0.031893
[Model stopped early]
Train loss       : 0.011834
Best valid loss  : 0.149357
Best test loss   : 0.030974
Pruning          : 1.00
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,210,710
--------------------------------
Total memory      : 8.56 MB
Total Flops       : 663.22 MFlops
Total Mem (Read)  : 11.54 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.819853 	 0.717831 	 0.695129
Epoch 10 	 0.368336 	 0.431066 	 0.277114
Epoch 20 	 0.186006 	 0.289982 	 0.137224
Epoch 30 	 0.121209 	 0.220129 	 0.081250
Epoch 40 	 0.082376 	 0.204044 	 0.061765
Epoch 50 	 0.066981 	 0.190717 	 0.053493
Epoch 60 	 0.059743 	 0.180607 	 0.042923
Epoch 70 	 0.047105 	 0.177390 	 0.039430
Epoch 80 	 0.026999 	 0.183364 	 0.040901
Epoch 90 	 0.018957 	 0.180607 	 0.037960
[Model stopped early]
Train loss       : 0.019301
Best valid loss  : 0.174632
Best test loss   : 0.039982
Pruning          : 0.75
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,156,677
--------------------------------
Total memory      : 8.56 MB
Total Flops       : 663.16 MFlops
Total Mem (Read)  : 11.34 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.889936 	 0.839154 	 0.817923
Epoch 10 	 0.354779 	 0.408548 	 0.273805
Epoch 20 	 0.186466 	 0.259651 	 0.125460
Epoch 30 	 0.112017 	 0.210478 	 0.077482
Epoch 40 	 0.082261 	 0.191636 	 0.057629
Epoch 50 	 0.060777 	 0.166820 	 0.044577
Epoch 60 	 0.045267 	 0.169118 	 0.041820
Epoch 70 	 0.040786 	 0.164982 	 0.036673
Epoch 80 	 0.024127 	 0.159007 	 0.032629
Epoch 90 	 0.023667 	 0.149816 	 0.030423
Epoch 100 	 0.012753 	 0.152114 	 0.031158
Epoch 110 	 0.014361 	 0.150276 	 0.030607
[Model stopped early]
Train loss       : 0.012178
Best valid loss  : 0.147518
Best test loss   : 0.030790
Pruning          : 0.56
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,111,827
--------------------------------
Total memory      : 8.56 MB
Total Flops       : 663.12 MFlops
Total Mem (Read)  : 11.16 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9862]
[Starting training]
Epoch 0 	 0.835823 	 0.741268 	 0.731618
Epoch 10 	 0.426815 	 0.485754 	 0.348805
Epoch 20 	 0.231847 	 0.325827 	 0.182904
Epoch 30 	 0.149357 	 0.243107 	 0.106158
Epoch 40 	 0.105928 	 0.215074 	 0.073529
Epoch 50 	 0.079848 	 0.186581 	 0.051930
Epoch 60 	 0.066981 	 0.183824 	 0.049816
Epoch 70 	 0.063764 	 0.176471 	 0.043382
Epoch 80 	 0.050896 	 0.163603 	 0.037684
Epoch 90 	 0.042624 	 0.161305 	 0.036213
Epoch 100 	 0.025965 	 0.166360 	 0.035478
Epoch 110 	 0.019072 	 0.159467 	 0.033088
[Model stopped early]
Train loss       : 0.018153
Best valid loss  : 0.154412
Best test loss   : 0.034191
Pruning          : 0.42
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 253,998
--------------------------------
Total memory      : 2.13 MB
Total Flops       : 135.04 MFlops
Total Mem (Read)  : 3.07 MB
Total Mem (Write) : 1.6 MB
[Supermasks testing]
[Untrained loss : 0.8765]
[Starting training]
Epoch 0 	 0.808249 	 0.726103 	 0.706342
Epoch 10 	 0.510915 	 0.535846 	 0.423713
Epoch 20 	 0.323415 	 0.383732 	 0.245221
Epoch 30 	 0.247358 	 0.291360 	 0.162132
Epoch 40 	 0.200023 	 0.263787 	 0.124265
Epoch 50 	 0.164292 	 0.222886 	 0.091360
Epoch 60 	 0.144531 	 0.219210 	 0.080423
Epoch 70 	 0.126608 	 0.192096 	 0.064798
Epoch 80 	 0.115234 	 0.184743 	 0.058180
Epoch 90 	 0.102482 	 0.181526 	 0.054871
Epoch 100 	 0.096048 	 0.169577 	 0.049724
Epoch 110 	 0.085018 	 0.165901 	 0.044393
Epoch 120 	 0.086742 	 0.165441 	 0.041085
Epoch 130 	 0.077091 	 0.162224 	 0.039982
Epoch 140 	 0.075597 	 0.167739 	 0.039062
Epoch 150 	 0.060777 	 0.160386 	 0.035937
Train loss       : 0.049517
Best valid loss  : 0.154412
Best test loss   : 0.037224
Pruning          : 0.32
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 242,706
--------------------------------
Total memory      : 2.13 MB
Total Flops       : 135.02 MFlops
Total Mem (Read)  : 3.03 MB
Total Mem (Write) : 1.6 MB
[Supermasks testing]
[Untrained loss : 0.8998]
[Starting training]
Epoch 0 	 0.897748 	 0.854779 	 0.855607
Epoch 10 	 0.561236 	 0.561581 	 0.450643
Epoch 20 	 0.399242 	 0.426011 	 0.296415
Epoch 30 	 0.304458 	 0.357996 	 0.217739
Epoch 40 	 0.256778 	 0.310662 	 0.164798
Epoch 50 	 0.211742 	 0.266544 	 0.124173
Epoch 60 	 0.187385 	 0.250000 	 0.110937
Epoch 70 	 0.166360 	 0.215074 	 0.086305
Epoch 80 	 0.146255 	 0.204504 	 0.072335
Epoch 90 	 0.135915 	 0.198529 	 0.064982
Epoch 100 	 0.124540 	 0.199908 	 0.064338
Epoch 110 	 0.107077 	 0.191636 	 0.056342
Epoch 120 	 0.106043 	 0.174173 	 0.046783
Epoch 130 	 0.091682 	 0.181066 	 0.050460
Epoch 140 	 0.082146 	 0.175092 	 0.044945
Epoch 150 	 0.077206 	 0.166360 	 0.041085
Train loss       : 0.074908
Best valid loss  : 0.166360
Best test loss   : 0.041085
Pruning          : 0.24
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 238,198
--------------------------------
Total memory      : 2.13 MB
Total Flops       : 135.02 MFlops
Total Mem (Read)  : 3.01 MB
Total Mem (Write) : 1.6 MB
[Supermasks testing]
[Untrained loss : 0.9550]
[Starting training]
Epoch 0 	 0.899357 	 0.882812 	 0.879779
Epoch 10 	 0.597771 	 0.622702 	 0.533823
Epoch 20 	 0.421875 	 0.465993 	 0.305423
Epoch 30 	 0.327895 	 0.398897 	 0.246232
Epoch 40 	 0.272748 	 0.324908 	 0.176195
Epoch 50 	 0.231503 	 0.285386 	 0.150460
Epoch 60 	 0.209903 	 0.255515 	 0.117555
Epoch 70 	 0.188189 	 0.235754 	 0.106158
Epoch 80 	 0.172105 	 0.218750 	 0.090717
Epoch 90 	 0.156020 	 0.216912 	 0.086121
Epoch 100 	 0.142693 	 0.203585 	 0.075460
Epoch 110 	 0.137523 	 0.187500 	 0.062868
Epoch 120 	 0.125000 	 0.187500 	 0.060294
Epoch 130 	 0.113626 	 0.182445 	 0.054412
Epoch 140 	 0.109260 	 0.173713 	 0.049081
Epoch 150 	 0.093176 	 0.170496 	 0.047518
Train loss       : 0.082606
Best valid loss  : 0.163603
Best test loss   : 0.043658
Pruning          : 0.18
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 235,518
--------------------------------
Total memory      : 2.13 MB
Total Flops       : 135.02 MFlops
Total Mem (Read)  : 3.0 MB
Total Mem (Write) : 1.6 MB
[Supermasks testing]
[Untrained loss : 0.9125]
[Starting training]
Epoch 0 	 0.911075 	 0.889246 	 0.893015
Epoch 10 	 0.670037 	 0.648897 	 0.562592
Epoch 20 	 0.473690 	 0.508272 	 0.373621
Epoch 30 	 0.393153 	 0.439798 	 0.315441
Epoch 40 	 0.344210 	 0.389706 	 0.256618
Epoch 50 	 0.313304 	 0.369485 	 0.246691
Epoch 60 	 0.292279 	 0.340533 	 0.210662
Epoch 70 	 0.259421 	 0.337316 	 0.210018
Epoch 80 	 0.249885 	 0.301930 	 0.169118
Epoch 90 	 0.225643 	 0.287224 	 0.153860
Epoch 100 	 0.215993 	 0.292739 	 0.155882
Epoch 110 	 0.201517 	 0.259651 	 0.120956
Epoch 120 	 0.180836 	 0.255974 	 0.124081
Epoch 130 	 0.178194 	 0.248162 	 0.112316
Epoch 140 	 0.176930 	 0.242188 	 0.108732
Epoch 150 	 0.159926 	 0.232077 	 0.095772
Train loss       : 0.152459
Best valid loss  : 0.215533
Best test loss   : 0.086213
Pruning          : 0.13
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 151,383
--------------------------------
Total memory      : 1.85 MB
Total Flops       : 93.75 MFlops
Total Mem (Read)  : 2.47 MB
Total Mem (Write) : 1.39 MB
[Supermasks testing]
[Untrained loss : 0.7537]
[Starting training]
Epoch 0 	 0.797335 	 0.754596 	 0.752941
Epoch 10 	 0.651540 	 0.615809 	 0.560938
Epoch 20 	 0.537914 	 0.522518 	 0.426103
Epoch 30 	 0.445083 	 0.466452 	 0.327114
Epoch 40 	 0.392119 	 0.404871 	 0.258915
Epoch 50 	 0.337201 	 0.356158 	 0.211489
Epoch 60 	 0.302390 	 0.329504 	 0.180974
Epoch 70 	 0.283663 	 0.319393 	 0.177849
Epoch 80 	 0.261029 	 0.282629 	 0.147518
Epoch 90 	 0.241843 	 0.261949 	 0.130239
Epoch 100 	 0.236443 	 0.257812 	 0.117279
Epoch 110 	 0.215993 	 0.251379 	 0.113419
Epoch 120 	 0.210018 	 0.227022 	 0.098254
Epoch 130 	 0.199334 	 0.235754 	 0.104871
Epoch 140 	 0.179113 	 0.217831 	 0.089706
Epoch 150 	 0.172449 	 0.207721 	 0.082077
Train loss       : 0.160846
Best valid loss  : 0.206342
Best test loss   : 0.079687
Pruning          : 0.10
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 140,416
--------------------------------
Total memory      : 1.83 MB
Total Flops       : 89.12 MFlops
Total Mem (Read)  : 2.41 MB
Total Mem (Write) : 1.37 MB
[Supermasks testing]
[Untrained loss : 0.9869]
[Starting training]
Epoch 0 	 0.977022 	 0.985294 	 0.985662
Epoch 10 	 0.834789 	 0.872243 	 0.820037
Epoch 20 	 0.645680 	 0.602941 	 0.507629
Epoch 30 	 0.543888 	 0.532629 	 0.406250
Epoch 40 	 0.480584 	 0.468290 	 0.339063
Epoch 50 	 0.427160 	 0.426471 	 0.289798
Epoch 60 	 0.394531 	 0.391544 	 0.256526
Epoch 70 	 0.368451 	 0.366268 	 0.234099
Epoch 80 	 0.339729 	 0.344669 	 0.211857
Epoch 90 	 0.330078 	 0.331342 	 0.202482
Epoch 100 	 0.309858 	 0.313419 	 0.188511
Epoch 110 	 0.301471 	 0.307904 	 0.171599
Epoch 120 	 0.302734 	 0.307904 	 0.181158
Epoch 130 	 0.285156 	 0.296415 	 0.168842
Epoch 140 	 0.277918 	 0.297335 	 0.165717
Epoch 150 	 0.276999 	 0.285846 	 0.167555
Train loss       : 0.261029
Best valid loss  : 0.273897
Best test loss   : 0.152114
Pruning          : 0.08
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 70,465
--------------------------------
Total memory      : 1.66 MB
Total Flops       : 56.66 MFlops
Total Mem (Read)  : 2.02 MB
Total Mem (Write) : 1.25 MB
[Supermasks testing]
[Untrained loss : 0.9121]
[Starting training]
Epoch 0 	 0.913373 	 0.905331 	 0.909007
Epoch 10 	 0.816406 	 0.801930 	 0.773070
Epoch 20 	 0.708525 	 0.665901 	 0.606985
Epoch 30 	 0.644646 	 0.633732 	 0.552757
Epoch 40 	 0.595244 	 0.590533 	 0.504963
Epoch 50 	 0.552964 	 0.560202 	 0.460938
Epoch 60 	 0.517119 	 0.512408 	 0.382812
Epoch 70 	 0.482537 	 0.490809 	 0.363879
Epoch 80 	 0.463695 	 0.475643 	 0.339246
Epoch 90 	 0.436351 	 0.459559 	 0.325460
Epoch 100 	 0.428653 	 0.436121 	 0.298437
Epoch 110 	 0.407514 	 0.428768 	 0.288327
Epoch 120 	 0.394876 	 0.417279 	 0.279136
Epoch 130 	 0.394187 	 0.400276 	 0.255974
Epoch 140 	 0.388902 	 0.398438 	 0.258548
Epoch 150 	 0.370519 	 0.398897 	 0.261673
Train loss       : 0.361213
Best valid loss  : 0.384191
Best test loss   : 0.243658
Pruning          : 0.06
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 48,949
--------------------------------
Total memory      : 1.16 MB
Total Flops       : 37.82 MFlops
Total Mem (Read)  : 1.56 MB
Total Mem (Write) : 891.69 KB
[Supermasks testing]
[Untrained loss : 0.9550]
[Starting training]
Epoch 0 	 0.950712 	 0.952206 	 0.952390
Epoch 10 	 0.850758 	 0.879136 	 0.870221
Epoch 20 	 0.716222 	 0.665901 	 0.607353
Epoch 30 	 0.647863 	 0.610294 	 0.525551
Epoch 40 	 0.600758 	 0.585478 	 0.480239
Epoch 50 	 0.556296 	 0.539522 	 0.424081
Epoch 60 	 0.524816 	 0.504596 	 0.386213
Epoch 70 	 0.504825 	 0.479779 	 0.350735
Epoch 80 	 0.468290 	 0.471967 	 0.344026
Epoch 90 	 0.459789 	 0.446691 	 0.318566
Epoch 100 	 0.449449 	 0.427390 	 0.300368
Epoch 110 	 0.440717 	 0.431985 	 0.308088
Epoch 120 	 0.424058 	 0.419577 	 0.290625
Epoch 130 	 0.409812 	 0.398897 	 0.278676
Epoch 140 	 0.403837 	 0.414062 	 0.290257
Epoch 150 	 0.392808 	 0.387408 	 0.270221
Train loss       : 0.385915
Best valid loss  : 0.378676
Best test loss   : 0.265257
Pruning          : 0.04
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 48,490
--------------------------------
Total memory      : 1.16 MB
Total Flops       : 37.82 MFlops
Total Mem (Read)  : 1.56 MB
Total Mem (Write) : 891.52 KB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.857537 	 0.838235 	 0.842463
Epoch 10 	 0.813879 	 0.797335 	 0.789614
Epoch 20 	 0.753217 	 0.723346 	 0.686305
Epoch 30 	 0.669347 	 0.642463 	 0.582261
Epoch 40 	 0.657973 	 0.612132 	 0.554504
Epoch 50 	 0.628562 	 0.597426 	 0.535662
Epoch 60 	 0.609835 	 0.574908 	 0.517831
Epoch 70 	 0.600184 	 0.569853 	 0.508732
Epoch 80 	 0.589040 	 0.556985 	 0.492831
Epoch 90 	 0.579389 	 0.545956 	 0.482629
Epoch 100 	 0.561351 	 0.547335 	 0.480423
Epoch 110 	 0.555492 	 0.536765 	 0.468658
Epoch 120 	 0.558479 	 0.526195 	 0.460018
Epoch 130 	 0.552619 	 0.518382 	 0.451930
Epoch 140 	 0.545496 	 0.521140 	 0.454871
Epoch 150 	 0.537454 	 0.519761 	 0.457996
Train loss       : 0.541245
Best valid loss  : 0.512408
Best test loss   : 0.446232
Pruning          : 0.03
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 46,115
--------------------------------
Total memory      : 1.15 MB
Total Flops       : 36.78 MFlops
Total Mem (Read)  : 1.55 MB
Total Mem (Write) : 885.6 KB
[Supermasks testing]
[Untrained loss : 0.8765]
[Starting training]
Epoch 0 	 0.868451 	 0.883732 	 0.876471
Epoch 10 	 0.821232 	 0.861213 	 0.838787
Epoch 20 	 0.697840 	 0.663143 	 0.607629
Epoch 30 	 0.636719 	 0.625460 	 0.576195
Epoch 40 	 0.614890 	 0.602482 	 0.550827
Epoch 50 	 0.603056 	 0.588235 	 0.537868
Epoch 60 	 0.584559 	 0.575827 	 0.521324
Epoch 70 	 0.572266 	 0.562500 	 0.511305
Epoch 80 	 0.559398 	 0.548713 	 0.491268
Epoch 90 	 0.552160 	 0.548713 	 0.474540
Epoch 100 	 0.540671 	 0.528493 	 0.445496
Epoch 110 	 0.536190 	 0.517463 	 0.437868
Epoch 120 	 0.523897 	 0.514706 	 0.430055
Epoch 130 	 0.512293 	 0.520680 	 0.433640
Epoch 140 	 0.508387 	 0.507812 	 0.419853
Epoch 150 	 0.508157 	 0.494026 	 0.400184
Train loss       : 0.500689
Best valid loss  : 0.492647
Best test loss   : 0.405607
Pruning          : 0.02
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 23,045
--------------------------------
Total memory      : 1.08 MB
Total Flops       : 26.46 MFlops
Total Mem (Read)  : 1.4 MB
Total Mem (Write) : 826.89 KB
[Supermasks testing]
[Untrained loss : 0.7969]
[Starting training]
Epoch 0 	 0.944278 	 0.933824 	 0.942096
Epoch 10 	 0.916705 	 0.920956 	 0.926471
Epoch 20 	 0.796415 	 0.881893 	 0.868474
Epoch 30 	 0.730469 	 0.680147 	 0.629412
Epoch 40 	 0.699449 	 0.644301 	 0.600919
Epoch 50 	 0.667165 	 0.632812 	 0.590349
Epoch 60 	 0.663603 	 0.617647 	 0.581618
Epoch 70 	 0.653033 	 0.617188 	 0.577849
Epoch 80 	 0.633961 	 0.610754 	 0.570221
Epoch 90 	 0.626953 	 0.600184 	 0.556618
Epoch 100 	 0.614660 	 0.612592 	 0.574449
Epoch 110 	 0.610754 	 0.605239 	 0.563695
Epoch 120 	 0.602252 	 0.597426 	 0.562960
Epoch 130 	 0.605239 	 0.594669 	 0.559559
Epoch 140 	 0.598001 	 0.585938 	 0.546232
Epoch 150 	 0.599954 	 0.590533 	 0.554136
Train loss       : 0.597771
Best valid loss  : 0.585938
Best test loss   : 0.546232
Pruning          : 0.02
