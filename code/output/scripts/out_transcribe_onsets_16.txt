Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288806.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, python-dateutil, pyparsing, kiwisolver, cycler, matplotlib, opt-einsum, keras-preprocessing, astor, termcolor, protobuf, absl-py, wrapt, google-pasta, h5py, keras-applications, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, oauthlib, chardet, certifi, idna, urllib3, requests, requests-oauthlib, google-auth-oauthlib, markdown, werkzeug, grpcio, tensorboard, gast, tensorflow-estimator, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288806.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:52:37.109886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:52:37.381573: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_trimming_information_rewind_local_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288806.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7750]
[Starting training]
/localscratch/esling.41288806.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
Epoch 0 	 22.963425 	 0.645876 	 0.626871
Epoch 10 	 21.512081 	 0.521315 	 0.515976
Epoch 20 	 20.565695 	 0.391899 	 0.386758
Epoch 30 	 19.075296 	 0.276753 	 0.270532
Epoch 40 	 18.087349 	 0.207932 	 0.202228
Epoch 50 	 17.451624 	 0.167702 	 0.167115
/localscratch/esling.41288806.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 60 	 17.073000 	 0.146879 	 0.153619
Epoch 70 	 16.833786 	 0.147155 	 0.149986
Epoch 80 	 16.635305 	 0.134400 	 0.144965
Epoch 90 	 16.499056 	 0.135057 	 0.143056
Epoch 100 	 16.282841 	 0.133936 	 0.137031
Epoch 110 	 16.220455 	 0.131340 	 0.133805
Epoch 120 	 16.195457 	 0.126552 	 0.136125
Epoch 130 	 16.150339 	 0.128160 	 0.133982
Epoch 140 	 16.099152 	 0.131329 	 0.132778
Epoch 150 	 16.069969 	 0.126734 	 0.133317
Epoch 160 	 16.049196 	 0.127456 	 0.132947
Epoch 170 	 16.040323 	 0.128171 	 0.131136
Epoch 180 	 16.034651 	 0.126784 	 0.130509
Epoch 190 	 16.023817 	 0.128911 	 0.131737
Train loss       : 16.022831
Best valid loss  : 0.123920
Best test loss   : 0.130690
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,031,141
--------------------------------
Total memory      : 15.85 MB
Total Flops       : 1.5 GFlops
Total Mem (Read)  : 23.9 MB
Total Mem (Write) : 12.33 MB
[Supermasks testing]
[Untrained loss : 0.2711]
[Starting training]
Epoch 0 	 17.361172 	 0.156466 	 0.159125
Epoch 10 	 16.914238 	 0.142298 	 0.147624
Epoch 20 	 16.688334 	 0.133299 	 0.142557
Epoch 30 	 16.555006 	 0.135157 	 0.142351
Epoch 40 	 16.336452 	 0.127481 	 0.135201
Epoch 50 	 16.257084 	 0.127884 	 0.135772
Epoch 60 	 16.178265 	 0.130952 	 0.134540
Epoch 70 	 16.142122 	 0.129288 	 0.134248
Epoch 80 	 16.125095 	 0.123901 	 0.132060
Epoch 90 	 16.104124 	 0.126598 	 0.133034
Epoch 100 	 16.091640 	 0.127925 	 0.133033
Epoch 110 	 16.077606 	 0.129754 	 0.133906
Epoch 120 	 16.076506 	 0.125608 	 0.132408
Epoch 130 	 16.067989 	 0.124766 	 0.131831
Epoch 140 	 16.075567 	 0.128031 	 0.131336
[Model stopped early]
Train loss       : 16.064028
Best valid loss  : 0.122666
Best test loss   : 0.132227
Pruning          : 0.75
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,015,845
--------------------------------
Total memory      : 11.89 MB
Total Flops       : 848.79 MFlops
Total Mem (Read)  : 16.95 MB
Total Mem (Write) : 9.25 MB
[Supermasks testing]
[Untrained loss : 0.3471]
[Starting training]
Epoch 0 	 17.742508 	 0.160347 	 0.163606
Epoch 10 	 17.059261 	 0.139621 	 0.147517
Epoch 20 	 16.821583 	 0.136323 	 0.141147
Epoch 30 	 16.645679 	 0.134702 	 0.140464
Epoch 40 	 16.529461 	 0.133041 	 0.142635
Epoch 50 	 16.347174 	 0.128621 	 0.133883
Epoch 60 	 16.267735 	 0.130848 	 0.136420
Epoch 70 	 16.209030 	 0.126896 	 0.133871
Epoch 80 	 16.151773 	 0.126195 	 0.133517
[Model stopped early]
Train loss       : 16.137442
Best valid loss  : 0.122162
Best test loss   : 0.135993
Pruning          : 0.56
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,378,357
--------------------------------
Total memory      : 8.92 MB
Total Flops       : 481.03 MFlops
Total Mem (Read)  : 12.21 MB
Total Mem (Write) : 6.94 MB
[Supermasks testing]
[Untrained loss : 0.4824]
[Starting training]
Epoch 0 	 18.270039 	 0.168537 	 0.169459
Epoch 10 	 17.237955 	 0.146202 	 0.149174
Epoch 20 	 16.963779 	 0.134612 	 0.142650
Epoch 30 	 16.811644 	 0.135401 	 0.139727
Epoch 40 	 16.691483 	 0.133434 	 0.141314
Epoch 50 	 16.544174 	 0.134875 	 0.137598
Epoch 60 	 16.405764 	 0.129224 	 0.134281
Epoch 70 	 16.326206 	 0.129856 	 0.135415
Epoch 80 	 16.288118 	 0.124273 	 0.133207
Epoch 90 	 16.230042 	 0.127761 	 0.134183
Epoch 100 	 16.222843 	 0.128058 	 0.132506
Epoch 110 	 16.201393 	 0.126894 	 0.132686
[Model stopped early]
Train loss       : 16.205530
Best valid loss  : 0.122729
Best test loss   : 0.134505
Pruning          : 0.42
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 969,982
--------------------------------
Total memory      : 6.69 MB
Total Flops       : 273.29 MFlops
Total Mem (Read)  : 8.92 MB
Total Mem (Write) : 5.21 MB
[Supermasks testing]
[Untrained loss : 0.5636]
[Starting training]
Epoch 0 	 18.923590 	 0.189002 	 0.188229
Epoch 10 	 17.527699 	 0.150835 	 0.151420
Epoch 20 	 17.275349 	 0.141524 	 0.147852
Epoch 30 	 17.079891 	 0.135885 	 0.143433
Epoch 40 	 16.924862 	 0.135926 	 0.139222
Epoch 50 	 16.789669 	 0.134631 	 0.138222
Epoch 60 	 16.647091 	 0.135344 	 0.138203
Epoch 70 	 16.543955 	 0.132009 	 0.135154
Epoch 80 	 16.505674 	 0.128078 	 0.138012
Epoch 90 	 16.461493 	 0.126340 	 0.136647
Epoch 100 	 16.445625 	 0.130721 	 0.136911
Epoch 110 	 16.407980 	 0.130004 	 0.135283
Epoch 120 	 16.384932 	 0.127247 	 0.135732
Epoch 130 	 16.405119 	 0.130166 	 0.134748
Epoch 140 	 16.405619 	 0.127118 	 0.135527
[Model stopped early]
Train loss       : 16.403120
Best valid loss  : 0.125519
Best test loss   : 0.135831
Pruning          : 0.32
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 700,624
--------------------------------
Total memory      : 4.96 MB
Total Flops       : 152.05 MFlops
Total Mem (Read)  : 6.54 MB
Total Mem (Write) : 3.86 MB
[Supermasks testing]
[Untrained loss : 0.6032]
[Starting training]
Epoch 0 	 19.493921 	 0.206362 	 0.215602
Epoch 10 	 17.883265 	 0.159377 	 0.161704
Epoch 20 	 17.600204 	 0.143003 	 0.150358
Epoch 30 	 17.391220 	 0.141201 	 0.144689
Epoch 40 	 17.244570 	 0.137419 	 0.145286
Epoch 50 	 17.133064 	 0.139529 	 0.145840
Epoch 60 	 16.970957 	 0.136232 	 0.141771
Epoch 70 	 16.832962 	 0.134608 	 0.138010
Epoch 80 	 16.795156 	 0.134820 	 0.138007
Epoch 90 	 16.744009 	 0.134535 	 0.139252
Epoch 100 	 16.714916 	 0.132533 	 0.139051
Epoch 110 	 16.698763 	 0.132749 	 0.138291
Epoch 120 	 16.682575 	 0.135347 	 0.138336
[Model stopped early]
Train loss       : 16.670719
Best valid loss  : 0.129704
Best test loss   : 0.139173
Pruning          : 0.24
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 522,854
--------------------------------
Total memory      : 3.72 MB
Total Flops       : 87.05 MFlops
Total Mem (Read)  : 4.9 MB
Total Mem (Write) : 2.9 MB
[Supermasks testing]
[Untrained loss : 0.5878]
[Starting training]
Epoch 0 	 20.242342 	 0.258674 	 0.260114
Epoch 10 	 18.327993 	 0.166435 	 0.171541
Epoch 20 	 17.999054 	 0.149292 	 0.159827
Epoch 30 	 17.822142 	 0.145725 	 0.154057
Epoch 40 	 17.682482 	 0.148591 	 0.149949
Epoch 50 	 17.576197 	 0.145052 	 0.149553
Epoch 60 	 17.458841 	 0.140956 	 0.148877
Epoch 70 	 17.359158 	 0.141255 	 0.148867
Epoch 80 	 17.248480 	 0.137901 	 0.144567
Epoch 90 	 17.183830 	 0.139589 	 0.144229
Epoch 100 	 17.097250 	 0.134809 	 0.142906
Epoch 110 	 17.024990 	 0.138993 	 0.141864
Epoch 120 	 17.037153 	 0.140275 	 0.142345
Epoch 130 	 16.952179 	 0.139877 	 0.141125
Epoch 140 	 16.991341 	 0.140447 	 0.141246
Epoch 150 	 16.969767 	 0.137852 	 0.140633
Epoch 160 	 16.936262 	 0.138908 	 0.140772
[Model stopped early]
Train loss       : 16.936262
Best valid loss  : 0.133899
Best test loss   : 0.140968
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 399,974
--------------------------------
Total memory      : 2.73 MB
Total Flops       : 48.02 MFlops
Total Mem (Read)  : 3.66 MB
Total Mem (Write) : 2.13 MB
[Supermasks testing]
[Untrained loss : 0.6983]
[Starting training]
Epoch 0 	 21.131586 	 0.328629 	 0.318577
Epoch 10 	 18.784414 	 0.176767 	 0.182007
Epoch 20 	 18.534632 	 0.171207 	 0.173626
Epoch 30 	 18.312876 	 0.163697 	 0.167592
Epoch 40 	 18.186514 	 0.152217 	 0.163162
Epoch 50 	 17.985023 	 0.150971 	 0.156975
Epoch 60 	 17.867783 	 0.145854 	 0.153872
Epoch 70 	 17.808651 	 0.146682 	 0.154321
Epoch 80 	 17.691284 	 0.148039 	 0.151885
Epoch 90 	 17.697149 	 0.145471 	 0.151654
[Model stopped early]
Train loss       : 17.705280
Best valid loss  : 0.144280
Best test loss   : 0.154740
Pruning          : 0.13
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 316,333
--------------------------------
Total memory      : 1.99 MB
Total Flops       : 26.32 MFlops
Total Mem (Read)  : 2.77 MB
Total Mem (Write) : 1.55 MB
[Supermasks testing]
[Untrained loss : 0.6631]
[Starting training]
Epoch 0 	 21.748085 	 0.420426 	 0.411632
Epoch 10 	 19.383419 	 0.215940 	 0.215606
Epoch 20 	 19.099632 	 0.210669 	 0.209246
Epoch 30 	 18.876169 	 0.188934 	 0.192217
Epoch 40 	 18.713713 	 0.188694 	 0.184237
Epoch 50 	 18.669928 	 0.184991 	 0.185358
Epoch 60 	 18.549583 	 0.179669 	 0.180507
Epoch 70 	 18.423019 	 0.169427 	 0.176812
Epoch 80 	 18.308445 	 0.171714 	 0.173805
Epoch 90 	 18.224209 	 0.168655 	 0.173025
Epoch 100 	 18.177870 	 0.166786 	 0.173428
Epoch 110 	 18.141077 	 0.173843 	 0.174103
Epoch 120 	 18.162872 	 0.165431 	 0.170123
Epoch 130 	 18.131996 	 0.170713 	 0.170083
Epoch 140 	 18.074690 	 0.168166 	 0.171085
Epoch 150 	 18.112854 	 0.166703 	 0.171134
[Model stopped early]
Train loss       : 18.077343
Best valid loss  : 0.165431
Best test loss   : 0.170123
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 257,489
--------------------------------
Total memory      : 1.49 MB
Total Flops       : 15.44 MFlops
Total Mem (Read)  : 2.16 MB
Total Mem (Write) : 1.16 MB
[Supermasks testing]
[Untrained loss : 0.6528]
[Starting training]
Epoch 0 	 22.018557 	 0.497327 	 0.488879
Epoch 10 	 19.960594 	 0.260296 	 0.263509
Epoch 20 	 19.595804 	 0.240755 	 0.239055
Epoch 30 	 19.453075 	 0.223652 	 0.225880
Epoch 40 	 19.325119 	 0.228075 	 0.224941
Epoch 50 	 19.249250 	 0.216478 	 0.210966
Epoch 60 	 19.095690 	 0.210424 	 0.209706
Epoch 70 	 19.040094 	 0.207284 	 0.209930
Epoch 80 	 18.939648 	 0.205193 	 0.202103
Epoch 90 	 18.918907 	 0.204669 	 0.202183
Epoch 100 	 18.891338 	 0.202443 	 0.206267
Epoch 110 	 18.809055 	 0.199393 	 0.202652
Epoch 120 	 18.749802 	 0.196069 	 0.203483
Epoch 130 	 18.779299 	 0.195868 	 0.199212
Epoch 140 	 18.767689 	 0.196245 	 0.199835
Epoch 150 	 18.701321 	 0.196145 	 0.200190
[Model stopped early]
Train loss       : 18.733816
Best valid loss  : 0.192441
Best test loss   : 0.200075
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 216,037
--------------------------------
Total memory      : 1.08 MB
Total Flops       : 8.58 MFlops
Total Mem (Read)  : 1.68 MB
Total Mem (Write) : 861.98 KB
[Supermasks testing]
[Untrained loss : 0.6423]
[Starting training]
Epoch 0 	 22.310415 	 0.582653 	 0.563100
Epoch 10 	 20.556576 	 0.325981 	 0.317343
Epoch 20 	 20.190222 	 0.295688 	 0.290760
Epoch 30 	 20.081808 	 0.285541 	 0.282270
Epoch 40 	 19.902964 	 0.279579 	 0.269064
Epoch 50 	 19.829168 	 0.259440 	 0.259618
Epoch 60 	 19.777918 	 0.250655 	 0.251564
Epoch 70 	 19.673599 	 0.243570 	 0.246546
Epoch 80 	 19.598438 	 0.249169 	 0.247582
Epoch 90 	 19.495132 	 0.242305 	 0.243885
Epoch 100 	 19.410854 	 0.243201 	 0.240899
Epoch 110 	 19.379555 	 0.239003 	 0.242114
Epoch 120 	 19.306660 	 0.243316 	 0.239522
[Model stopped early]
Train loss       : 19.386374
Best valid loss  : 0.234748
Best test loss   : 0.238947
Pruning          : 0.06
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 184,678
--------------------------------
Total memory      : 0.75 MB
Total Flops       : 4.54 MFlops
Total Mem (Read)  : 1.3 MB
Total Mem (Write) : 599.12 KB
[Supermasks testing]
[Untrained loss : 0.6592]
[Starting training]
Epoch 0 	 22.489353 	 0.621675 	 0.602657
Epoch 10 	 21.310940 	 0.455162 	 0.441125
Epoch 20 	 21.071438 	 0.430217 	 0.414406
Epoch 30 	 20.939798 	 0.399353 	 0.387777
Epoch 40 	 20.798422 	 0.385260 	 0.379235
Epoch 50 	 20.731476 	 0.382258 	 0.371597
Epoch 60 	 20.611429 	 0.364942 	 0.357392
Epoch 70 	 20.527761 	 0.358416 	 0.352392
Epoch 80 	 20.447699 	 0.355444 	 0.350041
Epoch 90 	 20.382120 	 0.343548 	 0.338411
Epoch 100 	 20.308779 	 0.339036 	 0.332350
Epoch 110 	 20.232204 	 0.331366 	 0.323411
Epoch 120 	 20.177450 	 0.328291 	 0.322316
Epoch 130 	 20.160666 	 0.326357 	 0.317360
Epoch 140 	 20.136183 	 0.325821 	 0.315731
Epoch 150 	 20.098963 	 0.320580 	 0.313325
Epoch 160 	 20.021061 	 0.321088 	 0.314516
Epoch 170 	 20.074106 	 0.320353 	 0.310701
Epoch 180 	 20.039114 	 0.319152 	 0.307670
Epoch 190 	 20.045889 	 0.318010 	 0.306344
Train loss       : 20.053438
Best valid loss  : 0.311991
Best test loss   : 0.305413
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 162,452
--------------------------------
Total memory      : 0.50 MB
Total Flops       : 2.34 MFlops
Total Mem (Read)  : 1.02 MB
Total Mem (Write) : 401.99 KB
[Supermasks testing]
[Untrained loss : 0.6800]
[Starting training]
Epoch 0 	 22.644903 	 0.639636 	 0.619098
Epoch 10 	 21.718832 	 0.522031 	 0.507626
Epoch 20 	 21.482464 	 0.475640 	 0.451678
Epoch 30 	 21.364084 	 0.466204 	 0.447876
Epoch 40 	 21.207270 	 0.450735 	 0.431745
Epoch 50 	 21.164444 	 0.449845 	 0.429357
Epoch 60 	 21.107874 	 0.429446 	 0.411976
Epoch 70 	 21.065104 	 0.426291 	 0.408289
Epoch 80 	 21.014055 	 0.409635 	 0.400130
Epoch 90 	 21.000515 	 0.413078 	 0.401460
Epoch 100 	 20.944553 	 0.427548 	 0.413093
Epoch 110 	 20.886238 	 0.400390 	 0.385427
Epoch 120 	 20.861160 	 0.398169 	 0.385929
Epoch 130 	 20.838984 	 0.397769 	 0.383724
Epoch 140 	 20.767538 	 0.403360 	 0.384214
Epoch 150 	 20.830997 	 0.393571 	 0.385169
Epoch 160 	 20.755667 	 0.399529 	 0.385624
Epoch 170 	 20.790157 	 0.397439 	 0.382915
[Model stopped early]
Train loss       : 20.775137
Best valid loss  : 0.393215
Best test loss   : 0.382970
Pruning          : 0.03
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 146,768
--------------------------------
Total memory      : 0.34 MB
Total Flops       : 1.28 MFlops
Total Mem (Read)  : 855.98 KB
Total Mem (Write) : 270.55 KB
[Supermasks testing]
[Untrained loss : 0.7265]
[Starting training]
Epoch 0 	 22.832932 	 0.686451 	 0.668126
Epoch 10 	 21.914238 	 0.541809 	 0.528585
Epoch 20 	 21.719673 	 0.534863 	 0.513620
Epoch 30 	 21.625633 	 0.498698 	 0.478131
Epoch 40 	 21.545048 	 0.483850 	 0.473664
Epoch 50 	 21.492247 	 0.487641 	 0.471478
Epoch 60 	 21.431936 	 0.483586 	 0.469421
Epoch 70 	 21.414101 	 0.481355 	 0.459299
Epoch 80 	 21.353230 	 0.475340 	 0.460429
Epoch 90 	 21.354883 	 0.470563 	 0.454579
Epoch 100 	 21.330114 	 0.462438 	 0.448548
Epoch 110 	 21.319216 	 0.453693 	 0.439247
Epoch 120 	 21.268148 	 0.452055 	 0.438263
Epoch 130 	 21.220192 	 0.450010 	 0.437807
Epoch 140 	 21.199656 	 0.450680 	 0.436884
Epoch 150 	 21.182617 	 0.453483 	 0.435665
Epoch 160 	 21.169474 	 0.447850 	 0.434264
Epoch 170 	 21.178238 	 0.448871 	 0.433593
Epoch 180 	 21.196810 	 0.447144 	 0.431806
[Model stopped early]
Train loss       : 21.196810
Best valid loss  : 0.443187
Best test loss   : 0.433391
Pruning          : 0.02
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 135,317
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 866.6 KFlops
Total Mem (Read)  : 745.48 KB
Total Mem (Write) : 204.79 KB
[Supermasks testing]
[Untrained loss : 0.7430]
[Starting training]
Epoch 0 	 22.862503 	 0.703754 	 0.677940
Epoch 10 	 22.161354 	 0.580295 	 0.560225
Epoch 20 	 21.914040 	 0.539951 	 0.523176
Epoch 30 	 21.821651 	 0.533854 	 0.522530
Epoch 40 	 21.811344 	 0.533261 	 0.516546
Epoch 50 	 21.819839 	 0.525826 	 0.514156
Epoch 60 	 21.744797 	 0.524975 	 0.510044
Epoch 70 	 21.703768 	 0.521672 	 0.508569
Epoch 80 	 21.699209 	 0.519518 	 0.509084
Epoch 90 	 21.680826 	 0.516287 	 0.502178
Epoch 100 	 21.695564 	 0.513422 	 0.502187
Epoch 110 	 21.661201 	 0.515821 	 0.501341
Epoch 120 	 21.629196 	 0.510259 	 0.501626
Epoch 130 	 21.612823 	 0.507415 	 0.498790
Epoch 140 	 21.624710 	 0.511635 	 0.497535
Epoch 150 	 21.607819 	 0.510745 	 0.498254
Epoch 160 	 21.590536 	 0.511773 	 0.497741
Epoch 170 	 21.597548 	 0.516101 	 0.498609
[Model stopped early]
Train loss       : 21.572090
Best valid loss  : 0.501838
Best test loss   : 0.494406
Pruning          : 0.02
