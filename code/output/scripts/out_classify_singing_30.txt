Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41289135.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, pyparsing, python-dateutil, cycler, kiwisolver, matplotlib, grpcio, h5py, keras-applications, tensorflow-estimator, keras-preprocessing, wrapt, gast, absl-py, urllib3, idna, certifi, chardet, requests, protobuf, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, werkzeug, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, termcolor, opt-einsum, google-pasta, astor, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41289135.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 05:01:04.142379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 05:01:04.490724: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is singing_classify_cnn_xavier_masking_magnitude_rewind_global_0.
*******
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.828699 	 0.750919 	 0.727574
Epoch 10 	 0.287109 	 0.393842 	 0.233640
Epoch 20 	 0.146599 	 0.248621 	 0.101287
Epoch 30 	 0.081112 	 0.203585 	 0.061949
Epoch 40 	 0.059972 	 0.184743 	 0.045772
Epoch 50 	 0.057215 	 0.177849 	 0.044853
Epoch 60 	 0.045956 	 0.176930 	 0.039338
Epoch 70 	 0.022978 	 0.171415 	 0.034743
Epoch 80 	 0.021140 	 0.170956 	 0.034743
Epoch 90 	 0.023208 	 0.174173 	 0.035294
Epoch 100 	 0.011489 	 0.176930 	 0.035662
Epoch 110 	 0.008502 	 0.178309 	 0.035846
[Model stopped early]
Train loss       : 0.008502
Best valid loss  : 0.169577
Best test loss   : 0.034375
Pruning          : 1.00
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.0485]
[Starting training]
Epoch 0 	 0.035156 	 0.175551 	 0.036949
Epoch 10 	 0.031020 	 0.179228 	 0.036765
Epoch 20 	 0.018957 	 0.175551 	 0.035662
Epoch 30 	 0.015740 	 0.170956 	 0.034375
[Model stopped early]
Train loss       : 0.012638
Best valid loss  : 0.169118
Best test loss   : 0.036029
Pruning          : 0.70
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.0478]
[Starting training]
Epoch 0 	 0.038603 	 0.175092 	 0.037868
Epoch 10 	 0.025161 	 0.169577 	 0.034926
Epoch 20 	 0.019646 	 0.169118 	 0.034007
Epoch 30 	 0.014246 	 0.170956 	 0.034375
Epoch 40 	 0.013442 	 0.173713 	 0.035110
Epoch 50 	 0.009076 	 0.173713 	 0.034926
[Model stopped early]
Train loss       : 0.010800
Best valid loss  : 0.164062
Best test loss   : 0.033272
Pruning          : 0.49
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.2274]
[Starting training]
Epoch 0 	 0.033088 	 0.166360 	 0.035478
Epoch 10 	 0.025391 	 0.163603 	 0.033272
Epoch 20 	 0.021025 	 0.161765 	 0.032904
Epoch 30 	 0.013787 	 0.167279 	 0.033640
[Model stopped early]
Train loss       : 0.011719
Best valid loss  : 0.160846
Best test loss   : 0.033272
Pruning          : 0.34
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.0842]
[Starting training]
Epoch 0 	 0.047449 	 0.167739 	 0.034743
Epoch 10 	 0.025276 	 0.162224 	 0.032904
Epoch 20 	 0.027459 	 0.166820 	 0.034191
Epoch 30 	 0.017119 	 0.172335 	 0.034926
Epoch 40 	 0.015970 	 0.171415 	 0.034559
[Model stopped early]
Train loss       : 0.013902
Best valid loss  : 0.160386
Best test loss   : 0.032353
Pruning          : 0.24
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.1373]
[Starting training]
Epoch 0 	 0.058594 	 0.173713 	 0.036581
Epoch 10 	 0.031939 	 0.162224 	 0.033456
Epoch 20 	 0.023208 	 0.167279 	 0.033640
Epoch 30 	 0.020795 	 0.163603 	 0.033088
[Model stopped early]
Train loss       : 0.017923
Best valid loss  : 0.162224
Best test loss   : 0.033824
Pruning          : 0.17
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.1285]
[Starting training]
Epoch 0 	 0.094554 	 0.176930 	 0.040993
Epoch 10 	 0.049517 	 0.178768 	 0.037868
Epoch 20 	 0.037454 	 0.171415 	 0.035662
Epoch 30 	 0.031250 	 0.172794 	 0.035662
Epoch 40 	 0.024931 	 0.168658 	 0.034191
Epoch 50 	 0.027688 	 0.170956 	 0.034743
Epoch 60 	 0.021255 	 0.171875 	 0.035110
[Model stopped early]
Train loss       : 0.020450
Best valid loss  : 0.165901
Best test loss   : 0.033640
Pruning          : 0.12
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.3934]
[Starting training]
Epoch 0 	 0.102597 	 0.179228 	 0.043107
Epoch 10 	 0.049403 	 0.172794 	 0.036029
Epoch 20 	 0.039752 	 0.167739 	 0.034007
Epoch 30 	 0.035041 	 0.169577 	 0.035662
Epoch 40 	 0.025965 	 0.170037 	 0.034926
Epoch 50 	 0.024931 	 0.169577 	 0.034375
Epoch 60 	 0.029412 	 0.165901 	 0.033640
Epoch 70 	 0.025850 	 0.165441 	 0.033456
Epoch 80 	 0.027918 	 0.164982 	 0.033456
[Model stopped early]
Train loss       : 0.024357
Best valid loss  : 0.163143
Best test loss   : 0.033088
Pruning          : 0.08
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.4893]
[Starting training]
Epoch 0 	 0.145795 	 0.190257 	 0.047978
Epoch 10 	 0.071806 	 0.173254 	 0.042463
Epoch 20 	 0.064568 	 0.168199 	 0.035846
Epoch 30 	 0.050092 	 0.169577 	 0.035110
Epoch 40 	 0.043313 	 0.172794 	 0.037132
[Model stopped early]
Train loss       : 0.046530
Best valid loss  : 0.164982
Best test loss   : 0.035846
Pruning          : 0.06
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.4198]
[Starting training]
Epoch 0 	 0.237362 	 0.209099 	 0.061949
Epoch 10 	 0.110869 	 0.179228 	 0.041912
Epoch 20 	 0.090763 	 0.181985 	 0.042831
Epoch 30 	 0.085018 	 0.179228 	 0.040257
Epoch 40 	 0.080997 	 0.175551 	 0.041728
Epoch 50 	 0.076976 	 0.177390 	 0.038787
Epoch 60 	 0.069738 	 0.171415 	 0.038787
Epoch 70 	 0.056296 	 0.171875 	 0.038603
Epoch 80 	 0.058709 	 0.174632 	 0.038787
[Model stopped early]
Train loss       : 0.057675
Best valid loss  : 0.167279
Best test loss   : 0.035846
Pruning          : 0.04
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.7520]
[Starting training]
Epoch 0 	 0.273552 	 0.235294 	 0.082445
Epoch 10 	 0.130859 	 0.189798 	 0.048529
Epoch 20 	 0.112707 	 0.173713 	 0.043199
Epoch 30 	 0.106043 	 0.179688 	 0.043382
Epoch 40 	 0.094210 	 0.179688 	 0.040809
Epoch 50 	 0.090188 	 0.174173 	 0.040809
[Model stopped early]
Train loss       : 0.087201
Best valid loss  : 0.173713
Best test loss   : 0.043199
Pruning          : 0.03
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.7945]
[Starting training]
Epoch 0 	 0.439568 	 0.352482 	 0.186581
Epoch 10 	 0.209444 	 0.235754 	 0.079044
Epoch 20 	 0.182445 	 0.215074 	 0.071324
Epoch 30 	 0.170841 	 0.204504 	 0.056434
Epoch 40 	 0.153378 	 0.190257 	 0.051838
Epoch 50 	 0.149242 	 0.197610 	 0.054044
Epoch 60 	 0.138557 	 0.202206 	 0.055882
Epoch 70 	 0.133042 	 0.194393 	 0.051287
Epoch 80 	 0.128562 	 0.199449 	 0.052941
Epoch 90 	 0.129710 	 0.187040 	 0.048346
Epoch 100 	 0.125115 	 0.193934 	 0.049265
Epoch 110 	 0.123392 	 0.197610 	 0.052757
Epoch 120 	 0.120979 	 0.189798 	 0.049081
Epoch 130 	 0.123162 	 0.190257 	 0.048162
[Model stopped early]
Train loss       : 0.119370
Best valid loss  : 0.184283
Best test loss   : 0.046507
Pruning          : 0.02
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.7537]
[Starting training]
Epoch 0 	 0.434858 	 0.380515 	 0.227941
Epoch 10 	 0.238166 	 0.236213 	 0.083640
Epoch 20 	 0.202206 	 0.222426 	 0.069301
Epoch 30 	 0.196461 	 0.215533 	 0.068750
Epoch 40 	 0.174977 	 0.205423 	 0.063419
Epoch 50 	 0.174747 	 0.201287 	 0.058088
Epoch 60 	 0.177964 	 0.198070 	 0.055147
Epoch 70 	 0.157744 	 0.195772 	 0.055147
Epoch 80 	 0.162684 	 0.197151 	 0.057721
Epoch 90 	 0.158318 	 0.195772 	 0.054963
Epoch 100 	 0.159582 	 0.200368 	 0.058640
[Model stopped early]
Train loss       : 0.159582
Best valid loss  : 0.186581
Best test loss   : 0.049816
Pruning          : 0.01
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9421]
[Starting training]
Epoch 0 	 0.509421 	 0.434283 	 0.287132
Epoch 10 	 0.295726 	 0.284926 	 0.117188
Epoch 20 	 0.267348 	 0.269761 	 0.103585
Epoch 30 	 0.240349 	 0.248621 	 0.090625
Epoch 40 	 0.232537 	 0.246324 	 0.085294
Epoch 50 	 0.235409 	 0.238051 	 0.084375
Epoch 60 	 0.224035 	 0.227941 	 0.080147
Epoch 70 	 0.211512 	 0.222886 	 0.077022
Epoch 80 	 0.218061 	 0.230699 	 0.075184
Epoch 90 	 0.206687 	 0.227022 	 0.073897
Epoch 100 	 0.197725 	 0.210938 	 0.065074
Epoch 110 	 0.194968 	 0.219669 	 0.068015
Epoch 120 	 0.195083 	 0.219669 	 0.069118
Epoch 130 	 0.197036 	 0.224265 	 0.070956
[Model stopped early]
Train loss       : 0.193819
Best valid loss  : 0.210938
Best test loss   : 0.065074
Pruning          : 0.01
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.7443]
[Starting training]
Epoch 0 	 0.586857 	 0.563419 	 0.444026
Epoch 10 	 0.369256 	 0.354320 	 0.175000
Epoch 20 	 0.312385 	 0.303768 	 0.133824
Epoch 30 	 0.283318 	 0.303768 	 0.130331
Epoch 40 	 0.278148 	 0.275276 	 0.113787
Epoch 50 	 0.273093 	 0.284007 	 0.115074
Epoch 60 	 0.255515 	 0.267923 	 0.105882
Epoch 70 	 0.249770 	 0.265165 	 0.098346
Epoch 80 	 0.248966 	 0.269301 	 0.095312
Epoch 90 	 0.242417 	 0.280790 	 0.103676
Epoch 100 	 0.240005 	 0.268382 	 0.097426
Epoch 110 	 0.250345 	 0.252757 	 0.086397
Epoch 120 	 0.241383 	 0.263327 	 0.090809
Epoch 130 	 0.235179 	 0.253217 	 0.086029
Epoch 140 	 0.236673 	 0.258732 	 0.090993
[Model stopped early]
Train loss       : 0.240924
Best valid loss  : 0.252757
Best test loss   : 0.086397
Pruning          : 0.01
