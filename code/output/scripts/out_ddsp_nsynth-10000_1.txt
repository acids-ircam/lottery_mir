Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41146324.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, kiwisolver, python-dateutil, cycler, pyparsing, matplotlib, gast, keras-preprocessing, wrapt, google-pasta, termcolor, grpcio, h5py, keras-applications, absl-py, astor, opt-einsum, tensorflow-estimator, protobuf, werkzeug, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, urllib3, chardet, idna, certifi, requests, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146324.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-27 03:03:16.339204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-27 03:03:16.712899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_ddsp_cnn_xavier_trimming_batchnorm_reinit_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41146324.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 83.7224]
[Starting training]
Epoch 0 	 75.002762 	 305.116180 	 506.550537
Epoch 10 	 61.541706 	 57.658772 	 57.504513
Epoch 20 	 50.077511 	 45.699398 	 47.317207
Epoch 30 	 44.056572 	 38.701191 	 40.835537
Epoch 40 	 41.247078 	 37.060417 	 38.964828
Epoch 50 	 42.860317 	 37.689545 	 39.586231
Epoch 60 	 38.908745 	 34.828426 	 36.625778
Epoch 70 	 40.403378 	 36.325989 	 38.281158
Epoch 80 	 35.065163 	 32.294044 	 34.370056
Epoch 90 	 33.313942 	 30.738047 	 32.865532
Epoch 100 	 31.403154 	 30.305256 	 32.461761
Epoch 110 	 29.915644 	 28.409420 	 30.373215
Epoch 120 	 28.828859 	 27.849474 	 29.805048
Epoch 130 	 27.732105 	 27.042843 	 29.240993
Epoch 140 	 27.108515 	 26.533520 	 28.473196
Epoch 150 	 26.420128 	 26.854435 	 28.769007
Epoch 160 	 25.846735 	 25.747654 	 27.602205
Epoch 170 	 25.175659 	 25.395311 	 27.452440
Epoch 180 	 24.945272 	 25.157194 	 27.002314
Epoch 190 	 24.768431 	 26.583345 	 28.407850
Train loss       : 24.290863
Best valid loss  : 24.790886
Best test loss   : 26.687366
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 3,792,824
--------------------------------
Total memory      : 35.14 MB
Total Flops       : 339.89 MFlops
Total Mem (Read)  : 35.15 MB
Total Mem (Write) : 28.95 MB
[Supermasks testing]
[Untrained loss : 95.3846]
[Starting training]
Epoch 0 	 687.926697 	 58.049274 	 59.376610
Epoch 10 	 52.155739 	 50.658066 	 51.611225
Epoch 20 	 44.966011 	 41.149529 	 42.234856
Epoch 30 	 40.018818 	 35.434231 	 37.191563
Epoch 40 	 36.559605 	 34.298943 	 36.561810
Epoch 50 	 34.149982 	 32.090740 	 34.084400
Epoch 60 	 35.949551 	 35.534252 	 36.451591
Epoch 70 	 35.184837 	 607.518372 	 36.711281
Epoch 80 	 31.382008 	 30.904736 	 33.161255
Epoch 90 	 29.873236 	 28.797897 	 31.009497
Epoch 100 	 29.178019 	 28.062998 	 29.956535
Epoch 110 	 29.016815 	 32.416637 	 34.515572
Epoch 120 	 27.869061 	 27.331726 	 29.382978
Epoch 130 	 34.880520 	 33.351151 	 34.992641
Epoch 140 	 28.565207 	 27.640507 	 29.465551
Epoch 150 	 27.406115 	 26.973501 	 28.717709
Epoch 160 	 26.764051 	 26.937101 	 28.454287
Epoch 170 	 26.255695 	 26.576916 	 28.206839
Epoch 180 	 26.046270 	 26.655094 	 28.318985
Epoch 190 	 25.803648 	 26.440369 	 28.055563
Train loss       : 25.585165
Best valid loss  : 26.175800
Best test loss   : 27.873020
Pruning          : 0.72
0.001
0.001
[Current model size]
================================
Total params      : 3,316,816
--------------------------------
Total memory      : 29.84 MB
Total Flops       : 192.02 MFlops
Total Mem (Read)  : 28.52 MB
Total Mem (Write) : 24.13 MB
[Supermasks testing]
[Untrained loss : 78.8405]
[Starting training]
Epoch 0 	 74.953354 	 58.433186 	 59.256180
Epoch 10 	 46.470108 	 44.271873 	 45.272369
Epoch 20 	 42.830246 	 36.454941 	 38.478584
Epoch 30 	 45.447536 	 37.980972 	 40.040321
Epoch 40 	 37.872631 	 33.985912 	 35.831902
Epoch 50 	 34.500435 	 31.784899 	 33.889961
Epoch 60 	 32.731106 	 31.307888 	 33.613335
Epoch 70 	 31.001360 	 29.509466 	 31.812855
Epoch 80 	 30.435131 	 29.139629 	 31.022255
Epoch 90 	 28.926138 	 28.585754 	 30.760624
Epoch 100 	 28.072308 	 27.738838 	 29.663752
Epoch 110 	 27.669546 	 27.393322 	 29.550062
Epoch 120 	 25.810280 	 26.590824 	 28.339344
Epoch 130 	 25.180750 	 26.416391 	 28.700569
Epoch 140 	 24.860218 	 25.970232 	 27.714649
Epoch 150 	 24.572411 	 26.062941 	 27.895428
Epoch 160 	 24.368567 	 25.808889 	 27.639481
Epoch 170 	 24.102068 	 25.733694 	 27.698919
Epoch 180 	 23.438112 	 25.336576 	 27.286249
Epoch 190 	 23.114708 	 24.839273 	 26.996235
Train loss       : 22.852448
Best valid loss  : 24.839273
Best test loss   : 26.996235
Pruning          : 0.52
0.001
0.001
[Current model size]
================================
Total params      : 3,028,378
--------------------------------
Total memory      : 26.00 MB
Total Flops       : 113.88 MFlops
Total Mem (Read)  : 23.92 MB
Total Mem (Write) : 20.63 MB
[Supermasks testing]
[Untrained loss : 81.2737]
[Starting training]
Epoch 0 	 69.450188 	 54.344650 	 53.886105
Epoch 10 	 42.104668 	 40.161213 	 42.217510
Epoch 20 	 36.423233 	 34.603554 	 36.286358
Epoch 30 	 34.244045 	 32.265381 	 33.982964
Epoch 40 	 32.877579 	 30.549046 	 32.697079
Epoch 50 	 30.795725 	 28.704210 	 30.808695
Epoch 60 	 29.463499 	 29.287930 	 31.217690
Epoch 70 	 28.935720 	 27.443304 	 29.559385
Epoch 80 	 28.234177 	 28.136139 	 30.258255
Epoch 90 	 27.241957 	 26.943577 	 28.999660
Epoch 100 	 26.938421 	 27.604172 	 29.241846
Epoch 110 	 26.474895 	 26.127596 	 28.281528
Epoch 120 	 26.267099 	 26.175667 	 28.041195
Epoch 130 	 24.710224 	 25.402510 	 27.328718
Epoch 140 	 24.597984 	 25.293787 	 27.186962
Epoch 150 	 24.333750 	 25.623930 	 27.445629
Epoch 160 	 24.113705 	 25.988354 	 27.839628
Epoch 170 	 23.966370 	 24.946142 	 26.949398
Epoch 180 	 23.766027 	 24.973543 	 27.013767
Epoch 190 	 23.164379 	 25.106783 	 26.843716
Train loss       : 22.806969
Best valid loss  : 24.575306
Best test loss   : 26.569500
Pruning          : 0.37
0.001
0.001
[Current model size]
================================
Total params      : 2,844,854
--------------------------------
Total memory      : 23.18 MB
Total Flops       : 72.48 MFlops
Total Mem (Read)  : 20.64 MB
Total Mem (Write) : 18.05 MB
[Supermasks testing]
[Untrained loss : 74.6392]
[Starting training]
Epoch 0 	 68.763435 	 57.448105 	 56.865078
Epoch 10 	 45.997307 	 47.517056 	 51.938087
Epoch 20 	 39.738235 	 36.751667 	 38.294273
Epoch 30 	 37.914894 	 34.355331 	 36.018272
Epoch 40 	 33.651138 	 32.665081 	 34.127171
Epoch 50 	 31.996704 	 30.708073 	 32.417393
Epoch 60 	 30.237551 	 30.589993 	 32.476807
Epoch 70 	 29.469965 	 28.850163 	 30.565533
Epoch 80 	 28.947966 	 28.345137 	 30.438955
Epoch 90 	 27.658493 	 28.097198 	 29.934845
Epoch 100 	 27.146372 	 28.469576 	 30.417719
Epoch 110 	 26.008944 	 28.246292 	 29.715176
Epoch 120 	 25.411552 	 27.232754 	 28.867777
Epoch 130 	 25.118752 	 26.711113 	 28.523516
Epoch 140 	 24.382030 	 26.689087 	 28.308031
Epoch 150 	 24.128040 	 27.021122 	 28.380844
Epoch 160 	 23.798906 	 26.628141 	 28.256512
Epoch 170 	 23.624842 	 26.785553 	 28.255167
Epoch 180 	 23.565739 	 26.388474 	 28.070492
Epoch 190 	 23.488491 	 26.426897 	 28.059937
Train loss       : 23.403278
Best valid loss  : 26.127687
Best test loss   : 28.009140
Pruning          : 0.27
0.001
0.001
[Current model size]
================================
Total params      : 2,726,701
--------------------------------
Total memory      : 21.15 MB
Total Flops       : 51.3 MFlops
Total Mem (Read)  : 18.35 MB
Total Mem (Write) : 16.21 MB
[Supermasks testing]
[Untrained loss : 94.7853]
[Starting training]
Epoch 0 	 73.191689 	 57.769150 	 59.040916
Epoch 10 	 49.757660 	 44.985748 	 45.979599
Epoch 20 	 43.811218 	 40.487572 	 41.858978
Epoch 30 	 39.662170 	 37.669346 	 39.526470
Epoch 40 	 37.602406 	 35.742352 	 37.357533
Epoch 50 	 34.833496 	 33.048595 	 35.044857
Epoch 60 	 33.619339 	 32.087818 	 34.164528
Epoch 70 	 31.491961 	 31.664648 	 33.534157
Epoch 80 	 30.197842 	 29.913317 	 32.199318
Epoch 90 	 30.824341 	 30.245413 	 31.873652
Epoch 100 	 27.925720 	 28.750666 	 30.684563
Epoch 110 	 27.446358 	 28.834417 	 30.650312
Epoch 120 	 26.265982 	 28.346231 	 30.075865
Epoch 130 	 25.786491 	 27.871881 	 30.059580
Epoch 140 	 25.744928 	 28.516279 	 29.931808
Epoch 150 	 25.075945 	 28.055241 	 29.672323
Epoch 160 	 24.914261 	 27.958858 	 29.652498
Epoch 170 	 24.743338 	 27.679359 	 29.618719
Epoch 180 	 24.644213 	 27.872009 	 29.561647
[Model stopped early]
Train loss       : 24.643877
Best valid loss  : 27.665249
Best test loss   : 29.551584
Pruning          : 0.19
0.001
0.001
[Current model size]
================================
Total params      : 2,649,351
--------------------------------
Total memory      : 19.73 MB
Total Flops       : 40.64 MFlops
Total Mem (Read)  : 16.76 MB
Total Mem (Write) : 14.91 MB
[Supermasks testing]
[Untrained loss : 76.7514]
[Starting training]
Epoch 0 	 73.150436 	 64.060493 	 63.581676
Epoch 10 	 52.507629 	 48.162476 	 48.544411
Epoch 20 	 45.691383 	 38.831787 	 41.209373
Epoch 30 	 39.974850 	 35.896103 	 37.158127
Epoch 40 	 36.723682 	 34.273998 	 35.847343
Epoch 50 	 33.641605 	 32.294548 	 33.982662
Epoch 60 	 31.835268 	 31.599823 	 33.422733
Epoch 70 	 30.807840 	 29.998428 	 31.739790
Epoch 80 	 29.702402 	 29.746138 	 31.486076
Epoch 90 	 28.937000 	 30.645283 	 32.738617
Epoch 100 	 28.468664 	 29.118164 	 30.974672
Epoch 110 	 31.042803 	 32.753403 	 35.912071
Epoch 120 	 28.058037 	 28.321020 	 29.920076
Epoch 130 	 25.879623 	 27.625397 	 29.294077
Epoch 140 	 25.569019 	 27.392036 	 29.265600
Epoch 150 	 24.893549 	 27.564848 	 29.186844
Epoch 160 	 24.687355 	 27.228266 	 29.025461
Epoch 170 	 24.378864 	 27.438894 	 29.095684
Epoch 180 	 24.162630 	 27.351122 	 28.937883
[Model stopped early]
Train loss       : 24.212198
Best valid loss  : 27.096073
Best test loss   : 29.015142
Pruning          : 0.14
0.001
0.001
[Current model size]
================================
Total params      : 2,597,821
--------------------------------
Total memory      : 18.71 MB
Total Flops       : 35.13 MFlops
Total Mem (Read)  : 15.63 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 79.7763]
[Starting training]
Epoch 0 	 75.267784 	 412.074371 	 74.792610
Epoch 10 	 49.773178 	 48.666840 	 45.923767
Epoch 20 	 44.685997 	 43.399773 	 44.119713
Epoch 30 	 41.615875 	 41.854874 	 41.867680
Epoch 40 	 39.166840 	 39.574249 	 40.317848
Epoch 50 	 36.967243 	 38.536987 	 39.213898
Epoch 60 	 37.399925 	 38.382481 	 38.996861
Epoch 70 	 34.459751 	 37.559807 	 38.340122
Epoch 80 	 32.792507 	 36.656307 	 37.108627
Epoch 90 	 31.350414 	 36.394108 	 36.762409
Epoch 100 	 30.204506 	 36.702595 	 37.227844
Epoch 110 	 29.571838 	 36.422005 	 36.819481
Epoch 120 	 29.055223 	 36.506943 	 36.818005
[Model stopped early]
Train loss       : 28.934158
Best valid loss  : 36.013306
Best test loss   : 36.677265
Pruning          : 0.10
0.001
0.001
[Current model size]
================================
Total params      : 2,563,373
--------------------------------
Total memory      : 17.92 MB
Total Flops       : 31.98 MFlops
Total Mem (Read)  : 14.78 MB
Total Mem (Write) : 13.26 MB
[Supermasks testing]
[Untrained loss : 89.7183]
[Starting training]
Epoch 0 	 75.961830 	 61.100136 	 62.662308
Epoch 10 	 51.565807 	 48.848103 	 49.564919
Epoch 20 	 48.330845 	 45.973320 	 46.692902
Epoch 30 	 47.281116 	 46.793652 	 48.483646
Epoch 40 	 43.617985 	 43.094582 	 44.272411
Epoch 50 	 40.400993 	 44.544899 	 44.721516
Epoch 60 	 38.751610 	 41.335751 	 42.264454
Epoch 70 	 36.088772 	 40.887592 	 42.072327
Epoch 80 	 34.752781 	 41.077179 	 41.843006
Epoch 90 	 33.205158 	 41.225113 	 42.127460
Epoch 100 	 32.368164 	 41.129051 	 42.087502
Epoch 110 	 32.198284 	 41.007488 	 42.178768
Epoch 120 	 31.795908 	 41.192341 	 42.018032
[Model stopped early]
Train loss       : 31.722317
Best valid loss  : 40.345165
Best test loss   : 41.925907
Pruning          : 0.07
0.001
0.001
[Current model size]
================================
Total params      : 2,540,077
--------------------------------
Total memory      : 17.49 MB
Total Flops       : 30.82 MFlops
Total Mem (Read)  : 14.3 MB
Total Mem (Write) : 12.87 MB
[Supermasks testing]
[Untrained loss : 74523.4062]
[Starting training]
Epoch 0 	 75.908333 	 64.454117 	 66.733414
Epoch 10 	 52.966583 	 49.870155 	 49.121628
Epoch 20 	 49.599854 	 46.056950 	 46.055500
Epoch 30 	 47.065887 	 44.929474 	 45.255630
Epoch 40 	 43.664768 	 43.839542 	 44.511425
Epoch 50 	 41.429722 	 42.640232 	 43.343586
Epoch 60 	 39.163589 	 42.323200 	 42.940716
Epoch 70 	 37.576340 	 42.229565 	 43.039692
Epoch 80 	 34.738533 	 41.272957 	 42.142799
Epoch 90 	 33.269241 	 41.286091 	 41.807606
Epoch 100 	 32.630932 	 41.265224 	 41.752178
[Model stopped early]
Train loss       : 31.779160
Best valid loss  : 40.855648
Best test loss   : 41.936703
Pruning          : 0.05
0.001
0.001
[Current model size]
================================
Total params      : 2,523,994
--------------------------------
Total memory      : 17.09 MB
Total Flops       : 29.95 MFlops
Total Mem (Read)  : 13.88 MB
Total Mem (Write) : 12.51 MB
[Supermasks testing]
[Untrained loss : 85.8940]
[Starting training]
Epoch 0 	 76.854240 	 65.735405 	 67.719818
Epoch 10 	 54.438782 	 53.396347 	 53.846714
Epoch 20 	 51.389820 	 48.011017 	 47.401810
Epoch 30 	 47.859325 	 47.155613 	 46.496063
Epoch 40 	 45.613956 	 44.381123 	 44.952679
Epoch 50 	 43.098186 	 43.741859 	 44.230991
Epoch 60 	 41.175278 	 42.966179 	 43.225857
Epoch 70 	 39.822464 	 43.047005 	 43.212971
Epoch 80 	 36.921463 	 43.262093 	 43.020210
Epoch 90 	 36.069645 	 43.723984 	 43.481548
Epoch 100 	 34.712868 	 43.429142 	 43.170666
Epoch 110 	 33.705585 	 43.521008 	 43.680237
[Model stopped early]
Train loss       : 33.008995
Best valid loss  : 42.870922
Best test loss   : 42.984676
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 2,511,046
--------------------------------
Total memory      : 17.01 MB
Total Flops       : 29.94 MFlops
Total Mem (Read)  : 13.77 MB
Total Mem (Write) : 12.45 MB
[Supermasks testing]
[Untrained loss : 81.0669]
[Starting training]
Epoch 0 	 75.088776 	 60.552376 	 61.550774
Epoch 10 	 53.531853 	 49.739899 	 49.195267
Epoch 20 	 51.400360 	 48.999584 	 48.685143
Epoch 30 	 49.415482 	 46.920044 	 46.768356
Epoch 40 	 46.874462 	 46.628815 	 46.662209
Epoch 50 	 44.929287 	 44.866432 	 45.717060
Epoch 60 	 42.729103 	 44.950401 	 46.181423
Epoch 70 	 40.671833 	 44.542999 	 44.881054
Epoch 80 	 38.073212 	 42.984039 	 44.191422
Epoch 90 	 36.492996 	 43.648125 	 44.493504
Epoch 100 	 35.081726 	 43.348797 	 44.249184
Epoch 110 	 34.450386 	 43.402252 	 44.299099
Epoch 120 	 34.028812 	 43.797272 	 44.512596
[Model stopped early]
Train loss       : 33.875217
Best valid loss  : 42.963215
Best test loss   : 44.015835
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 2,502,534
--------------------------------
Total memory      : 16.95 MB
Total Flops       : 29.94 MFlops
Total Mem (Read)  : 13.71 MB
Total Mem (Write) : 12.42 MB
[Supermasks testing]
[Untrained loss : 87.7525]
[Starting training]
Epoch 0 	 14755.695312 	 67.858330 	 69.321220
Epoch 10 	 68.164429 	 9361.068359 	 114.679642
Epoch 20 	 57.981152 	 86.680908 	 56.347214
Epoch 30 	 54.540329 	 39053.765625 	 278.489441
Epoch 40 	 53.231865 	 314.839325 	 54.405872
Epoch 50 	 51.590168 	 43082.519531 	 141.234909
Epoch 60 	 53.006016 	 73.889587 	 53.346203
Epoch 70 	 49.469585 	 67.346519 	 53.169128
Epoch 80 	 48.463318 	 93.504021 	 52.963398
Epoch 90 	 47.574600 	 50.917397 	 51.083366
Epoch 100 	 46.412727 	 49.869831 	 50.282322
Epoch 110 	 45.419590 	 49.719570 	 50.592636
Epoch 120 	 44.379242 	 49.022923 	 49.869125
Epoch 130 	 43.513248 	 48.296444 	 49.618122
Epoch 140 	 42.588593 	 48.478214 	 49.406563
Epoch 150 	 41.960209 	 47.695015 	 48.914780
Epoch 160 	 41.654686 	 47.856899 	 49.177467
Epoch 170 	 41.450497 	 48.204075 	 49.097359
Epoch 180 	 40.983322 	 48.012440 	 49.452976
[Model stopped early]
Train loss       : 41.079281
Best valid loss  : 47.590885
Best test loss   : 49.215317
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 2,496,213
--------------------------------
Total memory      : 16.91 MB
Total Flops       : 29.93 MFlops
Total Mem (Read)  : 13.65 MB
Total Mem (Write) : 12.39 MB
[Supermasks testing]
[Untrained loss : 81.5659]
[Starting training]
Epoch 0 	 78.223465 	 72.790085 	 74.468498
Epoch 10 	 55.658695 	 51.736450 	 51.648518
Epoch 20 	 54.760124 	 51.549793 	 51.355537
Epoch 30 	 53.199402 	 50.395847 	 49.709957
Epoch 40 	 51.097046 	 48.279099 	 47.929420
Epoch 50 	 47.920586 	 47.021736 	 47.255081
Epoch 60 	 45.893154 	 45.198460 	 45.226681
Epoch 70 	 42.953339 	 44.022091 	 44.806839
Epoch 80 	 41.212631 	 44.182228 	 44.327183
Epoch 90 	 39.417168 	 44.338486 	 44.164211
Epoch 100 	 37.711662 	 43.582291 	 44.004196
Epoch 110 	 37.120228 	 42.328930 	 43.320820
Epoch 120 	 35.754215 	 43.021996 	 43.161407
Epoch 130 	 34.703518 	 42.346264 	 43.438427
Epoch 140 	 33.948864 	 42.380432 	 43.155827
Epoch 150 	 33.585049 	 42.274750 	 43.416035
Epoch 160 	 33.273502 	 42.212505 	 43.645210
Epoch 170 	 33.106762 	 42.340088 	 43.532295
Epoch 180 	 32.967617 	 42.344189 	 43.433537
[Model stopped early]
Train loss       : 32.925720
Best valid loss  : 41.966908
Best test loss   : 43.426834
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 2,492,029
--------------------------------
Total memory      : 16.89 MB
Total Flops       : 29.93 MFlops
Total Mem (Read)  : 13.62 MB
Total Mem (Write) : 12.37 MB
[Supermasks testing]
[Untrained loss : 78.5363]
[Starting training]
Epoch 0 	 77.551979 	 68.037865 	 69.660759
Epoch 10 	 57.370831 	 52.418819 	 51.964718
Epoch 20 	 55.700321 	 52.337708 	 52.254669
Epoch 30 	 54.116203 	 50.969299 	 50.331184
Epoch 40 	 53.314274 	 49.995193 	 49.928894
Epoch 50 	 51.852085 	 49.673080 	 49.343128
Epoch 60 	 50.296913 	 48.781673 	 48.468124
Epoch 70 	 49.305450 	 50.229382 	 49.967583
Epoch 80 	 47.080223 	 47.525429 	 47.772213
Epoch 90 	 46.149292 	 47.602085 	 48.303307
Epoch 100 	 44.835537 	 47.251728 	 47.698807
Epoch 110 	 44.126236 	 47.152065 	 48.054394
Epoch 120 	 43.610924 	 47.377949 	 48.528568
Epoch 130 	 43.112732 	 47.572330 	 48.640476
[Model stopped early]
Train loss       : 43.251804
Best valid loss  : 46.988525
Best test loss   : 47.874325
Pruning          : 0.01
