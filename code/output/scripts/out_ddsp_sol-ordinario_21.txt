Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41281326.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, six, future, torch, torchvision, tqdm, pyparsing, cycler, kiwisolver, python-dateutil, matplotlib, keras-preprocessing, absl-py, protobuf, grpcio, tensorflow-estimator, gast, opt-einsum, termcolor, werkzeug, urllib3, chardet, idna, certifi, requests, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, tensorboard, google-pasta, h5py, keras-applications, wrapt, astor, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281326.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-29 02:24:36.259665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 02:24:36.586905: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is sol-ordinario_ddsp_cnn_xavier_trimming_activation_rewind_global_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41281326.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 108.2593]
[Starting training]
Epoch 0 	 92.078911 	 118.246490 	 118.285110
Epoch 10 	 80.234131 	 80.507561 	 84.180321
Epoch 20 	 78.856865 	 73.265099 	 79.374077
Epoch 30 	 72.602707 	 74.459785 	 75.603401
Epoch 40 	 69.859596 	 68.945206 	 70.841003
Epoch 50 	 67.738800 	 64.490105 	 70.529167
Epoch 60 	 62.303371 	 63.477814 	 68.256187
Epoch 70 	 54.385235 	 57.230988 	 59.313221
Epoch 80 	 54.561523 	 53.659100 	 55.302834
Epoch 90 	 44.613640 	 45.123890 	 49.891613
Epoch 100 	 43.133484 	 40.941864 	 43.410492
Epoch 110 	 39.533215 	 37.877106 	 39.979931
Epoch 120 	 35.209576 	 35.464619 	 38.126862
Epoch 130 	 34.489117 	 35.003151 	 37.117298
Epoch 140 	 32.760498 	 33.450916 	 35.573223
Epoch 150 	 31.103136 	 33.236473 	 35.109386
Epoch 160 	 30.385260 	 32.487003 	 34.329762
Epoch 170 	 28.750364 	 31.993835 	 33.865971
Epoch 180 	 28.335272 	 31.604149 	 33.460827
Epoch 190 	 4541.481445 	 39.208714 	 44.589581
Train loss       : 30.481714
Best valid loss  : 30.393373
Best test loss   : 32.550762
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,749,279
--------------------------------
Total memory      : 39.43 MB
Total Flops       : 614.84 MFlops
Total Mem (Read)  : 39.57 MB
Total Mem (Write) : 33.43 MB
[Supermasks testing]
[Untrained loss : 66.8621]
[Starting training]
Epoch 0 	 45.003910 	 42.013893 	 44.859337
Epoch 10 	 39.658802 	 38.355301 	 40.356808
Epoch 20 	 34.752327 	 35.531830 	 38.160610
Epoch 30 	 32.288246 	 33.167122 	 35.474827
Epoch 40 	 31.263510 	 32.679653 	 35.321140
Epoch 50 	 29.483295 	 31.836679 	 33.595898
Epoch 60 	 29.211843 	 31.339630 	 33.799831
Epoch 70 	 28.326796 	 30.942223 	 32.978294
Epoch 80 	 28.011349 	 30.681192 	 32.615429
Epoch 90 	 27.077538 	 30.430956 	 32.435402
Epoch 100 	 26.832262 	 29.872925 	 32.078304
Epoch 110 	 26.405285 	 29.831711 	 32.011608
Epoch 120 	 26.338049 	 29.697523 	 31.715830
Epoch 130 	 26.777601 	 29.505007 	 32.033504
Epoch 140 	 26.264288 	 28.856030 	 31.741098
Epoch 150 	 26.197289 	 29.590145 	 31.706846
Epoch 160 	 26.052479 	 29.442219 	 31.796255
Epoch 170 	 26.029100 	 29.571354 	 31.651663
[Model stopped early]
Train loss       : 26.252249
Best valid loss  : 28.803211
Best test loss   : 31.786264
Pruning          : 0.72
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,016,709
--------------------------------
Total memory      : 34.57 MB
Total Flops       : 495.46 MFlops
Total Mem (Read)  : 32.58 MB
Total Mem (Write) : 28.95 MB
[Supermasks testing]
[Untrained loss : 81.5594]
[Starting training]
Epoch 0 	 47.359356 	 42.122913 	 45.858906
Epoch 10 	 39.534195 	 42.552547 	 44.501293
Epoch 20 	 41.521553 	 38.929337 	 40.916267
Epoch 30 	 32.940456 	 34.493118 	 37.553768
Epoch 40 	 32.088669 	 34.424633 	 36.721375
Epoch 50 	 32.770573 	 32.804565 	 35.530579
Epoch 60 	 30.138357 	 34.242683 	 35.865929
Epoch 70 	 28.757751 	 30.757591 	 32.861076
Epoch 80 	 26.635912 	 29.774334 	 32.124905
Epoch 90 	 26.527473 	 29.088552 	 31.715002
Epoch 100 	 25.622002 	 28.951864 	 31.347853
Epoch 110 	 25.341578 	 29.383984 	 31.147398
Epoch 120 	 24.582087 	 28.632923 	 30.744268
Epoch 130 	 24.557133 	 28.045584 	 30.599203
Epoch 140 	 24.175327 	 28.363798 	 30.374960
Epoch 150 	 23.564915 	 28.048519 	 29.924780
Epoch 160 	 23.652811 	 27.195629 	 29.633701
Epoch 170 	 23.680613 	 27.430180 	 29.685429
Epoch 180 	 23.503813 	 27.923759 	 29.726282
Epoch 190 	 23.455166 	 27.682409 	 29.585770
Train loss       : 23.405720
Best valid loss  : 27.089417
Best test loss   : 29.556631
Pruning          : 0.52
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,417,135
--------------------------------
Total memory      : 30.64 MB
Total Flops       : 372.01 MFlops
Total Mem (Read)  : 26.74 MB
Total Mem (Write) : 23.64 MB
[Supermasks testing]
[Untrained loss : 88.5134]
[Starting training]
Epoch 0 	 55.477596 	 43.725895 	 49.170380
Epoch 10 	 39.375511 	 38.074390 	 40.777550
Epoch 20 	 35.829361 	 35.918297 	 38.373363
Epoch 30 	 33.887367 	 34.975163 	 37.127644
Epoch 40 	 33.582405 	 32.729130 	 35.908527
Epoch 50 	 30.733965 	 32.735451 	 35.001953
Epoch 60 	 30.225521 	 31.717983 	 34.285686
Epoch 70 	 28.979805 	 30.663488 	 33.192722
Epoch 80 	 27.307705 	 29.412725 	 31.855118
Epoch 90 	 26.690329 	 28.778961 	 31.226843
Epoch 100 	 26.126957 	 28.837860 	 31.005890
Epoch 110 	 25.312824 	 28.981184 	 30.533512
Epoch 120 	 25.000633 	 28.548250 	 30.403645
Epoch 130 	 24.873291 	 28.325600 	 30.409666
Epoch 140 	 24.855183 	 28.532227 	 30.176588
Epoch 150 	 24.456179 	 28.284554 	 29.873447
Epoch 160 	 24.371569 	 28.157841 	 29.939787
Epoch 170 	 24.069708 	 28.252745 	 29.811787
Epoch 180 	 23.991615 	 27.572815 	 29.609549
Epoch 190 	 23.877512 	 27.645464 	 29.666040
Train loss       : 23.836002
Best valid loss  : 27.069670
Best test loss   : 29.726633
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,002,403
--------------------------------
Total memory      : 27.44 MB
Total Flops       : 249.15 MFlops
Total Mem (Read)  : 22.19 MB
Total Mem (Write) : 19.25 MB
[Supermasks testing]
[Untrained loss : 88.0087]
[Starting training]
Epoch 0 	 67.885414 	 60.090199 	 63.280090
Epoch 10 	 42.700127 	 41.354317 	 44.141823
Epoch 20 	 41.067142 	 39.870304 	 42.939770
Epoch 30 	 37.225807 	 36.029510 	 40.426193
Epoch 40 	 35.246391 	 37.755318 	 40.832703
Epoch 50 	 33.259926 	 33.528160 	 36.902611
Epoch 60 	 32.552986 	 33.318272 	 36.537216
Epoch 70 	 30.957071 	 32.272865 	 35.265888
Epoch 80 	 31.253122 	 32.740673 	 35.882042
Epoch 90 	 29.980131 	 31.888058 	 35.229103
Epoch 100 	 28.783323 	 32.430637 	 34.934746
Epoch 110 	 27.658510 	 29.899136 	 32.511829
Epoch 120 	 27.999142 	 31.457487 	 33.692162
Epoch 130 	 26.808863 	 29.454660 	 32.042229
Epoch 140 	 27.686884 	 29.029219 	 31.800659
Epoch 150 	 26.508499 	 29.445656 	 31.862370
Epoch 160 	 25.987776 	 29.120014 	 30.937651
Epoch 170 	 25.469202 	 28.583984 	 31.074652
Epoch 180 	 25.013712 	 29.081785 	 31.131092
Epoch 190 	 24.942568 	 28.663931 	 30.857615
Train loss       : 24.754826
Best valid loss  : 28.182318
Best test loss   : 30.943283
Pruning          : 0.27
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,451,166
--------------------------------
Total memory      : 23.13 MB
Total Flops       : 103.0 MFlops
Total Mem (Read)  : 15.84 MB
Total Mem (Write) : 12.93 MB
[Supermasks testing]
[Untrained loss : 90.6961]
[Starting training]
Epoch 0 	 83.066711 	 69.637619 	 75.483727
Epoch 10 	 50.143700 	 45.846603 	 49.491840
Epoch 20 	 46.346786 	 43.077412 	 47.082966
Epoch 30 	 39.107475 	 38.298195 	 41.341972
Epoch 40 	 37.678879 	 36.971519 	 40.334682
Epoch 50 	 35.972481 	 36.971592 	 39.106792
Epoch 60 	 35.228779 	 35.538979 	 37.936905
Epoch 70 	 33.372673 	 33.696915 	 36.081707
Epoch 80 	 31.706896 	 33.444096 	 35.431976
Epoch 90 	 32.225273 	 33.169170 	 35.586285
Epoch 100 	 30.844337 	 32.485775 	 35.781425
Epoch 110 	 30.055286 	 32.111572 	 34.732388
Epoch 120 	 29.610317 	 31.122595 	 34.367901
Epoch 130 	 29.622915 	 30.893944 	 34.384842
Epoch 140 	 29.097776 	 31.091995 	 33.877537
Epoch 150 	 28.876877 	 30.870747 	 33.857189
Epoch 160 	 28.553741 	 30.870644 	 33.706955
Epoch 170 	 28.426325 	 30.347084 	 33.407936
Epoch 180 	 28.337013 	 29.285294 	 33.739029
Epoch 190 	 28.228315 	 30.474308 	 33.388908
Train loss       : 28.145979
Best valid loss  : 29.285294
Best test loss   : 33.739029
Pruning          : 0.19
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,035,855
--------------------------------
Total memory      : 21.48 MB
Total Flops       : 55.17 MFlops
Total Mem (Read)  : 12.64 MB
Total Mem (Write) : 9.86 MB
[Supermasks testing]
[Untrained loss : 98.0503]
[Starting training]
Epoch 0 	 85.715210 	 77.256828 	 83.554283
Epoch 10 	 63.623779 	 58.047211 	 62.978333
Epoch 20 	 56.849972 	 53.935822 	 58.353008
Epoch 30 	 50.557575 	 51.227242 	 51.465157
Epoch 40 	 45.738560 	 44.902523 	 46.145294
Epoch 50 	 42.696297 	 43.220947 	 45.387924
Epoch 60 	 45.538925 	 42.712734 	 44.397568
Epoch 70 	 38.357010 	 39.134338 	 40.849442
Epoch 80 	 38.206982 	 39.435749 	 41.142582
Epoch 90 	 36.957146 	 38.546078 	 40.039120
Epoch 100 	 36.270687 	 38.044506 	 39.683651
Epoch 110 	 35.952641 	 36.824028 	 38.527218
Epoch 120 	 35.324234 	 35.975246 	 38.751980
Epoch 130 	 35.205784 	 36.692947 	 38.160194
Epoch 140 	 34.192181 	 36.233311 	 38.002613
Epoch 150 	 34.222710 	 36.168674 	 38.236217
Epoch 160 	 34.160751 	 35.511105 	 37.438271
Epoch 170 	 33.396019 	 34.798916 	 37.455593
Epoch 180 	 33.076931 	 35.092896 	 37.392006
Epoch 190 	 32.784023 	 34.727966 	 37.189102
Train loss       : 33.044495
Best valid loss  : 34.333355
Best test loss   : 37.151691
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 743,939
--------------------------------
Total memory      : 19.75 MB
Total Flops       : 27.84 MFlops
Total Mem (Read)  : 9.81 MB
Total Mem (Write) : 7.24 MB
[Supermasks testing]
[Untrained loss : 98.7118]
[Starting training]
Epoch 0 	 87.558121 	 83.423538 	 86.862206
Epoch 10 	 64.370956 	 62.050884 	 65.145767
Epoch 20 	 59.701542 	 56.904057 	 60.814857
Epoch 30 	 56.779690 	 54.531517 	 59.390797
Epoch 40 	 52.351536 	 48.989838 	 52.668976
Epoch 50 	 48.199898 	 48.404797 	 50.623802
Epoch 60 	 44.490086 	 42.387035 	 45.225613
Epoch 70 	 44.005356 	 42.395737 	 44.608959
Epoch 80 	 41.446190 	 43.978088 	 46.011646
Epoch 90 	 39.971508 	 40.263809 	 42.128185
Epoch 100 	 39.479931 	 41.415550 	 44.136856
Epoch 110 	 37.753090 	 39.048927 	 41.014420
Epoch 120 	 37.262867 	 39.025120 	 41.358936
Epoch 130 	 36.495796 	 37.900723 	 39.876308
Epoch 140 	 36.977989 	 38.002663 	 39.625301
Epoch 150 	 36.118587 	 37.914257 	 40.219433
Epoch 160 	 35.839973 	 37.426365 	 39.891247
Epoch 170 	 35.769367 	 37.818447 	 39.595974
Epoch 180 	 35.754387 	 37.500214 	 39.703747
Epoch 190 	 35.848171 	 37.895607 	 39.647614
[Model stopped early]
Train loss       : 35.997280
Best valid loss  : 36.773521
Best test loss   : 39.648495
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 565,506
--------------------------------
Total memory      : 18.47 MB
Total Flops       : 13.38 MFlops
Total Mem (Read)  : 7.85 MB
Total Mem (Write) : 5.53 MB
[Supermasks testing]
[Untrained loss : 103.3270]
[Starting training]
Epoch 0 	 88.179779 	 83.561432 	 86.853104
Epoch 10 	 65.889511 	 61.567886 	 68.361526
Epoch 20 	 61.984665 	 62.713428 	 68.302010
Epoch 30 	 58.315228 	 59.091751 	 55473.312500
Epoch 40 	 59.415470 	 54.034554 	 60.153492
Epoch 50 	 55.756084 	 53.324535 	 58.386574
Epoch 60 	 55.777435 	 53.107559 	 59.203976
Epoch 70 	 54.816277 	 52.003311 	 56.911953
Epoch 80 	 54.926735 	 51.724709 	 57.565380
Epoch 90 	 52.501297 	 50.759560 	 55.750893
Epoch 100 	 52.453800 	 52.071083 	 57.098511
Epoch 110 	 51.590569 	 51.132763 	 54.955425
Epoch 120 	 51.442493 	 49.494732 	 54.731304
Epoch 130 	 50.888309 	 50.088112 	 54.500378
Epoch 140 	 51.334553 	 51.005589 	 54.968914
Epoch 150 	 50.893070 	 50.710583 	 54.477711
[Model stopped early]
Train loss       : 50.974529
Best valid loss  : 49.494732
Best test loss   : 54.731304
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 451,183
--------------------------------
Total memory      : 16.87 MB
Total Flops       : 3.55 MFlops
Total Mem (Read)  : 5.82 MB
Total Mem (Write) : 3.7 MB
[Supermasks testing]
[Untrained loss : 104.2605]
[Starting training]
Epoch 0 	 88.212547 	 84.567238 	 87.998489
Epoch 10 	 69.475739 	 67.244720 	 72.282768
Epoch 20 	 66.748497 	 66.085037 	 70.754936
Epoch 30 	 64.585205 	 64.675446 	 69.262535
Epoch 40 	 62.054089 	 60.200115 	 65.399994
Epoch 50 	 59.456661 	 59.508991 	 64.079330
Epoch 60 	 59.713581 	 56.348358 	 63.485981
Epoch 70 	 59.363861 	 58.494999 	 64.400322
Epoch 80 	 58.130009 	 56.847290 	 62.293194
Epoch 90 	 58.729046 	 57.448345 	 62.572529
Epoch 100 	 57.066620 	 56.564484 	 61.708729
Epoch 110 	 56.584938 	 56.186275 	 61.560078
Epoch 120 	 56.604881 	 56.256306 	 61.538654
Epoch 130 	 56.502625 	 56.158142 	 61.316845
Epoch 140 	 56.279640 	 55.371853 	 61.436043
[Model stopped early]
Train loss       : 56.515961
Best valid loss  : 54.978928
Best test loss   : 61.706600
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 359,113
--------------------------------
Total memory      : 16.66 MB
Total Flops       : 2.47 MFlops
Total Mem (Read)  : 5.25 MB
Total Mem (Write) : 3.34 MB
[Supermasks testing]
[Untrained loss : 103.9578]
[Starting training]
Epoch 0 	 88.995514 	 85.430420 	 89.192657
Epoch 10 	 72.350700 	 69.772194 	 73.097588
Epoch 20 	 69.770691 	 67.776260 	 71.236664
Epoch 30 	 67.821793 	 66.035873 	 70.864738
Epoch 40 	 66.652817 	 63.801060 	 69.292831
Epoch 50 	 66.635689 	 64.626472 	 68.739609
Epoch 60 	 65.922279 	 65.491234 	 70.507614
Epoch 70 	 63.220348 	 62.422401 	 65.820976
Epoch 80 	 59.712238 	 58.007088 	 61.780148
Epoch 90 	 58.866081 	 62.249641 	 65.534523
Epoch 100 	 58.115803 	 55.237816 	 60.084625
Epoch 110 	 57.361034 	 53.879593 	 58.770626
Epoch 120 	 57.815876 	 54.876247 	 58.463886
Epoch 130 	 58.375851 	 52.814766 	 58.716370
Epoch 140 	 57.247135 	 54.658955 	 58.194050
Epoch 150 	 56.148186 	 52.900673 	 57.316620
Epoch 160 	 56.083881 	 51.884705 	 56.867352
Epoch 170 	 55.599743 	 51.223579 	 56.887329
Epoch 180 	 54.969707 	 51.776081 	 56.701374
Epoch 190 	 55.012039 	 52.815090 	 56.837929
Train loss       : 54.914486
Best valid loss  : 51.164505
Best test loss   : 57.084167
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 285,022
--------------------------------
Total memory      : 16.60 MB
Total Flops       : 2.05 MFlops
Total Mem (Read)  : 4.91 MB
Total Mem (Write) : 3.18 MB
[Supermasks testing]
[Untrained loss : 103.7097]
[Starting training]
Epoch 0 	 89.131432 	 85.397598 	 88.856697
Epoch 10 	 74.828728 	 69.690811 	 74.307213
Epoch 20 	 71.774399 	 70.191994 	 72.066307
Epoch 30 	 66.885689 	 72.468163 	 76.214073
Epoch 40 	 62.538406 	 57.233173 	 61.863441
Epoch 50 	 64.316055 	 58.898739 	 62.463020
Epoch 60 	 60.815819 	 55.310913 	 60.189594
Epoch 70 	 59.800987 	 54.993156 	 59.454498
Epoch 80 	 57.911270 	 54.675304 	 58.905544
Epoch 90 	 57.805607 	 54.141464 	 58.143017
Epoch 100 	 57.392208 	 53.845276 	 57.743774
Epoch 110 	 56.627060 	 51.484829 	 57.066059
Epoch 120 	 55.327595 	 51.589432 	 56.590908
Epoch 130 	 55.326878 	 51.824226 	 56.412121
Epoch 140 	 55.208290 	 52.040047 	 56.280090
Epoch 150 	 54.657227 	 52.049988 	 56.507519
Epoch 160 	 54.797829 	 51.779053 	 56.181923
Epoch 170 	 54.723896 	 51.448322 	 56.133678
Epoch 180 	 54.532146 	 52.045799 	 56.034451
Epoch 190 	 54.664337 	 51.930740 	 56.015705
[Model stopped early]
Train loss       : 54.664337
Best valid loss  : 50.173664
Best test loss   : 56.003704
Pruning          : 0.03
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 230,233
--------------------------------
Total memory      : 16.57 MB
Total Flops       : 1.89 MFlops
Total Mem (Read)  : 4.68 MB
Total Mem (Write) : 3.11 MB
[Supermasks testing]
[Untrained loss : 104.7897]
[Starting training]
Epoch 0 	 88.747177 	 85.747681 	 88.571838
Epoch 10 	 71.853111 	 70.559799 	 72.898079
Epoch 20 	 70.017639 	 67.335541 	 72.148666
Epoch 30 	 68.980019 	 68.474480 	 73.205803
Epoch 40 	 68.985664 	 65.344887 	 70.521080
Epoch 50 	 65.279495 	 63.420509 	 68.559669
Epoch 60 	 65.175789 	 63.323059 	 68.219086
Epoch 70 	 64.656296 	 63.072529 	 67.758392
Epoch 80 	 64.168144 	 62.389664 	 67.531181
Epoch 90 	 63.847874 	 63.427368 	 67.300598
[Model stopped early]
Train loss       : 64.233322
Best valid loss  : 61.982407
Best test loss   : 68.057411
Pruning          : 0.02
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 196,893
--------------------------------
Total memory      : 16.57 MB
Total Flops       : 1.82 MFlops
Total Mem (Read)  : 4.54 MB
Total Mem (Write) : 3.08 MB
[Supermasks testing]
[Untrained loss : 104.5761]
[Starting training]
Epoch 0 	 88.614479 	 85.080765 	 88.538155
Epoch 10 	 71.080078 	 70.490166 	 74.566887
Epoch 20 	 70.015099 	 69.983925 	 73.412544
Epoch 30 	 67.542183 	 65.677116 	 70.117607
Epoch 40 	 64.690491 	 67.158157 	 70.655884
Epoch 50 	 63.144718 	 56.679626 	 62.321697
Epoch 60 	 61.136723 	 56.441113 	 61.629272
Epoch 70 	 58.476349 	 55.041111 	 59.771877
Epoch 80 	 59.628345 	 56.708515 	 61.304737
Epoch 90 	 59.027550 	 60.866371 	 65.013954
Epoch 100 	 59.648136 	 54.995453 	 58.956593
Epoch 110 	 58.642464 	 57.573608 	 62.552731
Epoch 120 	 58.101189 	 54.055252 	 58.153622
Epoch 130 	 57.200603 	 53.467247 	 57.412395
Epoch 140 	 55.844418 	 52.471500 	 57.112408
Epoch 150 	 56.283012 	 54.170341 	 58.980602
Epoch 160 	 56.327156 	 52.326267 	 56.845089
Epoch 170 	 56.190201 	 51.293533 	 56.671761
[Model stopped early]
Train loss       : 56.184429
Best valid loss  : 50.727203
Best test loss   : 56.802837
Pruning          : 0.01
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 183,725
--------------------------------
Total memory      : 16.56 MB
Total Flops       : 1.79 MFlops
Total Mem (Read)  : 4.49 MB
Total Mem (Write) : 3.07 MB
[Supermasks testing]
[Untrained loss : 104.3006]
[Starting training]
Epoch 0 	 88.373459 	 86.168053 	 88.397934
Epoch 10 	 73.785957 	 69.452744 	 73.170738
Epoch 20 	 70.779648 	 67.218529 	 72.315636
Epoch 30 	 66.886627 	 68.151695 	 72.603882
Epoch 40 	 61.682201 	 57.364136 	 61.506947
Epoch 50 	 60.100250 	 54.699299 	 59.366646
Epoch 60 	 58.223423 	 53.928268 	 57.753792
Epoch 70 	 58.028954 	 54.205551 	 57.284557
Epoch 80 	 56.497955 	 53.227322 	 58.249123
Epoch 90 	 56.083408 	 52.409397 	 55.961037
Epoch 100 	 56.876945 	 52.381962 	 55.914051
Epoch 110 	 55.081913 	 51.603901 	 55.346313
Epoch 120 	 54.263157 	 51.390518 	 55.356903
Epoch 130 	 54.417183 	 50.575256 	 55.047974
Epoch 140 	 54.596653 	 51.451916 	 55.061005
Epoch 150 	 54.588139 	 50.908649 	 55.050518
Epoch 160 	 53.958199 	 52.261078 	 54.808907
Epoch 170 	 54.786140 	 51.221867 	 55.014442
Epoch 180 	 54.637428 	 51.266029 	 54.779976
Epoch 190 	 54.065975 	 50.459431 	 54.814991
[Model stopped early]
Train loss       : 54.783054
Best valid loss  : 49.781841
Best test loss   : 54.969093
Pruning          : 0.01
