Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41289100.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, pillow-simd, future, torch, torchvision, tqdm, kiwisolver, python-dateutil, pyparsing, cycler, matplotlib, opt-einsum, grpcio, protobuf, absl-py, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, markdown, oauthlib, certifi, urllib3, chardet, idna, requests, requests-oauthlib, google-auth-oauthlib, werkzeug, tensorboard, astor, wrapt, keras-preprocessing, h5py, keras-applications, termcolor, tensorflow-estimator, google-pasta, gast, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41289100.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 05:01:04.090502: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 05:01:04.417258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is singing_classify_cnn_xavier_trimming_batchnorm_rewind_global_0.
*******
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9550]
[Starting training]
Epoch 0 	 0.803539 	 0.725184 	 0.723070
Epoch 10 	 0.314913 	 0.368107 	 0.234007
Epoch 20 	 0.145221 	 0.224724 	 0.088051
Epoch 30 	 0.089384 	 0.202206 	 0.068842
Epoch 40 	 0.065602 	 0.176930 	 0.048805
Epoch 50 	 0.050781 	 0.170496 	 0.043566
Epoch 60 	 0.044118 	 0.153493 	 0.033824
Epoch 70 	 0.037224 	 0.162224 	 0.037132
Epoch 80 	 0.032629 	 0.149357 	 0.030882
Epoch 90 	 0.032054 	 0.151654 	 0.032537
Epoch 100 	 0.015625 	 0.146599 	 0.029779
Epoch 110 	 0.015395 	 0.141544 	 0.028676
Epoch 120 	 0.009191 	 0.143382 	 0.029044
Epoch 130 	 0.009191 	 0.146140 	 0.029779
[Model stopped early]
Train loss       : 0.009191
Best valid loss  : 0.141544
Best test loss   : 0.029044
Pruning          : 1.00
0.0001
0.0001
[Current model size]
================================
Total params      : 536,917
--------------------------------
Total memory      : 4.72 MB
Total Flops       : 250.6 MFlops
Total Mem (Read)  : 6.09 MB
Total Mem (Write) : 3.54 MB
[Supermasks testing]
[Untrained loss : 0.5473]
[Starting training]
Epoch 0 	 0.200023 	 0.206801 	 0.067463
Epoch 10 	 0.075253 	 0.171875 	 0.039890
Epoch 20 	 0.057675 	 0.167279 	 0.037316
Epoch 30 	 0.040671 	 0.158548 	 0.034559
Epoch 40 	 0.036420 	 0.158088 	 0.033272
Epoch 50 	 0.031365 	 0.156250 	 0.033272
Epoch 60 	 0.024472 	 0.153952 	 0.032353
Epoch 70 	 0.022748 	 0.153493 	 0.031250
Epoch 80 	 0.021714 	 0.157169 	 0.032169
Epoch 90 	 0.023552 	 0.155790 	 0.031985
[Model stopped early]
Train loss       : 0.021599
Best valid loss  : 0.152574
Best test loss   : 0.031618
Pruning          : 0.75
0.0001
0.0001
[Current model size]
================================
Total params      : 397,451
--------------------------------
Total memory      : 4.66 MB
Total Flops       : 219.05 MFlops
Total Mem (Read)  : 5.52 MB
Total Mem (Write) : 3.5 MB
[Supermasks testing]
[Untrained loss : 0.5807]
[Starting training]
Epoch 0 	 0.259651 	 0.227022 	 0.083272
Epoch 10 	 0.095933 	 0.166820 	 0.041728
Epoch 20 	 0.083065 	 0.173254 	 0.044485
Epoch 30 	 0.056870 	 0.163603 	 0.035846
Epoch 40 	 0.048598 	 0.159007 	 0.034375
Epoch 50 	 0.042165 	 0.155790 	 0.033640
Epoch 60 	 0.038718 	 0.158548 	 0.033088
Epoch 70 	 0.033433 	 0.153493 	 0.032537
Epoch 80 	 0.029067 	 0.156250 	 0.032169
Epoch 90 	 0.028148 	 0.153033 	 0.031434
[Model stopped early]
Train loss       : 0.028722
Best valid loss  : 0.151195
Best test loss   : 0.031066
Pruning          : 0.56
0.0001
0.0001
[Current model size]
================================
Total params      : 249,647
--------------------------------
Total memory      : 4.09 MB
Total Flops       : 158.17 MFlops
Total Mem (Read)  : 4.52 MB
Total Mem (Write) : 3.07 MB
[Supermasks testing]
[Untrained loss : 0.6843]
[Starting training]
Epoch 0 	 0.370749 	 0.286305 	 0.153033
Epoch 10 	 0.153148 	 0.189798 	 0.059651
Epoch 20 	 0.117992 	 0.159007 	 0.040625
Epoch 30 	 0.103975 	 0.158548 	 0.039890
Epoch 40 	 0.081112 	 0.157169 	 0.036765
Epoch 50 	 0.070542 	 0.150735 	 0.034743
Epoch 60 	 0.067096 	 0.151654 	 0.035110
Epoch 70 	 0.062385 	 0.150276 	 0.034926
Epoch 80 	 0.060892 	 0.154412 	 0.033272
Epoch 90 	 0.049632 	 0.148897 	 0.031985
[Model stopped early]
Train loss       : 0.049403
Best valid loss  : 0.146599
Best test loss   : 0.032721
Pruning          : 0.42
0.0001
0.0001
[Current model size]
================================
Total params      : 215,375
--------------------------------
Total memory      : 4.08 MB
Total Flops       : 154.16 MFlops
Total Mem (Read)  : 4.38 MB
Total Mem (Write) : 3.06 MB
[Supermasks testing]
[Untrained loss : 0.7918]
[Starting training]
Epoch 0 	 0.466337 	 0.375919 	 0.237684
Epoch 10 	 0.157744 	 0.194393 	 0.059651
Epoch 20 	 0.125115 	 0.182904 	 0.052574
Epoch 30 	 0.115924 	 0.167739 	 0.045221
Epoch 40 	 0.087086 	 0.164062 	 0.040257
Epoch 50 	 0.076517 	 0.167279 	 0.041728
Epoch 60 	 0.078929 	 0.161305 	 0.037500
Epoch 70 	 0.073415 	 0.157629 	 0.036213
Epoch 80 	 0.061926 	 0.161305 	 0.036397
Epoch 90 	 0.051011 	 0.156710 	 0.035478
[Model stopped early]
Train loss       : 0.056296
Best valid loss  : 0.151654
Best test loss   : 0.034559
Pruning          : 0.32
0.0001
0.0001
[Current model size]
================================
Total params      : 189,430
--------------------------------
Total memory      : 3.94 MB
Total Flops       : 145.33 MFlops
Total Mem (Read)  : 4.18 MB
Total Mem (Write) : 2.96 MB
[Supermasks testing]
[Untrained loss : 0.7889]
[Starting training]
Epoch 0 	 0.602367 	 0.488051 	 0.391544
Epoch 10 	 0.188074 	 0.232077 	 0.087684
Epoch 20 	 0.150850 	 0.205423 	 0.065993
Epoch 30 	 0.126264 	 0.187960 	 0.056710
Epoch 40 	 0.112247 	 0.175551 	 0.048346
Epoch 50 	 0.112247 	 0.170037 	 0.044485
Epoch 60 	 0.093865 	 0.166820 	 0.040625
Epoch 70 	 0.076861 	 0.167279 	 0.041360
Epoch 80 	 0.075138 	 0.164982 	 0.038971
Epoch 90 	 0.063304 	 0.163603 	 0.037868
Epoch 100 	 0.064798 	 0.160846 	 0.036765
[Model stopped early]
Train loss       : 0.064338
Best valid loss  : 0.156250
Best test loss   : 0.037684
Pruning          : 0.24
0.0001
0.0001
[Current model size]
================================
Total params      : 149,239
--------------------------------
Total memory      : 3.67 MB
Total Flops       : 124.85 MFlops
Total Mem (Read)  : 3.82 MB
Total Mem (Write) : 2.75 MB
[Supermasks testing]
[Untrained loss : 0.7936]
[Starting training]
Epoch 0 	 0.694968 	 0.673254 	 0.633915
Epoch 10 	 0.254366 	 0.280790 	 0.127941
Epoch 20 	 0.189798 	 0.236673 	 0.087868
Epoch 30 	 0.173369 	 0.211397 	 0.066820
Epoch 40 	 0.152803 	 0.205882 	 0.064430
Epoch 50 	 0.141429 	 0.190257 	 0.053768
Epoch 60 	 0.132353 	 0.194853 	 0.055882
Epoch 70 	 0.104090 	 0.182904 	 0.050000
Epoch 80 	 0.099609 	 0.181985 	 0.047794
Epoch 90 	 0.097771 	 0.173254 	 0.042096
Epoch 100 	 0.093520 	 0.173254 	 0.044485
Epoch 110 	 0.082835 	 0.164982 	 0.039706
Epoch 120 	 0.083525 	 0.168199 	 0.040074
[Model stopped early]
Train loss       : 0.085708
Best valid loss  : 0.164522
Best test loss   : 0.040993
Pruning          : 0.18
0.0001
0.0001
[Current model size]
================================
Total params      : 117,882
--------------------------------
Total memory      : 3.27 MB
Total Flops       : 105.06 MFlops
Total Mem (Read)  : 3.41 MB
Total Mem (Write) : 2.45 MB
[Supermasks testing]
[Untrained loss : 0.7949]
[Starting training]
Epoch 0 	 0.725184 	 0.703585 	 0.675643
Epoch 10 	 0.314683 	 0.331342 	 0.179963
Epoch 20 	 0.249426 	 0.265165 	 0.115809
Epoch 30 	 0.204274 	 0.230239 	 0.086765
Epoch 40 	 0.198185 	 0.205882 	 0.073070
Epoch 50 	 0.176700 	 0.208180 	 0.068658
Epoch 60 	 0.153837 	 0.199449 	 0.062040
Epoch 70 	 0.144761 	 0.188879 	 0.057445
Epoch 80 	 0.142348 	 0.187960 	 0.054228
Epoch 90 	 0.133272 	 0.193474 	 0.058456
Epoch 100 	 0.136259 	 0.180607 	 0.052482
Epoch 110 	 0.131893 	 0.182445 	 0.052849
Epoch 120 	 0.120634 	 0.182904 	 0.051562
Epoch 130 	 0.112707 	 0.176011 	 0.047702
Epoch 140 	 0.107537 	 0.172794 	 0.045956
Epoch 150 	 0.106733 	 0.177390 	 0.049449
Train loss       : 0.108686
Best valid loss  : 0.168658
Best test loss   : 0.043750
Pruning          : 0.13
0.0001
0.0001
[Current model size]
================================
Total params      : 102,411
--------------------------------
Total memory      : 2.89 MB
Total Flops       : 92.46 MFlops
Total Mem (Read)  : 3.07 MB
Total Mem (Write) : 2.17 MB
[Supermasks testing]
[Untrained loss : 0.8754]
[Starting training]
Epoch 0 	 0.803998 	 0.784926 	 0.770129
Epoch 10 	 0.476103 	 0.487132 	 0.378952
Epoch 20 	 0.349609 	 0.358915 	 0.233364
Epoch 30 	 0.318704 	 0.302849 	 0.163051
Epoch 40 	 0.274127 	 0.265625 	 0.131250
Epoch 50 	 0.247243 	 0.256434 	 0.119485
Epoch 60 	 0.229550 	 0.249540 	 0.112868
Epoch 70 	 0.220014 	 0.237592 	 0.104596
Epoch 80 	 0.213580 	 0.237592 	 0.105699
Epoch 90 	 0.202551 	 0.238511 	 0.104044
Epoch 100 	 0.173943 	 0.220588 	 0.090993
Epoch 110 	 0.179917 	 0.218750 	 0.091176
Epoch 120 	 0.174747 	 0.211397 	 0.082721
Epoch 130 	 0.164982 	 0.203585 	 0.076654
Epoch 140 	 0.157399 	 0.207261 	 0.079044
Epoch 150 	 0.154527 	 0.197610 	 0.074449
Train loss       : 0.150161
Best valid loss  : 0.197610
Best test loss   : 0.074449
Pruning          : 0.10
0.0001
0.0001
[Current model size]
================================
Total params      : 101,126
--------------------------------
Total memory      : 2.89 MB
Total Flops       : 92.46 MFlops
Total Mem (Read)  : 3.06 MB
Total Mem (Write) : 2.17 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.853171 	 0.829044 	 0.822702
Epoch 10 	 0.650391 	 0.670496 	 0.613603
Epoch 20 	 0.498736 	 0.486673 	 0.410570
Epoch 30 	 0.424288 	 0.406250 	 0.294853
Epoch 40 	 0.381893 	 0.375460 	 0.261029
Epoch 50 	 0.354435 	 0.352022 	 0.244853
Epoch 60 	 0.347771 	 0.341452 	 0.234007
Epoch 70 	 0.346048 	 0.323529 	 0.218566
Epoch 80 	 0.327091 	 0.322151 	 0.221691
Epoch 90 	 0.322495 	 0.320772 	 0.212316
Epoch 100 	 0.319164 	 0.303309 	 0.194301
Epoch 110 	 0.291475 	 0.294118 	 0.181985
Epoch 120 	 0.294462 	 0.291360 	 0.180147
Epoch 130 	 0.281595 	 0.276654 	 0.170037
Epoch 140 	 0.281365 	 0.267463 	 0.160662
Epoch 150 	 0.273323 	 0.281710 	 0.174265
Train loss       : 0.270795
Best valid loss  : 0.264706
Best test loss   : 0.158640
Pruning          : 0.08
0.0001
0.0001
[Current model size]
================================
Total params      : 86,480
--------------------------------
Total memory      : 2.63 MB
Total Flops       : 81.49 MFlops
Total Mem (Read)  : 2.81 MB
Total Mem (Write) : 1.98 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.845703 	 0.831342 	 0.831250
Epoch 10 	 0.722656 	 0.721507 	 0.690349
Epoch 20 	 0.616039 	 0.599265 	 0.511121
Epoch 30 	 0.543428 	 0.538143 	 0.471691
Epoch 40 	 0.517693 	 0.525276 	 0.463603
Epoch 50 	 0.496553 	 0.504596 	 0.444485
Epoch 60 	 0.492532 	 0.479779 	 0.416912
Epoch 70 	 0.486558 	 0.476562 	 0.405239
Epoch 80 	 0.479779 	 0.475643 	 0.396599
Epoch 90 	 0.453010 	 0.425551 	 0.335202
Epoch 100 	 0.442900 	 0.412684 	 0.325092
Epoch 110 	 0.444049 	 0.409467 	 0.310570
Epoch 120 	 0.441636 	 0.404412 	 0.305423
Epoch 130 	 0.432330 	 0.393382 	 0.297243
Epoch 140 	 0.427619 	 0.394761 	 0.294118
Epoch 150 	 0.432790 	 0.380055 	 0.281250
Train loss       : 0.425781
Best valid loss  : 0.369945
Best test loss   : 0.277941
Pruning          : 0.06
0.0001
0.0001
[Current model size]
================================
Total params      : 59,369
--------------------------------
Total memory      : 2.60 MB
Total Flops       : 68.64 MFlops
Total Mem (Read)  : 2.68 MB
Total Mem (Write) : 1.95 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.855354 	 0.835478 	 0.834467
Epoch 10 	 0.739315 	 0.733456 	 0.697610
Epoch 20 	 0.642348 	 0.666820 	 0.564522
Epoch 30 	 0.576172 	 0.559283 	 0.486489
Epoch 40 	 0.547679 	 0.540901 	 0.474632
Epoch 50 	 0.522748 	 0.531710 	 0.454779
Epoch 60 	 0.514017 	 0.507353 	 0.434926
Epoch 70 	 0.508502 	 0.502298 	 0.428125
Epoch 80 	 0.508732 	 0.492188 	 0.422426
Epoch 90 	 0.499311 	 0.500460 	 0.423162
Epoch 100 	 0.476218 	 0.452665 	 0.358364
Epoch 110 	 0.458984 	 0.442096 	 0.354779
Epoch 120 	 0.461397 	 0.433824 	 0.347702
Epoch 130 	 0.454274 	 0.435202 	 0.347886
Epoch 140 	 0.444049 	 0.427390 	 0.343750
Epoch 150 	 0.438419 	 0.431066 	 0.341360
Train loss       : 0.442210
Best valid loss  : 0.426011
Best test loss   : 0.338787
Pruning          : 0.04
0.0001
0.0001
[Current model size]
================================
Total params      : 39,287
--------------------------------
Total memory      : 2.08 MB
Total Flops       : 50.8 MFlops
Total Mem (Read)  : 2.21 MB
Total Mem (Write) : 1.56 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.855699 	 0.837316 	 0.835938
Epoch 10 	 0.762293 	 0.750460 	 0.725919
Epoch 20 	 0.675551 	 0.681066 	 0.598989
Epoch 30 	 0.606158 	 0.588235 	 0.505882
Epoch 40 	 0.570312 	 0.546415 	 0.471140
Epoch 50 	 0.557904 	 0.534467 	 0.453125
Epoch 60 	 0.539637 	 0.525735 	 0.449081
Epoch 70 	 0.528493 	 0.524816 	 0.448713
Epoch 80 	 0.528263 	 0.510570 	 0.436581
Epoch 90 	 0.518153 	 0.505974 	 0.432169
Epoch 100 	 0.519876 	 0.500460 	 0.428585
Epoch 110 	 0.516314 	 0.501838 	 0.427941
Epoch 120 	 0.518382 	 0.493566 	 0.418658
Epoch 130 	 0.511144 	 0.487132 	 0.410846
Epoch 140 	 0.482996 	 0.459559 	 0.374908
Epoch 150 	 0.491498 	 0.453125 	 0.366912
Train loss       : 0.484605
Best valid loss  : 0.446232
Best test loss   : 0.367096
Pruning          : 0.03
0.0001
0.0001
[Current model size]
================================
Total params      : 20,042
--------------------------------
Total memory      : 1.55 MB
Total Flops       : 33.48 MFlops
Total Mem (Read)  : 1.74 MB
Total Mem (Write) : 1.16 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.854779 	 0.839614 	 0.838787
Epoch 10 	 0.774472 	 0.767004 	 0.750827
Epoch 20 	 0.710708 	 0.729320 	 0.676654
Epoch 30 	 0.655561 	 0.647518 	 0.568290
Epoch 40 	 0.625115 	 0.610294 	 0.526103
Epoch 50 	 0.604435 	 0.589614 	 0.506434
Epoch 60 	 0.598346 	 0.564798 	 0.490074
Epoch 70 	 0.586857 	 0.566636 	 0.488235
Epoch 80 	 0.581687 	 0.544577 	 0.476654
Epoch 90 	 0.572495 	 0.557445 	 0.481618
Epoch 100 	 0.560662 	 0.547335 	 0.472610
Epoch 110 	 0.563189 	 0.541360 	 0.466360
Epoch 120 	 0.558709 	 0.544577 	 0.470037
Epoch 130 	 0.562270 	 0.532169 	 0.462684
Epoch 140 	 0.555836 	 0.535846 	 0.464154
Epoch 150 	 0.555262 	 0.538143 	 0.462684
Train loss       : 0.554573
Best valid loss  : 0.522978
Best test loss   : 0.456434
Pruning          : 0.02
0.0001
0.0001
[Current model size]
================================
Total params      : 13,862
--------------------------------
Total memory      : 1.05 MB
Total Flops       : 22.33 MFlops
Total Mem (Read)  : 1.34 MB
Total Mem (Write) : 803.41 KB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.852252 	 0.844669 	 0.840809
Epoch 10 	 0.786420 	 0.771140 	 0.762132
Epoch 20 	 0.732767 	 0.746324 	 0.708272
Epoch 30 	 0.674862 	 0.662684 	 0.596875
Epoch 40 	 0.639591 	 0.622243 	 0.545037
Epoch 50 	 0.625574 	 0.607996 	 0.536581
Epoch 60 	 0.625115 	 0.598805 	 0.523162
Epoch 70 	 0.616728 	 0.592371 	 0.518750
Epoch 80 	 0.606733 	 0.582721 	 0.500735
Epoch 90 	 0.602597 	 0.585018 	 0.507721
Epoch 100 	 0.596278 	 0.578585 	 0.501103
Epoch 110 	 0.599954 	 0.569853 	 0.499632
Epoch 120 	 0.592486 	 0.572610 	 0.494485
Epoch 130 	 0.590533 	 0.568015 	 0.490625
Epoch 140 	 0.595588 	 0.569393 	 0.495404
Epoch 150 	 0.596967 	 0.568015 	 0.492831
Train loss       : 0.583869
Best valid loss  : 0.558364
Best test loss   : 0.487684
Pruning          : 0.02
