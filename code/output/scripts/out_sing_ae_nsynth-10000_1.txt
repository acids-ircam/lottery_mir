Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40871910.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, six, future, torch, torchvision, tqdm, python-dateutil, pyparsing, cycler, kiwisolver, matplotlib, h5py, keras-applications, protobuf, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, certifi, urllib3, idna, chardet, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, grpcio, absl-py, werkzeug, tensorboard, opt-einsum, tensorflow-estimator, keras-preprocessing, wrapt, termcolor, gast, astor, google-pasta, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871910.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-22 07:19:48.755237: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-22 07:19:49.133017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_sing_ae_cnn_xavier_trimming_batchnorm_reinit_local_0.
*******
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5818]
[Starting training]
Epoch 0 	 0.471433 	 0.429639 	 0.421627
Epoch 10 	 0.212214 	 0.209561 	 0.209132
Epoch 20 	 0.170603 	 0.179988 	 0.177394
Epoch 30 	 0.154122 	 0.165425 	 0.166291
Epoch 40 	 0.144385 	 0.155453 	 0.155199
Epoch 50 	 0.139036 	 0.151945 	 0.151513
Epoch 60 	 0.134954 	 0.150454 	 0.149430
Epoch 70 	 0.136135 	 0.147814 	 0.147641
Epoch 80 	 0.129589 	 0.144719 	 0.146259
Epoch 90 	 0.124828 	 0.138784 	 0.139679
Epoch 100 	 0.123998 	 0.140707 	 0.139466
Epoch 110 	 0.111117 	 0.130935 	 0.131407
Epoch 120 	 0.109788 	 0.130157 	 0.130460
Epoch 130 	 0.109013 	 0.128684 	 0.130490
Epoch 140 	 0.100240 	 0.125446 	 0.125493
Epoch 150 	 0.096973 	 0.123847 	 0.123344
Train loss       : 0.096374
Best valid loss  : 0.121058
Best test loss   : 0.123496
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.6004]
[Starting training]
Epoch 0 	 0.469619 	 0.428683 	 0.427574
Epoch 10 	 0.220225 	 0.221760 	 0.217923
Epoch 20 	 0.171138 	 0.180489 	 0.178567
Epoch 30 	 0.155213 	 0.166386 	 0.164342
Epoch 40 	 0.148804 	 0.158598 	 0.156482
Epoch 50 	 0.141856 	 0.153312 	 0.153672
Epoch 60 	 0.135243 	 0.143768 	 0.145909
Epoch 70 	 0.131239 	 0.146140 	 0.145932
Epoch 80 	 0.116109 	 0.135316 	 0.134407
Epoch 90 	 0.114721 	 0.136114 	 0.133858
Epoch 100 	 0.105712 	 0.129692 	 0.128542
Epoch 110 	 0.104639 	 0.127780 	 0.127834
Epoch 120 	 0.099926 	 0.127408 	 0.125713
Epoch 130 	 0.099097 	 0.126878 	 0.125076
Epoch 140 	 0.096940 	 0.125565 	 0.124190
Epoch 150 	 0.096448 	 0.125795 	 0.124333
Train loss       : 0.095333
Best valid loss  : 0.124069
Best test loss   : 0.124340
Pruning          : 0.78
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5547]
[Starting training]
Epoch 0 	 0.462056 	 0.422556 	 0.411746
Epoch 10 	 0.205555 	 0.204783 	 0.202442
Epoch 20 	 0.172049 	 0.179658 	 0.178229
Epoch 30 	 0.159940 	 0.172323 	 0.170171
Epoch 40 	 0.145781 	 0.158460 	 0.157640
Epoch 50 	 0.141225 	 0.156464 	 0.156561
Epoch 60 	 0.137435 	 0.149458 	 0.149581
Epoch 70 	 0.135193 	 0.148306 	 0.147108
Epoch 80 	 0.117334 	 0.135726 	 0.135987
Epoch 90 	 0.115759 	 0.137125 	 0.135936
Epoch 100 	 0.107344 	 0.129192 	 0.129750
Epoch 110 	 0.102642 	 0.126145 	 0.127050
Epoch 120 	 0.101659 	 0.126098 	 0.126686
Epoch 130 	 0.100905 	 0.125838 	 0.126401
Epoch 140 	 0.098533 	 0.126472 	 0.125304
Epoch 150 	 0.097714 	 0.125519 	 0.125112
[Model stopped early]
Train loss       : 0.096641
Best valid loss  : 0.122909
Best test loss   : 0.126158
Pruning          : 0.61
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5818]
[Starting training]
Epoch 0 	 0.467803 	 0.423716 	 0.417298
Epoch 10 	 0.219608 	 0.220882 	 0.219617
Epoch 20 	 0.169790 	 0.177814 	 0.174853
Epoch 30 	 0.160830 	 0.166639 	 0.166973
Epoch 40 	 0.146898 	 0.164474 	 0.164142
Epoch 50 	 0.140300 	 0.153715 	 0.151491
Epoch 60 	 0.135931 	 0.148811 	 0.149374
Epoch 70 	 0.132159 	 0.147430 	 0.146613
Epoch 80 	 0.129946 	 0.144234 	 0.145792
Epoch 90 	 0.126116 	 0.140486 	 0.144756
Epoch 100 	 0.127265 	 0.144341 	 0.144907
Epoch 110 	 0.112645 	 0.132912 	 0.133457
Epoch 120 	 0.105426 	 0.128572 	 0.129255
Epoch 130 	 0.104126 	 0.128426 	 0.129586
Epoch 140 	 0.103023 	 0.127993 	 0.128579
Epoch 150 	 0.102516 	 0.126649 	 0.128648
Train loss       : 0.098215
Best valid loss  : 0.124311
Best test loss   : 0.126233
Pruning          : 0.47
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5554]
[Starting training]
Epoch 0 	 0.460346 	 0.421943 	 0.413710
Epoch 10 	 0.210028 	 0.213577 	 0.209875
Epoch 20 	 0.174615 	 0.177474 	 0.179285
Epoch 30 	 0.160812 	 0.169480 	 0.170683
Epoch 40 	 0.148298 	 0.156484 	 0.156147
Epoch 50 	 0.138206 	 0.151625 	 0.150390
Epoch 60 	 0.134938 	 0.150065 	 0.149129
Epoch 70 	 0.133619 	 0.146282 	 0.146717
Epoch 80 	 0.130544 	 0.146429 	 0.145287
Epoch 90 	 0.127770 	 0.145295 	 0.142926
Epoch 100 	 0.125465 	 0.143049 	 0.142155
Epoch 110 	 0.111615 	 0.131767 	 0.131418
Epoch 120 	 0.110874 	 0.130399 	 0.130045
Epoch 130 	 0.109642 	 0.130588 	 0.129837
Epoch 140 	 0.101707 	 0.122377 	 0.124727
Epoch 150 	 0.101144 	 0.123581 	 0.124213
Train loss       : 0.097368
Best valid loss  : 0.121694
Best test loss   : 0.122322
Pruning          : 0.37
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5259]
[Starting training]
Epoch 0 	 0.469496 	 0.426663 	 0.417787
Epoch 10 	 0.214382 	 0.214196 	 0.210896
Epoch 20 	 0.172129 	 0.181530 	 0.177412
Epoch 30 	 0.159232 	 0.167373 	 0.167444
Epoch 40 	 0.149387 	 0.163090 	 0.161599
Epoch 50 	 0.140224 	 0.153242 	 0.153055
Epoch 60 	 0.142553 	 0.152741 	 0.152738
Epoch 70 	 0.133401 	 0.146031 	 0.146609
Epoch 80 	 0.130277 	 0.146411 	 0.145152
Epoch 90 	 0.126982 	 0.143365 	 0.142735
Epoch 100 	 0.126978 	 0.138772 	 0.140529
Epoch 110 	 0.127070 	 0.144912 	 0.144715
Epoch 120 	 0.111686 	 0.131519 	 0.132674
Epoch 130 	 0.104281 	 0.127329 	 0.127296
Epoch 140 	 0.101077 	 0.124872 	 0.125490
Epoch 150 	 0.099638 	 0.123359 	 0.125012
Train loss       : 0.097619
Best valid loss  : 0.123353
Best test loss   : 0.125456
Pruning          : 0.29
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5373]
[Starting training]
Epoch 0 	 0.463731 	 0.423124 	 0.416137
Epoch 10 	 0.208532 	 0.210257 	 0.207533
Epoch 20 	 0.172262 	 0.176544 	 0.177450
Epoch 30 	 0.154036 	 0.162861 	 0.162843
Epoch 40 	 0.143144 	 0.154446 	 0.155252
Epoch 50 	 0.139544 	 0.150471 	 0.152082
Epoch 60 	 0.134092 	 0.146379 	 0.148061
Epoch 70 	 0.130137 	 0.144728 	 0.144486
Epoch 80 	 0.127854 	 0.142118 	 0.141852
Epoch 90 	 0.125970 	 0.143466 	 0.142882
Epoch 100 	 0.123058 	 0.140331 	 0.139720
Epoch 110 	 0.123627 	 0.139563 	 0.141240
Epoch 120 	 0.109369 	 0.129563 	 0.130759
Epoch 130 	 0.108759 	 0.128900 	 0.129862
Epoch 140 	 0.107438 	 0.130126 	 0.130563
Epoch 150 	 0.100852 	 0.125819 	 0.125425
Train loss       : 0.100140
Best valid loss  : 0.122922
Best test loss   : 0.124739
Pruning          : 0.23
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.6008]
[Starting training]
Epoch 0 	 0.476331 	 0.439406 	 0.431490
Epoch 10 	 0.214677 	 0.213840 	 0.208554
Epoch 20 	 0.175088 	 0.176441 	 0.178442
Epoch 30 	 0.154002 	 0.163696 	 0.163533
Epoch 40 	 0.144455 	 0.154889 	 0.156065
Epoch 50 	 0.138135 	 0.149891 	 0.152093
Epoch 60 	 0.132063 	 0.146770 	 0.148614
Epoch 70 	 0.131891 	 0.146410 	 0.148295
Epoch 80 	 0.128291 	 0.143438 	 0.143668
Epoch 90 	 0.126310 	 0.144667 	 0.142387
Epoch 100 	 0.112404 	 0.133358 	 0.132930
Epoch 110 	 0.112530 	 0.132203 	 0.133359
Epoch 120 	 0.104465 	 0.130453 	 0.129254
Epoch 130 	 0.103276 	 0.129320 	 0.128981
Epoch 140 	 0.099115 	 0.125232 	 0.125919
Epoch 150 	 0.098524 	 0.126431 	 0.125786
Train loss       : 0.096309
Best valid loss  : 0.123616
Best test loss   : 0.125030
Pruning          : 0.18
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5417]
[Starting training]
Epoch 0 	 0.464789 	 0.428268 	 0.415669
Epoch 10 	 0.207787 	 0.207865 	 0.207228
Epoch 20 	 0.169226 	 0.179293 	 0.179992
Epoch 30 	 0.155290 	 0.164273 	 0.163883
Epoch 40 	 0.144641 	 0.156781 	 0.157575
Epoch 50 	 0.140178 	 0.150556 	 0.152348
Epoch 60 	 0.135888 	 0.148061 	 0.147645
Epoch 70 	 0.132523 	 0.149699 	 0.151305
Epoch 80 	 0.129915 	 0.145723 	 0.143864
Epoch 90 	 0.128373 	 0.145357 	 0.144924
Epoch 100 	 0.126785 	 0.140425 	 0.140771
Epoch 110 	 0.124321 	 0.141533 	 0.139768
Epoch 120 	 0.110268 	 0.128793 	 0.129708
Epoch 130 	 0.110429 	 0.130636 	 0.130719
Epoch 140 	 0.102633 	 0.125957 	 0.126151
Epoch 150 	 0.102007 	 0.126173 	 0.126599
Train loss       : 0.098594
Best valid loss  : 0.121548
Best test loss   : 0.124040
Pruning          : 0.14
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5880]
[Starting training]
Epoch 0 	 0.459495 	 0.421253 	 0.412413
Epoch 10 	 0.205374 	 0.212839 	 0.207139
Epoch 20 	 0.166749 	 0.175178 	 0.174889
Epoch 30 	 0.154119 	 0.158770 	 0.157607
Epoch 40 	 0.142676 	 0.152383 	 0.153168
Epoch 50 	 0.136652 	 0.149906 	 0.149087
Epoch 60 	 0.132365 	 0.145315 	 0.146940
Epoch 70 	 0.130191 	 0.146316 	 0.145647
Epoch 80 	 0.127255 	 0.142062 	 0.142003
Epoch 90 	 0.113721 	 0.134239 	 0.135052
Epoch 100 	 0.112330 	 0.133603 	 0.133742
Epoch 110 	 0.110344 	 0.132346 	 0.133413
Epoch 120 	 0.109240 	 0.133305 	 0.131917
Epoch 130 	 0.100908 	 0.127439 	 0.126430
Epoch 140 	 0.097277 	 0.123879 	 0.124787
Epoch 150 	 0.096224 	 0.123623 	 0.124689
Train loss       : 0.095716
Best valid loss  : 0.122150
Best test loss   : 0.124707
Pruning          : 0.11
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.6049]
[Starting training]
Epoch 0 	 0.478641 	 0.428667 	 0.419189
Epoch 10 	 0.210571 	 0.223559 	 0.217449
Epoch 20 	 0.178241 	 0.183254 	 0.181785
Epoch 30 	 0.161818 	 0.171566 	 0.170708
Epoch 40 	 0.155008 	 0.165696 	 0.166580
Epoch 50 	 0.149495 	 0.162097 	 0.161505
Epoch 60 	 0.145231 	 0.153993 	 0.156938
Epoch 70 	 0.139872 	 0.151862 	 0.152203
Epoch 80 	 0.138671 	 0.146993 	 0.150014
Epoch 90 	 0.134923 	 0.148604 	 0.150341
Epoch 100 	 0.131579 	 0.148737 	 0.147869
Epoch 110 	 0.131989 	 0.147316 	 0.146936
Epoch 120 	 0.115700 	 0.132314 	 0.134654
Epoch 130 	 0.114778 	 0.133325 	 0.134446
Epoch 140 	 0.108942 	 0.130123 	 0.130548
Epoch 150 	 0.106174 	 0.128883 	 0.130436
Train loss       : 0.102609
Best valid loss  : 0.125723
Best test loss   : 0.128303
Pruning          : 0.08
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5681]
[Starting training]
Epoch 0 	 0.469689 	 0.423129 	 0.413180
Epoch 10 	 0.220037 	 0.216465 	 0.213105
Epoch 20 	 0.176555 	 0.184662 	 0.181633
Epoch 30 	 0.161463 	 0.168218 	 0.172565
Epoch 40 	 0.149874 	 0.162838 	 0.161729
Epoch 50 	 0.145007 	 0.153688 	 0.154760
Epoch 60 	 0.140269 	 0.153150 	 0.154930
Epoch 70 	 0.137280 	 0.150135 	 0.149500
Epoch 80 	 0.132586 	 0.148157 	 0.150437
Epoch 90 	 0.129173 	 0.146049 	 0.144851
Epoch 100 	 0.128870 	 0.144216 	 0.144147
Epoch 110 	 0.114168 	 0.134737 	 0.134795
Epoch 120 	 0.113415 	 0.133591 	 0.134063
Epoch 130 	 0.105895 	 0.129002 	 0.129181
Epoch 140 	 0.104895 	 0.128265 	 0.129200
Epoch 150 	 0.101354 	 0.125600 	 0.126584
Train loss       : 0.100004
Best valid loss  : 0.124090
Best test loss   : 0.126646
Pruning          : 0.07
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5366]
[Starting training]
Epoch 0 	 0.458475 	 0.418559 	 0.411508
Epoch 10 	 0.209611 	 0.218171 	 0.215136
Epoch 20 	 0.170974 	 0.178123 	 0.174945
Epoch 30 	 0.155827 	 0.164924 	 0.165685
Epoch 40 	 0.145414 	 0.154947 	 0.154774
Epoch 50 	 0.138637 	 0.151612 	 0.151918
Epoch 60 	 0.133449 	 0.145256 	 0.145110
Epoch 70 	 0.127888 	 0.143412 	 0.143510
Epoch 80 	 0.127723 	 0.142122 	 0.142185
Epoch 90 	 0.123305 	 0.135665 	 0.137885
Epoch 100 	 0.121106 	 0.135300 	 0.136819
Epoch 110 	 0.119532 	 0.138106 	 0.136628
Epoch 120 	 0.105494 	 0.125264 	 0.125370
Epoch 130 	 0.100685 	 0.120264 	 0.122408
Epoch 140 	 0.098205 	 0.121998 	 0.122255
Epoch 150 	 0.097463 	 0.120436 	 0.121978
Train loss       : 0.093620
Best valid loss  : 0.116800
Best test loss   : 0.120082
Pruning          : 0.05
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5303]
[Starting training]
Epoch 0 	 0.464488 	 0.421280 	 0.411397
Epoch 10 	 0.221018 	 0.218767 	 0.213645
Epoch 20 	 0.173349 	 0.178795 	 0.178034
Epoch 30 	 0.155033 	 0.161252 	 0.162461
Epoch 40 	 0.149093 	 0.159211 	 0.157974
Epoch 50 	 0.138374 	 0.150347 	 0.149640
Epoch 60 	 0.138804 	 0.152182 	 0.152200
Epoch 70 	 0.133154 	 0.147267 	 0.145205
Epoch 80 	 0.131826 	 0.145432 	 0.143748
Epoch 90 	 0.115545 	 0.133422 	 0.133251
Epoch 100 	 0.114137 	 0.136307 	 0.134541
Epoch 110 	 0.106448 	 0.128454 	 0.128966
Epoch 120 	 0.104968 	 0.128074 	 0.128018
Epoch 130 	 0.100935 	 0.125648 	 0.126248
Epoch 140 	 0.100294 	 0.127843 	 0.125527
Epoch 150 	 0.097820 	 0.124139 	 0.124466
Train loss       : 0.097218
Best valid loss  : 0.123410
Best test loss   : 0.125773
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.6010]
[Starting training]
Epoch 0 	 0.462886 	 0.427339 	 0.418677
Epoch 10 	 0.208455 	 0.212705 	 0.210672
Epoch 20 	 0.170016 	 0.180767 	 0.178030
Epoch 30 	 0.156588 	 0.162333 	 0.163803
Epoch 40 	 0.147217 	 0.155470 	 0.157040
Epoch 50 	 0.138614 	 0.150889 	 0.151487
Epoch 60 	 0.135598 	 0.146279 	 0.148593
Epoch 70 	 0.131948 	 0.145176 	 0.147198
Epoch 80 	 0.129044 	 0.145792 	 0.144535
Epoch 90 	 0.128276 	 0.141362 	 0.140864
Epoch 100 	 0.123860 	 0.138951 	 0.141116
Epoch 110 	 0.122541 	 0.139416 	 0.138651
Epoch 120 	 0.109856 	 0.130791 	 0.131586
Epoch 130 	 0.108672 	 0.130743 	 0.130483
Epoch 140 	 0.107648 	 0.129972 	 0.130430
Epoch 150 	 0.099109 	 0.123517 	 0.123902
Train loss       : 0.098616
Best valid loss  : 0.123298
Best test loss   : 0.123974
Pruning          : 0.03
