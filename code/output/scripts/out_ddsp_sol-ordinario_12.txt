Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41281317.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, cycler, python-dateutil, pyparsing, kiwisolver, matplotlib, astor, h5py, keras-applications, opt-einsum, keras-preprocessing, gast, absl-py, tensorflow-estimator, termcolor, protobuf, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, urllib3, chardet, idna, certifi, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, grpcio, werkzeug, tensorboard, google-pasta, wrapt, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281317.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-29 02:24:46.194021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 02:24:46.514708: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is sol-ordinario_ddsp_cnn_xavier_trimming_magnitude_rewind_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41281317.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 97.5244]
[Starting training]
Epoch 0 	 89.896446 	 83.494102 	 89.726784
Epoch 10 	 77.932549 	 78.878021 	 79.627380
Epoch 20 	 71.514282 	 72.280952 	 77.201576
Epoch 30 	 68.301102 	 66.585960 	 71.471519
Epoch 40 	 64.181152 	 63.140137 	 64.494514
Epoch 50 	 61.137344 	 58.468651 	 85.970352
Epoch 60 	 55.643757 	 54.582153 	 55.747353
Epoch 70 	 48.294853 	 48.376354 	 49.513638
Epoch 80 	 43.972008 	 40.833420 	 43.791187
Epoch 90 	 44.073803 	 42.036606 	 44.275379
Epoch 100 	 39.504921 	 38.696400 	 40.034023
Epoch 110 	 36.266327 	 35.785477 	 37.044418
Epoch 120 	 34.365582 	 34.796371 	 35.745342
Epoch 130 	 32.477501 	 33.510796 	 35.191940
Epoch 140 	 31.360086 	 33.835140 	 37.121628
Epoch 150 	 29.932148 	 33.952118 	 35.929150
Epoch 160 	 26.900724 	 29.478119 	 31.895582
Epoch 170 	 26.263405 	 29.311085 	 31.310614
Epoch 180 	 25.534594 	 28.551338 	 30.785049
Epoch 190 	 25.240631 	 28.518064 	 30.508459
Train loss       : 25.159306
Best valid loss  : 27.413349
Best test loss   : 30.346552
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 2,909,528
--------------------------------
Total memory      : 35.03 MB
Total Flops       : 326.2 MFlops
Total Mem (Read)  : 31.67 MB
Total Mem (Write) : 24.49 MB
[Supermasks testing]
[Untrained loss : 126.1566]
[Starting training]
Epoch 0 	 65.657928 	 53.533192 	 57.141193
Epoch 10 	 37.905918 	 36.851089 	 37.931587
Epoch 20 	 34.987160 	 34.860638 	 35.841377
Epoch 30 	 32.600601 	 33.764248 	 34.653950
Epoch 40 	 30.842667 	 31.529097 	 33.680946
Epoch 50 	 28.718327 	 30.133162 	 31.788771
Epoch 60 	 31.114658 	 39.972145 	 44.432922
Epoch 70 	 27.000570 	 29.586771 	 30.920538
Epoch 80 	 26.551792 	 28.880465 	 30.404972
Epoch 90 	 26.141531 	 28.320591 	 29.709845
Epoch 100 	 24.219740 	 26.763395 	 28.660080
Epoch 110 	 23.948078 	 27.027985 	 28.618734
Epoch 120 	 23.882683 	 27.311249 	 28.721144
Epoch 130 	 23.047901 	 26.409924 	 28.063187
Epoch 140 	 22.743362 	 26.714207 	 28.030706
Epoch 150 	 22.542337 	 26.088364 	 27.885870
Epoch 160 	 22.382782 	 26.265038 	 27.924992
Epoch 170 	 21.789698 	 26.366611 	 27.721258
Epoch 180 	 21.816942 	 25.959963 	 27.361984
Epoch 190 	 21.527542 	 25.687069 	 27.275862
Train loss       : 21.450413
Best valid loss  : 25.193899
Best test loss   : 27.402628
Pruning          : 0.72
0.001
0.001
[Current model size]
================================
Total params      : 1,898,752
--------------------------------
Total memory      : 29.65 MB
Total Flops       : 171.22 MFlops
Total Mem (Read)  : 22.92 MB
Total Mem (Write) : 17.34 MB
[Supermasks testing]
[Untrained loss : 88.7589]
[Starting training]
Epoch 0 	 66.630226 	 57.165264 	 60.951244
Epoch 10 	 40.474083 	 40.318005 	 41.203522
Epoch 20 	 35.994804 	 36.223797 	 37.707058
Epoch 30 	 34.330528 	 35.784416 	 36.748821
Epoch 40 	 32.448017 	 32.473778 	 34.403362
Epoch 50 	 30.013968 	 30.641094 	 32.943199
Epoch 60 	 29.220987 	 30.674850 	 32.487011
Epoch 70 	 28.151089 	 28.966991 	 31.112366
Epoch 80 	 27.528082 	 29.814253 	 31.820906
Epoch 90 	 25.499889 	 27.246979 	 29.329559
Epoch 100 	 25.028952 	 27.777584 	 29.632187
Epoch 110 	 24.610641 	 27.002129 	 28.812948
Epoch 120 	 24.456682 	 27.333906 	 29.070271
Epoch 130 	 24.131544 	 27.097197 	 28.491922
Epoch 140 	 23.576908 	 26.975166 	 28.193823
Epoch 150 	 23.002869 	 26.377789 	 27.916265
Epoch 160 	 22.612034 	 26.302635 	 27.787573
Epoch 170 	 22.477484 	 25.847988 	 27.566328
Epoch 180 	 22.384836 	 25.931690 	 27.653345
Epoch 190 	 22.189631 	 26.083904 	 27.472504
Train loss       : 22.221231
Best valid loss  : 25.093819
Best test loss   : 27.478962
Pruning          : 0.52
0.001
0.001
[Current model size]
================================
Total params      : 1,282,494
--------------------------------
Total memory      : 25.75 MB
Total Flops       : 89.46 MFlops
Total Mem (Read)  : 17.01 MB
Total Mem (Write) : 12.62 MB
[Supermasks testing]
[Untrained loss : 93.3387]
[Starting training]
Epoch 0 	 70.458138 	 57.362179 	 60.707485
Epoch 10 	 42.659355 	 39.067562 	 40.118114
Epoch 20 	 39.514690 	 37.748356 	 42.950268
Epoch 30 	 36.227936 	 37.450314 	 40.856117
Epoch 40 	 31.936924 	 32.023628 	 34.380905
Epoch 50 	 31.442017 	 31.364002 	 33.574108
Epoch 60 	 30.339418 	 29.939478 	 31.970352
Epoch 70 	 29.598314 	 30.161085 	 32.360043
Epoch 80 	 29.308180 	 31.704073 	 34.382824
Epoch 90 	 27.783224 	 29.791487 	 31.704252
Epoch 100 	 27.335602 	 29.053717 	 31.615965
Epoch 110 	 25.345810 	 27.349884 	 29.378702
Epoch 120 	 25.362648 	 26.880636 	 29.338348
Epoch 130 	 24.488665 	 26.603867 	 28.950027
Epoch 140 	 23.825285 	 26.451672 	 28.491587
Epoch 150 	 23.679256 	 26.083971 	 28.441685
Epoch 160 	 23.318808 	 25.707424 	 28.158371
Epoch 170 	 23.540737 	 25.766788 	 28.105890
Epoch 180 	 23.330311 	 25.484756 	 28.054708
Epoch 190 	 23.147608 	 25.597334 	 27.922367
Train loss       : 23.109869
Best valid loss  : 25.057209
Best test loss   : 27.938740
Pruning          : 0.37
0.001
0.001
[Current model size]
================================
Total params      : 887,398
--------------------------------
Total memory      : 22.89 MB
Total Flops       : 46.15 MFlops
Total Mem (Read)  : 12.89 MB
Total Mem (Write) : 9.4 MB
[Supermasks testing]
[Untrained loss : 109.8231]
[Starting training]
Epoch 0 	 78.885902 	 63.878490 	 69.568871
Epoch 10 	 44.106247 	 42.176682 	 43.879696
Epoch 20 	 42.642780 	 39.734253 	 40.914394
Epoch 30 	 36.451801 	 33.378246 	 35.883263
Epoch 40 	 33.989719 	 33.361004 	 35.033062
Epoch 50 	 31.703344 	 32.152428 	 33.980755
Epoch 60 	 30.103964 	 30.539667 	 32.224056
Epoch 70 	 28.702028 	 29.575293 	 31.507233
Epoch 80 	 28.341700 	 29.163338 	 31.629349
Epoch 90 	 27.291542 	 28.612932 	 30.934088
Epoch 100 	 26.757702 	 29.036303 	 30.929035
Epoch 110 	 26.516800 	 27.986652 	 30.152472
Epoch 120 	 25.816719 	 27.565681 	 29.470295
Epoch 130 	 25.317038 	 27.229507 	 29.347731
Epoch 140 	 25.310564 	 27.736755 	 29.326284
Epoch 150 	 25.232748 	 27.088924 	 28.996077
Epoch 160 	 25.039812 	 27.815151 	 29.254990
Epoch 170 	 24.428587 	 27.334021 	 28.818167
Epoch 180 	 24.446882 	 26.486206 	 28.717798
Epoch 190 	 23.936235 	 26.512575 	 28.653669
[Model stopped early]
Train loss       : 24.084351
Best valid loss  : 26.441870
Best test loss   : 28.803827
Pruning          : 0.27
0.001
0.001
[Current model size]
================================
Total params      : 630,951
--------------------------------
Total memory      : 20.84 MB
Total Flops       : 23.99 MFlops
Total Mem (Read)  : 10.04 MB
Total Mem (Write) : 7.21 MB
[Supermasks testing]
[Untrained loss : 129.6736]
[Starting training]
Epoch 0 	 85.292564 	 70.800461 	 76.024437
Epoch 10 	 46.879700 	 44.458164 	 45.089077
Epoch 20 	 40.820473 	 38.561481 	 40.684460
Epoch 30 	 38.973972 	 37.178955 	 38.570141
Epoch 40 	 35.967983 	 36.823208 	 37.780972
Epoch 50 	 34.642300 	 35.437180 	 36.037197
Epoch 60 	 33.746609 	 35.442661 	 36.793255
Epoch 70 	 31.143200 	 32.070267 	 33.564472
Epoch 80 	 29.894098 	 31.038422 	 32.734341
Epoch 90 	 29.407135 	 30.423899 	 32.092403
Epoch 100 	 28.956566 	 30.987478 	 32.841221
Epoch 110 	 27.646206 	 29.675795 	 31.794424
Epoch 120 	 27.763216 	 30.236481 	 31.900543
Epoch 130 	 26.184801 	 28.311968 	 29.660816
Epoch 140 	 25.503910 	 27.709732 	 29.489973
Epoch 150 	 24.478861 	 27.532124 	 29.264105
Epoch 160 	 24.654501 	 27.404305 	 28.818369
Epoch 170 	 24.681337 	 27.478304 	 28.603498
Epoch 180 	 24.338854 	 27.376797 	 28.611225
Epoch 190 	 24.584356 	 27.290558 	 28.582224
Train loss       : 24.142727
Best valid loss  : 26.798691
Best test loss   : 28.543417
Pruning          : 0.19
0.001
0.001
[Current model size]
================================
Total params      : 461,817
--------------------------------
Total memory      : 19.39 MB
Total Flops       : 12.83 MFlops
Total Mem (Read)  : 8.07 MB
Total Mem (Write) : 5.74 MB
[Supermasks testing]
[Untrained loss : 157.9952]
[Starting training]
Epoch 0 	 89.866180 	 79.613693 	 82.326744
Epoch 10 	 49.014027 	 50.180809 	 51.805080
Epoch 20 	 43.372856 	 40.092392 	 41.620998
Epoch 30 	 42.161407 	 38.078846 	 39.720646
Epoch 40 	 39.885441 	 41.042126 	 42.406116
Epoch 50 	 37.937836 	 35.478794 	 37.571266
Epoch 60 	 35.643917 	 34.734646 	 36.981377
Epoch 70 	 33.224174 	 33.487759 	 35.542831
Epoch 80 	 33.608246 	 33.494949 	 35.765701
Epoch 90 	 32.639576 	 31.465887 	 33.968079
Epoch 100 	 31.893774 	 32.181198 	 34.216442
Epoch 110 	 28.699183 	 29.797258 	 32.910412
Epoch 120 	 28.424654 	 29.412170 	 32.248249
Epoch 130 	 28.639219 	 29.727037 	 32.721684
Epoch 140 	 28.230602 	 29.830107 	 32.496483
Epoch 150 	 27.723743 	 28.534309 	 31.980061
Epoch 160 	 27.526304 	 29.175472 	 32.083153
Epoch 170 	 27.114733 	 27.709717 	 31.297934
Epoch 180 	 26.925880 	 28.349524 	 31.509510
Epoch 190 	 26.928514 	 27.875637 	 31.457209
Train loss       : 26.737246
Best valid loss  : 27.709717
Best test loss   : 31.297934
Pruning          : 0.14
0.001
0.001
[Current model size]
================================
Total params      : 348,567
--------------------------------
Total memory      : 18.35 MB
Total Flops       : 7.06 MFlops
Total Mem (Read)  : 6.7 MB
Total Mem (Write) : 4.71 MB
[Supermasks testing]
[Untrained loss : 202.6266]
[Starting training]
Epoch 0 	 96.214714 	 83.045631 	 86.696411
Epoch 10 	 52.062222 	 48.004082 	 50.730324
Epoch 20 	 45.223846 	 40.789883 	 42.787033
Epoch 30 	 43.768475 	 41.093639 	 42.486546
Epoch 40 	 42.495636 	 38.154037 	 41.135300
Epoch 50 	 38.475430 	 39.340988 	 41.796661
Epoch 60 	 37.392181 	 39.431435 	 42.025208
Epoch 70 	 35.533707 	 36.292690 	 38.227306
Epoch 80 	 34.786678 	 34.147339 	 37.109222
Epoch 90 	 32.849144 	 33.283611 	 35.021900
Epoch 100 	 33.020107 	 33.196239 	 34.718369
Epoch 110 	 32.364887 	 32.633003 	 34.287243
Epoch 120 	 33.189018 	 32.849304 	 34.397995
Epoch 130 	 32.585480 	 31.971874 	 34.376209
Epoch 140 	 32.228935 	 32.217960 	 34.510544
Epoch 150 	 31.280130 	 32.524277 	 34.172817
[Model stopped early]
Train loss       : 31.785875
Best valid loss  : 31.497744
Best test loss   : 34.085430
Pruning          : 0.10
0.001
0.001
[Current model size]
================================
Total params      : 272,819
--------------------------------
Total memory      : 17.56 MB
Total Flops       : 3.79 MFlops
Total Mem (Read)  : 5.68 MB
Total Mem (Write) : 3.94 MB
[Supermasks testing]
[Untrained loss : 182.1528]
[Starting training]
Epoch 0 	 93.499992 	 84.259720 	 87.695976
Epoch 10 	 55.319538 	 51.606628 	 54.369686
Epoch 20 	 49.290581 	 44.905563 	 47.173832
Epoch 30 	 44.779919 	 40.530411 	 43.750393
Epoch 40 	 42.887875 	 41.354149 	 43.165859
Epoch 50 	 42.450111 	 40.667133 	 42.268963
Epoch 60 	 40.775311 	 38.484104 	 40.368816
Epoch 70 	 39.769176 	 38.943207 	 41.011078
Epoch 80 	 36.885738 	 36.663395 	 37.662350
Epoch 90 	 34.140514 	 34.831047 	 36.432461
Epoch 100 	 33.045368 	 35.263218 	 37.926517
Epoch 110 	 31.902338 	 33.471138 	 35.170593
Epoch 120 	 31.355015 	 31.970226 	 33.980724
Epoch 130 	 31.713436 	 32.342808 	 34.582943
Epoch 140 	 31.048603 	 32.050838 	 33.805069
Epoch 150 	 29.533451 	 31.359097 	 33.030315
Epoch 160 	 29.773075 	 31.471458 	 33.123550
Epoch 170 	 30.007160 	 30.414202 	 32.850441
Epoch 180 	 29.409599 	 31.838060 	 33.510159
Epoch 190 	 29.313969 	 31.107487 	 32.632355
Train loss       : 29.417124
Best valid loss  : 29.976938
Best test loss   : 32.385845
Pruning          : 0.07
0.001
0.001
[Current model size]
================================
Total params      : 220,983
--------------------------------
Total memory      : 17.11 MB
Total Flops       : 2.56 MFlops
Total Mem (Read)  : 5.08 MB
Total Mem (Write) : 3.52 MB
[Supermasks testing]
[Untrained loss : 208.6067]
[Starting training]
Epoch 0 	 98.726448 	 88.097046 	 91.597008
Epoch 10 	 60.513489 	 59.889351 	 63.886482
Epoch 20 	 56.202507 	 59.039608 	 63.590950
Epoch 30 	 53.011959 	 48.157516 	 51.492886
Epoch 40 	 49.978859 	 45.582115 	 50.215382
Epoch 50 	 47.812153 	 44.083817 	 47.843239
Epoch 60 	 44.946136 	 42.638817 	 45.342548
Epoch 70 	 45.408474 	 41.762539 	 44.109989
Epoch 80 	 45.981495 	 40.878120 	 43.804115
Epoch 90 	 43.664421 	 45.385925 	 47.287624
Epoch 100 	 44.205513 	 43.145424 	 45.771412
Epoch 110 	 39.899712 	 39.482494 	 41.346935
Epoch 120 	 41.557690 	 39.336601 	 40.338955
Epoch 130 	 38.403419 	 38.147781 	 40.258301
Epoch 140 	 38.386993 	 39.086815 	 40.931206
Epoch 150 	 38.123589 	 36.547062 	 39.169434
Epoch 160 	 37.252495 	 36.673855 	 38.517483
Epoch 170 	 36.938519 	 36.602833 	 38.534420
Epoch 180 	 36.390732 	 35.713795 	 37.648174
Epoch 190 	 35.920315 	 36.274704 	 37.889339
Train loss       : 35.618580
Best valid loss  : 35.695530
Best test loss   : 37.559921
Pruning          : 0.05
0.001
0.001
[Current model size]
================================
Total params      : 185,398
--------------------------------
Total memory      : 16.72 MB
Total Flops       : 1.66 MFlops
Total Mem (Read)  : 4.58 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 182.5882]
[Starting training]
Epoch 0 	 95.831703 	 87.377861 	 90.070488
Epoch 10 	 62.923779 	 57.352158 	 63.287666
Epoch 20 	 59.160271 	 54.902676 	 59.780209
Epoch 30 	 58.404293 	 58.261589 	 62.478737
Epoch 40 	 56.798138 	 52.933708 	 58.068523
Epoch 50 	 55.410046 	 53.156517 	 56.642311
Epoch 60 	 51.152180 	 49.344227 	 52.155251
Epoch 70 	 48.232082 	 48.925770 	 50.193062
Epoch 80 	 48.588921 	 49.051491 	 50.689358
Epoch 90 	 45.080578 	 45.507366 	 46.493568
Epoch 100 	 45.839764 	 44.313480 	 45.879391
Epoch 110 	 44.179905 	 43.898582 	 45.545410
Epoch 120 	 44.904232 	 42.600655 	 44.191563
Epoch 130 	 44.281719 	 42.052574 	 44.365013
Epoch 140 	 42.450085 	 41.608711 	 43.001415
Epoch 150 	 43.228687 	 40.365501 	 43.297962
Epoch 160 	 41.704479 	 41.193001 	 42.378368
Epoch 170 	 42.495472 	 40.847080 	 43.124722
Epoch 180 	 41.414967 	 40.733238 	 42.006920
Epoch 190 	 42.172459 	 40.255188 	 41.705048
Train loss       : 40.067059
Best valid loss  : 39.765293
Best test loss   : 41.761539
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 156,046
--------------------------------
Total memory      : 16.63 MB
Total Flops       : 1.63 MFlops
Total Mem (Read)  : 4.41 MB
Total Mem (Write) : 3.08 MB
[Supermasks testing]
[Untrained loss : 155.7538]
[Starting training]
Epoch 0 	 95.145805 	 88.161560 	 91.593590
Epoch 10 	 63.373966 	 58.134571 	 63.811043
Epoch 20 	 60.937485 	 56.420250 	 60.734783
Epoch 30 	 58.208084 	 55.983692 	 61.309090
Epoch 40 	 55.912632 	 52.232491 	 56.206837
Epoch 50 	 52.522518 	 49.604137 	 52.770515
Epoch 60 	 50.491314 	 48.331318 	 52.177864
Epoch 70 	 47.914330 	 46.943211 	 49.677353
Epoch 80 	 47.609932 	 45.195789 	 47.278519
Epoch 90 	 45.435436 	 44.024963 	 45.645802
Epoch 100 	 45.623684 	 42.894203 	 44.576519
Epoch 110 	 44.186581 	 43.361431 	 44.856869
Epoch 120 	 42.414303 	 40.787312 	 42.996487
Epoch 130 	 42.412643 	 41.207775 	 42.838135
Epoch 140 	 41.650864 	 40.861576 	 42.632229
Epoch 150 	 41.349419 	 41.203609 	 42.386486
Epoch 160 	 41.696743 	 40.330437 	 42.538956
Epoch 170 	 41.238636 	 40.445911 	 41.918869
Epoch 180 	 40.961376 	 39.682312 	 42.039364
Epoch 190 	 41.156315 	 39.592308 	 42.009296
Train loss       : 40.338814
Best valid loss  : 39.293705
Best test loss   : 41.815193
Pruning          : 0.03
Traceback (most recent call last):
  File "main.py", line 261, in <module>
    if (args.prune_selection in ['activation', 'information', 'info_target']):
  File "/scratch/esling/lottery/pruning.py", line 781, in reset
    replace_recurrent(m, l, m.unprune_idx[l], prev_kept)
  File "/scratch/esling/lottery/pruning.py", line 752, in replace_recurrent
    cur_ih = nn.Parameter(cur_ih[rep_id0])#torch.from_numpy(cur_ih[rep_id0]).to(self.args.device))
IndexError: too many indices for tensor of dimension 2
