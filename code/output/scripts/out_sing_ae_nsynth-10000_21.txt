Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40871933.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, pillow-simd, six, torchvision, tqdm, python-dateutil, kiwisolver, cycler, pyparsing, matplotlib, gast, absl-py, keras-preprocessing, tensorflow-estimator, h5py, keras-applications, protobuf, idna, urllib3, certifi, chardet, requests, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, werkzeug, markdown, tensorboard, astor, google-pasta, termcolor, opt-einsum, wrapt, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871933.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-22 07:33:53.033304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-22 07:33:53.372045: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_sing_ae_cnn_xavier_trimming_activation_rewind_global_0.
*******
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.6126]
[Starting training]
Epoch 0 	 0.462086 	 0.443322 	 0.435401
Epoch 10 	 0.207971 	 0.228635 	 0.218918
Epoch 20 	 0.169817 	 0.197358 	 0.185385
Epoch 30 	 0.154040 	 0.180012 	 0.171316
Epoch 40 	 0.144573 	 0.171520 	 0.163417
Epoch 50 	 0.138415 	 0.170109 	 0.159007
Epoch 60 	 0.133753 	 0.165520 	 0.159043
Epoch 70 	 0.131769 	 0.160331 	 0.151007
Epoch 80 	 0.129219 	 0.163774 	 0.153013
Epoch 90 	 0.115658 	 0.149896 	 0.140630
Epoch 100 	 0.113332 	 0.153254 	 0.140352
Epoch 110 	 0.104830 	 0.145220 	 0.134119
Epoch 120 	 0.100787 	 0.145385 	 0.132316
Epoch 130 	 0.100242 	 0.144739 	 0.131858
Epoch 140 	 0.099529 	 0.143660 	 0.131801
Epoch 150 	 0.096859 	 0.143520 	 0.130758
Train loss       : 0.095916
Best valid loss  : 0.140118
Best test loss   : 0.131891
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 14,407,208
--------------------------------
Total memory      : 18.11 MB
Total Flops       : 3.12 GFlops
Total Mem (Read)  : 65.2 MB
Total Mem (Write) : 17.88 MB
[Supermasks testing]
[Untrained loss : 0.1538]
[Starting training]
Epoch 0 	 0.133104 	 0.165605 	 0.154887
Epoch 10 	 0.128698 	 0.162011 	 0.151520
Epoch 20 	 0.113771 	 0.151939 	 0.140138
Epoch 30 	 0.111958 	 0.151136 	 0.140279
Epoch 40 	 0.104437 	 0.146934 	 0.133901
Epoch 50 	 0.103208 	 0.146439 	 0.133863
Epoch 60 	 0.099172 	 0.143795 	 0.131025
Epoch 70 	 0.098628 	 0.143583 	 0.130972
Epoch 80 	 0.096436 	 0.140498 	 0.129864
Epoch 90 	 0.095937 	 0.142530 	 0.129801
Epoch 100 	 0.094861 	 0.142918 	 0.129139
Epoch 110 	 0.094387 	 0.139359 	 0.128913
[Model stopped early]
Train loss       : 0.094220
Best valid loss  : 0.138284
Best test loss   : 0.129709
Pruning          : 0.78
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,803,725
--------------------------------
Total memory      : 14.15 MB
Total Flops       : 2.02 GFlops
Total Mem (Read)  : 44.41 MB
Total Mem (Write) : 13.92 MB
[Supermasks testing]
[Untrained loss : 0.1537]
[Starting training]
Epoch 0 	 0.133168 	 0.164071 	 0.153326
Epoch 10 	 0.128896 	 0.161514 	 0.151065
Epoch 20 	 0.125315 	 0.161828 	 0.150070
Epoch 30 	 0.111749 	 0.149515 	 0.138718
Epoch 40 	 0.110842 	 0.147359 	 0.138229
Epoch 50 	 0.109737 	 0.151588 	 0.137211
Epoch 60 	 0.102109 	 0.145175 	 0.133201
Epoch 70 	 0.101355 	 0.143714 	 0.132613
Epoch 80 	 0.097018 	 0.140936 	 0.129304
Epoch 90 	 0.096474 	 0.141322 	 0.128912
Epoch 100 	 0.094247 	 0.139828 	 0.128088
Epoch 110 	 0.093376 	 0.141605 	 0.127556
Epoch 120 	 0.092652 	 0.139634 	 0.127402
[Model stopped early]
Train loss       : 0.092461
Best valid loss  : 0.137549
Best test loss   : 0.128121
Pruning          : 0.61
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 6,917,315
--------------------------------
Total memory      : 11.07 MB
Total Flops       : 1.38 GFlops
Total Mem (Read)  : 31.64 MB
Total Mem (Write) : 10.83 MB
[Supermasks testing]
[Untrained loss : 0.1535]
[Starting training]
Epoch 0 	 0.132874 	 0.156729 	 0.148528
Epoch 10 	 0.127152 	 0.163492 	 0.152112
Epoch 20 	 0.113952 	 0.151251 	 0.138937
Epoch 30 	 0.111919 	 0.150059 	 0.138460
Epoch 40 	 0.106162 	 0.145578 	 0.133469
Epoch 50 	 0.103112 	 0.145240 	 0.133664
Epoch 60 	 0.099799 	 0.145019 	 0.130796
Epoch 70 	 0.098488 	 0.144864 	 0.130913
Epoch 80 	 0.096306 	 0.140225 	 0.129800
Epoch 90 	 0.096039 	 0.141874 	 0.129569
Epoch 100 	 0.094552 	 0.141031 	 0.128967
Epoch 110 	 0.094016 	 0.141121 	 0.128701
[Model stopped early]
Train loss       : 0.093943
Best valid loss  : 0.137905
Best test loss   : 0.129672
Pruning          : 0.47
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,474,244
--------------------------------
Total memory      : 8.69 MB
Total Flops       : 836.73 MFlops
Total Mem (Read)  : 20.88 MB
Total Mem (Write) : 8.45 MB
[Supermasks testing]
[Untrained loss : 0.1534]
[Starting training]
Epoch 0 	 0.131489 	 0.162371 	 0.151064
Epoch 10 	 0.127529 	 0.163491 	 0.150094
Epoch 20 	 0.124183 	 0.157475 	 0.146275
Epoch 30 	 0.124200 	 0.161789 	 0.149595
Epoch 40 	 0.111286 	 0.149890 	 0.136788
Epoch 50 	 0.103961 	 0.145093 	 0.133372
Epoch 60 	 0.099986 	 0.144337 	 0.131058
Epoch 70 	 0.098991 	 0.143075 	 0.130574
Epoch 80 	 0.098305 	 0.142335 	 0.130252
Epoch 90 	 0.096250 	 0.142492 	 0.129021
Epoch 100 	 0.094865 	 0.140755 	 0.128360
Epoch 110 	 0.094282 	 0.142672 	 0.128223
Epoch 120 	 0.094204 	 0.138982 	 0.128192
Epoch 130 	 0.093866 	 0.140106 	 0.128034
Epoch 140 	 0.093805 	 0.139985 	 0.128019
[Model stopped early]
Train loss       : 0.093790
Best valid loss  : 0.137348
Best test loss   : 0.128234
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,952,020
--------------------------------
Total memory      : 6.83 MB
Total Flops       : 522.3 MFlops
Total Mem (Read)  : 14.29 MB
Total Mem (Write) : 6.6 MB
[Supermasks testing]
[Untrained loss : 0.1531]
[Starting training]
Epoch 0 	 0.132711 	 0.162107 	 0.152052
Epoch 10 	 0.128069 	 0.161713 	 0.150759
Epoch 20 	 0.126457 	 0.158734 	 0.148932
Epoch 30 	 0.125755 	 0.160396 	 0.148565
Epoch 40 	 0.111229 	 0.149083 	 0.138174
Epoch 50 	 0.110136 	 0.148673 	 0.137797
Epoch 60 	 0.102300 	 0.143201 	 0.132423
Epoch 70 	 0.101467 	 0.145488 	 0.131784
Epoch 80 	 0.097191 	 0.140011 	 0.129210
Epoch 90 	 0.096627 	 0.141336 	 0.128850
Epoch 100 	 0.094715 	 0.139852 	 0.127580
Epoch 110 	 0.094253 	 0.140085 	 0.127498
Epoch 120 	 0.093339 	 0.139655 	 0.127210
Epoch 130 	 0.092682 	 0.139058 	 0.127005
Epoch 140 	 0.092517 	 0.140218 	 0.126870
Epoch 150 	 0.092505 	 0.137790 	 0.126852
[Model stopped early]
Train loss       : 0.092397
Best valid loss  : 0.136128
Best test loss   : 0.126975
Pruning          : 0.29
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,008,839
--------------------------------
Total memory      : 5.39 MB
Total Flops       : 316.44 MFlops
Total Mem (Read)  : 9.7 MB
Total Mem (Write) : 5.15 MB
[Supermasks testing]
[Untrained loss : 0.1532]
[Starting training]
Epoch 0 	 0.132390 	 0.161375 	 0.151706
Epoch 10 	 0.127946 	 0.164650 	 0.158726
Epoch 20 	 0.114099 	 0.152996 	 0.139816
Epoch 30 	 0.112970 	 0.152381 	 0.139700
Epoch 40 	 0.110420 	 0.150565 	 0.139031
Epoch 50 	 0.110231 	 0.148087 	 0.138383
Epoch 60 	 0.109001 	 0.149658 	 0.137421
Epoch 70 	 0.108520 	 0.146878 	 0.136444
Epoch 80 	 0.105474 	 0.147121 	 0.134000
Epoch 90 	 0.098244 	 0.142449 	 0.130441
Epoch 100 	 0.097802 	 0.142386 	 0.129936
Epoch 110 	 0.097667 	 0.140622 	 0.130448
Epoch 120 	 0.096586 	 0.140090 	 0.129645
Epoch 130 	 0.092717 	 0.139679 	 0.127795
Epoch 140 	 0.092435 	 0.141034 	 0.126955
Epoch 150 	 0.092053 	 0.140428 	 0.127138
Train loss       : 0.091779
Best valid loss  : 0.136528
Best test loss   : 0.126538
Pruning          : 0.23
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,332,737
--------------------------------
Total memory      : 4.26 MB
Total Flops       : 185.4 MFlops
Total Mem (Read)  : 6.64 MB
Total Mem (Write) : 4.02 MB
[Supermasks testing]
[Untrained loss : 0.1536]
[Starting training]
Epoch 0 	 0.132707 	 0.162470 	 0.152857
Epoch 10 	 0.128329 	 0.162548 	 0.150802
Epoch 20 	 0.118168 	 0.151799 	 0.140357
Epoch 30 	 0.111648 	 0.148808 	 0.138963
Epoch 40 	 0.112551 	 0.149103 	 0.138462
Epoch 50 	 0.104085 	 0.147039 	 0.133369
Epoch 60 	 0.102823 	 0.145773 	 0.133557
Epoch 70 	 0.102238 	 0.147647 	 0.133995
Epoch 80 	 0.100673 	 0.142700 	 0.131950
Epoch 90 	 0.100201 	 0.143224 	 0.131193
Epoch 100 	 0.095478 	 0.142081 	 0.128770
Epoch 110 	 0.093417 	 0.141771 	 0.127926
Epoch 120 	 0.093376 	 0.139616 	 0.127634
Epoch 130 	 0.092222 	 0.139408 	 0.127263
Epoch 140 	 0.091860 	 0.140378 	 0.127206
[Model stopped early]
Train loss       : 0.091713
Best valid loss  : 0.137128
Best test loss   : 0.127676
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 941,184
--------------------------------
Total memory      : 3.38 MB
Total Flops       : 107.75 MFlops
Total Mem (Read)  : 4.61 MB
Total Mem (Write) : 3.15 MB
[Supermasks testing]
[Untrained loss : 0.1559]
[Starting training]
Epoch 0 	 0.133989 	 0.163621 	 0.152737
Epoch 10 	 0.127484 	 0.160022 	 0.149230
Epoch 20 	 0.126376 	 0.164506 	 0.152736
Epoch 30 	 0.129029 	 0.156032 	 0.146700
Epoch 40 	 0.125088 	 0.164035 	 0.151422
Epoch 50 	 0.110748 	 0.148863 	 0.137622
Epoch 60 	 0.109438 	 0.148115 	 0.136279
Epoch 70 	 0.101662 	 0.144704 	 0.131104
Epoch 80 	 0.100903 	 0.144180 	 0.130589
Epoch 90 	 0.096860 	 0.140978 	 0.128805
Epoch 100 	 0.096451 	 0.140414 	 0.128526
Epoch 110 	 0.094066 	 0.140650 	 0.127271
Epoch 120 	 0.093162 	 0.137543 	 0.126698
[Model stopped early]
Train loss       : 0.092880
Best valid loss  : 0.135337
Best test loss   : 0.128522
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 743,929
--------------------------------
Total memory      : 2.69 MB
Total Flops       : 71.73 MFlops
Total Mem (Read)  : 3.4 MB
Total Mem (Write) : 2.46 MB
[Supermasks testing]
[Untrained loss : 0.2713]
[Starting training]
Epoch 0 	 0.170733 	 0.185954 	 0.174579
Epoch 10 	 0.136512 	 0.168679 	 0.157875
Epoch 20 	 0.132336 	 0.165400 	 0.153111
Epoch 30 	 0.130092 	 0.166719 	 0.155524
Epoch 40 	 0.117506 	 0.155203 	 0.144470
Epoch 50 	 0.116936 	 0.154748 	 0.143066
Epoch 60 	 0.109910 	 0.148301 	 0.139237
Epoch 70 	 0.108846 	 0.151194 	 0.139480
Epoch 80 	 0.108109 	 0.149818 	 0.139596
Epoch 90 	 0.104801 	 0.149781 	 0.136737
Epoch 100 	 0.104377 	 0.145262 	 0.136166
Epoch 110 	 0.102513 	 0.148258 	 0.135303
Epoch 120 	 0.101686 	 0.145482 	 0.135073
Epoch 130 	 0.101150 	 0.147692 	 0.134957
Epoch 140 	 0.101071 	 0.146237 	 0.134969
Epoch 150 	 0.100614 	 0.146565 	 0.134943
Train loss       : 0.100745
Best valid loss  : 0.143762
Best test loss   : 0.134897
Pruning          : 0.11
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 496,844
--------------------------------
Total memory      : 2.15 MB
Total Flops       : 44.15 MFlops
Total Mem (Read)  : 2.56 MB
Total Mem (Write) : 1.92 MB
[Supermasks testing]
[Untrained loss : 0.4993]
[Starting training]
Epoch 0 	 0.275378 	 0.244326 	 0.233349
Epoch 10 	 0.157333 	 0.185683 	 0.176678
Epoch 20 	 0.148544 	 0.180202 	 0.169615
Epoch 30 	 0.146182 	 0.180682 	 0.169435
Epoch 40 	 0.143621 	 0.176302 	 0.166855
Epoch 50 	 0.140323 	 0.172922 	 0.163203
Epoch 60 	 0.138996 	 0.173806 	 0.165073
Epoch 70 	 0.127667 	 0.165573 	 0.154333
Epoch 80 	 0.127448 	 0.163654 	 0.153404
Epoch 90 	 0.121088 	 0.159617 	 0.149688
Epoch 100 	 0.120403 	 0.160314 	 0.150018
Epoch 110 	 0.117245 	 0.158040 	 0.148330
Epoch 120 	 0.116031 	 0.156818 	 0.147947
Epoch 130 	 0.115746 	 0.157973 	 0.147705
Epoch 140 	 0.115044 	 0.157410 	 0.147447
Epoch 150 	 0.114604 	 0.157977 	 0.147363
Train loss       : 0.114589
Best valid loss  : 0.154358
Best test loss   : 0.147292
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 319,087
--------------------------------
Total memory      : 1.74 MB
Total Flops       : 28.85 MFlops
Total Mem (Read)  : 2.01 MB
Total Mem (Write) : 1.5 MB
[Supermasks testing]
[Untrained loss : 0.5291]
[Starting training]
Epoch 0 	 0.367268 	 0.345075 	 0.330990
Epoch 10 	 0.196733 	 0.222089 	 0.210507
Epoch 20 	 0.178539 	 0.207732 	 0.196278
Epoch 30 	 0.170826 	 0.200716 	 0.190202
Epoch 40 	 0.165640 	 0.194918 	 0.184247
Epoch 50 	 0.163186 	 0.193432 	 0.183692
Epoch 60 	 0.160348 	 0.190639 	 0.180889
Epoch 70 	 0.149606 	 0.181947 	 0.172424
Epoch 80 	 0.148908 	 0.181896 	 0.171863
Epoch 90 	 0.148320 	 0.179203 	 0.170489
Epoch 100 	 0.146936 	 0.179761 	 0.169486
Epoch 110 	 0.146348 	 0.180936 	 0.169978
Epoch 120 	 0.145540 	 0.179713 	 0.168835
Epoch 130 	 0.140698 	 0.174283 	 0.165564
Epoch 140 	 0.139992 	 0.174884 	 0.165539
Epoch 150 	 0.137886 	 0.175335 	 0.163675
Train loss       : 0.137538
Best valid loss  : 0.172415
Best test loss   : 0.163935
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 170,252
--------------------------------
Total memory      : 1.41 MB
Total Flops       : 24.69 MFlops
Total Mem (Read)  : 1.74 MB
Total Mem (Write) : 1.18 MB
[Supermasks testing]
[Untrained loss : 0.5458]
[Starting training]
Epoch 0 	 0.406372 	 0.364442 	 0.360770
Epoch 10 	 0.223198 	 0.250402 	 0.242214
Epoch 20 	 0.205938 	 0.233241 	 0.222085
Epoch 30 	 0.197275 	 0.222968 	 0.215310
Epoch 40 	 0.191900 	 0.217569 	 0.208712
Epoch 50 	 0.187629 	 0.216763 	 0.205425
Epoch 60 	 0.184010 	 0.209658 	 0.201716
Epoch 70 	 0.183836 	 0.212450 	 0.203257
Epoch 80 	 0.179775 	 0.203911 	 0.197040
Epoch 90 	 0.178903 	 0.207443 	 0.196580
Epoch 100 	 0.170380 	 0.198016 	 0.189256
Epoch 110 	 0.169784 	 0.197120 	 0.188802
Epoch 120 	 0.169024 	 0.196363 	 0.187777
Epoch 130 	 0.165640 	 0.194458 	 0.185151
Epoch 140 	 0.165321 	 0.192108 	 0.184602
Epoch 150 	 0.164667 	 0.194315 	 0.184657
Train loss       : 0.163199
Best valid loss  : 0.190199
Best test loss   : 0.183623
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 131,514
--------------------------------
Total memory      : 1.15 MB
Total Flops       : 21.93 MFlops
Total Mem (Read)  : 1.46 MB
Total Mem (Write) : 942.36 KB
[Supermasks testing]
[Untrained loss : 0.5447]
[Starting training]
Epoch 0 	 0.427262 	 0.407122 	 0.397531
Epoch 10 	 0.240122 	 0.264422 	 0.254792
Epoch 20 	 0.219820 	 0.244831 	 0.235910
Epoch 30 	 0.210593 	 0.236577 	 0.226729
Epoch 40 	 0.204474 	 0.232661 	 0.222556
Epoch 50 	 0.200126 	 0.226787 	 0.217394
Epoch 60 	 0.196391 	 0.223862 	 0.213527
Epoch 70 	 0.193571 	 0.223159 	 0.213136
Epoch 80 	 0.191994 	 0.218287 	 0.209948
Epoch 90 	 0.190158 	 0.214741 	 0.207208
Epoch 100 	 0.187631 	 0.214745 	 0.206308
Epoch 110 	 0.181727 	 0.209826 	 0.201573
Epoch 120 	 0.181101 	 0.210143 	 0.200726
Epoch 130 	 0.178089 	 0.206954 	 0.198893
Epoch 140 	 0.177500 	 0.206097 	 0.198790
Epoch 150 	 0.176046 	 0.207352 	 0.197496
Train loss       : 0.175655
Best valid loss  : 0.203132
Best test loss   : 0.197464
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 95,422
--------------------------------
Total memory      : 0.95 MB
Total Flops       : 18.08 MFlops
Total Mem (Read)  : 1.22 MB
Total Mem (Write) : 738.27 KB
[Supermasks testing]
[Untrained loss : 0.5451]
[Starting training]
Epoch 0 	 0.445784 	 0.426965 	 0.422668
Epoch 10 	 0.291408 	 0.318190 	 0.306381
Epoch 20 	 0.265456 	 0.296002 	 0.282563
Epoch 30 	 0.254504 	 0.284091 	 0.271857
Epoch 40 	 0.246359 	 0.273996 	 0.263418
Epoch 50 	 0.241660 	 0.271834 	 0.257500
Epoch 60 	 0.237605 	 0.269001 	 0.255145
Epoch 70 	 0.236274 	 0.264332 	 0.251269
Epoch 80 	 0.233588 	 0.263522 	 0.251163
Epoch 90 	 0.232046 	 0.260740 	 0.248167
Epoch 100 	 0.226787 	 0.256541 	 0.243799
Epoch 110 	 0.224009 	 0.252007 	 0.241887
Epoch 120 	 0.223530 	 0.255003 	 0.240983
Epoch 130 	 0.221801 	 0.251821 	 0.239944
Epoch 140 	 0.221203 	 0.255378 	 0.239569
Epoch 150 	 0.220548 	 0.250343 	 0.239149
Train loss       : 0.220156
Best valid loss  : 0.249192
Best test loss   : 0.239087
Pruning          : 0.03
