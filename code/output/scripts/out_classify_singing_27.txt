Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41289127.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, six, pillow-simd, torchvision, tqdm, kiwisolver, pyparsing, python-dateutil, cycler, matplotlib, astor, tensorflow-estimator, wrapt, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, protobuf, markdown, grpcio, werkzeug, idna, chardet, urllib3, certifi, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, tensorboard, keras-preprocessing, gast, opt-einsum, termcolor, h5py, keras-applications, google-pasta, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41289127.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 05:00:30.615539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 05:00:30.627797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is singing_classify_cnn_xavier_masking_gradient_min_reinit_global_0.
*******
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.8996]
[Starting training]
Epoch 0 	 0.761949 	 0.688419 	 0.683548
Epoch 10 	 0.312615 	 0.380974 	 0.238327
Epoch 20 	 0.140395 	 0.237592 	 0.098254
Epoch 30 	 0.085708 	 0.188879 	 0.058456
Epoch 40 	 0.059628 	 0.169577 	 0.044669
Epoch 50 	 0.047220 	 0.164982 	 0.039430
Epoch 60 	 0.042969 	 0.158548 	 0.035202
Epoch 70 	 0.021255 	 0.155331 	 0.032812
Epoch 80 	 0.018612 	 0.152114 	 0.031342
Epoch 90 	 0.014246 	 0.154412 	 0.031342
[Model stopped early]
Train loss       : 0.010340
Best valid loss  : 0.148438
Best test loss   : 0.030974
Pruning          : 1.00
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9121]
[Starting training]
Epoch 0 	 0.775391 	 0.687040 	 0.672335
Epoch 10 	 0.325253 	 0.384191 	 0.246783
Epoch 20 	 0.158663 	 0.245864 	 0.099908
Epoch 30 	 0.099150 	 0.195312 	 0.060386
Epoch 40 	 0.069164 	 0.180147 	 0.048437
Epoch 50 	 0.059858 	 0.173713 	 0.041820
Epoch 60 	 0.049747 	 0.164062 	 0.036121
Epoch 70 	 0.047220 	 0.153952 	 0.034099
Epoch 80 	 0.037569 	 0.157629 	 0.033732
Epoch 90 	 0.026540 	 0.162224 	 0.032812
Epoch 100 	 0.017808 	 0.154871 	 0.031526
[Model stopped early]
Train loss       : 0.012638
Best valid loss  : 0.152114
Best test loss   : 0.032445
Pruning          : 0.70
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.811466 	 0.743566 	 0.724908
Epoch 10 	 0.345014 	 0.384191 	 0.255423
Epoch 20 	 0.176585 	 0.248621 	 0.118290
Epoch 30 	 0.120404 	 0.209099 	 0.079687
Epoch 40 	 0.089384 	 0.186581 	 0.056250
Epoch 50 	 0.066866 	 0.173713 	 0.044945
Epoch 60 	 0.055147 	 0.173713 	 0.042371
Epoch 70 	 0.049977 	 0.164982 	 0.038511
Epoch 80 	 0.037569 	 0.159007 	 0.034467
Epoch 90 	 0.024816 	 0.153952 	 0.032996
Epoch 100 	 0.026425 	 0.160386 	 0.033180
Epoch 110 	 0.019187 	 0.153952 	 0.031342
Epoch 120 	 0.017233 	 0.157629 	 0.033180
Epoch 130 	 0.015280 	 0.163143 	 0.033364
Epoch 140 	 0.013097 	 0.158088 	 0.032445
[Model stopped early]
Train loss       : 0.014246
Best valid loss  : 0.151654
Best test loss   : 0.030974
Pruning          : 0.49
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.768957 	 0.701287 	 0.699357
Epoch 10 	 0.398438 	 0.435202 	 0.308732
Epoch 20 	 0.221163 	 0.285846 	 0.156893
Epoch 30 	 0.155101 	 0.227941 	 0.095680
Epoch 40 	 0.115809 	 0.209099 	 0.074173
Epoch 50 	 0.093980 	 0.188419 	 0.057445
Epoch 60 	 0.075138 	 0.179688 	 0.046967
Epoch 70 	 0.071691 	 0.175092 	 0.048070
Epoch 80 	 0.065832 	 0.174173 	 0.040165
Epoch 90 	 0.057790 	 0.168658 	 0.038511
Epoch 100 	 0.050551 	 0.167279 	 0.038695
Epoch 110 	 0.040786 	 0.168658 	 0.036673
Epoch 120 	 0.037569 	 0.160846 	 0.033915
Epoch 130 	 0.031824 	 0.164062 	 0.034835
Epoch 140 	 0.029412 	 0.165901 	 0.035754
Epoch 150 	 0.028378 	 0.160386 	 0.032812
Train loss       : 0.018957
Best valid loss  : 0.158548
Best test loss   : 0.032996
Pruning          : 0.34
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9550]
[Starting training]
Epoch 0 	 0.788948 	 0.740809 	 0.724081
Epoch 10 	 0.441062 	 0.499081 	 0.362408
Epoch 20 	 0.271599 	 0.340074 	 0.184467
Epoch 30 	 0.202206 	 0.277574 	 0.122518
Epoch 40 	 0.168313 	 0.240349 	 0.093290
Epoch 50 	 0.139361 	 0.214614 	 0.074724
Epoch 60 	 0.123736 	 0.200827 	 0.066268
Epoch 70 	 0.118451 	 0.194393 	 0.058732
Epoch 80 	 0.102367 	 0.189338 	 0.055055
Epoch 90 	 0.096622 	 0.186581 	 0.051930
Epoch 100 	 0.072725 	 0.182904 	 0.050184
Epoch 110 	 0.070542 	 0.176930 	 0.044301
Epoch 120 	 0.062960 	 0.178309 	 0.044026
Epoch 130 	 0.054802 	 0.175551 	 0.041268
Epoch 140 	 0.049517 	 0.173713 	 0.041085
Epoch 150 	 0.053883 	 0.176930 	 0.042004
Train loss       : 0.047909
Best valid loss  : 0.168199
Best test loss   : 0.039982
Pruning          : 0.24
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.836282 	 0.749540 	 0.731250
Epoch 10 	 0.537684 	 0.541820 	 0.443107
Epoch 20 	 0.397748 	 0.460938 	 0.314062
Epoch 30 	 0.327321 	 0.379136 	 0.223254
Epoch 40 	 0.279412 	 0.330882 	 0.182996
Epoch 50 	 0.254021 	 0.291360 	 0.142371
Epoch 60 	 0.238281 	 0.278493 	 0.136673
Epoch 70 	 0.218980 	 0.260110 	 0.120864
Epoch 80 	 0.208410 	 0.243107 	 0.100092
Epoch 90 	 0.195772 	 0.240809 	 0.105055
Epoch 100 	 0.184513 	 0.225184 	 0.089798
Epoch 110 	 0.179343 	 0.212316 	 0.086949
Epoch 120 	 0.177964 	 0.215074 	 0.083732
Epoch 130 	 0.172449 	 0.201746 	 0.076195
Epoch 140 	 0.168199 	 0.205423 	 0.074816
Epoch 150 	 0.165786 	 0.204504 	 0.078768
Train loss       : 0.158778
Best valid loss  : 0.195772
Best test loss   : 0.070588
Pruning          : 0.17
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9871]
[Starting training]
Epoch 0 	 0.917050 	 0.875000 	 0.870037
Epoch 10 	 0.689453 	 0.646599 	 0.609375
Epoch 20 	 0.600414 	 0.607537 	 0.516728
Epoch 30 	 0.549977 	 0.587776 	 0.458824
Epoch 40 	 0.520450 	 0.568474 	 0.418382
Epoch 50 	 0.506319 	 0.569393 	 0.409191
Epoch 60 	 0.493107 	 0.555147 	 0.390901
Epoch 70 	 0.485064 	 0.541820 	 0.379504
Epoch 80 	 0.480354 	 0.531710 	 0.359926
Epoch 90 	 0.476677 	 0.520221 	 0.348621
Epoch 100 	 0.467142 	 0.518382 	 0.351746
Epoch 110 	 0.458410 	 0.500000 	 0.332537
Epoch 120 	 0.464844 	 0.505515 	 0.331985
Epoch 130 	 0.448759 	 0.496324 	 0.322059
Epoch 140 	 0.454044 	 0.510570 	 0.336489
Epoch 150 	 0.434972 	 0.490349 	 0.312684
Train loss       : 0.445312
Best valid loss  : 0.476103
Best test loss   : 0.295864
Pruning          : 0.12
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.8996]
[Starting training]
Epoch 0 	 0.801700 	 0.768842 	 0.754228
Epoch 10 	 0.720129 	 0.702206 	 0.676562
Epoch 20 	 0.696806 	 0.680607 	 0.645037
Epoch 30 	 0.678768 	 0.663603 	 0.615625
Epoch 40 	 0.658088 	 0.670496 	 0.608915
Epoch 50 	 0.647978 	 0.656250 	 0.588235
Epoch 60 	 0.647748 	 0.657169 	 0.582537
Epoch 70 	 0.642004 	 0.647518 	 0.576103
Epoch 80 	 0.633732 	 0.642923 	 0.570312
Epoch 90 	 0.638557 	 0.634191 	 0.557445
Epoch 100 	 0.631319 	 0.638787 	 0.561489
Epoch 110 	 0.632123 	 0.632353 	 0.557077
Epoch 120 	 0.630055 	 0.640625 	 0.564522
Epoch 130 	 0.629940 	 0.636029 	 0.561397
Epoch 140 	 0.627068 	 0.635570 	 0.556710
[Model stopped early]
Train loss       : 0.624885
Best valid loss  : 0.629136
Best test loss   : 0.553952
Pruning          : 0.08
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9550]
[Starting training]
Epoch 0 	 0.835708 	 0.940717 	 0.942096
Epoch 10 	 0.774472 	 0.921415 	 0.912684
Epoch 20 	 0.775965 	 0.920956 	 0.912684
Epoch 30 	 0.762063 	 0.916360 	 0.911581
[Model stopped early]
Train loss       : 0.769646
Best valid loss  : 0.746783
Best test loss   : 0.753676
Pruning          : 0.06
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.840763 	 0.747243 	 0.753676
Epoch 10 	 0.759766 	 0.747243 	 0.753676
Epoch 20 	 0.761029 	 0.747243 	 0.753676
Epoch 30 	 0.766314 	 0.747243 	 0.753676
[Model stopped early]
Train loss       : 0.763212
Best valid loss  : 0.746783
Best test loss   : 0.753676
Pruning          : 0.04
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9421]
[Starting training]
Epoch 0 	 0.849494 	 0.748162 	 0.753676
Epoch 10 	 0.760110 	 0.893382 	 0.899632
Epoch 20 	 0.757927 	 0.920956 	 0.912684
Epoch 30 	 0.762178 	 0.921415 	 0.912684
[Model stopped early]
Train loss       : 0.764706
Best valid loss  : 0.748162
Best test loss   : 0.753676
Pruning          : 0.03
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9421]
[Starting training]
Epoch 0 	 0.943934 	 0.940717 	 0.942096
Epoch 10 	 0.763787 	 0.786305 	 0.794853
Epoch 20 	 0.766199 	 0.785846 	 0.794853
Epoch 30 	 0.766199 	 0.785846 	 0.794853
[Model stopped early]
Train loss       : 0.769531
Best valid loss  : 0.785386
Best test loss   : 0.794853
Pruning          : 0.02
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.7949]
[Starting training]
Epoch 0 	 0.962431 	 0.785846 	 0.794853
Epoch 10 	 0.760915 	 0.747243 	 0.753676
Epoch 20 	 0.763787 	 0.747243 	 0.753676
Epoch 30 	 0.759766 	 0.747243 	 0.753676
[Model stopped early]
Train loss       : 0.763097
Best valid loss  : 0.746783
Best test loss   : 0.753676
Pruning          : 0.01
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9871]
[Starting training]
Epoch 0 	 0.805951 	 0.886949 	 0.876471
Epoch 10 	 0.760110 	 0.916360 	 0.911581
Epoch 20 	 0.756549 	 0.916820 	 0.911581
Epoch 30 	 0.756893 	 0.746783 	 0.753676
Epoch 40 	 0.756434 	 0.747702 	 0.753676
Epoch 50 	 0.755400 	 0.747702 	 0.753676
[Model stopped early]
Train loss       : 0.756664
Best valid loss  : 0.746783
Best test loss   : 0.753676
Pruning          : 0.01
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.965648 	 0.886029 	 0.876471
Epoch 10 	 0.764246 	 0.920956 	 0.912684
Epoch 20 	 0.758961 	 0.921875 	 0.912684
Epoch 30 	 0.761029 	 0.785846 	 0.794853
Epoch 40 	 0.759881 	 0.747702 	 0.753676
Epoch 50 	 0.760915 	 0.786305 	 0.794853
Epoch 60 	 0.756434 	 0.746783 	 0.753676
Epoch 70 	 0.754940 	 0.747243 	 0.753676
[Model stopped early]
Train loss       : 0.758732
Best valid loss  : 0.746783
Best test loss   : 0.753676
Pruning          : 0.01
