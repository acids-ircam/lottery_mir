Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288815.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, future, torch, six, torchvision, tqdm, cycler, python-dateutil, pyparsing, kiwisolver, matplotlib, google-pasta, wrapt, protobuf, absl-py, keras-preprocessing, termcolor, grpcio, gast, opt-einsum, tensorflow-estimator, astor, h5py, keras-applications, urllib3, chardet, certifi, idna, requests, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, werkzeug, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288815.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:52:46.477183: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:52:46.490243: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_trimming_info_target_rewind_global_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288815.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7931]
[Starting training]
/localscratch/esling.41288815.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
Epoch 0 	 22.590370 	 0.600927 	 0.598227
Epoch 10 	 21.434765 	 0.526808 	 0.515699
Epoch 20 	 20.653353 	 0.423936 	 0.427455
Epoch 30 	 19.239187 	 0.291636 	 0.287565
Epoch 40 	 18.236296 	 0.218580 	 0.210145
Epoch 50 	 17.527859 	 0.179350 	 0.162521
Epoch 60 	 17.137014 	 0.166428 	 0.155911
Epoch 70 	 16.905210 	 0.171742 	 0.156558
Epoch 80 	 16.842869 	 0.169493 	 0.151044
Epoch 90 	 16.610447 	 0.162220 	 0.151179
Epoch 100 	 16.540939 	 0.158814 	 0.148530
Epoch 110 	 16.512852 	 0.159548 	 0.146117
/localscratch/esling.41288815.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 120 	 16.423834 	 0.152397 	 0.141802
Epoch 130 	 16.386881 	 0.149677 	 0.141408
Epoch 140 	 16.328548 	 0.150505 	 0.141925
Epoch 150 	 16.299236 	 0.149497 	 0.138313
Epoch 160 	 16.285936 	 0.152193 	 0.139252
Epoch 170 	 16.271194 	 0.150656 	 0.138323
Epoch 180 	 16.258806 	 0.152572 	 0.140642
Epoch 190 	 16.253050 	 0.150244 	 0.138839
Train loss       : 16.246351
Best valid loss  : 0.146502
Best test loss   : 0.139500
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,516,712
--------------------------------
Total memory      : 21.12 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 29.86 MB
Total Mem (Write) : 16.43 MB
[Supermasks testing]
[Untrained loss : 0.2256]
[Starting training]
Epoch 0 	 17.505531 	 0.167549 	 0.157935
Epoch 10 	 16.962057 	 0.162116 	 0.147905
Epoch 20 	 16.775187 	 0.158165 	 0.142458
Epoch 30 	 16.628139 	 0.156242 	 0.143846
Epoch 40 	 16.489830 	 0.152613 	 0.141182
Epoch 50 	 16.324982 	 0.150445 	 0.136974
Epoch 60 	 16.274582 	 0.145707 	 0.133561
Epoch 70 	 16.256542 	 0.146510 	 0.132339
Epoch 80 	 16.195271 	 0.144422 	 0.135028
Epoch 90 	 16.161762 	 0.144249 	 0.131613
Epoch 100 	 16.149200 	 0.144605 	 0.133000
Epoch 110 	 16.124992 	 0.142297 	 0.131568
Epoch 120 	 16.123159 	 0.141233 	 0.132268
Epoch 130 	 16.112560 	 0.142019 	 0.131030
Epoch 140 	 16.116180 	 0.144418 	 0.131952
Epoch 150 	 16.112600 	 0.140398 	 0.130971
[Model stopped early]
Train loss       : 16.112600
Best valid loss  : 0.138999
Best test loss   : 0.131054
Pruning          : 0.75
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,779,839
--------------------------------
Total memory      : 21.12 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 27.04 MB
Total Mem (Write) : 16.42 MB
[Supermasks testing]
[Untrained loss : 0.4014]
[Starting training]
Epoch 0 	 18.216740 	 0.182866 	 0.169435
Epoch 10 	 17.166370 	 0.160173 	 0.147950
Epoch 20 	 16.962860 	 0.159891 	 0.145194
Epoch 30 	 16.793344 	 0.162377 	 0.146406
Epoch 40 	 16.605022 	 0.158275 	 0.141479
Epoch 50 	 16.496861 	 0.152142 	 0.138895
Epoch 60 	 16.409548 	 0.149978 	 0.138923
Epoch 70 	 16.379833 	 0.149888 	 0.138906
Epoch 80 	 16.353071 	 0.152477 	 0.137509
Epoch 90 	 16.327791 	 0.150267 	 0.135687
Epoch 100 	 16.327261 	 0.148473 	 0.137193
[Model stopped early]
Train loss       : 16.327261
Best valid loss  : 0.146176
Best test loss   : 0.139396
Pruning          : 0.56
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,322,263
--------------------------------
Total memory      : 21.11 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 25.29 MB
Total Mem (Write) : 16.42 MB
[Supermasks testing]
[Untrained loss : 0.5551]
[Starting training]
Epoch 0 	 19.524618 	 0.212951 	 0.208009
Epoch 10 	 17.673334 	 0.171532 	 0.151107
Epoch 20 	 17.412985 	 0.165758 	 0.151007
Epoch 30 	 17.266537 	 0.160262 	 0.150426
Epoch 40 	 17.124680 	 0.158189 	 0.151227
Epoch 50 	 16.941441 	 0.159838 	 0.144596
Epoch 60 	 16.868616 	 0.158493 	 0.147464
Epoch 70 	 16.766922 	 0.155624 	 0.145107
Epoch 80 	 16.730116 	 0.152939 	 0.147098
Epoch 90 	 16.710276 	 0.156550 	 0.145311
Epoch 100 	 16.700497 	 0.154767 	 0.145098
Epoch 110 	 16.691408 	 0.155979 	 0.145245
Epoch 120 	 16.696650 	 0.154691 	 0.144754
[Model stopped early]
Train loss       : 16.667208
Best valid loss  : 0.151187
Best test loss   : 0.146916
Pruning          : 0.42
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,028,060
--------------------------------
Total memory      : 21.11 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 24.17 MB
Total Mem (Write) : 16.42 MB
[Supermasks testing]
[Untrained loss : 0.6239]
[Starting training]
Epoch 0 	 21.158802 	 0.366613 	 0.363993
Epoch 10 	 18.834105 	 0.206706 	 0.200273
Epoch 20 	 18.510015 	 0.189923 	 0.183479
Epoch 30 	 18.303225 	 0.181905 	 0.175194
Epoch 40 	 18.192539 	 0.180062 	 0.169527
Epoch 50 	 18.088224 	 0.179030 	 0.168307
Epoch 60 	 17.941456 	 0.174652 	 0.163638
Epoch 70 	 17.870724 	 0.173928 	 0.163525
Epoch 80 	 17.780602 	 0.172253 	 0.162495
Epoch 90 	 17.747679 	 0.170195 	 0.161416
[Model stopped early]
Train loss       : 17.687366
Best valid loss  : 0.166328
Best test loss   : 0.158164
Pruning          : 0.32
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,841,300
--------------------------------
Total memory      : 21.10 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 23.45 MB
Total Mem (Write) : 16.41 MB
[Supermasks testing]
[Untrained loss : 0.6811]
[Starting training]
Epoch 0 	 22.133621 	 0.567209 	 0.559547
Epoch 10 	 20.810760 	 0.400280 	 0.403083
Epoch 20 	 20.511745 	 0.358350 	 0.364022
Epoch 30 	 20.348385 	 0.345059 	 0.344751
Epoch 40 	 20.184299 	 0.330729 	 0.327343
Epoch 50 	 20.159033 	 0.308461 	 0.306987
Epoch 60 	 20.048130 	 0.302290 	 0.299155
Epoch 70 	 19.990831 	 0.310263 	 0.307706
Epoch 80 	 19.883961 	 0.297430 	 0.293264
Epoch 90 	 19.815979 	 0.298819 	 0.292566
Epoch 100 	 19.737942 	 0.292406 	 0.293922
Epoch 110 	 19.662266 	 0.284638 	 0.285877
Epoch 120 	 19.670279 	 0.285814 	 0.283342
Epoch 130 	 19.673731 	 0.285562 	 0.283346
Epoch 140 	 19.672733 	 0.284327 	 0.281899
Epoch 150 	 19.616121 	 0.283428 	 0.283603
[Model stopped early]
Train loss       : 19.616959
Best valid loss  : 0.279271
Best test loss   : 0.280852
Pruning          : 0.24
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,841,300
--------------------------------
Total memory      : 21.10 MB
Total Flops       : 2.65 GFlops
Total Mem (Read)  : 23.45 MB
Total Mem (Write) : 16.41 MB
[Supermasks testing]
[Untrained loss : 0.6684]
[Starting training]
Epoch 0 	 22.200045 	 0.568034 	 0.569330
Epoch 10 	 20.808182 	 0.394749 	 0.392954
Epoch 20 	 20.489193 	 0.365022 	 0.369438
Epoch 30 	 20.341366 	 0.339731 	 0.348149
Epoch 40 	 20.223835 	 0.328533 	 0.336036
Epoch 50 	 20.168634 	 0.332761 	 0.330643
Epoch 60 	 20.123871 	 0.322226 	 0.327487
Epoch 70 	 19.983437 	 0.305927 	 0.308122
Epoch 80 	 19.905254 	 0.305043 	 0.303212
Epoch 90 	 19.825602 	 0.293795 	 0.294800
Epoch 100 	 19.796432 	 0.295866 	 0.293608
Epoch 110 	 19.723679 	 0.297270 	 0.294647
Epoch 120 	 19.713333 	 0.291605 	 0.293117
Epoch 130 	 19.679737 	 0.294819 	 0.292185
Epoch 140 	 19.683029 	 0.292933 	 0.292165
[Model stopped early]
Train loss       : 19.713346
Best valid loss  : 0.288801
Best test loss   : 0.292421
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,403,106
--------------------------------
Total memory      : 17.83 MB
Total Flops       : 1.95 GFlops
Total Mem (Read)  : 19.24 MB
Total Mem (Write) : 13.87 MB
[Supermasks testing]
[Untrained loss : 0.6668]
[Starting training]
Epoch 0 	 22.183092 	 0.558860 	 0.560298
Epoch 10 	 20.806215 	 0.401733 	 0.401704
Epoch 20 	 20.517452 	 0.371495 	 0.371567
Epoch 30 	 20.347471 	 0.348178 	 0.348781
Epoch 40 	 20.212418 	 0.330496 	 0.327591
Epoch 50 	 20.101398 	 0.323954 	 0.321259
Epoch 60 	 20.004320 	 0.314405 	 0.318141
Epoch 70 	 19.935310 	 0.306535 	 0.312456
Epoch 80 	 19.901968 	 0.301019 	 0.304158
Epoch 90 	 19.845671 	 0.298269 	 0.292376
Epoch 100 	 19.794312 	 0.283310 	 0.287114
Epoch 110 	 19.773420 	 0.291511 	 0.294422
Epoch 120 	 19.699167 	 0.292594 	 0.288789
Epoch 130 	 19.648371 	 0.286717 	 0.280393
Epoch 140 	 19.591953 	 0.277312 	 0.279854
Epoch 150 	 19.609091 	 0.282210 	 0.278192
Epoch 160 	 19.588015 	 0.278420 	 0.280191
[Model stopped early]
Train loss       : 19.540493
Best valid loss  : 0.272759
Best test loss   : 0.278680
Pruning          : 0.13
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,403,106
--------------------------------
Total memory      : 17.83 MB
Total Flops       : 1.95 GFlops
Total Mem (Read)  : 19.24 MB
Total Mem (Write) : 13.87 MB
[Supermasks testing]
[Untrained loss : 0.6659]
[Starting training]
Epoch 0 	 22.175243 	 0.568313 	 0.570048
Epoch 10 	 20.788452 	 0.388027 	 0.383332
Epoch 20 	 20.468355 	 0.352179 	 0.353985
Epoch 30 	 20.328451 	 0.331955 	 0.336986
Epoch 40 	 20.239422 	 0.326443 	 0.329444
Epoch 50 	 20.111143 	 0.322779 	 0.321756
Epoch 60 	 20.038651 	 0.311623 	 0.309496
Epoch 70 	 19.980450 	 0.300621 	 0.296287
Epoch 80 	 19.865301 	 0.294463 	 0.294598
Epoch 90 	 19.842438 	 0.299849 	 0.294481
Epoch 100 	 19.769215 	 0.292160 	 0.292043
Epoch 110 	 19.739616 	 0.292577 	 0.291569
Epoch 120 	 19.665035 	 0.293301 	 0.289200
Epoch 130 	 19.639683 	 0.288311 	 0.286799
Epoch 140 	 19.624096 	 0.292210 	 0.285473
Epoch 150 	 19.624062 	 0.289633 	 0.286748
Epoch 160 	 19.566433 	 0.288744 	 0.284188
Epoch 170 	 19.607088 	 0.288190 	 0.286528
Epoch 180 	 19.603123 	 0.287422 	 0.286208
[Model stopped early]
Train loss       : 19.600384
Best valid loss  : 0.281301
Best test loss   : 0.285778
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,176,152
--------------------------------
Total memory      : 15.99 MB
Total Flops       : 1.59 GFlops
Total Mem (Read)  : 16.94 MB
Total Mem (Write) : 12.44 MB
[Supermasks testing]
[Untrained loss : 0.6554]
[Starting training]
Epoch 0 	 22.194139 	 0.571892 	 0.568429
Epoch 10 	 20.777954 	 0.402406 	 0.395198
Epoch 20 	 20.484442 	 0.361632 	 0.358858
Epoch 30 	 20.336349 	 0.341796 	 0.345575
Epoch 40 	 20.165592 	 0.312894 	 0.321332
Epoch 50 	 20.062136 	 0.307135 	 0.306486
Epoch 60 	 19.978270 	 0.304779 	 0.304054
Epoch 70 	 19.963377 	 0.298377 	 0.300663
Epoch 80 	 19.867798 	 0.297974 	 0.299070
Epoch 90 	 19.814371 	 0.288483 	 0.291231
Epoch 100 	 19.732956 	 0.292367 	 0.290004
Epoch 110 	 19.683853 	 0.287022 	 0.285592
Epoch 120 	 19.693699 	 0.288071 	 0.287403
Epoch 130 	 19.657358 	 0.286533 	 0.286122
Epoch 140 	 19.707455 	 0.283808 	 0.285506
Epoch 150 	 19.668142 	 0.287221 	 0.285615
Epoch 160 	 19.664602 	 0.281916 	 0.286221
Epoch 170 	 19.636086 	 0.284423 	 0.285872
[Model stopped early]
Train loss       : 19.670509
Best valid loss  : 0.280850
Best test loss   : 0.285298
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 876,760
--------------------------------
Total memory      : 13.14 MB
Total Flops       : 1.11 GFlops
Total Mem (Read)  : 13.58 MB
Total Mem (Write) : 10.22 MB
[Supermasks testing]
[Untrained loss : 0.7040]
[Starting training]
Epoch 0 	 22.226063 	 0.575241 	 0.571568
Epoch 10 	 20.788528 	 0.394449 	 0.390126
Epoch 20 	 20.490799 	 0.357111 	 0.357265
Epoch 30 	 20.368673 	 0.348936 	 0.354083
Epoch 40 	 20.180901 	 0.329020 	 0.332278
Epoch 50 	 20.102640 	 0.316375 	 0.318328
Epoch 60 	 20.008511 	 0.320216 	 0.320576
Epoch 70 	 19.877954 	 0.305215 	 0.310656
Epoch 80 	 19.845341 	 0.298269 	 0.301271
Epoch 90 	 19.832829 	 0.292409 	 0.294979
Epoch 100 	 19.779751 	 0.285211 	 0.290832
Epoch 110 	 19.709995 	 0.283454 	 0.287079
Epoch 120 	 19.696857 	 0.283834 	 0.287928
Epoch 130 	 19.684580 	 0.279329 	 0.283565
Epoch 140 	 19.618473 	 0.280377 	 0.283415
Epoch 150 	 19.647324 	 0.278695 	 0.281091
[Model stopped early]
Train loss       : 19.589848
Best valid loss  : 0.276583
Best test loss   : 0.282792
Pruning          : 0.06
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 740,530
--------------------------------
Total memory      : 11.76 MB
Total Flops       : 887.25 MFlops
Total Mem (Read)  : 11.99 MB
Total Mem (Write) : 9.15 MB
[Supermasks testing]
[Untrained loss : 0.6741]
[Starting training]
Epoch 0 	 22.199305 	 0.577330 	 0.578404
Epoch 10 	 20.805834 	 0.406689 	 0.406819
Epoch 20 	 20.490757 	 0.368447 	 0.365969
Epoch 30 	 20.301069 	 0.336437 	 0.341091
Epoch 40 	 20.168528 	 0.321961 	 0.324644
Epoch 50 	 20.061426 	 0.323802 	 0.327483
Epoch 60 	 19.985703 	 0.314600 	 0.318704
Epoch 70 	 19.923195 	 0.312003 	 0.313114
Epoch 80 	 19.854305 	 0.305622 	 0.307773
Epoch 90 	 19.792126 	 0.300246 	 0.303751
Epoch 100 	 19.760479 	 0.296662 	 0.299976
Epoch 110 	 19.760536 	 0.297839 	 0.297412
Epoch 120 	 19.754484 	 0.296944 	 0.297946
Epoch 130 	 19.701572 	 0.296283 	 0.296296
Epoch 140 	 19.702810 	 0.294049 	 0.295749
Epoch 150 	 19.717014 	 0.293975 	 0.295440
Epoch 160 	 19.746504 	 0.295673 	 0.294911
Epoch 170 	 19.735760 	 0.295764 	 0.295827
Epoch 180 	 19.701534 	 0.294703 	 0.297303
Epoch 190 	 19.704977 	 0.297873 	 0.295632
Train loss       : 19.700640
Best valid loss  : 0.291073
Best test loss   : 0.295279
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 714,209
--------------------------------
Total memory      : 11.46 MB
Total Flops       : 844.98 MFlops
Total Mem (Read)  : 11.65 MB
Total Mem (Write) : 8.92 MB
[Supermasks testing]
[Untrained loss : 0.6597]
[Starting training]
Epoch 0 	 22.209373 	 0.584039 	 0.582165
Epoch 10 	 20.796631 	 0.394999 	 0.396952
Epoch 20 	 20.484413 	 0.362631 	 0.364709
Epoch 30 	 20.290474 	 0.340666 	 0.340379
Epoch 40 	 20.160175 	 0.331676 	 0.325492
Epoch 50 	 20.082142 	 0.327318 	 0.327195
Epoch 60 	 20.015652 	 0.315259 	 0.318328
Epoch 70 	 19.970829 	 0.314132 	 0.315864
Epoch 80 	 19.889542 	 0.316066 	 0.313218
Epoch 90 	 19.895102 	 0.308657 	 0.303864
Epoch 100 	 19.848330 	 0.298044 	 0.299314
Epoch 110 	 19.725325 	 0.296709 	 0.290077
Epoch 120 	 19.721571 	 0.295260 	 0.290913
Epoch 130 	 19.680180 	 0.291674 	 0.287607
Epoch 140 	 19.663525 	 0.292970 	 0.289539
Epoch 150 	 19.663948 	 0.291964 	 0.283654
slurmstepd: error: *** JOB 41288815 ON cdr351 CANCELLED AT 2020-04-29T16:49:02 DUE TO TIME LIMIT ***
