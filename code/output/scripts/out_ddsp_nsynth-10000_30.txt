Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40977524.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, six, pillow-simd, torchvision, tqdm, kiwisolver, pyparsing, python-dateutil, cycler, matplotlib, h5py, keras-applications, gast, tensorflow-estimator, markdown, protobuf, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, chardet, urllib3, idna, certifi, requests, absl-py, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, grpcio, tensorboard, termcolor, google-pasta, keras-preprocessing, wrapt, opt-einsum, astor, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40977524.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-24 12:15:50.409535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-24 12:15:50.736489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_ddsp_cnn_xavier_masking_magnitude_rewind_global_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.40977524.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 85.7021]
[Starting training]
Epoch 0 	 76.405357 	 66.684212 	 68.394119
Epoch 10 	 63.463802 	 58.450268 	 60.143654
Epoch 20 	 52.171928 	 46.255314 	 46.784592
Epoch 30 	 46.110905 	 44.461975 	 46.028526
Epoch 40 	 50.593979 	 56.300999 	 56.632473
Epoch 50 	 46.742550 	 45.490780 	 46.431747
Epoch 60 	 45.238148 	 41.167786 	 42.604301
Epoch 70 	 43.383350 	 39.320183 	 41.189987
Epoch 80 	 42.181370 	 37.009682 	 38.917023
Epoch 90 	 41.891174 	 36.411617 	 38.233307
Epoch 100 	 40.201431 	 37.656437 	 39.052254
Epoch 110 	 40.157001 	 36.609344 	 38.287430
Epoch 120 	 38.476887 	 34.722549 	 36.483242
Epoch 130 	 37.223358 	 33.310875 	 35.405258
Epoch 140 	 35.658054 	 32.731358 	 36.293293
Epoch 150 	 35.095798 	 32.177139 	 34.355766
Epoch 160 	 34.005272 	 31.551090 	 33.591850
Epoch 170 	 32.813194 	 31.502386 	 33.289410
Epoch 180 	 32.013977 	 30.394493 	 32.208435
Epoch 190 	 30.216261 	 29.760460 	 31.437883
Train loss       : 29.420149
Best valid loss  : 28.944382
Best test loss   : 30.825514
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 17605268.0000]
[Starting training]
Epoch 0 	 43.478516 	 39.972454 	 41.958035
Epoch 10 	 35.856148 	 34.730759 	 35.991837
Epoch 20 	 32.361187 	 30.359028 	 32.520329
Epoch 30 	 33.443836 	 30.487316 	 32.463829
Epoch 40 	 32.041222 	 30.651470 	 32.369415
Epoch 50 	 31.399412 	 29.196907 	 31.215631
Epoch 60 	 31.205805 	 28.421320 	 30.412369
Epoch 70 	 30.499313 	 27.966534 	 30.014198
Epoch 80 	 29.962326 	 27.921930 	 29.792189
Epoch 90 	 29.148102 	 28.049749 	 29.922941
Epoch 100 	 28.317263 	 27.556910 	 29.417349
Epoch 110 	 28.174644 	 27.311312 	 29.210033
Epoch 120 	 27.883608 	 27.364403 	 29.192179
Epoch 130 	 27.278334 	 27.163883 	 28.884001
Epoch 140 	 27.036100 	 27.308504 	 29.003700
Epoch 150 	 27.172945 	 27.149015 	 28.902164
Epoch 160 	 26.754000 	 27.305653 	 29.006884
Epoch 170 	 26.643166 	 26.947044 	 28.914206
Epoch 180 	 26.475834 	 26.987392 	 28.864498
Epoch 190 	 26.507408 	 26.820446 	 28.660952
Train loss       : 26.502401
Best valid loss  : 26.644148
Best test loss   : 28.651127
Pruning          : 0.70
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 60.6697]
[Starting training]
Epoch 0 	 723090.562500 	 38.017117 	 40.064953
Epoch 10 	 39.658146 	 35.885822 	 38.198620
Epoch 20 	 38.636703 	 34.774147 	 36.533680
Epoch 30 	 37.261890 	 33.367905 	 35.374603
Epoch 40 	 35.190193 	 32.072323 	 33.924389
Epoch 50 	 34.039913 	 31.437723 	 33.215981
Epoch 60 	 34.126083 	 31.313667 	 33.035324
Epoch 70 	 33.726196 	 31.191156 	 33.090740
Epoch 80 	 32.408321 	 30.519266 	 32.342007
Epoch 90 	 32.133247 	 30.371113 	 32.352406
Epoch 100 	 31.367929 	 30.154593 	 32.380764
Epoch 110 	 30.976477 	 30.248446 	 32.160336
Epoch 120 	 29.711052 	 29.687410 	 31.487497
Epoch 130 	 29.603985 	 29.611244 	 31.518148
Epoch 140 	 29.288401 	 29.451599 	 31.353765
[Model stopped early]
Train loss       : 28.951292
Best valid loss  : 29.343439
Best test loss   : 31.633749
Pruning          : 0.49
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 42.5967]
[Starting training]
Epoch 0 	 41.852997 	 37.486126 	 39.449085
Epoch 10 	 36.140541 	 31.787512 	 34.056168
Epoch 20 	 32.158707 	 30.258402 	 32.546684
Epoch 30 	 30.097534 	 29.691109 	 31.814728
Epoch 40 	 30.298952 	 29.736324 	 31.472376
Epoch 50 	 28.533276 	 27.509819 	 29.416414
Epoch 60 	 28.128105 	 27.363930 	 29.480560
Epoch 70 	 28.382729 	 26.838051 	 28.851320
Epoch 80 	 25.862190 	 27.236731 	 29.276377
Epoch 90 	 25.373882 	 25.508642 	 27.556698
Epoch 100 	 25.000431 	 25.635782 	 27.376417
Epoch 110 	 24.295080 	 25.087748 	 26.945286
Epoch 120 	 23.999741 	 24.999699 	 26.798065
Epoch 130 	 23.830282 	 24.775070 	 26.685324
Epoch 140 	 23.467869 	 24.497942 	 26.345423
Epoch 150 	 23.367413 	 24.818521 	 26.592514
Epoch 160 	 23.268114 	 24.372395 	 26.346409
Epoch 170 	 23.201460 	 24.464535 	 26.451262
Epoch 180 	 22.981674 	 24.410824 	 26.193781
Epoch 190 	 22.863003 	 24.295483 	 26.148077
Train loss       : 22.815735
Best valid loss  : 24.036015
Best test loss   : 26.175795
Pruning          : 0.34
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 45826452.0000]
[Starting training]
Epoch 0 	 41.439587 	 34.356937 	 36.166859
Epoch 10 	 32.899815 	 30.668552 	 32.344360
Epoch 20 	 30.051548 	 28.410856 	 30.508009
Epoch 30 	 29.314837 	 28.510651 	 30.254250
Epoch 40 	 27.743107 	 27.556585 	 29.389303
Epoch 50 	 28.370012 	 27.666533 	 29.425838
Epoch 60 	 27.228594 	 26.538525 	 28.062172
Epoch 70 	 26.605631 	 26.454119 	 28.198271
Epoch 80 	 25.957155 	 25.765362 	 27.713137
Epoch 90 	 26.061060 	 26.138777 	 27.830967
Epoch 100 	 25.089693 	 25.847887 	 27.507784
Epoch 110 	 24.165274 	 24.983988 	 26.630672
Epoch 120 	 23.951241 	 24.851839 	 26.639292
Epoch 130 	 23.376825 	 24.687250 	 26.387711
Epoch 140 	 23.248110 	 24.512415 	 26.185017
Epoch 150 	 23.117685 	 24.345942 	 26.222288
Epoch 160 	 22.943579 	 24.208950 	 25.997732
Epoch 170 	 22.649734 	 24.590111 	 26.184772
Epoch 180 	 22.574389 	 23.973946 	 25.862543
Epoch 190 	 22.514380 	 23.760246 	 25.773859
Train loss       : 22.423372
Best valid loss  : 23.760246
Best test loss   : 25.773859
Pruning          : 0.24
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 135.9235]
[Starting training]
Epoch 0 	 41.164764 	 34.518253 	 36.303890
Epoch 10 	 31.544884 	 28.264429 	 30.568098
Epoch 20 	 28.908260 	 27.988636 	 29.623652
Epoch 30 	 27.926764 	 26.668468 	 28.283661
Epoch 40 	 28.963833 	 27.891153 	 29.791531
Epoch 50 	 25.748449 	 25.642694 	 27.271994
Epoch 60 	 25.191469 	 25.211973 	 26.970156
Epoch 70 	 24.810774 	 25.340124 	 27.003847
Epoch 80 	 24.503645 	 24.751753 	 26.479141
Epoch 90 	 24.157053 	 24.732504 	 26.447401
Epoch 100 	 23.607798 	 24.823483 	 26.468653
Epoch 110 	 23.451014 	 24.406181 	 26.043215
Epoch 120 	 23.164309 	 24.126249 	 25.755098
Epoch 130 	 22.996359 	 24.042526 	 25.732958
Epoch 140 	 22.952528 	 23.876610 	 25.668495
Epoch 150 	 22.902201 	 23.958040 	 25.688040
Epoch 160 	 22.851303 	 24.009186 	 25.714802
Epoch 170 	 22.850634 	 23.923632 	 25.689230
Epoch 180 	 22.713413 	 23.934790 	 25.629320
Epoch 190 	 22.694530 	 23.846497 	 25.596336
Train loss       : 22.773336
Best valid loss  : 23.679365
Best test loss   : 25.602884
Pruning          : 0.17
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 83756.1406]
[Starting training]
Epoch 0 	 40.318588 	 33.338493 	 35.413052
Epoch 10 	 30.261862 	 33.357628 	 34.630508
Epoch 20 	 28.272821 	 28.827423 	 30.526587
Epoch 30 	 26.718515 	 26.720873 	 28.377256
Epoch 40 	 26.148794 	 25.897305 	 27.614128
Epoch 50 	 25.300158 	 25.635056 	 27.297703
Epoch 60 	 25.026342 	 25.308207 	 27.131479
Epoch 70 	 24.979416 	 25.242758 	 27.014799
Epoch 80 	 24.499168 	 24.495443 	 26.337971
Epoch 90 	 23.458954 	 24.476387 	 26.079660
Epoch 100 	 23.343086 	 24.436367 	 26.085194
Epoch 110 	 23.313072 	 23.969116 	 25.799782
Epoch 120 	 22.785074 	 23.880949 	 25.538900
Epoch 130 	 22.559824 	 23.743670 	 25.353554
Epoch 140 	 22.391930 	 23.649088 	 25.286688
Epoch 150 	 22.451792 	 23.590620 	 25.262783
Epoch 160 	 22.340212 	 23.550791 	 25.218807
Epoch 170 	 22.314835 	 23.589516 	 25.216478
Epoch 180 	 22.327856 	 23.630062 	 25.276991
Epoch 190 	 22.242693 	 23.586056 	 25.178190
Train loss       : 22.247179
Best valid loss  : 23.312883
Best test loss   : 25.241350
Pruning          : 0.12
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 706.5366]
[Starting training]
Epoch 0 	 39.005356 	 34.303749 	 36.123146
Epoch 10 	 29.335651 	 27.731798 	 29.534527
Epoch 20 	 27.064600 	 26.002470 	 27.856564
Epoch 30 	 25.939489 	 25.999420 	 28.188040
Epoch 40 	 25.455410 	 25.321117 	 26.923349
Epoch 50 	 25.566603 	 26.023169 	 27.713467
Epoch 60 	 24.680714 	 24.664625 	 26.502052
Epoch 70 	 24.559643 	 24.582710 	 26.254740
Epoch 80 	 24.095648 	 24.410545 	 26.073061
Epoch 90 	 23.124271 	 23.743574 	 25.482960
Epoch 100 	 23.104052 	 24.219978 	 25.843235
Epoch 110 	 23.150042 	 23.732271 	 25.518108
Epoch 120 	 22.597763 	 23.438261 	 25.209536
Epoch 130 	 22.432924 	 23.538567 	 25.109131
Epoch 140 	 22.294062 	 23.441887 	 25.053980
Epoch 150 	 22.272127 	 23.437746 	 25.119354
[Model stopped early]
Train loss       : 22.250725
Best valid loss  : 23.187679
Best test loss   : 25.077618
Pruning          : 0.08
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 39750.5820]
[Starting training]
Epoch 0 	 39.163517 	 32.643711 	 34.651688
Epoch 10 	 29.330658 	 27.571787 	 29.606010
Epoch 20 	 26.910126 	 26.136332 	 27.861759
Epoch 30 	 25.997406 	 25.419434 	 27.127857
Epoch 40 	 25.241066 	 25.245922 	 26.785913
Epoch 50 	 25.152651 	 24.390541 	 26.362932
Epoch 60 	 24.534849 	 24.327860 	 26.400536
Epoch 70 	 24.846153 	 25.053476 	 26.781294
Epoch 80 	 23.414503 	 23.837267 	 25.525921
Epoch 90 	 23.320648 	 23.528622 	 25.421501
Epoch 100 	 23.263889 	 23.424736 	 25.291382
Epoch 110 	 22.779886 	 23.371809 	 25.125536
Epoch 120 	 22.817419 	 23.489916 	 25.164482
Epoch 130 	 22.699547 	 23.391840 	 25.172543
Epoch 140 	 22.508917 	 23.255428 	 24.952595
Epoch 150 	 22.499094 	 23.182272 	 24.989876
Epoch 160 	 22.415304 	 23.277046 	 24.938194
Epoch 170 	 22.405336 	 22.921135 	 24.897612
Epoch 180 	 22.311216 	 23.228601 	 24.992979
Epoch 190 	 22.341621 	 23.248198 	 24.981146
Train loss       : 22.321596
Best valid loss  : 22.910433
Best test loss   : 24.951698
Pruning          : 0.06
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 158.5727]
[Starting training]
Epoch 0 	 40.871044 	 33.879337 	 35.886227
Epoch 10 	 29.841228 	 28.017231 	 29.772364
Epoch 20 	 27.940187 	 41.599495 	 29.653360
Epoch 30 	 27.569870 	 26.822104 	 28.343971
Epoch 40 	 28.496496 	 26.756754 	 28.652485
Epoch 50 	 27.106894 	 25.903173 	 27.636320
Epoch 60 	 26.905825 	 25.550678 	 27.562092
Epoch 70 	 26.606541 	 25.723417 	 27.382444
Epoch 80 	 26.003805 	 25.294464 	 27.059675
Epoch 90 	 25.860378 	 24.868000 	 26.857773
Epoch 100 	 25.747259 	 24.878475 	 26.848099
Epoch 110 	 25.482439 	 24.898533 	 26.651943
Epoch 120 	 25.474266 	 24.894402 	 26.588343
[Model stopped early]
Train loss       : 25.427078
Best valid loss  : 24.663921
Best test loss   : 26.905159
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 76.4903]
[Starting training]
Epoch 0 	 41.634594 	 33.832886 	 35.760162
Epoch 10 	 30.900774 	 28.960230 	 30.945915
Epoch 20 	 28.740423 	 28.023876 	 30.010780
Epoch 30 	 27.448086 	 26.570005 	 28.229839
Epoch 40 	 27.260933 	 26.112671 	 27.771387
Epoch 50 	 26.755680 	 26.206257 	 27.869289
Epoch 60 	 26.457140 	 25.655550 	 27.509892
Epoch 70 	 26.147844 	 25.139172 	 26.987917
Epoch 80 	 25.822933 	 25.405411 	 27.133595
Epoch 90 	 24.836514 	 24.373554 	 26.210781
Epoch 100 	 24.758015 	 24.517399 	 26.411663
Epoch 110 	 24.620996 	 24.297926 	 26.126396
Epoch 120 	 24.196676 	 23.975801 	 25.876320
Epoch 130 	 24.155228 	 24.288174 	 26.053457
Epoch 140 	 23.940258 	 23.990320 	 25.855942
Epoch 150 	 23.947456 	 23.990395 	 25.758482
Epoch 160 	 23.821499 	 23.850285 	 25.676275
Epoch 170 	 23.766516 	 23.956148 	 25.716419
Epoch 180 	 23.750597 	 23.816488 	 25.643402
Epoch 190 	 23.694557 	 23.667368 	 25.659311
Train loss       : 23.715923
Best valid loss  : 23.565319
Best test loss   : 25.643032
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 98.7434]
[Starting training]
Epoch 0 	 53.687920 	 44.033840 	 45.080406
Epoch 10 	 34.426949 	 32.193882 	 34.387470
Epoch 20 	 32.219894 	 30.232792 	 32.360332
Epoch 30 	 31.470295 	 29.117319 	 30.919310
Epoch 40 	 30.045322 	 28.302904 	 30.099169
Epoch 50 	 29.723757 	 28.280167 	 30.212229
Epoch 60 	 29.003372 	 27.724434 	 29.465242
Epoch 70 	 29.550741 	 27.467464 	 29.390776
Epoch 80 	 27.522329 	 26.693342 	 28.440430
Epoch 90 	 27.147488 	 27.229382 	 28.912474
Epoch 100 	 27.022556 	 26.306824 	 28.139645
Epoch 110 	 26.898325 	 26.408979 	 28.112150
Epoch 120 	 26.837215 	 26.085976 	 27.900066
Epoch 130 	 26.495922 	 25.983236 	 27.819824
Epoch 140 	 26.387453 	 25.729643 	 27.628826
Epoch 150 	 26.207258 	 25.695732 	 27.550756
Epoch 160 	 26.123430 	 25.273352 	 27.309574
Epoch 170 	 25.656193 	 25.432858 	 27.277020
Epoch 180 	 25.338018 	 25.233023 	 27.042347
Epoch 190 	 25.372728 	 25.232893 	 27.011478
Train loss       : 25.242445
Best valid loss  : 24.867371
Best test loss   : 27.079298
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : 329.1011]
[Starting training]
Epoch 0 	 86.567780 	 72.092285 	 74.071808
Epoch 10 	 76.854134 	 72.101486 	 73.988945
Epoch 20 	 76.800423 	 71.831367 	 73.987610
Epoch 30 	 76.778671 	 71.991699 	 73.986916
[Model stopped early]
Train loss       : 76.766922
Best valid loss  : 71.554214
Best test loss   : 73.990013
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : nan]
[Starting training]
Epoch 0 	 86.657501 	 71.929474 	 74.007896
Epoch 10 	 76.757881 	 72.306496 	 73.991379
Epoch 20 	 76.662262 	 72.292236 	 73.989883
Epoch 30 	 76.770264 	 72.261162 	 74.031342
Epoch 40 	 76.802567 	 72.239311 	 73.985138
Epoch 50 	 76.766220 	 72.280922 	 73.976212
Epoch 60 	 76.743378 	 72.122009 	 73.977455
Epoch 70 	 76.757225 	 72.128319 	 73.977455
[Model stopped early]
Train loss       : 76.839813
Best valid loss  : 71.527077
Best test loss   : 73.979332
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
[Untrained loss : nan]
[Starting training]
Epoch 0 	 86.558289 	 71.877998 	 74.001236
Epoch 10 	 76.702995 	 72.036690 	 73.987000
Epoch 20 	 76.871078 	 72.078705 	 73.989281
Epoch 30 	 76.812630 	 72.000427 	 73.988449
Epoch 40 	 76.832199 	 72.180519 	 73.979912
[Model stopped early]
Train loss       : 76.832199
Best valid loss  : 71.513756
Best test loss   : 73.984924
Pruning          : 0.01
