Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41281314.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, six, future, torch, torchvision, tqdm, python-dateutil, kiwisolver, pyparsing, cycler, matplotlib, idna, urllib3, certifi, chardet, requests, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, protobuf, werkzeug, markdown, absl-py, tensorboard, h5py, keras-applications, tensorflow-estimator, wrapt, google-pasta, termcolor, keras-preprocessing, opt-einsum, astor, gast, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281314.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-29 02:24:46.192950: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 02:24:46.514552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is sol-ordinario_ddsp_cnn_xavier_trimming_activation_reinit_global_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41281314.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 100.6576]
[Starting training]
Epoch 0 	 90.971214 	 85.731102 	 89.693871
Epoch 10 	 79.867569 	 79.448036 	 82.133057
Epoch 20 	 74.979416 	 74.225082 	 75.402443
Epoch 30 	 70.876778 	 103.167931 	 87.394806
Epoch 40 	 65.710503 	 64.274902 	 67.541161
Epoch 50 	 67.047646 	 61.145382 	 65.398048
Epoch 60 	 56.670330 	 57.855209 	 60.465862
Epoch 70 	 50.659985 	 49.217056 	 53.396633
Epoch 80 	 44.353642 	 44.154163 	 46.563267
Epoch 90 	 42.636543 	 42.055828 	 43.364582
Epoch 100 	 39.630856 	 38.873230 	 41.803154
Epoch 110 	 38.213364 	 37.410439 	 39.189800
Epoch 120 	 36.331944 	 34.488571 	 37.717346
Epoch 130 	 33.070347 	 33.825691 	 36.383823
Epoch 140 	 31.808561 	 37.463009 	 39.785427
Epoch 150 	 30.362925 	 32.745346 	 35.631390
Epoch 160 	 29.344938 	 32.366570 	 34.829124
Epoch 170 	 28.292349 	 30.255270 	 32.730270
Epoch 180 	 26.582621 	 30.542282 	 32.572941
Epoch 190 	 27.331636 	 30.380844 	 33.079994
Train loss       : 24.430670
Best valid loss  : 28.471437
Best test loss   : 30.021772
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,788,693
--------------------------------
Total memory      : 39.25 MB
Total Flops       : 607.72 MFlops
Total Mem (Read)  : 39.54 MB
Total Mem (Write) : 33.32 MB
[Supermasks testing]
[Untrained loss : 96.6788]
[Starting training]
Epoch 0 	 89.612358 	 82.411484 	 86.628021
Epoch 10 	 60.977333 	 54.209553 	 57.719002
Epoch 20 	 61.501163 	 65.436302 	 69.729218
Epoch 30 	 55.374718 	 47.088459 	 49.291599
Epoch 40 	 41.354450 	 39.280258 	 41.112507
Epoch 50 	 37.437626 	 40.110451 	 41.888458
Epoch 60 	 37.637772 	 40.516655 	 41.713634
Epoch 70 	 34.146763 	 35.584999 	 37.417965
Epoch 80 	 40.175854 	 40.651123 	 45.993168
Epoch 90 	 30.724760 	 31.268587 	 32.748360
Epoch 100 	 27.909124 	 30.776276 	 33.016911
Epoch 110 	 27.053503 	 31.653997 	 34.143009
Epoch 120 	 27.135729 	 30.784971 	 32.449459
Epoch 130 	 26.335829 	 31.058331 	 32.598206
Epoch 140 	 23.572590 	 28.183838 	 29.565786
Epoch 150 	 23.389185 	 27.638536 	 29.233023
Epoch 160 	 22.887878 	 28.026745 	 29.428654
Epoch 170 	 22.162861 	 27.884434 	 29.013556
Epoch 180 	 21.541578 	 27.171501 	 28.448423
Epoch 190 	 21.507746 	 26.991337 	 28.313831
Train loss       : 21.240915
Best valid loss  : 26.859310
Best test loss   : 28.411024
Pruning          : 0.72
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,295,564
--------------------------------
Total memory      : 36.63 MB
Total Flops       : 584.56 MFlops
Total Mem (Read)  : 35.73 MB
Total Mem (Write) : 31.39 MB
[Supermasks testing]
[Untrained loss : 98.4769]
[Starting training]
Epoch 0 	 89.604675 	 81.537292 	 85.643677
Epoch 10 	 57.972404 	 55.912998 	 60.377224
Epoch 20 	 50.151394 	 55.952393 	 60.735352
Epoch 30 	 42.891006 	 43.066826 	 47.272778
Epoch 40 	 39.354366 	 39.408989 	 43.488281
Epoch 50 	 36.289185 	 38.688404 	 41.336433
Epoch 60 	 32.919353 	 35.520435 	 39.642052
Epoch 70 	 31.444469 	 34.562275 	 40.575706
Epoch 80 	 29.243671 	 33.979004 	 37.698254
Epoch 90 	 28.410336 	 32.181000 	 37.102329
Epoch 100 	 26.782074 	 31.485323 	 36.820286
Epoch 110 	 25.774858 	 30.984446 	 36.565002
Epoch 120 	 25.166992 	 30.430387 	 35.721706
Epoch 130 	 23.435905 	 29.530926 	 34.924480
Epoch 140 	 23.406519 	 29.519402 	 34.641949
Epoch 150 	 23.251547 	 29.718287 	 34.100391
Epoch 160 	 22.685600 	 28.993307 	 33.830654
Epoch 170 	 22.132616 	 28.826553 	 33.998837
Epoch 180 	 22.077820 	 29.179770 	 33.291931
Epoch 190 	 21.793631 	 28.967934 	 33.210808
Train loss       : 21.770685
Best valid loss  : 27.721430
Best test loss   : 33.061184
Pruning          : 0.52
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,828,660
--------------------------------
Total memory      : 31.38 MB
Total Flops       : 416.78 MFlops
Total Mem (Read)  : 29.1 MB
Total Mem (Write) : 26.29 MB
[Supermasks testing]
[Untrained loss : 103.7436]
[Starting training]
Epoch 0 	 96.581772 	 89.745560 	 93.839409
Epoch 10 	 69.223396 	 66.530533 	 69.795891
Epoch 20 	 67.403793 	 66.003723 	 69.503532
Epoch 30 	 61.273987 	 63.493034 	 65.479393
Epoch 40 	 61.036591 	 61.357941 	 63.581524
Epoch 50 	 57.890213 	 59.381195 	 63.058887
Epoch 60 	 55.464390 	 57.594902 	 60.289570
Epoch 70 	 53.780064 	 56.306190 	 71.972260
Epoch 80 	 53.633934 	 59.735207 	 126.220901
Epoch 90 	 49.568317 	 55.636005 	 526.425232
Epoch 100 	 48.150677 	 54.584194 	 2663.463623
Epoch 110 	 47.243122 	 53.563660 	 58.540833
Epoch 120 	 44.596958 	 51.194698 	 56.166229
Epoch 130 	 43.317852 	 53.424992 	 57.099422
Epoch 140 	 44.681149 	 52.632019 	 60.101948
Epoch 150 	 42.984627 	 51.488594 	 88.058846
Epoch 160 	 42.020393 	 52.088657 	 114.344376
Epoch 170 	 41.678196 	 51.287758 	 56.934750
Epoch 180 	 42.262463 	 52.040253 	 56.324390
Epoch 190 	 41.373409 	 52.776093 	 83.755821
Train loss       : 42.003139
Best valid loss  : 49.431870
Best test loss   : 68.304962
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,557,891
--------------------------------
Total memory      : 23.80 MB
Total Flops       : 134.79 MFlops
Total Mem (Read)  : 20.7 MB
Total Mem (Write) : 18.92 MB
[Supermasks testing]
[Untrained loss : 94.9566]
[Starting training]
Epoch 0 	 91.639748 	 87.859680 	 91.996063
Epoch 10 	 63.501831 	 60.569401 	 65.390190
Epoch 20 	 57.873180 	 56.440331 	 61.097271
Epoch 30 	 55.114258 	 53.848801 	 58.722847
Epoch 40 	 49.440536 	 47.801678 	 52.157856
Epoch 50 	 43.989632 	 45.021065 	 49.919445
Epoch 60 	 42.695896 	 42.205925 	 48.824837
Epoch 70 	 40.230667 	 42.029224 	 47.488167
Epoch 80 	 41.715549 	 41.874569 	 46.347511
Epoch 90 	 37.992329 	 41.009590 	 46.179523
Epoch 100 	 37.808174 	 41.757595 	 44.953735
Epoch 110 	 35.299370 	 37.889694 	 44.057159
Epoch 120 	 35.376663 	 38.857224 	 43.739563
Epoch 130 	 34.615681 	 38.344456 	 44.530704
Epoch 140 	 34.250160 	 38.666798 	 43.066963
[Model stopped early]
Train loss       : 34.336647
Best valid loss  : 37.394199
Best test loss   : 43.290462
Pruning          : 0.27
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,919,262
--------------------------------
Total memory      : 20.31 MB
Total Flops       : 43.1 MFlops
Total Mem (Read)  : 14.81 MB
Total Mem (Write) : 12.44 MB
[Supermasks testing]
[Untrained loss : 95.2105]
[Starting training]
Epoch 0 	 92.192734 	 89.529770 	 92.541321
Epoch 10 	 64.551376 	 64.166954 	 68.479431
Epoch 20 	 59.935009 	 56.825966 	 60.738995
Epoch 30 	 56.655727 	 53.956081 	 59.395191
Epoch 40 	 51.670765 	 50.667053 	 54.276134
Epoch 50 	 46.164917 	 47.495403 	 50.061775
Epoch 60 	 45.086525 	 51.952442 	 54.145599
Epoch 70 	 41.947899 	 44.463890 	 47.183407
Epoch 80 	 40.880577 	 46.385956 	 49.165455
Epoch 90 	 38.464088 	 43.086124 	 45.627697
Epoch 100 	 36.855728 	 42.721916 	 45.542187
Epoch 110 	 36.185974 	 42.135162 	 44.242645
Epoch 120 	 34.914570 	 42.961887 	 44.071785
Epoch 130 	 35.028759 	 42.409801 	 44.059216
Epoch 140 	 34.512497 	 42.032001 	 43.737621
Epoch 150 	 34.261402 	 43.029297 	 43.730869
Epoch 160 	 34.215382 	 41.116890 	 43.763893
Epoch 170 	 34.342056 	 42.322300 	 43.964909
Epoch 180 	 34.058983 	 41.492439 	 43.822552
Epoch 190 	 34.170860 	 41.764275 	 43.915279
[Model stopped early]
Train loss       : 33.865795
Best valid loss  : 40.793816
Best test loss   : 43.726486
Pruning          : 0.19
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,396,419
--------------------------------
Total memory      : 19.38 MB
Total Flops       : 28.24 MFlops
Total Mem (Read)  : 11.9 MB
Total Mem (Write) : 9.14 MB
[Supermasks testing]
[Untrained loss : 95.4788]
[Starting training]
Epoch 0 	 92.840363 	 89.147011 	 92.599480
Epoch 10 	 66.123322 	 61.269669 	 67.424599
Epoch 20 	 65.878014 	 61.042080 	 65.408310
Epoch 30 	 55.495865 	 55.039196 	 60.802929
Epoch 40 	 53.295036 	 52.851261 	 58.239254
Epoch 50 	 49.362061 	 48.645851 	 54.608318
Epoch 60 	 45.553555 	 45.904472 	 50.934296
Epoch 70 	 43.340984 	 45.989807 	 52.646038
Epoch 80 	 42.993496 	 43.587589 	 48.422623
Epoch 90 	 39.554443 	 42.748196 	 46.718262
Epoch 100 	 40.052505 	 43.135773 	 47.423916
Epoch 110 	 38.552109 	 41.848248 	 46.641621
Epoch 120 	 37.007828 	 40.099442 	 46.055733
Epoch 130 	 37.495331 	 44.479511 	 47.429516
Epoch 140 	 35.194141 	 40.319519 	 44.986473
Epoch 150 	 34.477688 	 39.876968 	 44.095623
Epoch 160 	 33.875763 	 39.001266 	 44.360210
Epoch 170 	 33.369179 	 39.774738 	 44.802238
Epoch 180 	 33.513691 	 39.204960 	 44.174088
Epoch 190 	 33.792953 	 39.389381 	 43.846981
Train loss       : 33.552643
Best valid loss  : 37.769974
Best test loss   : 44.738190
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,009,753
--------------------------------
Total memory      : 19.04 MB
Total Flops       : 20.04 MFlops
Total Mem (Read)  : 10.09 MB
Total Mem (Write) : 7.3 MB
[Supermasks testing]
[Untrained loss : 103.3969]
[Starting training]
Epoch 0 	 95.935898 	 90.339165 	 94.224060
Epoch 10 	 64.754173 	 62.314987 	 67.286530
Epoch 20 	 58.364090 	 57.889160 	 61.379162
Epoch 30 	 56.915714 	 55.400360 	 60.989502
Epoch 40 	 54.859745 	 53.599403 	 58.745388
Epoch 50 	 53.068291 	 53.552937 	 58.221668
Epoch 60 	 50.800438 	 48.889458 	 54.418987
Epoch 70 	 45.369900 	 47.463974 	 50.857803
Epoch 80 	 43.063480 	 44.963745 	 49.229534
Epoch 90 	 44.215176 	 50.738373 	 55.801128
Epoch 100 	 40.962917 	 44.669090 	 49.679199
Epoch 110 	 40.284275 	 45.288635 	 49.706718
Epoch 120 	 40.597202 	 43.897751 	 49.060722
Epoch 130 	 39.815983 	 44.877270 	 48.113735
Epoch 140 	 38.746994 	 42.865475 	 48.831142
Epoch 150 	 39.173332 	 44.611607 	 49.507656
Epoch 160 	 37.910744 	 42.418434 	 47.055195
Epoch 170 	 36.772869 	 43.141888 	 47.272423
Epoch 180 	 36.347572 	 41.559109 	 46.446545
Epoch 190 	 36.293762 	 42.047764 	 46.334045
Train loss       : 35.915882
Best valid loss  : 41.500992
Best test loss   : 47.000332
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 824,082
--------------------------------
Total memory      : 17.76 MB
Total Flops       : 9.32 MFlops
Total Mem (Read)  : 8.1 MB
Total Mem (Write) : 5.44 MB
[Supermasks testing]
[Untrained loss : 95.4603]
[Starting training]
Epoch 0 	 91.808601 	 89.550621 	 92.135719
Epoch 10 	 67.312447 	 63.532742 	 68.645889
Epoch 20 	 60.357536 	 58.344387 	 63.161243
Epoch 30 	 59.979855 	 58.609161 	 63.061745
Epoch 40 	 57.236866 	 54.938854 	 61.864685
Epoch 50 	 56.964710 	 55.151577 	 59.482300
Epoch 60 	 55.564106 	 53.604252 	 58.411572
Epoch 70 	 54.186317 	 53.336208 	 57.482662
Epoch 80 	 52.521690 	 53.539948 	 58.038277
Epoch 90 	 52.579758 	 52.995369 	 57.011921
Epoch 100 	 51.854279 	 52.211552 	 56.657990
Epoch 110 	 51.827969 	 51.968021 	 56.171528
Epoch 120 	 51.708572 	 52.686848 	 55.722786
Epoch 130 	 51.423920 	 51.962658 	 56.042080
Epoch 140 	 51.434128 	 51.891129 	 55.935871
[Model stopped early]
Train loss       : 51.442986
Best valid loss  : 50.729053
Best test loss   : 55.947262
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 642,318
--------------------------------
Total memory      : 17.72 MB
Total Flops       : 7.73 MFlops
Total Mem (Read)  : 7.37 MB
Total Mem (Write) : 4.9 MB
[Supermasks testing]
[Untrained loss : 97.6804]
[Starting training]
Epoch 0 	 93.280197 	 89.902267 	 93.319244
Epoch 10 	 66.077988 	 62.077511 	 66.247337
Epoch 20 	 59.823776 	 58.840733 	 62.249214
Epoch 30 	 58.927658 	 57.114185 	 61.008511
Epoch 40 	 57.556690 	 55.802803 	 60.588207
Epoch 50 	 55.255146 	 54.258732 	 58.842983
Epoch 60 	 55.103424 	 55.129982 	 59.157063
Epoch 70 	 54.463497 	 53.945724 	 59.021038
Epoch 80 	 54.200184 	 53.650791 	 58.369167
Epoch 90 	 53.994926 	 53.491852 	 58.181149
Epoch 100 	 53.880939 	 53.728512 	 58.375977
Epoch 110 	 53.539448 	 53.895981 	 57.885273
Epoch 120 	 53.785217 	 54.009487 	 58.128704
[Model stopped early]
Train loss       : 53.785217
Best valid loss  : 52.058163
Best test loss   : 58.028393
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 513,413
--------------------------------
Total memory      : 17.41 MB
Total Flops       : 6.09 MFlops
Total Mem (Read)  : 6.58 MB
Total Mem (Write) : 4.35 MB
[Supermasks testing]
[Untrained loss : nan]
[Starting training]
Epoch 0 	 93.444695 	 89.623169 	 93.221359
Epoch 10 	 68.645485 	 66.242737 	 71.026001
Epoch 20 	 63.284531 	 62.972260 	 66.508644
Epoch 30 	 60.735809 	 63.195908 	 67.300659
Epoch 40 	 58.914261 	 57.084160 	 61.339184
Epoch 50 	 60.341667 	 57.873497 	 61.569794
Epoch 60 	 56.834969 	 57.593342 	 61.666836
Epoch 70 	 54.799072 	 55.754448 	 59.150398
Epoch 80 	 54.464901 	 54.042488 	 59.015285
Epoch 90 	 54.207253 	 55.119263 	 58.610714
Epoch 100 	 53.425049 	 53.182144 	 57.391968
Epoch 110 	 53.323940 	 52.577553 	 57.170162
[Model stopped early]
Train loss       : 53.903885
Best valid loss  : 52.159966
Best test loss   : 57.937477
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 408,665
--------------------------------
Total memory      : 17.38 MB
Total Flops       : 5.47 MFlops
Total Mem (Read)  : 6.16 MB
Total Mem (Write) : 4.13 MB
[Supermasks testing]
[Untrained loss : 97.4931]
[Starting training]
Epoch 0 	 92.995514 	 87.617142 	 93.549126
Epoch 10 	 67.462769 	 64.448227 	 69.546059
Epoch 20 	 63.888206 	 59.462864 	 64.799255
Epoch 30 	 61.543144 	 60.540493 	 65.516098
Epoch 40 	 56.663609 	 58.350994 	 61.807961
Epoch 50 	 57.008430 	 57.938503 	 62.479443
Epoch 60 	 56.186874 	 56.084526 	 61.417366
Epoch 70 	 54.295139 	 54.348309 	 59.783150
Epoch 80 	 53.046001 	 53.843193 	 59.078487
Epoch 90 	 52.999535 	 55.523682 	 59.336010
Epoch 100 	 52.397778 	 52.323013 	 57.680561
Epoch 110 	 53.045795 	 52.922096 	 57.849621
Epoch 120 	 51.950962 	 54.009258 	 57.883495
Epoch 130 	 51.348969 	 52.747360 	 56.729736
[Model stopped early]
Train loss       : 51.739559
Best valid loss  : 51.885475
Best test loss   : 58.107494
Pruning          : 0.03
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 329,143
--------------------------------
Total memory      : 17.26 MB
Total Flops       : 4.61 MFlops
Total Mem (Read)  : 5.74 MB
Total Mem (Write) : 3.89 MB
[Supermasks testing]
[Untrained loss : 101.7731]
[Starting training]
Epoch 0 	 94.375999 	 89.364273 	 93.454063
Epoch 10 	 67.580475 	 66.071289 	 70.618492
Epoch 20 	 62.329605 	 59.214088 	 66.277809
Epoch 30 	 60.810066 	 61.921036 	 65.970360
Epoch 40 	 58.158726 	 57.371174 	 62.553432
Epoch 50 	 57.800125 	 57.328407 	 62.074871
Epoch 60 	 56.051964 	 53.836800 	 59.767757
Epoch 70 	 55.831280 	 54.312160 	 59.359356
Epoch 80 	 54.371960 	 53.563557 	 58.700741
Epoch 90 	 54.235500 	 55.251541 	 59.479866
Epoch 100 	 52.922047 	 54.155514 	 58.261730
Epoch 110 	 52.918381 	 53.544033 	 57.971638
Epoch 120 	 52.958195 	 53.927135 	 58.509533
Epoch 130 	 52.554268 	 52.812675 	 57.902737
Epoch 140 	 52.001244 	 52.933777 	 57.657177
Epoch 150 	 52.388176 	 53.262901 	 57.618813
[Model stopped early]
Train loss       : 52.186737
Best valid loss  : 52.074528
Best test loss   : 57.856575
Pruning          : 0.02
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 276,783
--------------------------------
Total memory      : 17.16 MB
Total Flops       : 4.11 MFlops
Total Mem (Read)  : 5.44 MB
Total Mem (Write) : 3.73 MB
[Supermasks testing]
[Untrained loss : 95.7955]
[Starting training]
Epoch 0 	 92.348595 	 91.310387 	 93.202660
Epoch 10 	 67.242508 	 65.543587 	 69.691490
Epoch 20 	 61.859055 	 59.654522 	 64.297974
Epoch 30 	 61.094353 	 58.095436 	 62.911053
Epoch 40 	 57.357861 	 54.907082 	 60.090992
Epoch 50 	 56.633072 	 56.500397 	 62.046516
Epoch 60 	 55.408028 	 53.784008 	 59.717865
Epoch 70 	 54.964325 	 53.525433 	 59.487026
Epoch 80 	 55.050022 	 53.092560 	 59.990051
Epoch 90 	 54.511303 	 53.165047 	 59.281685
Epoch 100 	 53.851799 	 52.754086 	 59.837429
Epoch 110 	 53.629627 	 53.630219 	 60.060432
Epoch 120 	 54.195156 	 53.588047 	 59.794380
[Model stopped early]
Train loss       : 53.857296
Best valid loss  : 52.474777
Best test loss   : 59.626869
Pruning          : 0.01
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 232,715
--------------------------------
Total memory      : 17.05 MB
Total Flops       : 3.68 MFlops
Total Mem (Read)  : 5.16 MB
Total Mem (Write) : 3.58 MB
[Supermasks testing]
[Untrained loss : 97.9978]
[Starting training]
Epoch 0 	 92.673141 	 90.240143 	 92.749718
Epoch 10 	 67.740608 	 63.106934 	 68.025467
Epoch 20 	 61.582989 	 59.723320 	 64.431717
Epoch 30 	 59.433674 	 58.164227 	 62.242397
Epoch 40 	 59.283649 	 57.145779 	 61.402222
Epoch 50 	 56.944588 	 55.963116 	 60.737118
Epoch 60 	 56.609745 	 55.354107 	 60.732979
Epoch 70 	 55.705872 	 54.617149 	 59.928448
Epoch 80 	 55.332649 	 55.169090 	 59.927029
Epoch 90 	 55.542267 	 54.634750 	 59.590027
Epoch 100 	 55.080872 	 55.812443 	 59.977222
Epoch 110 	 55.449978 	 53.760586 	 59.763454
Epoch 120 	 54.698746 	 53.883530 	 59.239887
Epoch 130 	 54.491035 	 54.695908 	 59.274441
Epoch 140 	 54.535667 	 54.405830 	 59.273048
Epoch 150 	 54.233086 	 54.502541 	 59.326515
Epoch 160 	 54.673050 	 53.833252 	 59.498299
[Model stopped early]
Train loss       : 54.415409
Best valid loss  : 52.913773
Best test loss   : 59.322842
Pruning          : 0.01
