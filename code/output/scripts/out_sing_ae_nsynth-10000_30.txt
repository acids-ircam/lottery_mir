Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40871942.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, future, torch, six, torchvision, tqdm, python-dateutil, pyparsing, cycler, kiwisolver, matplotlib, keras-preprocessing, tensorflow-estimator, absl-py, astor, gast, termcolor, protobuf, urllib3, idna, chardet, certifi, requests, oauthlib, requests-oauthlib, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, grpcio, werkzeug, markdown, tensorboard, wrapt, opt-einsum, google-pasta, h5py, keras-applications, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871942.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-22 07:38:50.960300: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-22 07:38:51.310716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_sing_ae_cnn_xavier_masking_magnitude_rewind_global_0.
*******
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5293]
[Starting training]
Epoch 0 	 0.469250 	 0.428129 	 0.437310
Epoch 10 	 0.206400 	 0.216349 	 0.216995
Epoch 20 	 0.171641 	 0.174812 	 0.178745
Epoch 30 	 0.151564 	 0.174426 	 0.174208
Epoch 40 	 0.143816 	 0.161797 	 0.164299
Epoch 50 	 0.139468 	 0.157306 	 0.159065
Epoch 60 	 0.135103 	 0.154742 	 0.156950
Epoch 70 	 0.134903 	 0.155357 	 0.158331
Epoch 80 	 0.129449 	 0.152953 	 0.153514
Epoch 90 	 0.115729 	 0.140503 	 0.142583
Epoch 100 	 0.109497 	 0.137511 	 0.137890
Epoch 110 	 0.106666 	 0.138481 	 0.138114
Epoch 120 	 0.105703 	 0.136761 	 0.137009
Epoch 130 	 0.104300 	 0.134584 	 0.136558
Epoch 140 	 0.102697 	 0.133799 	 0.135888
Epoch 150 	 0.102011 	 0.133791 	 0.135131
Train loss       : 0.100818
Best valid loss  : 0.133791
Best test loss   : 0.135131
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1521]
[Starting training]
Epoch 0 	 0.133976 	 0.151426 	 0.153335
Epoch 10 	 0.130015 	 0.153636 	 0.153116
Epoch 20 	 0.118237 	 0.140411 	 0.141727
Epoch 30 	 0.113004 	 0.142932 	 0.142686
Epoch 40 	 0.105298 	 0.135178 	 0.135859
Epoch 50 	 0.103923 	 0.134108 	 0.135687
Epoch 60 	 0.100425 	 0.133422 	 0.133551
Epoch 70 	 0.099636 	 0.132391 	 0.133058
Epoch 80 	 0.097503 	 0.131478 	 0.132225
Epoch 90 	 0.096536 	 0.130245 	 0.131926
[Model stopped early]
Train loss       : 0.096562
Best valid loss  : 0.130202
Best test loss   : 0.133533
Pruning          : 0.70
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1521]
[Starting training]
Epoch 0 	 0.133053 	 0.148770 	 0.151325
Epoch 10 	 0.127152 	 0.151551 	 0.154208
Epoch 20 	 0.114685 	 0.141871 	 0.142503
Epoch 30 	 0.112583 	 0.141493 	 0.141624
Epoch 40 	 0.111440 	 0.137128 	 0.140029
Epoch 50 	 0.110700 	 0.137978 	 0.140396
Epoch 60 	 0.102525 	 0.135163 	 0.134901
Epoch 70 	 0.101569 	 0.134429 	 0.134313
Epoch 80 	 0.100681 	 0.133504 	 0.133419
Epoch 90 	 0.096891 	 0.131003 	 0.131143
Epoch 100 	 0.096288 	 0.131936 	 0.131301
Epoch 110 	 0.095651 	 0.130167 	 0.130805
Epoch 120 	 0.095299 	 0.130775 	 0.130279
Epoch 130 	 0.093384 	 0.130752 	 0.129520
Epoch 140 	 0.092328 	 0.130501 	 0.129472
Epoch 150 	 0.092101 	 0.130991 	 0.129412
Train loss       : 0.091691
Best valid loss  : 0.128085
Best test loss   : 0.129379
Pruning          : 0.49
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1521]
[Starting training]
Epoch 0 	 0.134466 	 0.150762 	 0.152980
Epoch 10 	 0.128746 	 0.151443 	 0.152703
Epoch 20 	 0.125730 	 0.145579 	 0.150026
Epoch 30 	 0.124002 	 0.145635 	 0.146859
Epoch 40 	 0.111729 	 0.141183 	 0.141338
Epoch 50 	 0.112026 	 0.140717 	 0.141275
Epoch 60 	 0.108515 	 0.137095 	 0.137988
Epoch 70 	 0.101213 	 0.132783 	 0.133451
Epoch 80 	 0.098393 	 0.129659 	 0.131078
Epoch 90 	 0.097039 	 0.130526 	 0.130866
Epoch 100 	 0.096554 	 0.130635 	 0.130675
Epoch 110 	 0.094551 	 0.129870 	 0.129640
Epoch 120 	 0.093581 	 0.129873 	 0.129700
Epoch 130 	 0.093374 	 0.130418 	 0.129632
Epoch 140 	 0.092825 	 0.130678 	 0.129419
Epoch 150 	 0.092614 	 0.129188 	 0.129408
Train loss       : 0.092505
Best valid loss  : 0.127491
Best test loss   : 0.129373
Pruning          : 0.34
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1523]
[Starting training]
Epoch 0 	 0.133155 	 0.151891 	 0.152046
Epoch 10 	 0.127858 	 0.152200 	 0.154598
Epoch 20 	 0.126393 	 0.147206 	 0.150702
Epoch 30 	 0.125322 	 0.150741 	 0.151642
Epoch 40 	 0.122573 	 0.145382 	 0.147082
Epoch 50 	 0.120526 	 0.146205 	 0.147359
Epoch 60 	 0.108618 	 0.134999 	 0.136702
Epoch 70 	 0.107583 	 0.137545 	 0.136890
Epoch 80 	 0.106974 	 0.134064 	 0.136574
Epoch 90 	 0.099533 	 0.132460 	 0.131901
Epoch 100 	 0.098973 	 0.132003 	 0.131307
Epoch 110 	 0.095572 	 0.129711 	 0.129864
Epoch 120 	 0.095138 	 0.128252 	 0.129878
Epoch 130 	 0.094636 	 0.128420 	 0.129835
Epoch 140 	 0.094134 	 0.129290 	 0.129584
Epoch 150 	 0.092461 	 0.128666 	 0.128542
Train loss       : 0.091596
Best valid loss  : 0.126690
Best test loss   : 0.128475
Pruning          : 0.24
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1530]
[Starting training]
Epoch 0 	 0.133471 	 0.149374 	 0.151936
Epoch 10 	 0.128620 	 0.151116 	 0.152845
Epoch 20 	 0.127113 	 0.152990 	 0.155638
Epoch 30 	 0.112671 	 0.139097 	 0.140700
Epoch 40 	 0.111343 	 0.138308 	 0.140696
Epoch 50 	 0.104013 	 0.134470 	 0.135192
Epoch 60 	 0.100506 	 0.133224 	 0.132951
Epoch 70 	 0.099392 	 0.133657 	 0.132818
Epoch 80 	 0.097250 	 0.130893 	 0.131573
Epoch 90 	 0.096844 	 0.132647 	 0.131453
Epoch 100 	 0.096133 	 0.131626 	 0.131275
Epoch 110 	 0.095682 	 0.132005 	 0.131099
Epoch 120 	 0.095304 	 0.130786 	 0.130889
Epoch 130 	 0.094948 	 0.128685 	 0.130874
[Model stopped early]
Train loss       : 0.095006
Best valid loss  : 0.128658
Best test loss   : 0.131153
Pruning          : 0.17
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1530]
[Starting training]
Epoch 0 	 0.133781 	 0.151456 	 0.153548
Epoch 10 	 0.128434 	 0.154575 	 0.157950
Epoch 20 	 0.126073 	 0.146045 	 0.149320
Epoch 30 	 0.127171 	 0.152234 	 0.154659
Epoch 40 	 0.111115 	 0.138825 	 0.139575
Epoch 50 	 0.110596 	 0.138947 	 0.139434
Epoch 60 	 0.104181 	 0.134367 	 0.134163
Epoch 70 	 0.101745 	 0.134190 	 0.133928
Epoch 80 	 0.098198 	 0.132783 	 0.131588
Epoch 90 	 0.096383 	 0.131466 	 0.130901
Epoch 100 	 0.095879 	 0.129796 	 0.130797
Epoch 110 	 0.094806 	 0.131171 	 0.130619
Epoch 120 	 0.094590 	 0.127591 	 0.130494
Epoch 130 	 0.094438 	 0.129567 	 0.130300
Epoch 140 	 0.094232 	 0.130069 	 0.130283
Epoch 150 	 0.094191 	 0.129814 	 0.130288
Train loss       : 0.094007
Best valid loss  : 0.127500
Best test loss   : 0.130247
Pruning          : 0.12
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1531]
[Starting training]
Epoch 0 	 0.133869 	 0.153519 	 0.154920
Epoch 10 	 0.128122 	 0.151808 	 0.152257
Epoch 20 	 0.114824 	 0.141631 	 0.142412
Epoch 30 	 0.108490 	 0.138147 	 0.137566
Epoch 40 	 0.105959 	 0.135056 	 0.137104
Epoch 50 	 0.101561 	 0.134254 	 0.134318
Epoch 60 	 0.100986 	 0.134734 	 0.134373
Epoch 70 	 0.098629 	 0.133291 	 0.133105
Epoch 80 	 0.098437 	 0.132255 	 0.132871
Epoch 90 	 0.097428 	 0.134099 	 0.132603
Epoch 100 	 0.097322 	 0.131892 	 0.132339
Epoch 110 	 0.096992 	 0.132883 	 0.132304
Epoch 120 	 0.096416 	 0.130937 	 0.132201
Epoch 130 	 0.096087 	 0.132336 	 0.132124
[Model stopped early]
Train loss       : 0.096087
Best valid loss  : 0.130361
Best test loss   : 0.132469
Pruning          : 0.08
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1531]
[Starting training]
Epoch 0 	 0.134800 	 0.153525 	 0.154610
Epoch 10 	 0.130919 	 0.149091 	 0.152805
Epoch 20 	 0.127865 	 0.153264 	 0.154581
Epoch 30 	 0.125910 	 0.150626 	 0.151784
Epoch 40 	 0.111193 	 0.140037 	 0.140949
Epoch 50 	 0.104419 	 0.135933 	 0.135297
Epoch 60 	 0.103115 	 0.133865 	 0.134521
Epoch 70 	 0.099507 	 0.132297 	 0.132201
Epoch 80 	 0.097505 	 0.130630 	 0.131547
Epoch 90 	 0.097139 	 0.131049 	 0.131508
Epoch 100 	 0.096063 	 0.131523 	 0.131183
Epoch 110 	 0.095512 	 0.130325 	 0.130999
[Model stopped early]
Train loss       : 0.095724
Best valid loss  : 0.129750
Best test loss   : 0.131527
Pruning          : 0.06
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1533]
[Starting training]
Epoch 0 	 0.134078 	 0.152218 	 0.153728
Epoch 10 	 0.128204 	 0.149770 	 0.151568
Epoch 20 	 0.126038 	 0.150264 	 0.151976
Epoch 30 	 0.125349 	 0.147531 	 0.149304
Epoch 40 	 0.111345 	 0.138628 	 0.138985
Epoch 50 	 0.110073 	 0.137695 	 0.139052
Epoch 60 	 0.102930 	 0.133932 	 0.133781
Epoch 70 	 0.099137 	 0.132775 	 0.132147
Epoch 80 	 0.098518 	 0.131091 	 0.131582
Epoch 90 	 0.097852 	 0.129743 	 0.131341
Epoch 100 	 0.095973 	 0.130461 	 0.130575
Epoch 110 	 0.095078 	 0.128894 	 0.130411
[Model stopped early]
Train loss       : 0.095078
Best valid loss  : 0.128729
Best test loss   : 0.131498
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1586]
[Starting training]
Epoch 0 	 0.134819 	 0.151462 	 0.153261
Epoch 10 	 0.129265 	 0.149148 	 0.150910
Epoch 20 	 0.124090 	 0.145939 	 0.147443
Epoch 30 	 0.121511 	 0.144949 	 0.148529
Epoch 40 	 0.118181 	 0.144658 	 0.146568
Epoch 50 	 0.117965 	 0.143111 	 0.145977
Epoch 60 	 0.108990 	 0.135272 	 0.135672
Epoch 70 	 0.105858 	 0.135249 	 0.136012
Epoch 80 	 0.099071 	 0.130223 	 0.131298
Epoch 90 	 0.095939 	 0.130062 	 0.129672
Epoch 100 	 0.095575 	 0.129151 	 0.129681
Epoch 110 	 0.094092 	 0.129641 	 0.129015
Epoch 120 	 0.093744 	 0.129413 	 0.128675
Epoch 130 	 0.092940 	 0.129190 	 0.128420
Epoch 140 	 0.092636 	 0.127400 	 0.128441
Epoch 150 	 0.092372 	 0.128786 	 0.128239
Train loss       : 0.092374
Best valid loss  : 0.126237
Best test loss   : 0.128277
Pruning          : 0.03
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.1999]
[Starting training]
Epoch 0 	 0.140515 	 0.150697 	 0.153134
Epoch 10 	 0.120962 	 0.145073 	 0.144453
Epoch 20 	 0.114879 	 0.139867 	 0.141650
Epoch 30 	 0.113226 	 0.139872 	 0.140750
Epoch 40 	 0.111420 	 0.138384 	 0.140132
Epoch 50 	 0.110458 	 0.139280 	 0.140041
Epoch 60 	 0.109953 	 0.139379 	 0.141698
Epoch 70 	 0.109037 	 0.136742 	 0.137269
Epoch 80 	 0.099133 	 0.132101 	 0.131021
Epoch 90 	 0.096469 	 0.128351 	 0.129250
Epoch 100 	 0.095325 	 0.129093 	 0.128505
Epoch 110 	 0.094965 	 0.129587 	 0.129413
Epoch 120 	 0.092681 	 0.126970 	 0.127563
Epoch 130 	 0.091650 	 0.126231 	 0.127171
Epoch 140 	 0.091042 	 0.127689 	 0.127070
Epoch 150 	 0.090958 	 0.128533 	 0.126968
Train loss       : 0.090620
Best valid loss  : 0.125099
Best test loss   : 0.127018
Pruning          : 0.02
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.3029]
[Starting training]
Epoch 0 	 0.170538 	 0.163738 	 0.165215
Epoch 10 	 0.121392 	 0.143079 	 0.144806
Epoch 20 	 0.113173 	 0.139087 	 0.140758
Epoch 30 	 0.110705 	 0.140447 	 0.138243
Epoch 40 	 0.101778 	 0.132510 	 0.132038
Epoch 50 	 0.099151 	 0.132001 	 0.130520
Epoch 60 	 0.097988 	 0.131935 	 0.130424
Epoch 70 	 0.097890 	 0.130664 	 0.130071
Epoch 80 	 0.097618 	 0.130448 	 0.130328
Epoch 90 	 0.097235 	 0.128597 	 0.130308
Epoch 100 	 0.095596 	 0.128088 	 0.129422
Epoch 110 	 0.095530 	 0.129911 	 0.129414
Epoch 120 	 0.094633 	 0.130249 	 0.129077
Epoch 130 	 0.094023 	 0.128633 	 0.128859
Epoch 140 	 0.093845 	 0.129509 	 0.128890
Epoch 150 	 0.093819 	 0.130900 	 0.128883
[Model stopped early]
Train loss       : 0.093807
Best valid loss  : 0.127643
Best test loss   : 0.128878
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.4299]
[Starting training]
Epoch 0 	 0.226103 	 0.189057 	 0.189307
Epoch 10 	 0.129877 	 0.148867 	 0.149925
Epoch 20 	 0.122587 	 0.145260 	 0.145840
Epoch 30 	 0.120231 	 0.145191 	 0.143823
Epoch 40 	 0.117389 	 0.143498 	 0.143029
Epoch 50 	 0.117484 	 0.143183 	 0.142312
Epoch 60 	 0.111060 	 0.136374 	 0.137794
Epoch 70 	 0.110903 	 0.138405 	 0.138249
Epoch 80 	 0.108038 	 0.136319 	 0.136292
Epoch 90 	 0.106637 	 0.136006 	 0.135459
Epoch 100 	 0.106355 	 0.134537 	 0.135363
Epoch 110 	 0.106327 	 0.136691 	 0.135421
Epoch 120 	 0.105510 	 0.135636 	 0.135180
Epoch 130 	 0.105152 	 0.137660 	 0.135097
Epoch 140 	 0.104989 	 0.137007 	 0.135031
Epoch 150 	 0.104893 	 0.136686 	 0.134993
[Model stopped early]
Train loss       : 0.104735
Best valid loss  : 0.133695
Best test loss   : 0.135016
Pruning          : 0.01
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.4824]
[Starting training]
Epoch 0 	 0.277184 	 0.217991 	 0.218134
Epoch 10 	 0.142967 	 0.158674 	 0.160835
Epoch 20 	 0.134841 	 0.153901 	 0.154699
Epoch 30 	 0.129856 	 0.150363 	 0.151087
Epoch 40 	 0.128795 	 0.150077 	 0.150234
Epoch 50 	 0.122707 	 0.144332 	 0.145958
Epoch 60 	 0.122232 	 0.145896 	 0.145954
Epoch 70 	 0.119379 	 0.143988 	 0.144610
Epoch 80 	 0.119518 	 0.144511 	 0.144199
Epoch 90 	 0.117966 	 0.141852 	 0.143781
Epoch 100 	 0.117373 	 0.142906 	 0.143686
[Model stopped early]
Train loss       : 0.117431
Best valid loss  : 0.141677
Best test loss   : 0.144447
Pruning          : 0.01
