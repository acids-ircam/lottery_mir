Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41289067.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, pillow-simd, future, torch, torchvision, tqdm, cycler, python-dateutil, pyparsing, kiwisolver, matplotlib, astor, h5py, keras-applications, tensorflow-estimator, keras-preprocessing, wrapt, protobuf, gast, opt-einsum, google-pasta, grpcio, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, urllib3, certifi, idna, chardet, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, werkzeug, markdown, tensorboard, termcolor, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41289067.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 05:00:22.805263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 05:00:23.133283: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is singing_classify_cnn_xavier_trimming_info_target_reinit_local_0.
*******
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9550]
[Starting training]
Epoch 0 	 0.853745 	 0.753217 	 0.742371
Epoch 10 	 0.325483 	 0.401195 	 0.248805
Epoch 20 	 0.150735 	 0.264246 	 0.114154
Epoch 30 	 0.091682 	 0.196232 	 0.062684
Epoch 40 	 0.063304 	 0.179688 	 0.046048
Epoch 50 	 0.049288 	 0.173254 	 0.042463
Epoch 60 	 0.040901 	 0.166360 	 0.036949
Epoch 70 	 0.040097 	 0.164982 	 0.035018
Epoch 80 	 0.029182 	 0.159926 	 0.033364
Epoch 90 	 0.027918 	 0.161305 	 0.033548
Epoch 100 	 0.024357 	 0.156250 	 0.032261
Epoch 110 	 0.011374 	 0.150735 	 0.030423
Epoch 120 	 0.012638 	 0.154871 	 0.031158
Epoch 130 	 0.008387 	 0.151654 	 0.030423
Epoch 140 	 0.006089 	 0.151654 	 0.030423
Epoch 150 	 0.005630 	 0.154412 	 0.030974
[Model stopped early]
Train loss       : 0.006089
Best valid loss  : 0.147059
Best test loss   : 0.029688
Pruning          : 1.00
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 774,827
--------------------------------
Total memory      : 6.42 MB
Total Flops       : 398.98 MFlops
Total Mem (Read)  : 8.28 MB
Total Mem (Write) : 4.82 MB
[Supermasks testing]
[Untrained loss : 0.9871]
[Starting training]
Epoch 0 	 0.814223 	 0.729320 	 0.725368
Epoch 10 	 0.360409 	 0.428309 	 0.292923
Epoch 20 	 0.178079 	 0.260570 	 0.129320
Epoch 30 	 0.117302 	 0.221048 	 0.081526
Epoch 40 	 0.092831 	 0.192555 	 0.057445
Epoch 50 	 0.066751 	 0.181066 	 0.046507
Epoch 60 	 0.059398 	 0.166360 	 0.040993
Epoch 70 	 0.049403 	 0.166820 	 0.038143
Epoch 80 	 0.030561 	 0.176930 	 0.037776
Epoch 90 	 0.025276 	 0.168199 	 0.035662
Epoch 100 	 0.020680 	 0.166820 	 0.034835
Epoch 110 	 0.016314 	 0.169118 	 0.034926
[Model stopped early]
Train loss       : 0.016659
Best valid loss  : 0.162224
Best test loss   : 0.034191
Pruning          : 0.75
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 442,883
--------------------------------
Total memory      : 4.82 MB
Total Flops       : 243.8 MFlops
Total Mem (Read)  : 5.81 MB
Total Mem (Write) : 3.61 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.814453 	 0.703585 	 0.697426
Epoch 10 	 0.445083 	 0.491728 	 0.351746
Epoch 20 	 0.247472 	 0.321232 	 0.168750
Epoch 30 	 0.174747 	 0.262868 	 0.111213
Epoch 40 	 0.124540 	 0.224724 	 0.077206
Epoch 50 	 0.110754 	 0.209559 	 0.065901
Epoch 60 	 0.092716 	 0.186121 	 0.054688
Epoch 70 	 0.084903 	 0.182445 	 0.049540
Epoch 80 	 0.075597 	 0.180147 	 0.044118
Epoch 90 	 0.053653 	 0.175092 	 0.040993
Epoch 100 	 0.045381 	 0.174632 	 0.041360
[Model stopped early]
Train loss       : 0.041475
Best valid loss  : 0.166360
Best test loss   : 0.043015
Pruning          : 0.56
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 254,405
--------------------------------
Total memory      : 3.61 MB
Total Flops       : 151.67 MFlops
Total Mem (Read)  : 4.19 MB
Total Mem (Write) : 2.71 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.845703 	 0.746324 	 0.742647
Epoch 10 	 0.497702 	 0.502298 	 0.396415
Epoch 20 	 0.318704 	 0.366728 	 0.227298
Epoch 30 	 0.241153 	 0.306985 	 0.157812
Epoch 40 	 0.191981 	 0.273897 	 0.119210
Epoch 50 	 0.163373 	 0.227482 	 0.089706
Epoch 60 	 0.136604 	 0.219669 	 0.081710
Epoch 70 	 0.122243 	 0.212776 	 0.069761
Epoch 80 	 0.112822 	 0.193474 	 0.061581
Epoch 90 	 0.100873 	 0.186581 	 0.049724
Epoch 100 	 0.092601 	 0.189338 	 0.051379
Epoch 110 	 0.085248 	 0.176930 	 0.041820
Epoch 120 	 0.069393 	 0.175092 	 0.043474
Epoch 130 	 0.059513 	 0.170037 	 0.039062
Epoch 140 	 0.054802 	 0.170496 	 0.038787
Epoch 150 	 0.049403 	 0.175551 	 0.040717
[Model stopped early]
Train loss       : 0.049403
Best valid loss  : 0.164522
Best test loss   : 0.039154
Pruning          : 0.42
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 144,361
--------------------------------
Total memory      : 2.68 MB
Total Flops       : 94.38 MFlops
Total Mem (Read)  : 3.06 MB
Total Mem (Write) : 2.01 MB
[Supermasks testing]
[Untrained loss : 0.8866]
[Starting training]
Epoch 0 	 0.872702 	 0.859835 	 0.873162
Epoch 10 	 0.629481 	 0.607996 	 0.532996
Epoch 20 	 0.449793 	 0.476103 	 0.359651
Epoch 30 	 0.333295 	 0.372243 	 0.250735
Epoch 40 	 0.278837 	 0.339614 	 0.210110
Epoch 50 	 0.239200 	 0.292739 	 0.166912
Epoch 60 	 0.216912 	 0.277574 	 0.142279
Epoch 70 	 0.200253 	 0.255974 	 0.129228
Epoch 80 	 0.177045 	 0.237132 	 0.110018
Epoch 90 	 0.169347 	 0.216912 	 0.093934
Epoch 100 	 0.157973 	 0.208640 	 0.087316
Epoch 110 	 0.149816 	 0.196691 	 0.079687
Epoch 120 	 0.139936 	 0.187960 	 0.066544
Epoch 130 	 0.143153 	 0.183824 	 0.068382
Epoch 140 	 0.121783 	 0.188879 	 0.061765
Epoch 150 	 0.116383 	 0.173713 	 0.058088
Train loss       : 0.112822
Best valid loss  : 0.173713
Best test loss   : 0.058088
Pruning          : 0.32
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 84,015
--------------------------------
Total memory      : 2.01 MB
Total Flops       : 61.16 MFlops
Total Mem (Read)  : 2.33 MB
Total Mem (Write) : 1.51 MB
[Supermasks testing]
[Untrained loss : 0.8851]
[Starting training]
Epoch 0 	 0.832146 	 0.791360 	 0.797794
Epoch 10 	 0.643842 	 0.630515 	 0.567371
Epoch 20 	 0.491958 	 0.522059 	 0.416636
Epoch 30 	 0.406020 	 0.463235 	 0.331066
Epoch 40 	 0.356618 	 0.414982 	 0.281985
Epoch 50 	 0.326861 	 0.372243 	 0.231710
Epoch 60 	 0.300322 	 0.357077 	 0.217004
Epoch 70 	 0.287569 	 0.334099 	 0.193382
Epoch 80 	 0.261029 	 0.322610 	 0.183088
Epoch 90 	 0.252528 	 0.322151 	 0.167831
Epoch 100 	 0.242073 	 0.290441 	 0.147610
Epoch 110 	 0.235064 	 0.274816 	 0.129320
Epoch 120 	 0.217256 	 0.272518 	 0.130607
Epoch 130 	 0.202551 	 0.262868 	 0.118658
Epoch 140 	 0.201402 	 0.265625 	 0.121507
Epoch 150 	 0.194278 	 0.258732 	 0.112132
Train loss       : 0.194623
Best valid loss  : 0.245404
Best test loss   : 0.105515
Pruning          : 0.24
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 47,755
--------------------------------
Total memory      : 1.47 MB
Total Flops       : 39.21 MFlops
Total Mem (Read)  : 1.79 MB
Total Mem (Write) : 1.1 MB
[Supermasks testing]
[Untrained loss : 0.8996]
[Starting training]
Epoch 0 	 0.896484 	 0.879596 	 0.883364
Epoch 10 	 0.675896 	 0.655331 	 0.603768
Epoch 20 	 0.576402 	 0.562960 	 0.474357
Epoch 30 	 0.501034 	 0.522518 	 0.403033
Epoch 40 	 0.453699 	 0.497702 	 0.367463
Epoch 50 	 0.426011 	 0.443474 	 0.311121
Epoch 60 	 0.394187 	 0.439338 	 0.294669
Epoch 70 	 0.385915 	 0.419577 	 0.264706
Epoch 80 	 0.371898 	 0.407629 	 0.251195
Epoch 90 	 0.350988 	 0.384651 	 0.227114
Epoch 100 	 0.343750 	 0.369945 	 0.217647
Epoch 110 	 0.331342 	 0.386029 	 0.236765
Epoch 120 	 0.329619 	 0.371783 	 0.210846
Epoch 130 	 0.318244 	 0.352941 	 0.202390
Epoch 140 	 0.314913 	 0.346048 	 0.196691
Epoch 150 	 0.306526 	 0.330882 	 0.178768
Train loss       : 0.315947
Best valid loss  : 0.322151
Best test loss   : 0.178217
Pruning          : 0.18
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 27,239
--------------------------------
Total memory      : 1.07 MB
Total Flops       : 25.43 MFlops
Total Mem (Read)  : 1.41 MB
Total Mem (Write) : 822.46 KB
[Supermasks testing]
[Untrained loss : 0.9421]
[Starting training]
Epoch 0 	 0.868336 	 0.848805 	 0.841176
Epoch 10 	 0.703240 	 0.666360 	 0.649908
Epoch 20 	 0.660616 	 0.607537 	 0.571599
Epoch 30 	 0.602367 	 0.582721 	 0.499449
Epoch 40 	 0.565028 	 0.546415 	 0.446691
Epoch 50 	 0.531135 	 0.527114 	 0.404688
Epoch 60 	 0.505400 	 0.505974 	 0.377574
Epoch 70 	 0.478860 	 0.487132 	 0.361949
Epoch 80 	 0.463006 	 0.487132 	 0.345864
Epoch 90 	 0.461857 	 0.477941 	 0.334835
Epoch 100 	 0.451976 	 0.455423 	 0.305515
Epoch 110 	 0.430836 	 0.459559 	 0.308732
Epoch 120 	 0.433249 	 0.454504 	 0.303860
Epoch 130 	 0.425781 	 0.436581 	 0.281618
Epoch 140 	 0.420841 	 0.434283 	 0.292279
Epoch 150 	 0.411420 	 0.432904 	 0.278860
Train loss       : 0.405676
Best valid loss  : 0.412684
Best test loss   : 0.264338
Pruning          : 0.13
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 16,485
--------------------------------
Total memory      : 0.80 MB
Total Flops       : 17.54 MFlops
Total Mem (Read)  : 1.17 MB
Total Mem (Write) : 616.86 KB
[Supermasks testing]
[Untrained loss : 0.9296]
[Starting training]
Epoch 0 	 0.871898 	 0.863971 	 0.867096
Epoch 10 	 0.720703 	 0.695312 	 0.678217
Epoch 20 	 0.693244 	 0.667279 	 0.647978
Epoch 30 	 0.664752 	 0.651195 	 0.610018
Epoch 40 	 0.642578 	 0.635570 	 0.581158
Epoch 50 	 0.620864 	 0.618107 	 0.559191
Epoch 60 	 0.587546 	 0.560202 	 0.478401
Epoch 70 	 0.556411 	 0.545037 	 0.455882
Epoch 80 	 0.552390 	 0.528493 	 0.439154
Epoch 90 	 0.529871 	 0.524357 	 0.416820
Epoch 100 	 0.522633 	 0.508272 	 0.396048
Epoch 110 	 0.509421 	 0.511949 	 0.394210
Epoch 120 	 0.497817 	 0.505974 	 0.389614
Epoch 130 	 0.495060 	 0.505515 	 0.388879
Epoch 140 	 0.501494 	 0.496783 	 0.381158
Epoch 150 	 0.487132 	 0.502298 	 0.383088
Train loss       : 0.488511
Best valid loss  : 0.488511
Best test loss   : 0.368015
Pruning          : 0.10
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 8,769
--------------------------------
Total memory      : 0.54 MB
Total Flops       : 10.66 MFlops
Total Mem (Read)  : 962.38 KB
Total Mem (Write) : 411.36 KB
[Supermasks testing]
[Untrained loss : 0.9871]
[Starting training]
Epoch 0 	 0.947610 	 0.957261 	 0.954412
Epoch 10 	 0.743222 	 0.679228 	 0.684467
Epoch 20 	 0.719554 	 0.660846 	 0.665074
Epoch 30 	 0.706572 	 0.662224 	 0.657812
Epoch 40 	 0.692555 	 0.660386 	 0.646415
Epoch 50 	 0.672105 	 0.647518 	 0.629412
Epoch 60 	 0.665556 	 0.633272 	 0.603860
Epoch 70 	 0.650161 	 0.628676 	 0.593566
Epoch 80 	 0.631549 	 0.614430 	 0.561581
Epoch 90 	 0.623277 	 0.592371 	 0.536581
Epoch 100 	 0.612247 	 0.575368 	 0.529412
Epoch 110 	 0.599724 	 0.569853 	 0.513143
Epoch 120 	 0.600184 	 0.568474 	 0.505423
Epoch 130 	 0.584559 	 0.564798 	 0.505607
Epoch 140 	 0.586167 	 0.567096 	 0.506066
Epoch 150 	 0.575712 	 0.558824 	 0.496783
Train loss       : 0.576746
Best valid loss  : 0.555147
Best test loss   : 0.495956
Pruning          : 0.08
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 5,489
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 846.73 KB
Total Mem (Write) : 308.53 KB
[Supermasks testing]
[Untrained loss : 0.9563]
[Starting training]
Epoch 0 	 0.838810 	 0.806526 	 0.802206
Epoch 10 	 0.747702 	 0.731618 	 0.710110
Epoch 20 	 0.728975 	 0.687040 	 0.674540
Epoch 30 	 0.713006 	 0.679688 	 0.664338
Epoch 40 	 0.695542 	 0.682445 	 0.660202
Epoch 50 	 0.687500 	 0.688419 	 0.656066
Epoch 60 	 0.670152 	 0.681526 	 0.645037
Epoch 70 	 0.676126 	 0.670496 	 0.636029
Epoch 80 	 0.664292 	 0.676011 	 0.636305
Epoch 90 	 0.660846 	 0.653952 	 0.612040
Epoch 100 	 0.663143 	 0.673254 	 0.630515
Epoch 110 	 0.659467 	 0.676011 	 0.632812
Epoch 120 	 0.654756 	 0.657629 	 0.618750
[Model stopped early]
Train loss       : 0.651310
Best valid loss  : 0.634651
Best test loss   : 0.592371
Pruning          : 0.06
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 4,929
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 844.38 KB
Total Mem (Write) : 308.37 KB
[Supermasks testing]
[Untrained loss : 0.9143]
[Starting training]
Epoch 0 	 0.925092 	 0.922335 	 0.920588
Epoch 10 	 0.834329 	 0.840533 	 0.832077
Epoch 20 	 0.742647 	 0.717371 	 0.704963
Epoch 30 	 0.716797 	 0.658088 	 0.661765
Epoch 40 	 0.706112 	 0.641544 	 0.644761
Epoch 50 	 0.698759 	 0.645221 	 0.637960
Epoch 60 	 0.694853 	 0.640165 	 0.624357
Epoch 70 	 0.684743 	 0.636029 	 0.624540
Epoch 80 	 0.681181 	 0.638327 	 0.623438
Epoch 90 	 0.679458 	 0.630055 	 0.611397
Epoch 100 	 0.671530 	 0.629596 	 0.613419
Epoch 110 	 0.671186 	 0.634651 	 0.615993
Epoch 120 	 0.674173 	 0.620404 	 0.609559
Epoch 130 	 0.682215 	 0.624540 	 0.608364
Epoch 140 	 0.665556 	 0.624081 	 0.602114
Epoch 150 	 0.668428 	 0.625000 	 0.607537
Train loss       : 0.670267
Best valid loss  : 0.616268
Best test loss   : 0.598254
Pruning          : 0.04
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 4,589
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 842.93 KB
Total Mem (Write) : 308.25 KB
[Supermasks testing]
[Untrained loss : 0.9587]
[Starting training]
Epoch 0 	 0.898093 	 0.937960 	 0.943474
Epoch 10 	 0.785616 	 0.747702 	 0.728860
Epoch 20 	 0.740809 	 0.728860 	 0.712684
Epoch 30 	 0.716912 	 0.708180 	 0.696324
Epoch 40 	 0.704159 	 0.676011 	 0.664430
Epoch 50 	 0.704963 	 0.644301 	 0.640257
Epoch 60 	 0.699678 	 0.638787 	 0.622886
Epoch 70 	 0.688189 	 0.623621 	 0.611673
Epoch 80 	 0.685317 	 0.641085 	 0.618566
Epoch 90 	 0.681641 	 0.625919 	 0.608640
Epoch 100 	 0.671760 	 0.624081 	 0.598713
Epoch 110 	 0.674173 	 0.626379 	 0.599540
[Model stopped early]
Train loss       : 0.668888
Best valid loss  : 0.619026
Best test loss   : 0.603217
Pruning          : 0.03
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 4,353
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 841.92 KB
Total Mem (Write) : 308.16 KB
[Supermasks testing]
[Untrained loss : 0.9658]
[Starting training]
Epoch 0 	 0.919462 	 0.922794 	 0.915441
Epoch 10 	 0.895795 	 0.915901 	 0.907537
Epoch 20 	 0.831342 	 0.905790 	 0.895221
Epoch 30 	 0.797335 	 0.764246 	 0.751838
Epoch 40 	 0.774586 	 0.746324 	 0.731526
Epoch 50 	 0.755285 	 0.739890 	 0.714982
Epoch 60 	 0.748392 	 0.732077 	 0.703860
Epoch 70 	 0.730469 	 0.718290 	 0.685018
Epoch 80 	 0.721048 	 0.718290 	 0.688879
Epoch 90 	 0.709329 	 0.710018 	 0.670956
Epoch 100 	 0.700827 	 0.698989 	 0.662776
Epoch 110 	 0.697725 	 0.692096 	 0.655147
Epoch 120 	 0.693359 	 0.692096 	 0.652574
Epoch 130 	 0.681870 	 0.687960 	 0.647518
Epoch 140 	 0.674403 	 0.688879 	 0.646415
Epoch 150 	 0.676700 	 0.689338 	 0.648529
Train loss       : 0.673713
Best valid loss  : 0.681526
Best test loss   : 0.641268
Pruning          : 0.02
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 4,197
--------------------------------
Total memory      : 0.40 MB
Total Flops       : 7.61 MFlops
Total Mem (Read)  : 841.24 KB
Total Mem (Write) : 308.09 KB
[Supermasks testing]
[Untrained loss : 0.9221]
[Starting training]
Epoch 0 	 0.914177 	 0.898897 	 0.897610
Epoch 10 	 0.858571 	 0.841452 	 0.828309
Epoch 20 	 0.792739 	 0.799173 	 0.787132
Epoch 30 	 0.767119 	 0.747243 	 0.724632
Epoch 40 	 0.752528 	 0.735754 	 0.713511
Epoch 50 	 0.735294 	 0.731618 	 0.707904
Epoch 60 	 0.731503 	 0.723805 	 0.705790
Epoch 70 	 0.724954 	 0.696232 	 0.683915
Epoch 80 	 0.723116 	 0.700368 	 0.687500
Epoch 90 	 0.712661 	 0.671415 	 0.664430
Epoch 100 	 0.718290 	 0.674632 	 0.660937
Epoch 110 	 0.705423 	 0.665441 	 0.652298
Epoch 120 	 0.699449 	 0.655331 	 0.643107
Epoch 130 	 0.703585 	 0.658548 	 0.644577
Epoch 140 	 0.702091 	 0.656250 	 0.639430
Epoch 150 	 0.703355 	 0.648897 	 0.637960
Train loss       : 0.694164
Best valid loss  : 0.644761
Best test loss   : 0.635754
Pruning          : 0.02
