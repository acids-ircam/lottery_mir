Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41281319.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, pillow-simd, six, torchvision, tqdm, cycler, pyparsing, kiwisolver, python-dateutil, matplotlib, protobuf, wrapt, absl-py, keras-preprocessing, opt-einsum, google-pasta, termcolor, gast, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, grpcio, werkzeug, urllib3, chardet, idna, certifi, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, tensorboard, astor, tensorflow-estimator, h5py, keras-applications, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41281319.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-29 02:25:22.842930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 02:25:23.174165: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is sol-ordinario_ddsp_cnn_xavier_trimming_gradient_min_rewind_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41281319.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 109.8199]
[Starting training]
Epoch 0 	 91.898575 	 89.271667 	 92.080399
Epoch 10 	 78.530571 	 76.147636 	 78.913666
Epoch 20 	 80.366074 	 75.465836 	 78.563194
Epoch 30 	 76.335968 	 76.324341 	 77.136642
Epoch 40 	 69.639946 	 67.485840 	 71.438774
Epoch 50 	 67.659256 	 68.258499 	 71.125298
Epoch 60 	 64.611099 	 63.519314 	 62.521423
Epoch 70 	 61.475990 	 60.389114 	 63.430279
Epoch 80 	 58.126312 	 59.594967 	 62.469025
Epoch 90 	 53.955669 	 54.056042 	 52.694763
Epoch 100 	 51.067356 	 51.441727 	 53.746765
Epoch 110 	 47.309536 	 47.898602 	 48.524700
Epoch 120 	 43.697067 	 44.171692 	 44.064152
Epoch 130 	 42.988495 	 43.197083 	 45.893940
Epoch 140 	 41.519485 	 40.169731 	 42.818756
Epoch 150 	 40.180893 	 40.918831 	 41.226353
Epoch 160 	 37.708309 	 38.960457 	 41.310776
Epoch 170 	 36.948597 	 38.585030 	 39.571949
Epoch 180 	 36.349636 	 38.373753 	 40.591953
Epoch 190 	 36.136589 	 35.534378 	 37.168957
Train loss       : 34.209328
Best valid loss  : 34.469776
Best test loss   : 37.180325
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,909,528
--------------------------------
Total memory      : 35.03 MB
Total Flops       : 326.2 MFlops
Total Mem (Read)  : 31.67 MB
Total Mem (Write) : 24.49 MB
[Supermasks testing]
[Untrained loss : 116.0829]
[Starting training]
Epoch 0 	 79.713570 	 71.041611 	 75.280724
Epoch 10 	 55.503410 	 50.442490 	 54.489849
Epoch 20 	 45.801422 	 44.070263 	 44.554504
Epoch 30 	 43.634075 	 45.046894 	 46.829182
Epoch 40 	 41.609577 	 40.399685 	 43.017735
Epoch 50 	 37.254395 	 37.181595 	 38.879154
Epoch 60 	 38.085613 	 34.945988 	 37.166592
Epoch 70 	 32.911423 	 34.286129 	 36.364391
Epoch 80 	 32.354130 	 34.674870 	 36.835308
Epoch 90 	 29.813011 	 30.616234 	 33.194241
Epoch 100 	 28.737556 	 30.557262 	 33.381866
Epoch 110 	 28.166391 	 30.212946 	 32.191189
Epoch 120 	 28.338703 	 29.489166 	 31.719091
Epoch 130 	 26.126730 	 28.268604 	 30.642460
Epoch 140 	 25.570539 	 28.664789 	 30.759207
Epoch 150 	 25.925360 	 28.287230 	 30.438404
Epoch 160 	 25.029764 	 28.523516 	 30.319044
Epoch 170 	 24.618732 	 28.156855 	 29.873091
Epoch 180 	 24.758951 	 28.581573 	 29.866659
Epoch 190 	 24.160597 	 27.603386 	 29.514753
Train loss       : 24.385023
Best valid loss  : 27.017466
Best test loss   : 29.746235
Pruning          : 0.72
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,898,752
--------------------------------
Total memory      : 29.65 MB
Total Flops       : 171.22 MFlops
Total Mem (Read)  : 22.92 MB
Total Mem (Write) : 17.34 MB
[Supermasks testing]
[Untrained loss : 112.4199]
[Starting training]
Epoch 0 	 86.724525 	 74.150391 	 81.163467
Epoch 10 	 60.995872 	 54.931934 	 60.884499
Epoch 20 	 54.120033 	 57.644657 	 60.230549
Epoch 30 	 46.349209 	 45.015167 	 47.406216
Epoch 40 	 45.934700 	 49.265106 	 50.582504
Epoch 50 	 39.320217 	 40.998974 	 41.577465
Epoch 60 	 43.258415 	 47.345692 	 52.708752
Epoch 70 	 37.691628 	 39.341301 	 44.316406
Epoch 80 	 35.400665 	 37.027813 	 38.616577
Epoch 90 	 32.317402 	 33.643036 	 35.307049
Epoch 100 	 34.435543 	 34.042645 	 36.208107
Epoch 110 	 30.502216 	 32.194225 	 35.216881
Epoch 120 	 29.791546 	 31.367752 	 34.458824
Epoch 130 	 29.224703 	 30.999632 	 33.680016
Epoch 140 	 27.109720 	 29.850266 	 32.222736
Epoch 150 	 26.768953 	 29.288221 	 31.809202
Epoch 160 	 26.452751 	 28.807074 	 31.488520
Epoch 170 	 25.062431 	 28.081623 	 30.697662
Epoch 180 	 24.329720 	 28.169668 	 30.209852
Epoch 190 	 23.876513 	 27.897100 	 29.973072
Train loss       : 23.462273
Best valid loss  : 27.292784
Best test loss   : 30.002283
Pruning          : 0.52
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,282,494
--------------------------------
Total memory      : 25.75 MB
Total Flops       : 89.46 MFlops
Total Mem (Read)  : 17.01 MB
Total Mem (Write) : 12.62 MB
[Supermasks testing]
[Untrained loss : 91.5887]
[Starting training]
Epoch 0 	 89.610992 	 80.490860 	 84.028770
Epoch 10 	 69.520409 	 68.740036 	 73.604332
Epoch 20 	 59.433956 	 57.038902 	 59.573544
Epoch 30 	 52.534557 	 50.807911 	 55.551086
Epoch 40 	 51.789089 	 49.422237 	 53.700146
Epoch 50 	 45.700554 	 45.245872 	 49.816772
Epoch 60 	 42.209526 	 46.855339 	 48.537445
Epoch 70 	 39.346088 	 42.363514 	 46.468655
Epoch 80 	 38.008472 	 42.245041 	 45.707481
Epoch 90 	 35.239227 	 40.180145 	 44.079433
Epoch 100 	 33.323620 	 37.722664 	 42.347744
Epoch 110 	 32.245575 	 36.457428 	 40.606785
Epoch 120 	 32.102222 	 38.364124 	 42.031040
Epoch 130 	 30.726162 	 35.879707 	 39.640957
Epoch 140 	 30.040392 	 35.851391 	 39.066692
Epoch 150 	 29.298197 	 34.358059 	 38.311729
Epoch 160 	 28.763861 	 34.845272 	 37.842308
Epoch 170 	 28.942764 	 34.613003 	 37.813583
Epoch 180 	 28.593248 	 34.140877 	 37.881187
Epoch 190 	 28.773262 	 34.267365 	 37.557377
Train loss       : 28.295729
Best valid loss  : 32.594433
Best test loss   : 37.603233
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 887,398
--------------------------------
Total memory      : 22.89 MB
Total Flops       : 46.15 MFlops
Total Mem (Read)  : 12.89 MB
Total Mem (Write) : 9.4 MB
[Supermasks testing]
[Untrained loss : 132.8795]
[Starting training]
Epoch 0 	 206.669632 	 83.992744 	 89.463173
Epoch 10 	 69.353424 	 64.809471 	 68.505684
Epoch 20 	 62.731712 	 68.540085 	 191.274094
Epoch 30 	 60.600586 	 53.715660 	 61.548645
Epoch 40 	 56.240303 	 49.495491 	 52.786720
Epoch 50 	 58.956726 	 51.101295 	 55.533905
Epoch 60 	 47.350281 	 44.211548 	 47.008438
Epoch 70 	 42.542534 	 41.841160 	 44.073189
Epoch 80 	 42.829048 	 40.516155 	 42.526638
Epoch 90 	 39.555691 	 37.429771 	 40.697678
Epoch 100 	 40.827374 	 36.823521 	 38.745747
Epoch 110 	 35.159519 	 35.213047 	 37.396786
Epoch 120 	 34.344753 	 34.253414 	 37.052784
Epoch 130 	 34.529575 	 33.730400 	 34.885967
Epoch 140 	 32.761261 	 32.361763 	 34.978729
Epoch 150 	 32.792610 	 31.975214 	 34.983826
Epoch 160 	 32.723644 	 33.129379 	 35.244518
Epoch 170 	 31.348778 	 32.658871 	 34.609493
Epoch 180 	 31.025291 	 31.856936 	 34.357155
Epoch 190 	 31.134445 	 32.042248 	 33.855511
Train loss       : 30.295033
Best valid loss  : 31.075703
Best test loss   : 33.849915
Pruning          : 0.27
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 630,951
--------------------------------
Total memory      : 20.84 MB
Total Flops       : 23.99 MFlops
Total Mem (Read)  : 10.04 MB
Total Mem (Write) : 7.21 MB
[Supermasks testing]
[Untrained loss : 97.5116]
[Starting training]
Epoch 0 	 87.919563 	 85.883034 	 88.603912
Epoch 10 	 71.112999 	 67.261368 	 70.445396
Epoch 20 	 65.068161 	 60.693378 	 64.858063
Epoch 30 	 56.785728 	 57.115379 	 59.352627
Epoch 40 	 53.191929 	 55.805561 	 58.264126
Epoch 50 	 50.149628 	 46.496765 	 51.139816
Epoch 60 	 45.312088 	 47.520683 	 49.240856
Epoch 70 	 43.632225 	 47.317085 	 51.095478
Epoch 80 	 41.754898 	 41.717194 	 45.684776
Epoch 90 	 41.850544 	 41.480385 	 43.566471
Epoch 100 	 39.433506 	 44.317268 	 47.824768
Epoch 110 	 37.874477 	 41.579147 	 43.258915
Epoch 120 	 36.886108 	 39.137039 	 41.079998
Epoch 130 	 38.194908 	 38.662735 	 42.549896
Epoch 140 	 34.135410 	 37.260670 	 38.941650
Epoch 150 	 33.626568 	 37.195061 	 38.719761
Epoch 160 	 32.674446 	 36.914169 	 39.393414
Epoch 170 	 31.966938 	 34.739906 	 37.678131
Epoch 180 	 31.368502 	 35.087822 	 36.962910
Epoch 190 	 30.940111 	 33.707603 	 36.861439
Train loss       : 31.118420
Best valid loss  : 33.602535
Best test loss   : 36.975700
Pruning          : 0.19
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 461,817
--------------------------------
Total memory      : 19.39 MB
Total Flops       : 12.83 MFlops
Total Mem (Read)  : 8.07 MB
Total Mem (Write) : 5.74 MB
[Supermasks testing]
[Untrained loss : 93.8441]
[Starting training]
Epoch 0 	 87.481834 	 85.346886 	 88.970665
Epoch 10 	 68.018356 	 67.331604 	 71.979385
Epoch 20 	 65.810143 	 62.642204 	 67.523888
Epoch 30 	 60.267769 	 58.410717 	 60.486858
Epoch 40 	 54.551067 	 58.556091 	 59.922169
Epoch 50 	 51.739555 	 51.663795 	 53.787251
Epoch 60 	 50.362488 	 53.564068 	 55.002094
Epoch 70 	 50.641258 	 50.050941 	 50.782452
Epoch 80 	 43.097759 	 43.591038 	 43.591568
Epoch 90 	 45.764286 	 45.020477 	 45.922401
Epoch 100 	 44.265282 	 45.700031 	 50.378574
Epoch 110 	 39.894836 	 39.721493 	 42.208878
Epoch 120 	 39.928371 	 38.900921 	 41.445034
Epoch 130 	 37.459126 	 38.395111 	 40.478878
Epoch 140 	 36.458298 	 37.837330 	 40.230873
Epoch 150 	 35.935471 	 37.769882 	 39.429710
Epoch 160 	 36.839359 	 37.540974 	 40.005558
Epoch 170 	 35.647545 	 36.456085 	 39.501503
Epoch 180 	 35.508812 	 36.857746 	 39.901810
Epoch 190 	 35.058933 	 35.484959 	 39.307568
Train loss       : 34.314293
Best valid loss  : 35.484959
Best test loss   : 39.307568
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 348,567
--------------------------------
Total memory      : 18.35 MB
Total Flops       : 7.06 MFlops
Total Mem (Read)  : 6.7 MB
Total Mem (Write) : 4.71 MB
[Supermasks testing]
[Untrained loss : 96.8666]
[Starting training]
Epoch 0 	 88.265015 	 85.076080 	 88.256699
Epoch 10 	 70.508255 	 70.918190 	 75.405449
Epoch 20 	 67.241524 	 65.012474 	 68.377319
Epoch 30 	 65.919594 	 62.110134 	 65.603302
Epoch 40 	 62.972591 	 61.740879 	 64.526466
Epoch 50 	 58.639496 	 56.795521 	 59.695137
Epoch 60 	 56.199139 	 51.016541 	 53.874786
Epoch 70 	 53.061333 	 48.818085 	 53.646008
Epoch 80 	 47.694637 	 58.718613 	 61.807552
Epoch 90 	 48.790646 	 48.680374 	 52.345577
Epoch 100 	 44.598930 	 47.420876 	 49.796162
Epoch 110 	 44.263500 	 41.597214 	 45.037312
Epoch 120 	 42.487885 	 40.716232 	 44.204449
Epoch 130 	 40.789021 	 39.268196 	 43.574596
Epoch 140 	 39.818394 	 41.484047 	 44.499859
Epoch 150 	 39.866924 	 38.310677 	 42.164501
Epoch 160 	 38.459820 	 38.408600 	 42.022732
Epoch 170 	 38.309540 	 38.343220 	 41.280117
Epoch 180 	 38.764107 	 38.084854 	 41.334988
Epoch 190 	 38.541084 	 37.463974 	 41.110767
Train loss       : 38.087093
Best valid loss  : 36.852516
Best test loss   : 41.183815
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 272,819
--------------------------------
Total memory      : 17.56 MB
Total Flops       : 3.79 MFlops
Total Mem (Read)  : 5.68 MB
Total Mem (Write) : 3.94 MB
[Supermasks testing]
[Untrained loss : 96.8639]
[Starting training]
Epoch 0 	 87.567741 	 85.915916 	 88.501236
Epoch 10 	 74.045349 	 72.399376 	 75.415886
Epoch 20 	 69.361427 	 67.134621 	 69.878044
Epoch 30 	 67.207245 	 68.752457 	 71.296890
Epoch 40 	 65.635887 	 62.559631 	 67.032219
Epoch 50 	 64.733818 	 65.271660 	 66.559418
Epoch 60 	 62.209106 	 56.755142 	 60.614033
Epoch 70 	 58.836327 	 56.645359 	 59.977936
Epoch 80 	 55.244267 	 52.886330 	 56.556068
Epoch 90 	 56.174423 	 52.369980 	 55.713940
Epoch 100 	 54.976765 	 53.453873 	 55.428661
Epoch 110 	 53.119526 	 48.787914 	 50.510860
Epoch 120 	 52.331905 	 47.775826 	 49.784557
Epoch 130 	 48.875900 	 49.472031 	 50.861084
Epoch 140 	 46.868176 	 43.530560 	 45.443199
Epoch 150 	 48.905186 	 50.256588 	 51.791542
Epoch 160 	 45.274944 	 42.602478 	 44.271885
Epoch 170 	 44.301266 	 42.942986 	 43.876179
Epoch 180 	 43.770237 	 42.286606 	 43.047512
Epoch 190 	 44.171818 	 41.606888 	 43.461239
Train loss       : 45.333427
Best valid loss  : 40.619350
Best test loss   : 44.073807
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 220,983
--------------------------------
Total memory      : 17.11 MB
Total Flops       : 2.56 MFlops
Total Mem (Read)  : 5.08 MB
Total Mem (Write) : 3.52 MB
[Supermasks testing]
[Untrained loss : 93.6765]
[Starting training]
Epoch 0 	 87.242393 	 85.048149 	 88.456467
Epoch 10 	 73.430687 	 69.887207 	 72.653709
Epoch 20 	 69.175858 	 66.466217 	 69.400848
Epoch 30 	 67.593216 	 68.490898 	 69.929199
Epoch 40 	 67.399361 	 65.190742 	 68.698822
Epoch 50 	 65.645737 	 64.946144 	 67.406227
Epoch 60 	 65.869942 	 69.540688 	 71.513870
Epoch 70 	 64.444572 	 66.558884 	 70.909637
Epoch 80 	 63.276871 	 60.457973 	 65.288170
Epoch 90 	 61.461910 	 57.711945 	 63.013618
Epoch 100 	 59.331100 	 58.319233 	 61.752907
Epoch 110 	 56.918003 	 53.116039 	 56.924049
Epoch 120 	 55.691811 	 51.576492 	 55.788784
Epoch 130 	 53.750790 	 50.693741 	 53.630695
Epoch 140 	 52.428234 	 51.395660 	 54.634335
Epoch 150 	 48.755272 	 54.151279 	 57.013489
Epoch 160 	 48.230526 	 49.190140 	 51.686451
Epoch 170 	 48.149036 	 52.793407 	 55.152699
Epoch 180 	 46.537521 	 44.182781 	 47.979534
Epoch 190 	 44.949490 	 42.442741 	 44.504330
Train loss       : 42.979870
Best valid loss  : 40.947144
Best test loss   : 43.567394
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 185,398
--------------------------------
Total memory      : 16.72 MB
Total Flops       : 1.66 MFlops
Total Mem (Read)  : 4.58 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 92.7543]
[Starting training]
Epoch 0 	 87.014389 	 84.728767 	 88.369560
Epoch 10 	 72.258949 	 66.323662 	 71.425987
Epoch 20 	 70.078804 	 66.900063 	 69.584656
Epoch 30 	 67.508507 	 68.144814 	 70.633163
Epoch 40 	 64.740433 	 62.405285 	 64.614334
Epoch 50 	 62.233105 	 60.871956 	 64.316414
Epoch 60 	 61.117378 	 58.512268 	 63.952869
Epoch 70 	 62.477917 	 69.475983 	 73.009666
Epoch 80 	 58.987354 	 56.565514 	 61.377319
Epoch 90 	 59.578091 	 58.557228 	 61.957417
Epoch 100 	 58.683479 	 58.334652 	 61.375805
Epoch 110 	 59.377876 	 57.355122 	 61.077950
Epoch 120 	 59.405754 	 56.274494 	 59.980385
Epoch 130 	 58.742920 	 56.800816 	 59.898445
Epoch 140 	 57.861584 	 56.100975 	 59.967773
Epoch 150 	 58.400230 	 57.708984 	 60.444916
Epoch 160 	 58.009743 	 54.934490 	 59.403786
Epoch 170 	 58.129753 	 54.682350 	 59.710125
Epoch 180 	 57.667797 	 55.412117 	 59.601238
Epoch 190 	 57.731228 	 54.441380 	 59.324501
Train loss       : 58.707924
Best valid loss  : 54.317871
Best test loss   : 58.903473
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 156,046
--------------------------------
Total memory      : 16.63 MB
Total Flops       : 1.63 MFlops
Total Mem (Read)  : 4.41 MB
Total Mem (Write) : 3.08 MB
[Supermasks testing]
[Untrained loss : 93.3894]
[Starting training]
Epoch 0 	 87.132462 	 85.816742 	 88.508110
Epoch 10 	 76.345657 	 73.709389 	 77.276894
Epoch 20 	 72.277893 	 69.691383 	 74.419701
Epoch 30 	 69.589348 	 67.442955 	 71.864220
Epoch 40 	 70.342072 	 66.037987 	 69.309273
Epoch 50 	 68.858421 	 68.309067 	 71.946060
Epoch 60 	 65.269730 	 63.095798 	 68.224884
Epoch 70 	 61.900982 	 58.523197 	 63.698956
Epoch 80 	 61.741226 	 59.628067 	 64.432495
Epoch 90 	 60.605286 	 56.742428 	 61.665924
Epoch 100 	 59.961449 	 56.564095 	 61.263161
Epoch 110 	 59.857971 	 55.712708 	 60.068306
Epoch 120 	 59.466766 	 62.151268 	 65.402809
Epoch 130 	 58.473965 	 59.042458 	 63.109173
Epoch 140 	 60.777832 	 56.975552 	 61.096416
Epoch 150 	 57.508045 	 56.029488 	 59.992920
Epoch 160 	 57.814697 	 53.620140 	 58.373669
Epoch 170 	 56.635681 	 55.071297 	 59.802570
Epoch 180 	 58.309261 	 53.237411 	 57.709564
Epoch 190 	 57.468376 	 53.528534 	 57.332806
Train loss       : 56.113064
Best valid loss  : 51.729980
Best test loss   : 56.947826
Pruning          : 0.03
[Performing one full cumulative epoch]
Traceback (most recent call last):
  File "main.py", line 261, in <module>
    if (args.prune_selection in ['activation', 'information', 'info_target']):
  File "/scratch/esling/lottery/pruning.py", line 781, in reset
    replace_recurrent(m, l, m.unprune_idx[l], prev_kept)
  File "/scratch/esling/lottery/pruning.py", line 752, in replace_recurrent
    cur_ih = nn.Parameter(cur_ih[rep_id0])#torch.from_numpy(cur_ih[rep_id0]).to(self.args.device))
IndexError: too many indices for tensor of dimension 2
