Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288793.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, pyparsing, python-dateutil, kiwisolver, cycler, matplotlib, gast, tensorflow-estimator, h5py, keras-applications, google-pasta, absl-py, grpcio, protobuf, markdown, certifi, chardet, urllib3, idna, requests, werkzeug, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, keras-preprocessing, wrapt, opt-einsum, termcolor, astor, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288793.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 04:52:09.671411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 04:52:09.682855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is onsets_transcribe_cnn_xavier_trimming_gradient_min_reinit_global_0.
*******
[Current model size]
================================
Total params      : 4,678,757
--------------------------------
Total memory      : 21.13 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 34.3 MB
Total Mem (Write) : 16.44 MB
[Supermasks testing]
/localscratch/esling.41288793.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.7722]
[Starting training]
/localscratch/esling.41288793.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
Epoch 0 	 23.030039 	 0.684713 	 0.685256
Epoch 10 	 21.813284 	 0.556604 	 0.565892
Epoch 20 	 21.227173 	 0.501070 	 0.506564
Epoch 30 	 20.132847 	 0.378842 	 0.377037
Epoch 40 	 19.215973 	 0.278719 	 0.275830
Epoch 50 	 18.463415 	 0.227381 	 0.228461
/localscratch/esling.41288793.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 60 	 17.990585 	 0.203265 	 0.200223
Epoch 70 	 17.626501 	 0.187425 	 0.185655
Epoch 80 	 17.249819 	 0.160293 	 0.165125
Epoch 90 	 16.927095 	 0.153450 	 0.154640
Epoch 100 	 16.745993 	 0.146422 	 0.148892
Epoch 110 	 16.635576 	 0.145208 	 0.148216
Epoch 120 	 16.531969 	 0.136820 	 0.137785
Epoch 130 	 16.433353 	 0.139840 	 0.140763
Epoch 140 	 16.302242 	 0.133875 	 0.133797
Epoch 150 	 16.202007 	 0.136377 	 0.136509
Epoch 160 	 16.153725 	 0.133915 	 0.134230
Epoch 170 	 16.117073 	 0.129797 	 0.132703
[Model stopped early]
Train loss       : 16.097542
Best valid loss  : 0.128633
Best test loss   : 0.134036
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,145,216
--------------------------------
Total memory      : 18.38 MB
Total Flops       : 2.14 GFlops
Total Mem (Read)  : 26.3 MB
Total Mem (Write) : 14.29 MB
[Supermasks testing]
[Untrained loss : 0.7121]
[Starting training]
Epoch 0 	 22.973763 	 0.635763 	 0.644290
Epoch 10 	 21.728834 	 0.553600 	 0.560741
Epoch 20 	 20.472330 	 0.389417 	 0.406549
Epoch 30 	 18.753992 	 0.236313 	 0.235960
Epoch 40 	 17.766167 	 0.175832 	 0.179631
Epoch 50 	 17.288879 	 0.153593 	 0.155644
Epoch 60 	 17.003323 	 0.152635 	 0.153066
Epoch 70 	 16.801836 	 0.145809 	 0.147736
Epoch 80 	 16.604965 	 0.136683 	 0.138469
Epoch 90 	 16.489048 	 0.140818 	 0.144944
Epoch 100 	 16.430260 	 0.140860 	 0.139747
Epoch 110 	 16.273880 	 0.137706 	 0.138337
Epoch 120 	 16.170021 	 0.137658 	 0.135976
Epoch 130 	 16.128162 	 0.135647 	 0.136618
Epoch 140 	 16.109200 	 0.132462 	 0.135632
Epoch 150 	 16.085117 	 0.134021 	 0.134046
[Model stopped early]
Train loss       : 16.081238
Best valid loss  : 0.131429
Best test loss   : 0.135784
Pruning          : 0.75
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,662,868
--------------------------------
Total memory      : 8.98 MB
Total Flops       : 370.36 MFlops
Total Mem (Read)  : 13.34 MB
Total Mem (Write) : 6.98 MB
[Supermasks testing]
[Untrained loss : 0.7426]
[Starting training]
Epoch 0 	 22.938503 	 0.606952 	 0.615158
Epoch 10 	 21.824533 	 0.561205 	 0.571610
Epoch 20 	 20.862562 	 0.439513 	 0.444717
Epoch 30 	 19.143711 	 0.264892 	 0.276625
Epoch 40 	 18.112139 	 0.187534 	 0.196235
Epoch 50 	 17.577932 	 0.166862 	 0.173334
Epoch 60 	 17.153543 	 0.149176 	 0.148944
Epoch 70 	 16.893375 	 0.145441 	 0.139641
Epoch 80 	 16.621994 	 0.139015 	 0.135917
Epoch 90 	 16.501272 	 0.133352 	 0.131561
Epoch 100 	 16.412207 	 0.135621 	 0.133596
Epoch 110 	 16.347857 	 0.130997 	 0.131502
Epoch 120 	 16.278511 	 0.129711 	 0.127993
Epoch 130 	 16.249336 	 0.133863 	 0.132299
Epoch 140 	 16.205709 	 0.134102 	 0.130124
Epoch 150 	 16.195938 	 0.133003 	 0.129882
[Model stopped early]
Train loss       : 16.191860
Best valid loss  : 0.129711
Best test loss   : 0.127993
Pruning          : 0.56
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,002,429
--------------------------------
Total memory      : 8.26 MB
Total Flops       : 268.19 MFlops
Total Mem (Read)  : 10.26 MB
Total Mem (Write) : 6.43 MB
[Supermasks testing]
[Untrained loss : 0.6991]
[Starting training]
Epoch 0 	 23.569084 	 0.675654 	 0.678198
Epoch 10 	 21.887894 	 0.572998 	 0.586159
Epoch 20 	 20.999939 	 0.470435 	 0.481318
Epoch 30 	 20.137079 	 0.366599 	 0.372008
Epoch 40 	 19.466745 	 0.305084 	 0.312990
Epoch 50 	 19.003729 	 0.251543 	 0.257957
Epoch 60 	 18.644644 	 0.225783 	 0.235710
Epoch 70 	 18.258776 	 0.197625 	 0.199158
Epoch 80 	 18.073446 	 0.194267 	 0.196847
Epoch 90 	 17.920359 	 0.188105 	 0.186497
Epoch 100 	 17.723991 	 0.178282 	 0.183359
Epoch 110 	 17.598383 	 0.170755 	 0.171093
Epoch 120 	 17.465107 	 0.163187 	 0.165660
Epoch 130 	 17.396048 	 0.159206 	 0.162537
slurmstepd: error: _is_a_lwp: open() /proc/28500/status failed: No such file or directory
Epoch 140 	 17.307690 	 0.162301 	 0.158502
Epoch 150 	 17.271389 	 0.156636 	 0.156420
Epoch 160 	 17.206478 	 0.160061 	 0.158074
Epoch 170 	 17.047909 	 0.152652 	 0.154639
Epoch 180 	 17.048563 	 0.151821 	 0.154827
Epoch 190 	 17.000677 	 0.151008 	 0.155150
Train loss       : 16.957993
Best valid loss  : 0.149988
Best test loss   : 0.151633
Pruning          : 0.42
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 774,649
--------------------------------
Total memory      : 6.55 MB
Total Flops       : 201.79 MFlops
Total Mem (Read)  : 8.07 MB
Total Mem (Write) : 5.1 MB
[Supermasks testing]
[Untrained loss : 0.8091]
[Starting training]
Epoch 0 	 23.570381 	 0.731889 	 0.736443
Epoch 10 	 21.801800 	 0.560010 	 0.564084
Epoch 20 	 21.370018 	 0.517294 	 0.517974
Epoch 30 	 20.618204 	 0.406951 	 0.415100
Epoch 40 	 19.903444 	 0.337050 	 0.346917
Epoch 50 	 19.355131 	 0.286922 	 0.296033
Epoch 60 	 19.018442 	 0.254954 	 0.269390
Epoch 70 	 18.727859 	 0.227680 	 0.239856
Epoch 80 	 18.448923 	 0.208897 	 0.221651
Epoch 90 	 18.191635 	 0.205126 	 0.208909
Epoch 100 	 18.013079 	 0.189144 	 0.193819
Epoch 110 	 17.872629 	 0.184160 	 0.189447
Epoch 120 	 17.718002 	 0.177554 	 0.184245
Epoch 130 	 17.584282 	 0.172976 	 0.177327
Epoch 140 	 17.496624 	 0.167522 	 0.173962
Epoch 150 	 17.427750 	 0.168620 	 0.170275
Epoch 160 	 17.361443 	 0.165528 	 0.174133
Epoch 170 	 17.310112 	 0.165409 	 0.171956
Epoch 180 	 17.194830 	 0.164113 	 0.168138
Epoch 190 	 17.138676 	 0.164852 	 0.165752
Train loss       : 17.087801
Best valid loss  : 0.160141
Best test loss   : 0.165928
Pruning          : 0.32
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 634,985
--------------------------------
Total memory      : 1.25 MB
Total Flops       : 11.97 MFlops
Total Mem (Read)  : 3.41 MB
Total Mem (Write) : 998.62 KB
[Supermasks testing]
[Untrained loss : 0.7145]
[Starting training]
Epoch 0 	 23.521679 	 0.715818 	 0.717783
Epoch 10 	 21.550987 	 0.541313 	 0.540605
Epoch 20 	 21.101341 	 0.480289 	 0.484957
Epoch 30 	 20.502934 	 0.416589 	 0.420618
Epoch 40 	 20.101625 	 0.357482 	 0.361182
Epoch 50 	 19.484482 	 0.294276 	 0.296201
Epoch 60 	 18.988743 	 0.252566 	 0.261334
Epoch 70 	 18.634689 	 0.227843 	 0.241131
Epoch 80 	 18.362110 	 0.200902 	 0.207601
Epoch 90 	 18.149651 	 0.189055 	 0.193004
Epoch 100 	 17.928406 	 0.180740 	 0.185373
Epoch 110 	 17.826302 	 0.179467 	 0.180159
Epoch 120 	 17.678682 	 0.171659 	 0.175356
Epoch 130 	 17.615671 	 0.169635 	 0.174291
Epoch 140 	 17.538044 	 0.168142 	 0.174286
Epoch 150 	 17.478712 	 0.165357 	 0.172063
Epoch 160 	 17.378164 	 0.164735 	 0.170086
Epoch 170 	 17.296936 	 0.164926 	 0.164420
Epoch 180 	 17.284876 	 0.164161 	 0.164007
Epoch 190 	 17.209438 	 0.160233 	 0.163512
Train loss       : 17.181396
Best valid loss  : 0.157211
Best test loss   : 0.162489
Pruning          : 0.24
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 499,249
--------------------------------
Total memory      : 1.25 MB
Total Flops       : 11.83 MFlops
Total Mem (Read)  : 2.89 MB
Total Mem (Write) : 996.84 KB
[Supermasks testing]
[Untrained loss : 0.7914]
[Starting training]
Epoch 0 	 23.622780 	 0.685329 	 0.688054
Epoch 10 	 21.877811 	 0.560048 	 0.570789
Epoch 20 	 21.157970 	 0.486400 	 0.489388
Epoch 30 	 20.460897 	 0.395506 	 0.397747
Epoch 40 	 19.968887 	 0.337864 	 0.352156
Epoch 50 	 19.580421 	 0.308754 	 0.319506
Epoch 60 	 19.254776 	 0.263919 	 0.280372
Epoch 70 	 18.991428 	 0.248418 	 0.263680
Epoch 80 	 18.789309 	 0.235052 	 0.249717
Epoch 90 	 18.586094 	 0.229586 	 0.240494
Epoch 100 	 18.394199 	 0.219453 	 0.227157
Epoch 110 	 18.284054 	 0.210138 	 0.222031
Epoch 120 	 18.079926 	 0.197283 	 0.205981
Epoch 130 	 17.982752 	 0.183306 	 0.190418
Epoch 140 	 17.862801 	 0.185628 	 0.190144
Epoch 150 	 17.789549 	 0.179241 	 0.181935
Epoch 160 	 17.677679 	 0.174046 	 0.179013
Epoch 170 	 17.596956 	 0.174273 	 0.178867
Epoch 180 	 17.540064 	 0.170873 	 0.171907
Epoch 190 	 17.498524 	 0.172536 	 0.170009
Train loss       : 17.431095
Best valid loss  : 0.166343
Best test loss   : 0.167580
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 395,367
--------------------------------
Total memory      : 1.19 MB
Total Flops       : 10.95 MFlops
Total Mem (Read)  : 2.45 MB
Total Mem (Write) : 951.76 KB
[Supermasks testing]
[Untrained loss : 0.7344]
[Starting training]
Epoch 0 	 23.518330 	 0.653440 	 0.660732
Epoch 10 	 21.931986 	 0.571580 	 0.582627
Epoch 20 	 21.293402 	 0.497636 	 0.505773
Epoch 30 	 20.571991 	 0.390624 	 0.399628
Epoch 40 	 20.126999 	 0.344043 	 0.355557
Epoch 50 	 19.743504 	 0.294366 	 0.299504
Epoch 60 	 19.429733 	 0.267753 	 0.274991
Epoch 70 	 19.222609 	 0.257229 	 0.265182
Epoch 80 	 19.014065 	 0.248212 	 0.252472
Epoch 90 	 18.899055 	 0.238621 	 0.250772
Epoch 100 	 18.747728 	 0.232494 	 0.233845
Epoch 110 	 18.611931 	 0.224409 	 0.225776
Epoch 120 	 18.467550 	 0.218661 	 0.227387
Epoch 130 	 18.372746 	 0.223454 	 0.223052
Epoch 140 	 18.265732 	 0.211584 	 0.213383
Epoch 150 	 18.170357 	 0.204381 	 0.206455
Epoch 160 	 18.061829 	 0.197871 	 0.202185
Epoch 170 	 18.033970 	 0.196853 	 0.197803
Epoch 180 	 17.969765 	 0.193265 	 0.195042
Epoch 190 	 17.907967 	 0.189216 	 0.189655
Train loss       : 17.886354
Best valid loss  : 0.186028
Best test loss   : 0.190783
Pruning          : 0.13
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 320,057
--------------------------------
Total memory      : 1.19 MB
Total Flops       : 10.88 MFlops
Total Mem (Read)  : 2.16 MB
Total Mem (Write) : 950.77 KB
[Supermasks testing]
[Untrained loss : 0.7824]
[Starting training]
Epoch 0 	 23.574093 	 0.713421 	 0.715526
Epoch 10 	 21.812077 	 0.555311 	 0.566803
Epoch 20 	 21.524658 	 0.532786 	 0.532098
Epoch 30 	 21.149258 	 0.480479 	 0.486198
Epoch 40 	 20.565203 	 0.403175 	 0.409874
Epoch 50 	 20.138775 	 0.352491 	 0.360618
Epoch 60 	 19.899059 	 0.328796 	 0.329859
Epoch 70 	 19.699341 	 0.307024 	 0.318254
Epoch 80 	 19.474949 	 0.283495 	 0.292736
Epoch 90 	 19.314035 	 0.271570 	 0.279656
Epoch 100 	 19.172394 	 0.250887 	 0.263185
Epoch 110 	 18.981075 	 0.250660 	 0.251821
Epoch 120 	 18.848591 	 0.230858 	 0.239834
Epoch 130 	 18.737638 	 0.221541 	 0.228277
Epoch 140 	 18.621346 	 0.220545 	 0.224048
Epoch 150 	 18.567453 	 0.217388 	 0.218500
Epoch 160 	 18.497238 	 0.211996 	 0.214174
Epoch 170 	 18.461575 	 0.209560 	 0.214165
Epoch 180 	 18.371981 	 0.206939 	 0.207720
Epoch 190 	 18.325401 	 0.199586 	 0.206904
Train loss       : 18.302366
Best valid loss  : 0.193260
Best test loss   : 0.204438
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 263,786
--------------------------------
Total memory      : 1.14 MB
Total Flops       : 10.05 MFlops
Total Mem (Read)  : 1.9 MB
Total Mem (Write) : 906.28 KB
[Supermasks testing]
[Untrained loss : 0.7271]
[Starting training]
Epoch 0 	 23.542988 	 0.711607 	 0.716667
Epoch 10 	 21.795872 	 0.558382 	 0.558040
Epoch 20 	 21.486403 	 0.519200 	 0.519010
Epoch 30 	 21.139591 	 0.475460 	 0.478145
Epoch 40 	 20.916519 	 0.448364 	 0.453112
Epoch 50 	 20.633663 	 0.403766 	 0.402539
Epoch 60 	 20.430716 	 0.378467 	 0.381071
Epoch 70 	 20.298380 	 0.375628 	 0.374377
Epoch 80 	 20.224686 	 0.362736 	 0.357828
Epoch 90 	 20.111637 	 0.343624 	 0.347928
Epoch 100 	 19.930141 	 0.332517 	 0.333001
Epoch 110 	 19.888027 	 0.323153 	 0.328563
Epoch 120 	 19.838764 	 0.316913 	 0.323387
Epoch 130 	 19.693472 	 0.316181 	 0.317486
Epoch 140 	 19.613203 	 0.314524 	 0.318371
Epoch 150 	 19.602407 	 0.307611 	 0.306486
Epoch 160 	 19.487576 	 0.304341 	 0.309795
Epoch 170 	 19.444178 	 0.301687 	 0.303138
Epoch 180 	 19.384165 	 0.297061 	 0.299146
Epoch 190 	 19.351767 	 0.295890 	 0.298571
Train loss       : 19.350420
Best valid loss  : 0.287548
Best test loss   : 0.295763
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 210,746
--------------------------------
Total memory      : 1.14 MB
Total Flops       : 9.99 MFlops
Total Mem (Read)  : 1.7 MB
Total Mem (Write) : 905.71 KB
[Supermasks testing]
[Untrained loss : 0.7155]
[Starting training]
Epoch 0 	 23.790707 	 0.743705 	 0.745315
Epoch 10 	 21.981176 	 0.573063 	 0.579991
Epoch 20 	 21.659311 	 0.541516 	 0.551179
Epoch 30 	 21.458273 	 0.508936 	 0.512698
Epoch 40 	 21.250431 	 0.492231 	 0.490042
Epoch 50 	 21.088196 	 0.482087 	 0.482081
Epoch 60 	 20.986916 	 0.458631 	 0.457498
Epoch 70 	 20.896296 	 0.455863 	 0.449382
Epoch 80 	 20.806173 	 0.429515 	 0.434987
Epoch 90 	 20.673433 	 0.410502 	 0.409907
Epoch 100 	 20.540176 	 0.394062 	 0.399124
Epoch 110 	 20.451023 	 0.387294 	 0.390477
Epoch 120 	 20.381105 	 0.383613 	 0.383488
Epoch 130 	 20.348291 	 0.377705 	 0.378254
Epoch 140 	 20.280306 	 0.367622 	 0.372346
Epoch 150 	 20.228769 	 0.363229 	 0.367340
Epoch 160 	 20.179913 	 0.367413 	 0.374784
Epoch 170 	 20.116764 	 0.353156 	 0.359443
Epoch 180 	 20.054056 	 0.348054 	 0.355606
Epoch 190 	 20.027124 	 0.343738 	 0.347633
Train loss       : 19.994112
Best valid loss  : 0.336870
Best test loss   : 0.341145
Pruning          : 0.06
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 186,968
--------------------------------
Total memory      : 0.64 MB
Total Flops       : 3.39 MFlops
Total Mem (Read)  : 1.22 MB
Total Mem (Write) : 511.71 KB
[Supermasks testing]
[Untrained loss : 0.7206]
[Starting training]
Epoch 0 	 23.662558 	 0.779010 	 0.782195
Epoch 10 	 22.034256 	 0.578846 	 0.585206
Epoch 20 	 21.823605 	 0.551885 	 0.557485
Epoch 30 	 21.668383 	 0.543920 	 0.541000
Epoch 40 	 21.431242 	 0.505075 	 0.511191
Epoch 50 	 21.300079 	 0.482518 	 0.491920
Epoch 60 	 21.208347 	 0.473997 	 0.476461
Epoch 70 	 21.119030 	 0.450865 	 0.458228
Epoch 80 	 21.027143 	 0.448006 	 0.446518
Epoch 90 	 20.961788 	 0.439350 	 0.441024
Epoch 100 	 20.896702 	 0.435210 	 0.441657
Epoch 110 	 20.817129 	 0.430256 	 0.435188
Epoch 120 	 20.766527 	 0.424816 	 0.428813
Epoch 130 	 20.719793 	 0.429432 	 0.427933
Epoch 140 	 20.720016 	 0.424059 	 0.424985
Epoch 150 	 20.710178 	 0.420106 	 0.421630
Epoch 160 	 20.680166 	 0.422293 	 0.423818
Epoch 170 	 20.682102 	 0.421527 	 0.424301
[Model stopped early]
Train loss       : 20.678207
Best valid loss  : 0.414911
Best test loss   : 0.423219
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 168,984
--------------------------------
Total memory      : 0.59 MB
Total Flops       : 2.68 MFlops
Total Mem (Read)  : 1.11 MB
Total Mem (Write) : 467.73 KB
[Supermasks testing]
[Untrained loss : 0.7815]
[Starting training]
Epoch 0 	 23.823002 	 0.719503 	 0.719756
Epoch 10 	 21.949936 	 0.554292 	 0.565224
Epoch 20 	 21.754938 	 0.534202 	 0.542350
Epoch 30 	 21.574022 	 0.515932 	 0.517743
Epoch 40 	 21.495920 	 0.516718 	 0.518105
Epoch 50 	 21.467478 	 0.512372 	 0.511031
Epoch 60 	 21.352501 	 0.505205 	 0.506953
Epoch 70 	 21.318121 	 0.497862 	 0.499470
Epoch 80 	 21.248165 	 0.475160 	 0.482771
Epoch 90 	 21.200085 	 0.472847 	 0.477929
Epoch 100 	 21.125343 	 0.463862 	 0.466902
Epoch 110 	 21.104422 	 0.452414 	 0.463750
Epoch 120 	 21.031755 	 0.447266 	 0.453875
Epoch 130 	 21.019545 	 0.444419 	 0.456290
Epoch 140 	 20.973770 	 0.444658 	 0.455240
Epoch 150 	 20.993450 	 0.442209 	 0.451109
Epoch 160 	 20.937019 	 0.444390 	 0.453806
Epoch 170 	 20.955561 	 0.444684 	 0.454270
[Model stopped early]
Train loss       : 20.935345
Best valid loss  : 0.439019
Best test loss   : 0.451634
Pruning          : 0.03
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 152,129
--------------------------------
Total memory      : 0.42 MB
Total Flops       : 1.77 MFlops
Total Mem (Read)  : 942.62 KB
Total Mem (Write) : 336.26 KB
[Supermasks testing]
[Untrained loss : 0.7751]
[Starting training]
Epoch 0 	 23.755850 	 0.699149 	 0.703853
Epoch 10 	 22.103214 	 0.583925 	 0.595000
Epoch 20 	 21.949417 	 0.561282 	 0.570457
Epoch 30 	 21.819265 	 0.556115 	 0.560099
Epoch 40 	 21.739981 	 0.541795 	 0.551063
Epoch 50 	 21.653620 	 0.537830 	 0.538529
Epoch 60 	 21.611874 	 0.525417 	 0.529326
Epoch 70 	 21.545574 	 0.518934 	 0.524709
Epoch 80 	 21.510786 	 0.510790 	 0.517306
Epoch 90 	 21.457272 	 0.499643 	 0.509088
Epoch 100 	 21.446981 	 0.498980 	 0.503039
Epoch 110 	 21.424145 	 0.491918 	 0.497677
Epoch 120 	 21.370195 	 0.498608 	 0.500848
Epoch 130 	 21.333811 	 0.491480 	 0.493889
Epoch 140 	 21.317781 	 0.488188 	 0.494524
Epoch 150 	 21.311039 	 0.486745 	 0.489929
Epoch 160 	 21.277893 	 0.488846 	 0.491348
Epoch 170 	 21.254290 	 0.488027 	 0.492216
slurmstepd: error: _is_a_lwp: open() /proc/26236/status failed: No such file or directory
Epoch 180 	 21.287920 	 0.482963 	 0.488443
Epoch 190 	 21.262640 	 0.486253 	 0.486288
Train loss       : 21.263950
Best valid loss  : 0.479012
Best test loss   : 0.488758
Pruning          : 0.02
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 138,588
--------------------------------
Total memory      : 0.31 MB
Total Flops       : 1.09 MFlops
Total Mem (Read)  : 802.07 KB
Total Mem (Write) : 248.6 KB
[Supermasks testing]
[Untrained loss : 0.7461]
[Starting training]
Epoch 0 	 23.772678 	 0.778999 	 0.782195
Epoch 10 	 22.363029 	 0.614002 	 0.623735
Epoch 20 	 22.249746 	 0.600739 	 0.612587
Epoch 30 	 22.147915 	 0.576219 	 0.587920
Epoch 40 	 21.990911 	 0.558563 	 0.568552
Epoch 50 	 21.911781 	 0.548083 	 0.553737
Epoch 60 	 21.807941 	 0.539906 	 0.543821
Epoch 70 	 21.798048 	 0.534287 	 0.541143
Epoch 80 	 21.731588 	 0.530802 	 0.535827
Epoch 90 	 21.739475 	 0.536021 	 0.541993
Epoch 100 	 21.667868 	 0.524873 	 0.535266
Epoch 110 	 21.665333 	 0.529408 	 0.535064
Epoch 120 	 21.626133 	 0.527451 	 0.531814
Epoch 130 	 21.641642 	 0.517870 	 0.529500
Epoch 140 	 21.602547 	 0.511541 	 0.524102
Epoch 150 	 21.619162 	 0.514858 	 0.526364
Epoch 160 	 21.628119 	 0.511370 	 0.522583
Epoch 170 	 21.579699 	 0.519049 	 0.527761
[Model stopped early]
Train loss       : 21.568777
Best valid loss  : 0.507287
Best test loss   : 0.525468
Pruning          : 0.02
