Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40871931.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, python-dateutil, cycler, pyparsing, kiwisolver, matplotlib, absl-py, tensorflow-estimator, werkzeug, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, urllib3, chardet, certifi, idna, requests, requests-oauthlib, google-auth-oauthlib, grpcio, markdown, protobuf, tensorboard, google-pasta, keras-preprocessing, termcolor, opt-einsum, wrapt, h5py, keras-applications, astor, gast, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871931.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-22 07:33:53.330897: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-22 07:33:53.661947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_sing_ae_cnn_xavier_trimming_gradient_min_rewind_global_0.
*******
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5271]
[Starting training]
Epoch 0 	 0.459379 	 0.424962 	 0.419596
Epoch 10 	 0.205248 	 0.219610 	 0.220797
Epoch 20 	 0.174440 	 0.181790 	 0.186080
Epoch 30 	 0.156126 	 0.172039 	 0.172823
Epoch 40 	 0.145796 	 0.163616 	 0.166740
Epoch 50 	 0.139104 	 0.159866 	 0.159594
Epoch 60 	 0.134957 	 0.156233 	 0.158518
Epoch 70 	 0.129854 	 0.150774 	 0.152781
Epoch 80 	 0.126563 	 0.149290 	 0.151904
Epoch 90 	 0.123750 	 0.147948 	 0.149587
Epoch 100 	 0.121621 	 0.145566 	 0.148536
Epoch 110 	 0.120787 	 0.146507 	 0.149116
Epoch 120 	 0.119227 	 0.143170 	 0.146961
Epoch 130 	 0.118782 	 0.141123 	 0.144261
Epoch 140 	 0.107672 	 0.132415 	 0.136434
Epoch 150 	 0.103688 	 0.131409 	 0.135078
Train loss       : 0.102544
Best valid loss  : 0.127134
Best test loss   : 0.135557
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 14,565,411
--------------------------------
Total memory      : 18.14 MB
Total Flops       : 3.19 GFlops
Total Mem (Read)  : 66.27 MB
Total Mem (Write) : 17.91 MB
[Supermasks testing]
[Untrained loss : 0.5326]
[Starting training]
Epoch 0 	 0.465988 	 0.434567 	 0.429439
Epoch 10 	 0.245187 	 0.248829 	 0.248684
Epoch 20 	 0.184677 	 0.193597 	 0.195676
Epoch 30 	 0.168634 	 0.184340 	 0.184526
Epoch 40 	 0.156289 	 0.171929 	 0.172027
Epoch 50 	 0.151892 	 0.168078 	 0.169281
Epoch 60 	 0.143408 	 0.163522 	 0.164624
Epoch 70 	 0.142561 	 0.164848 	 0.163809
Epoch 80 	 0.138096 	 0.156683 	 0.159561
Epoch 90 	 0.134536 	 0.152155 	 0.154457
Epoch 100 	 0.120658 	 0.145031 	 0.145739
Epoch 110 	 0.118657 	 0.142785 	 0.144725
Epoch 120 	 0.117559 	 0.142053 	 0.145338
Epoch 130 	 0.115779 	 0.144245 	 0.144623
Epoch 140 	 0.107838 	 0.137730 	 0.139007
Epoch 150 	 0.107199 	 0.136897 	 0.138408
Train loss       : 0.106721
Best valid loss  : 0.133071
Best test loss   : 0.138695
Pruning          : 0.78
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 14,565,411
--------------------------------
Total memory      : 18.14 MB
Total Flops       : 3.19 GFlops
Total Mem (Read)  : 66.27 MB
Total Mem (Write) : 17.91 MB
[Supermasks testing]
[Untrained loss : 0.5326]
[Starting training]
Epoch 0 	 0.459756 	 0.433160 	 0.425722
Epoch 10 	 0.244691 	 0.244776 	 0.245752
Epoch 20 	 0.181699 	 0.194918 	 0.195444
Epoch 30 	 0.172162 	 0.183499 	 0.184521
Epoch 40 	 0.150643 	 0.169755 	 0.169042
Epoch 50 	 0.144680 	 0.164042 	 0.164738
Epoch 60 	 0.144879 	 0.161745 	 0.160821
Epoch 70 	 0.136767 	 0.157817 	 0.159670
Epoch 80 	 0.134615 	 0.156160 	 0.158078
Epoch 90 	 0.132964 	 0.153338 	 0.155234
Epoch 100 	 0.128231 	 0.151192 	 0.154026
Epoch 110 	 0.128825 	 0.151597 	 0.152775
Epoch 120 	 0.127861 	 0.154358 	 0.153640
Epoch 130 	 0.124586 	 0.145679 	 0.148896
Epoch 140 	 0.121704 	 0.145350 	 0.146394
Epoch 150 	 0.119939 	 0.144236 	 0.146139
Train loss       : 0.111440
Best valid loss  : 0.134915
Best test loss   : 0.137082
Pruning          : 0.61
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,724,907
--------------------------------
Total memory      : 14.22 MB
Total Flops       : 2.09 GFlops
Total Mem (Read)  : 45.57 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 0.5327]
[Starting training]
Epoch 0 	 0.513976 	 0.510540 	 0.505441
Epoch 10 	 0.500089 	 0.510830 	 0.505946
Epoch 20 	 0.498791 	 0.508140 	 0.504850
Epoch 30 	 0.499573 	 0.512271 	 0.504709
Epoch 40 	 0.499174 	 0.512533 	 0.504625
Epoch 50 	 0.499276 	 0.508556 	 0.504662
Epoch 60 	 0.500020 	 0.513663 	 0.504618
[Model stopped early]
Train loss       : 0.500020
Best valid loss  : 0.504215
Best test loss   : 0.504718
Pruning          : 0.47
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,724,907
--------------------------------
Total memory      : 14.22 MB
Total Flops       : 2.09 GFlops
Total Mem (Read)  : 45.57 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 0.5327]
[Starting training]
Epoch 0 	 0.510873 	 0.507235 	 0.505765
Epoch 10 	 0.500438 	 0.509899 	 0.504973
Epoch 20 	 0.499779 	 0.507554 	 0.504929
Epoch 30 	 0.499762 	 0.509065 	 0.504757
Epoch 40 	 0.499664 	 0.501104 	 0.504653
Epoch 50 	 0.499641 	 0.509384 	 0.504538
Epoch 60 	 0.499609 	 0.506286 	 0.504602
Epoch 70 	 0.499645 	 0.509446 	 0.504667
[Model stopped early]
Train loss       : 0.498491
Best valid loss  : 0.501104
Best test loss   : 0.504653
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,724,907
--------------------------------
Total memory      : 14.22 MB
Total Flops       : 2.09 GFlops
Total Mem (Read)  : 45.57 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 0.5327]
[Starting training]
Epoch 0 	 0.508572 	 0.507288 	 0.505635
Epoch 10 	 0.500560 	 0.508888 	 0.505265
Epoch 20 	 0.500115 	 0.511937 	 0.504658
Epoch 30 	 0.499804 	 0.510277 	 0.504693
Epoch 40 	 0.499014 	 0.509816 	 0.504600
Epoch 50 	 0.499754 	 0.509150 	 0.504660
Epoch 60 	 0.499523 	 0.507946 	 0.504570
[Model stopped early]
Train loss       : 0.500125
Best valid loss  : 0.498078
Best test loss   : 0.504557
Pruning          : 0.29
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,724,907
--------------------------------
Total memory      : 14.22 MB
Total Flops       : 2.09 GFlops
Total Mem (Read)  : 45.57 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 0.5327]
[Starting training]
Epoch 0 	 0.510935 	 0.508497 	 0.505112
Epoch 10 	 0.500074 	 0.509797 	 0.504797
Epoch 20 	 0.500252 	 0.507771 	 0.504967
Epoch 30 	 0.499908 	 0.508401 	 0.504658
Epoch 40 	 0.499804 	 0.507411 	 0.504940
Epoch 50 	 0.499220 	 0.511311 	 0.504641
Epoch 60 	 0.499111 	 0.510557 	 0.504618
Epoch 70 	 0.499771 	 0.506436 	 0.504728
Epoch 80 	 0.499661 	 0.506226 	 0.504584
[Model stopped early]
Train loss       : 0.499478
Best valid loss  : 0.501910
Best test loss   : 0.504578
Pruning          : 0.23
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,724,907
--------------------------------
Total memory      : 14.22 MB
Total Flops       : 2.09 GFlops
Total Mem (Read)  : 45.57 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 0.5327]
[Starting training]
Epoch 0 	 0.510600 	 0.508758 	 0.505388
Epoch 10 	 0.499432 	 0.511244 	 0.504957
Epoch 20 	 0.500290 	 0.503888 	 0.504811
Epoch 30 	 0.500133 	 0.503703 	 0.504754
Epoch 40 	 0.499856 	 0.508716 	 0.504985
Epoch 50 	 0.499761 	 0.507340 	 0.504933
Epoch 60 	 0.499534 	 0.509219 	 0.504953
Epoch 70 	 0.499515 	 0.510391 	 0.504571
[Model stopped early]
Train loss       : 0.500001
Best valid loss  : 0.502810
Best test loss   : 0.504728
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,724,907
--------------------------------
Total memory      : 14.22 MB
Total Flops       : 2.09 GFlops
Total Mem (Read)  : 45.57 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 0.5327]
[Starting training]
Epoch 0 	 0.526134 	 0.539910 	 0.532506
Epoch 10 	 0.526005 	 0.534744 	 0.532573
Epoch 20 	 0.500227 	 0.510736 	 0.504843
Epoch 30 	 0.500038 	 0.509258 	 0.504913
Epoch 40 	 0.499371 	 0.509375 	 0.504656
Epoch 50 	 0.499353 	 0.508919 	 0.504690
Epoch 60 	 0.499048 	 0.507918 	 0.504565
Epoch 70 	 0.499661 	 0.508037 	 0.504578
[Model stopped early]
Train loss       : 0.499807
Best valid loss  : 0.501757
Best test loss   : 0.504587
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,724,907
--------------------------------
Total memory      : 14.22 MB
Total Flops       : 2.09 GFlops
Total Mem (Read)  : 45.57 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 0.5327]
[Starting training]
Epoch 0 	 0.510686 	 0.502994 	 0.505017
Epoch 10 	 0.500460 	 0.504024 	 0.504887
Epoch 20 	 0.499078 	 0.512352 	 0.504685
Epoch 30 	 0.499593 	 0.508373 	 0.504653
Epoch 40 	 0.500084 	 0.511325 	 0.505035
Epoch 50 	 0.498942 	 0.507251 	 0.504582
Epoch 60 	 0.499418 	 0.511557 	 0.504664
[Model stopped early]
Train loss       : 0.499418
Best valid loss  : 0.500359
Best test loss   : 0.504662
Pruning          : 0.11
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,724,907
--------------------------------
Total memory      : 14.22 MB
Total Flops       : 2.09 GFlops
Total Mem (Read)  : 45.57 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 0.5327]
[Starting training]
Epoch 0 	 0.509342 	 0.510360 	 0.507563
Epoch 10 	 0.500277 	 0.509738 	 0.504958
Epoch 20 	 0.499609 	 0.508324 	 0.504714
Epoch 30 	 0.500294 	 0.509523 	 0.504851
Epoch 40 	 0.499595 	 0.506505 	 0.504616
Epoch 50 	 0.499660 	 0.510466 	 0.504670
Epoch 60 	 0.499771 	 0.508121 	 0.504577
[Model stopped early]
Train loss       : 0.499771
Best valid loss  : 0.501869
Best test loss   : 0.504658
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,724,907
--------------------------------
Total memory      : 14.22 MB
Total Flops       : 2.09 GFlops
Total Mem (Read)  : 45.57 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 0.5327]
[Starting training]
Epoch 0 	 0.509639 	 0.507805 	 0.505085
Epoch 10 	 0.500030 	 0.508937 	 0.504988
Epoch 20 	 0.500175 	 0.512074 	 0.505046
Epoch 30 	 0.499243 	 0.509034 	 0.504686
Epoch 40 	 0.499888 	 0.507287 	 0.504635
[Model stopped early]
Train loss       : 0.499586
Best valid loss  : 0.502294
Best test loss   : 0.504869
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,724,907
--------------------------------
Total memory      : 14.22 MB
Total Flops       : 2.09 GFlops
Total Mem (Read)  : 45.57 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 0.5327]
[Starting training]
Epoch 0 	 0.507495 	 0.504764 	 0.505419
Epoch 10 	 0.500518 	 0.511829 	 0.505685
Epoch 20 	 0.499530 	 0.511382 	 0.504742
Epoch 30 	 0.499535 	 0.505986 	 0.504589
Epoch 40 	 0.499410 	 0.509753 	 0.504629
Epoch 50 	 0.499517 	 0.502053 	 0.504557
Epoch 60 	 0.499029 	 0.511756 	 0.504610
Epoch 70 	 0.500174 	 0.509355 	 0.504573
Epoch 80 	 0.499989 	 0.509816 	 0.504569
[Model stopped early]
Train loss       : 0.499777
Best valid loss  : 0.502053
Best test loss   : 0.504557
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,724,907
--------------------------------
Total memory      : 14.22 MB
Total Flops       : 2.09 GFlops
Total Mem (Read)  : 45.57 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 0.5327]
[Starting training]
Epoch 0 	 0.509330 	 0.508321 	 0.505149
Epoch 10 	 0.500548 	 0.510505 	 0.504792
Epoch 20 	 0.500263 	 0.509498 	 0.505351
Epoch 30 	 0.498755 	 0.506821 	 0.504720
Epoch 40 	 0.499636 	 0.506609 	 0.504621
Epoch 50 	 0.499655 	 0.510986 	 0.504651
Epoch 60 	 0.499822 	 0.511229 	 0.504588
Epoch 70 	 0.498687 	 0.510514 	 0.504586
[Model stopped early]
Train loss       : 0.499957
Best valid loss  : 0.504397
Best test loss   : 0.504706
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,724,907
--------------------------------
Total memory      : 14.22 MB
Total Flops       : 2.09 GFlops
Total Mem (Read)  : 45.57 MB
Total Mem (Write) : 13.98 MB
[Supermasks testing]
[Untrained loss : 0.5327]
[Starting training]
Epoch 0 	 0.523154 	 0.511165 	 0.507577
Epoch 10 	 0.497903 	 0.509749 	 0.504754
Epoch 20 	 0.499852 	 0.506342 	 0.504707
Epoch 30 	 0.499691 	 0.509787 	 0.504605
[Model stopped early]
Train loss       : 0.499841
Best valid loss  : 0.503062
Best test loss   : 0.504822
Pruning          : 0.03
