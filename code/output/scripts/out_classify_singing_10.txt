Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41289076.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, pillow-simd, six, torchvision, tqdm, pyparsing, cycler, python-dateutil, kiwisolver, matplotlib, gast, keras-preprocessing, grpcio, opt-einsum, protobuf, absl-py, h5py, keras-applications, termcolor, wrapt, chardet, certifi, idna, urllib3, requests, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, markdown, tensorboard, astor, tensorflow-estimator, google-pasta, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41289076.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 05:00:16.487776: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 05:00:16.817062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is singing_classify_cnn_xavier_trimming_information_reinit_global_0.
*******
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.8765]
[Starting training]
Epoch 0 	 0.782744 	 0.711857 	 0.707169
Epoch 10 	 0.347312 	 0.384651 	 0.254779
Epoch 20 	 0.157054 	 0.262408 	 0.120221
Epoch 30 	 0.093520 	 0.194393 	 0.061949
Epoch 40 	 0.065142 	 0.179228 	 0.050551
Epoch 50 	 0.048828 	 0.170496 	 0.040993
Epoch 60 	 0.040786 	 0.165901 	 0.039706
Epoch 70 	 0.037914 	 0.148438 	 0.030882
Epoch 80 	 0.031939 	 0.153033 	 0.031066
Epoch 90 	 0.016774 	 0.148897 	 0.030331
Epoch 100 	 0.017348 	 0.146140 	 0.029596
Epoch 110 	 0.011029 	 0.151195 	 0.030515
Epoch 120 	 0.008042 	 0.150735 	 0.030515
[Model stopped early]
Train loss       : 0.008157
Best valid loss  : 0.140625
Best test loss   : 0.028676
Pruning          : 1.00
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,207,031
--------------------------------
Total memory      : 8.56 MB
Total Flops       : 663.22 MFlops
Total Mem (Read)  : 11.53 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.8765]
[Starting training]
Epoch 0 	 0.801356 	 0.719210 	 0.704044
Epoch 10 	 0.362592 	 0.403952 	 0.267831
Epoch 20 	 0.178768 	 0.263327 	 0.124265
Epoch 30 	 0.107192 	 0.204963 	 0.069301
Epoch 40 	 0.076746 	 0.188879 	 0.054596
Epoch 50 	 0.053883 	 0.181985 	 0.045588
Epoch 60 	 0.048369 	 0.164982 	 0.037684
Epoch 70 	 0.025735 	 0.161305 	 0.035294
Epoch 80 	 0.025391 	 0.157169 	 0.032721
Epoch 90 	 0.024242 	 0.158548 	 0.032721
Epoch 100 	 0.020221 	 0.156250 	 0.031801
Epoch 110 	 0.014131 	 0.157169 	 0.031618
Epoch 120 	 0.010340 	 0.156710 	 0.031985
Epoch 130 	 0.011604 	 0.152574 	 0.030882
[Model stopped early]
Train loss       : 0.008272
Best valid loss  : 0.150276
Best test loss   : 0.030515
Pruning          : 0.75
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,104,249
--------------------------------
Total memory      : 8.56 MB
Total Flops       : 663.11 MFlops
Total Mem (Read)  : 11.14 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9871]
[Starting training]
Epoch 0 	 0.762063 	 0.712776 	 0.703125
Epoch 10 	 0.474265 	 0.472426 	 0.362592
Epoch 20 	 0.275161 	 0.295037 	 0.188603
Epoch 30 	 0.195657 	 0.250460 	 0.132721
Epoch 40 	 0.154412 	 0.217831 	 0.098162
Epoch 50 	 0.127872 	 0.207261 	 0.079412
Epoch 60 	 0.103860 	 0.190717 	 0.063787
Epoch 70 	 0.093750 	 0.175092 	 0.053493
Epoch 80 	 0.079044 	 0.178309 	 0.051103
Epoch 90 	 0.072495 	 0.170956 	 0.045956
Epoch 100 	 0.055262 	 0.163603 	 0.040257
Epoch 110 	 0.045037 	 0.163143 	 0.037684
Epoch 120 	 0.036420 	 0.165901 	 0.036581
Epoch 130 	 0.031365 	 0.162684 	 0.035662
[Model stopped early]
Train loss       : 0.031250
Best valid loss  : 0.152114
Best test loss   : 0.035478
Pruning          : 0.56
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,090,239
--------------------------------
Total memory      : 8.56 MB
Total Flops       : 663.1 MFlops
Total Mem (Read)  : 11.08 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.826057 	 0.767463 	 0.749449
Epoch 10 	 0.542739 	 0.567096 	 0.468382
Epoch 20 	 0.390625 	 0.416820 	 0.292923
Epoch 30 	 0.306985 	 0.340074 	 0.218934
Epoch 40 	 0.263327 	 0.297794 	 0.176287
Epoch 50 	 0.222082 	 0.262868 	 0.134375
Epoch 60 	 0.196461 	 0.244485 	 0.122978
Epoch 70 	 0.167854 	 0.214614 	 0.087408
Epoch 80 	 0.152918 	 0.201746 	 0.077298
Epoch 90 	 0.129710 	 0.197610 	 0.067371
Epoch 100 	 0.122013 	 0.201746 	 0.074265
Epoch 110 	 0.109145 	 0.186581 	 0.058824
Epoch 120 	 0.108341 	 0.180147 	 0.056066
Epoch 130 	 0.101562 	 0.173254 	 0.050551
Epoch 140 	 0.091337 	 0.170037 	 0.044853
Epoch 150 	 0.085938 	 0.185202 	 0.054044
Train loss       : 0.073070
Best valid loss  : 0.164982
Best test loss   : 0.039706
Pruning          : 0.42
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,086,645
--------------------------------
Total memory      : 8.55 MB
Total Flops       : 663.09 MFlops
Total Mem (Read)  : 11.07 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9871]
[Starting training]
Epoch 0 	 0.777918 	 0.747243 	 0.742647
Epoch 10 	 0.581457 	 0.539982 	 0.459743
Epoch 20 	 0.426471 	 0.435662 	 0.307996
Epoch 30 	 0.352711 	 0.342831 	 0.221691
Epoch 40 	 0.300666 	 0.330423 	 0.215165
Epoch 50 	 0.268038 	 0.306526 	 0.188787
Epoch 60 	 0.246324 	 0.293658 	 0.175735
Epoch 70 	 0.227252 	 0.274816 	 0.156893
Epoch 80 	 0.199334 	 0.261029 	 0.141636
Epoch 90 	 0.199908 	 0.250460 	 0.124632
Epoch 100 	 0.156595 	 0.235294 	 0.110110
Epoch 110 	 0.154527 	 0.228401 	 0.110294
Epoch 120 	 0.140740 	 0.224265 	 0.104044
Epoch 130 	 0.146369 	 0.210478 	 0.084559
Epoch 140 	 0.135915 	 0.198989 	 0.071875
Epoch 150 	 0.133502 	 0.205882 	 0.079412
Train loss       : 0.127757
Best valid loss  : 0.195312
Best test loss   : 0.068382
Pruning          : 0.32
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,084,462
--------------------------------
Total memory      : 8.55 MB
Total Flops       : 663.09 MFlops
Total Mem (Read)  : 11.06 MB
Total Mem (Write) : 6.41 MB
[Supermasks testing]
[Untrained loss : 0.9412]
[Starting training]
Epoch 0 	 0.908548 	 0.798713 	 0.801930
Epoch 10 	 0.601677 	 0.579963 	 0.504871
Epoch 20 	 0.487362 	 0.462316 	 0.349632
Epoch 30 	 0.416705 	 0.381893 	 0.268382
Epoch 40 	 0.367073 	 0.347886 	 0.232721
Epoch 50 	 0.345244 	 0.324908 	 0.209191
Epoch 60 	 0.316521 	 0.323989 	 0.205147
Epoch 70 	 0.292165 	 0.292739 	 0.167096
Epoch 80 	 0.281135 	 0.282629 	 0.152574
Epoch 90 	 0.269187 	 0.261949 	 0.141728
Epoch 100 	 0.249655 	 0.260570 	 0.130147
Epoch 110 	 0.238971 	 0.254136 	 0.125000
Epoch 120 	 0.220358 	 0.237132 	 0.108640
Epoch 130 	 0.210363 	 0.249540 	 0.118107
Epoch 140 	 0.204504 	 0.224265 	 0.090809
Epoch 150 	 0.186581 	 0.228860 	 0.090993
Train loss       : 0.171415
Best valid loss  : 0.216912
Best test loss   : 0.080515
Pruning          : 0.24
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,083,151
--------------------------------
Total memory      : 8.55 MB
Total Flops       : 663.09 MFlops
Total Mem (Read)  : 11.05 MB
Total Mem (Write) : 6.41 MB
[Supermasks testing]
[Untrained loss : 0.9546]
[Starting training]
Epoch 0 	 0.904986 	 0.863051 	 0.844577
Epoch 10 	 0.654871 	 0.628217 	 0.589982
Epoch 20 	 0.520450 	 0.503676 	 0.422610
Epoch 30 	 0.428653 	 0.437040 	 0.320864
Epoch 40 	 0.380285 	 0.369485 	 0.237500
Epoch 50 	 0.348001 	 0.332261 	 0.204136
Epoch 60 	 0.311811 	 0.310202 	 0.187684
Epoch 70 	 0.290211 	 0.295956 	 0.176562
Epoch 80 	 0.264706 	 0.281710 	 0.155790
Epoch 90 	 0.248277 	 0.263327 	 0.140074
Epoch 100 	 0.227941 	 0.255055 	 0.128676
Epoch 110 	 0.219210 	 0.235754 	 0.111581
Epoch 120 	 0.215188 	 0.242188 	 0.116820
Epoch 130 	 0.208065 	 0.230239 	 0.104044
Epoch 140 	 0.201976 	 0.226103 	 0.097610
Epoch 150 	 0.193934 	 0.217371 	 0.091728
Train loss       : 0.191406
Best valid loss  : 0.211397
Best test loss   : 0.085294
Pruning          : 0.18
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,082,258
--------------------------------
Total memory      : 8.55 MB
Total Flops       : 663.09 MFlops
Total Mem (Read)  : 11.05 MB
Total Mem (Write) : 6.41 MB
[Supermasks testing]
[Untrained loss : 0.9669]
[Starting training]
Epoch 0 	 0.804917 	 0.782629 	 0.795588
Epoch 10 	 0.681411 	 0.696691 	 0.667279
Epoch 20 	 0.553079 	 0.549632 	 0.469301
Epoch 30 	 0.474150 	 0.465533 	 0.375000
Epoch 40 	 0.433594 	 0.403033 	 0.298346
Epoch 50 	 0.388902 	 0.346507 	 0.232537
Epoch 60 	 0.368222 	 0.336857 	 0.222426
Epoch 70 	 0.351792 	 0.324908 	 0.207353
Epoch 80 	 0.328470 	 0.313879 	 0.200368
Epoch 90 	 0.328929 	 0.289062 	 0.182537
Epoch 100 	 0.318589 	 0.282169 	 0.173529
Epoch 110 	 0.300437 	 0.295956 	 0.184191
Epoch 120 	 0.289062 	 0.279412 	 0.172243
Epoch 130 	 0.283088 	 0.267923 	 0.165257
Epoch 140 	 0.279067 	 0.270221 	 0.161765
Epoch 150 	 0.275965 	 0.280790 	 0.168934
Train loss       : 0.272863
Best valid loss  : 0.259651
Best test loss   : 0.155699
Pruning          : 0.13
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 839,269
--------------------------------
Total memory      : 6.90 MB
Total Flops       : 515.39 MFlops
Total Mem (Read)  : 8.88 MB
Total Mem (Write) : 5.17 MB
[Supermasks testing]
[Untrained loss : 0.9555]
[Starting training]
Epoch 0 	 0.895680 	 0.894301 	 0.898897
Epoch 10 	 0.770910 	 0.827206 	 0.789522
Epoch 20 	 0.629136 	 0.618566 	 0.496232
Epoch 30 	 0.536075 	 0.512868 	 0.390809
Epoch 40 	 0.486213 	 0.472426 	 0.360386
Epoch 50 	 0.443934 	 0.414522 	 0.314154
Epoch 60 	 0.406020 	 0.373621 	 0.259835
Epoch 70 	 0.374311 	 0.352482 	 0.224816
Epoch 80 	 0.350184 	 0.319853 	 0.185202
Epoch 90 	 0.332606 	 0.296415 	 0.163327
Epoch 100 	 0.316981 	 0.292279 	 0.154320
Epoch 110 	 0.297335 	 0.274357 	 0.139430
Epoch 120 	 0.277688 	 0.262868 	 0.133272
Epoch 130 	 0.276769 	 0.264706 	 0.132537
Epoch 140 	 0.271599 	 0.254136 	 0.122243
Epoch 150 	 0.249540 	 0.246324 	 0.113787
Train loss       : 0.246783
Best valid loss  : 0.230699
Best test loss   : 0.102574
Pruning          : 0.10
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 667,204
--------------------------------
Total memory      : 5.63 MB
Total Flops       : 409.14 MFlops
Total Mem (Read)  : 7.27 MB
Total Mem (Write) : 4.22 MB
[Supermasks testing]
[Untrained loss : 0.9507]
[Starting training]
Epoch 0 	 0.843980 	 0.823529 	 0.817004
Epoch 10 	 0.655446 	 0.618107 	 0.533088
Epoch 20 	 0.526425 	 0.515165 	 0.420496
Epoch 30 	 0.460363 	 0.450827 	 0.358272
Epoch 40 	 0.424403 	 0.421415 	 0.327849
Epoch 50 	 0.403033 	 0.391544 	 0.302390
Epoch 60 	 0.369600 	 0.373621 	 0.278309
Epoch 70 	 0.353631 	 0.361213 	 0.266360
Epoch 80 	 0.346392 	 0.343750 	 0.245037
Epoch 90 	 0.329963 	 0.344669 	 0.237500
Epoch 100 	 0.317785 	 0.321232 	 0.217279
Epoch 110 	 0.309398 	 0.317555 	 0.215257
Epoch 120 	 0.302160 	 0.310662 	 0.202574
Epoch 130 	 0.297909 	 0.296875 	 0.194118
Epoch 140 	 0.295152 	 0.296415 	 0.189154
Epoch 150 	 0.290211 	 0.299173 	 0.194485
Train loss       : 0.270795
Best valid loss  : 0.238971
Best test loss   : 0.114338
Pruning          : 0.08
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 515,536
--------------------------------
Total memory      : 4.49 MB
Total Flops       : 315.31 MFlops
Total Mem (Read)  : 5.84 MB
Total Mem (Write) : 3.37 MB
[Supermasks testing]
[Untrained loss : 0.7949]
[Starting training]
Epoch 0 	 0.800207 	 0.782169 	 0.794853
Epoch 10 	 0.794807 	 0.781250 	 0.792096
Epoch 20 	 0.789062 	 0.780790 	 0.790625
Epoch 30 	 0.667394 	 0.649357 	 0.597978
Epoch 40 	 0.599380 	 0.579963 	 0.508364
Epoch 50 	 0.571576 	 0.525276 	 0.446967
Epoch 60 	 0.554802 	 0.517923 	 0.442004
Epoch 70 	 0.535501 	 0.470129 	 0.397151
Epoch 80 	 0.518957 	 0.476103 	 0.399724
Epoch 90 	 0.505974 	 0.443474 	 0.368658
Epoch 100 	 0.505400 	 0.432445 	 0.363051
Epoch 110 	 0.494026 	 0.440257 	 0.362684
Epoch 120 	 0.500689 	 0.409926 	 0.341360
Epoch 130 	 0.479435 	 0.408548 	 0.337500
Epoch 140 	 0.470358 	 0.399816 	 0.329963
Epoch 150 	 0.467371 	 0.403493 	 0.326654
Train loss       : 0.463120
Best valid loss  : 0.387408
Best test loss   : 0.317831
Pruning          : 0.06
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 241,815
--------------------------------
Total memory      : 2.33 MB
Total Flops       : 144.14 MFlops
Total Mem (Read)  : 3.18 MB
Total Mem (Write) : 1.75 MB
[Supermasks testing]
[Untrained loss : 0.9421]
[Starting training]
Epoch 0 	 0.878906 	 0.882812 	 0.883088
Epoch 10 	 0.790097 	 0.810662 	 0.774081
Epoch 20 	 0.757927 	 0.733915 	 0.699449
Epoch 30 	 0.744945 	 0.737132 	 0.697243
Epoch 40 	 0.730584 	 0.721048 	 0.684007
Epoch 50 	 0.694738 	 0.708180 	 0.653493
Epoch 60 	 0.659926 	 0.638787 	 0.554136
Epoch 70 	 0.604894 	 0.593750 	 0.495864
Epoch 80 	 0.587201 	 0.518842 	 0.404779
Epoch 90 	 0.573300 	 0.475643 	 0.350919
Epoch 100 	 0.551815 	 0.426471 	 0.304779
Epoch 110 	 0.532399 	 0.395221 	 0.277665
Epoch 120 	 0.518612 	 0.382812 	 0.260662
Epoch 130 	 0.504366 	 0.370864 	 0.245588
Epoch 140 	 0.500345 	 0.380515 	 0.254504
Epoch 150 	 0.480469 	 0.345129 	 0.228952
Train loss       : 0.475528
Best valid loss  : 0.325368
Best test loss   : 0.210294
Pruning          : 0.04
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 147,562
--------------------------------
Total memory      : 1.74 MB
Total Flops       : 90.21 MFlops
Total Mem (Read)  : 2.38 MB
Total Mem (Write) : 1.31 MB
[Supermasks testing]
[Untrained loss : 0.9669]
[Starting training]
Epoch 0 	 0.900965 	 0.892923 	 0.878493
Epoch 10 	 0.801815 	 0.747702 	 0.726471
Epoch 20 	 0.731733 	 0.697610 	 0.642279
Epoch 30 	 0.688764 	 0.663143 	 0.608456
Epoch 40 	 0.671186 	 0.644301 	 0.590625
Epoch 50 	 0.651310 	 0.628217 	 0.581985
Epoch 60 	 0.625345 	 0.592831 	 0.532353
Epoch 70 	 0.610064 	 0.578585 	 0.502390
Epoch 80 	 0.584099 	 0.551471 	 0.467279
Epoch 90 	 0.578355 	 0.528493 	 0.438235
Epoch 100 	 0.557445 	 0.499540 	 0.408456
Epoch 110 	 0.553539 	 0.482537 	 0.388787
Epoch 120 	 0.542165 	 0.452665 	 0.360846
Epoch 130 	 0.538603 	 0.452665 	 0.363603
Epoch 140 	 0.530331 	 0.432904 	 0.345221
Epoch 150 	 0.531250 	 0.423713 	 0.337868
Train loss       : 0.524586
Best valid loss  : 0.408548
Best test loss   : 0.320221
Pruning          : 0.03
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 75,337
--------------------------------
Total memory      : 1.07 MB
Total Flops       : 44.69 MFlops
Total Mem (Read)  : 1.59 MB
Total Mem (Write) : 821.29 KB
[Supermasks testing]
[Untrained loss : 0.9126]
[Starting training]
Epoch 0 	 0.977137 	 0.986213 	 0.986765
Epoch 10 	 0.921760 	 0.957721 	 0.940809
Epoch 20 	 0.850873 	 0.910386 	 0.862868
Epoch 30 	 0.794118 	 0.739430 	 0.673713
Epoch 40 	 0.739660 	 0.682445 	 0.620772
Epoch 50 	 0.713120 	 0.641544 	 0.592647
Epoch 60 	 0.674058 	 0.617188 	 0.570312
Epoch 70 	 0.661994 	 0.604320 	 0.559559
Epoch 80 	 0.642463 	 0.586857 	 0.544026
Epoch 90 	 0.639821 	 0.584559 	 0.533456
Epoch 100 	 0.583640 	 0.571232 	 0.527298
Epoch 110 	 0.573415 	 0.564798 	 0.521415
Epoch 120 	 0.571806 	 0.546415 	 0.493474
Epoch 130 	 0.561811 	 0.535846 	 0.485202
Epoch 140 	 0.555607 	 0.526195 	 0.473070
Epoch 150 	 0.546875 	 0.515165 	 0.460478
Train loss       : 0.549862
Best valid loss  : 0.501838
Best test loss   : 0.446691
Pruning          : 0.02
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 37,937
--------------------------------
Total memory      : 0.65 MB
Total Flops       : 20.52 MFlops
Total Mem (Read)  : 1.14 MB
Total Mem (Write) : 501.75 KB
[Supermasks testing]
[Untrained loss : 0.9421]
[Starting training]
Epoch 0 	 0.940487 	 0.943015 	 0.941728
Epoch 10 	 0.923369 	 0.935662 	 0.931893
Epoch 20 	 0.877757 	 0.903952 	 0.889982
Epoch 30 	 0.794347 	 0.862592 	 0.819026
Epoch 40 	 0.706801 	 0.723346 	 0.639706
Epoch 50 	 0.666705 	 0.614430 	 0.546875
Epoch 60 	 0.653493 	 0.580423 	 0.522243
Epoch 70 	 0.647059 	 0.578585 	 0.520312
Epoch 80 	 0.627413 	 0.562960 	 0.510294
Epoch 90 	 0.626149 	 0.563879 	 0.506710
Epoch 100 	 0.620749 	 0.553768 	 0.494026
Epoch 110 	 0.613166 	 0.555147 	 0.495680
Epoch 120 	 0.609490 	 0.538603 	 0.478952
Epoch 130 	 0.607537 	 0.560202 	 0.502573
Epoch 140 	 0.608915 	 0.550092 	 0.492279
Epoch 150 	 0.610754 	 0.550092 	 0.492096
Train loss       : 0.610409
Best valid loss  : 0.522978
Best test loss   : 0.453768
Pruning          : 0.02
