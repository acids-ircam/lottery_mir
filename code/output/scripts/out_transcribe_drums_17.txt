Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41288859.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, cycler, python-dateutil, kiwisolver, pyparsing, matplotlib, gast, termcolor, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, urllib3, idna, chardet, certifi, requests, grpcio, absl-py, werkzeug, markdown, protobuf, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, keras-preprocessing, h5py, keras-applications, opt-einsum, wrapt, tensorflow-estimator, astor, google-pasta, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41288859.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 08:58:31.713627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 08:58:31.729847: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is drums_transcribe_cnn_xavier_trimming_info_target_rewind_local_0.
*******
[Current model size]
================================
Total params      : 7,597,357
--------------------------------
Total memory      : 21.14 MB
Total Flops       : 2.66 GFlops
Total Mem (Read)  : 45.46 MB
Total Mem (Write) : 16.45 MB
[Supermasks testing]
/localscratch/esling.41288859.0/env/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[Untrained loss : 0.8944]
[Starting training]
/localscratch/esling.41288859.0/env/lib/python3.7/site-packages/mir_eval/onset.py:51: UserWarning: Estimated onsets are empty.
  warnings.warn("Estimated onsets are empty.")
/localscratch/esling.41288859.0/env/lib/python3.7/site-packages/mir_eval/onset.py:49: UserWarning: Reference onsets are empty.
  warnings.warn("Reference onsets are empty.")
Epoch 0 	 57.460682 	 0.882786 	 0.880478
Epoch 10 	 56.039852 	 0.883788 	 0.880478
Epoch 20 	 56.032784 	 0.884112 	 0.880478
Epoch 30 	 56.047295 	 0.882515 	 0.880478
Epoch 40 	 56.051384 	 0.882830 	 0.880478
Epoch 50 	 56.046551 	 0.883332 	 0.880478
Epoch 60 	 56.068501 	 0.882897 	 0.880478
Epoch 70 	 56.056595 	 0.883358 	 0.880478
Epoch 80 	 56.055851 	 0.882361 	 0.880478
Epoch 90 	 56.046921 	 0.882895 	 0.880478
[Model stopped early]
Train loss       : 56.048409
Best valid loss  : 0.880970
Best test loss   : 0.880478
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 5,425,453
--------------------------------
Total memory      : 15.86 MB
Total Flops       : 1.5 GFlops
Total Mem (Read)  : 33.07 MB
Total Mem (Write) : 12.34 MB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 56.026691 	 0.883540 	 0.880478
Epoch 10 	 55.862591 	 0.878959 	 0.877033
Epoch 20 	 55.824394 	 0.882353 	 0.880233
Epoch 30 	 55.822586 	 0.882577 	 0.879957
[Model stopped early]
Train loss       : 55.820992
Best valid loss  : 0.853289
Best test loss   : 0.843521
Pruning          : 0.75
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 4,016,941
--------------------------------
Total memory      : 11.90 MB
Total Flops       : 850.78 MFlops
Total Mem (Read)  : 24.62 MB
Total Mem (Write) : 9.26 MB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 56.003086 	 0.862997 	 0.858664
Epoch 10 	 55.477615 	 0.813468 	 0.807102
Epoch 20 	 55.425251 	 0.826234 	 0.814752
Epoch 30 	 55.277851 	 0.821331 	 0.814891
Epoch 40 	 55.249535 	 0.804123 	 0.801335
Epoch 50 	 55.122314 	 0.804339 	 0.796203
Epoch 60 	 55.152298 	 0.799594 	 0.791587
Epoch 70 	 55.033916 	 0.789824 	 0.786253
Epoch 80 	 54.989189 	 0.787762 	 0.785970
Epoch 90 	 54.940033 	 0.782818 	 0.779415
Epoch 100 	 54.853302 	 0.785228 	 0.777027
Epoch 110 	 54.810921 	 0.784817 	 0.778225
Epoch 120 	 54.757725 	 0.782526 	 0.775793
Epoch 130 	 54.764595 	 0.783637 	 0.777247
Epoch 140 	 54.740410 	 0.780464 	 0.773979
[Model stopped early]
Train loss       : 54.748894
Best valid loss  : 0.778793
Best test loss   : 0.775715
Pruning          : 0.56
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,084,541
--------------------------------
Total memory      : 8.93 MB
Total Flops       : 482.74 MFlops
Total Mem (Read)  : 18.75 MB
Total Mem (Write) : 6.95 MB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 55.985939 	 0.878013 	 0.871879
Epoch 10 	 55.424793 	 0.812288 	 0.802949
Epoch 20 	 55.213810 	 0.798065 	 0.789699
Epoch 30 	 55.149113 	 0.784066 	 0.777973
Epoch 40 	 55.145100 	 0.780568 	 0.776026
Epoch 50 	 54.954254 	 0.779632 	 0.771982
Epoch 60 	 54.926960 	 0.774574 	 0.769252
Epoch 70 	 54.863747 	 0.777162 	 0.767090
Epoch 80 	 54.761654 	 0.776332 	 0.767223
Epoch 90 	 54.693172 	 0.768586 	 0.762159
Epoch 100 	 54.631989 	 0.763416 	 0.757102
Epoch 110 	 54.570015 	 0.762279 	 0.755115
Epoch 120 	 54.532715 	 0.761121 	 0.754785
Epoch 130 	 54.521740 	 0.758716 	 0.753802
Epoch 140 	 54.531364 	 0.760964 	 0.752921
Epoch 150 	 54.520329 	 0.761934 	 0.754382
[Model stopped early]
Train loss       : 54.522705
Best valid loss  : 0.757193
Best test loss   : 0.754416
Pruning          : 0.42
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,454,982
--------------------------------
Total memory      : 6.70 MB
Total Flops       : 274.77 MFlops
Total Mem (Read)  : 14.61 MB
Total Mem (Write) : 5.21 MB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 55.963005 	 0.875892 	 0.872977
Epoch 10 	 55.362595 	 0.803334 	 0.796941
Epoch 20 	 55.233650 	 0.790529 	 0.787387
Epoch 30 	 55.049126 	 0.796588 	 0.789855
Epoch 40 	 54.994053 	 0.800687 	 0.790408
Epoch 50 	 54.916317 	 0.787394 	 0.779400
Epoch 60 	 54.891487 	 0.780487 	 0.774756
Epoch 70 	 54.910965 	 0.787956 	 0.784743
Epoch 80 	 54.763382 	 0.775973 	 0.771286
Epoch 90 	 54.725075 	 0.778553 	 0.768061
Epoch 100 	 54.680786 	 0.774600 	 0.770097
Epoch 110 	 54.643635 	 0.777462 	 0.767902
Epoch 120 	 54.644444 	 0.770441 	 0.766619
Epoch 130 	 54.626373 	 0.774900 	 0.767459
Epoch 140 	 54.630981 	 0.770439 	 0.767166
Epoch 150 	 54.594624 	 0.774378 	 0.768033
Epoch 160 	 54.622215 	 0.773457 	 0.767378
Epoch 170 	 54.601707 	 0.772328 	 0.767068
[Model stopped early]
Train loss       : 54.597469
Best valid loss  : 0.767133
Best test loss   : 0.765137
Pruning          : 0.32
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,019,736
--------------------------------
Total memory      : 4.97 MB
Total Flops       : 153.36 MFlops
Total Mem (Read)  : 11.61 MB
Total Mem (Write) : 3.87 MB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 55.990852 	 0.883078 	 0.880405
Epoch 10 	 55.438824 	 0.812797 	 0.801930
Epoch 20 	 55.247101 	 0.801756 	 0.795654
Epoch 30 	 55.047417 	 0.791785 	 0.783420
Epoch 40 	 54.946159 	 0.782395 	 0.775398
Epoch 50 	 54.858532 	 0.773780 	 0.769679
Epoch 60 	 54.794201 	 0.780076 	 0.773212
Epoch 70 	 54.743042 	 0.780859 	 0.772730
Epoch 80 	 54.704880 	 0.775373 	 0.768699
Epoch 90 	 54.681263 	 0.774311 	 0.767546
Epoch 100 	 54.687809 	 0.772569 	 0.765723
Epoch 110 	 54.656036 	 0.770955 	 0.766460
Epoch 120 	 54.637920 	 0.770864 	 0.767527
Epoch 130 	 54.601948 	 0.768488 	 0.765103
Epoch 140 	 54.590668 	 0.768890 	 0.764908
Epoch 150 	 54.590012 	 0.772772 	 0.765686
Epoch 160 	 54.590744 	 0.769823 	 0.765140
[Model stopped early]
Train loss       : 54.572697
Best valid loss  : 0.768012
Best test loss   : 0.763929
Pruning          : 0.24
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,717,038
--------------------------------
Total memory      : 3.73 MB
Total Flops       : 88.24 MFlops
Total Mem (Read)  : 9.49 MB
Total Mem (Write) : 2.9 MB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 56.024239 	 0.882926 	 0.880478
Epoch 10 	 55.543022 	 0.823345 	 0.814092
Epoch 20 	 55.358673 	 0.818051 	 0.813171
Epoch 30 	 55.222332 	 0.819396 	 0.815917
Epoch 40 	 55.156219 	 0.819164 	 0.811033
Epoch 50 	 55.051804 	 0.800472 	 0.797652
Epoch 60 	 54.989502 	 0.795579 	 0.796395
Epoch 70 	 54.933052 	 0.797663 	 0.793402
Epoch 80 	 54.911751 	 0.798097 	 0.798524
Epoch 90 	 54.849674 	 0.794162 	 0.794871
Epoch 100 	 54.843559 	 0.802167 	 0.796337
Epoch 110 	 54.833549 	 0.799028 	 0.795863
Epoch 120 	 54.824898 	 0.796359 	 0.797183
[Model stopped early]
Train loss       : 54.833698
Best valid loss  : 0.794162
Best test loss   : 0.794871
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,499,950
--------------------------------
Total memory      : 2.74 MB
Total Flops       : 49.12 MFlops
Total Mem (Read)  : 7.89 MB
Total Mem (Write) : 2.13 MB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 56.031548 	 0.881613 	 0.879565
Epoch 10 	 55.528885 	 0.816711 	 0.808926
Epoch 20 	 55.437267 	 0.824741 	 0.821044
Epoch 30 	 55.294388 	 0.820153 	 0.811973
Epoch 40 	 55.266666 	 0.811290 	 0.805084
Epoch 50 	 55.185555 	 0.810225 	 0.802814
Epoch 60 	 55.157650 	 0.812429 	 0.805291
Epoch 70 	 55.125618 	 0.810522 	 0.800606
Epoch 80 	 55.112785 	 0.809663 	 0.803578
Epoch 90 	 55.089157 	 0.806830 	 0.800271
Epoch 100 	 55.080830 	 0.810205 	 0.801135
[Model stopped early]
Train loss       : 55.081997
Best valid loss  : 0.806534
Best test loss   : 0.800459
Pruning          : 0.13
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,346,677
--------------------------------
Total memory      : 2.00 MB
Total Flops       : 27.34 MFlops
Total Mem (Read)  : 6.73 MB
Total Mem (Write) : 1.56 MB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 56.038963 	 0.881297 	 0.879456
Epoch 10 	 55.451118 	 0.806616 	 0.802012
Epoch 20 	 55.386700 	 0.809549 	 0.801720
Epoch 30 	 55.178547 	 0.802059 	 0.798203
Epoch 40 	 55.081966 	 0.801737 	 0.795536
Epoch 50 	 55.050392 	 0.799604 	 0.793766
Epoch 60 	 54.949932 	 0.792227 	 0.786249
Epoch 70 	 54.938469 	 0.790903 	 0.781766
Epoch 80 	 54.900833 	 0.790548 	 0.783958
Epoch 90 	 54.907219 	 0.789806 	 0.783290
Epoch 100 	 54.855339 	 0.791594 	 0.783399
Epoch 110 	 54.811024 	 0.785522 	 0.780359
Epoch 120 	 54.797859 	 0.787433 	 0.781581
[Model stopped early]
Train loss       : 54.770393
Best valid loss  : 0.783320
Best test loss   : 0.780857
Pruning          : 0.10
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,234,585
--------------------------------
Total memory      : 1.50 MB
Total Flops       : 16.41 MFlops
Total Mem (Read)  : 5.92 MB
Total Mem (Write) : 1.17 MB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 56.038033 	 0.883620 	 0.880478
Epoch 10 	 55.729801 	 0.841981 	 0.840844
Epoch 20 	 55.385273 	 0.813611 	 0.807743
Epoch 30 	 55.279209 	 0.805640 	 0.797846
Epoch 40 	 55.210106 	 0.802810 	 0.794562
Epoch 50 	 55.180653 	 0.800209 	 0.793492
Epoch 60 	 55.077766 	 0.804674 	 0.798055
Epoch 70 	 55.071499 	 0.794131 	 0.789113
Epoch 80 	 55.009857 	 0.797762 	 0.791166
Epoch 90 	 54.989128 	 0.794660 	 0.786665
Epoch 100 	 54.915657 	 0.789297 	 0.782502
Epoch 110 	 54.899715 	 0.786677 	 0.781693
Epoch 120 	 54.856609 	 0.793826 	 0.784630
Epoch 130 	 54.843994 	 0.788037 	 0.779724
Epoch 140 	 54.827351 	 0.788596 	 0.777036
Epoch 150 	 54.817272 	 0.787133 	 0.778275
Epoch 160 	 54.816994 	 0.787819 	 0.776549
[Model stopped early]
Train loss       : 54.800491
Best valid loss  : 0.784614
Best test loss   : 0.776707
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,154,221
--------------------------------
Total memory      : 1.09 MB
Total Flops       : 9.52 MFlops
Total Mem (Read)  : 5.29 MB
Total Mem (Write) : 870.76 KB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 56.032440 	 0.882782 	 0.880478
Epoch 10 	 55.739143 	 0.853261 	 0.848438
Epoch 20 	 55.448250 	 0.825527 	 0.813633
Epoch 30 	 55.326607 	 0.825819 	 0.817912
Epoch 40 	 55.255482 	 0.818155 	 0.809333
Epoch 50 	 55.217693 	 0.807851 	 0.804990
Epoch 60 	 55.133781 	 0.805306 	 0.800312
Epoch 70 	 55.115852 	 0.803376 	 0.799448
Epoch 80 	 55.102303 	 0.800351 	 0.794494
Epoch 90 	 55.024971 	 0.796675 	 0.792638
Epoch 100 	 55.049324 	 0.799278 	 0.792018
Epoch 110 	 55.008320 	 0.793727 	 0.792254
Epoch 120 	 55.040508 	 0.794135 	 0.791544
Epoch 130 	 55.020794 	 0.795550 	 0.791513
Epoch 140 	 54.998623 	 0.793566 	 0.789325
Epoch 150 	 54.935040 	 0.787512 	 0.785386
Epoch 160 	 54.931107 	 0.786836 	 0.779225
Epoch 170 	 54.895519 	 0.782609 	 0.776813
Epoch 180 	 54.901543 	 0.783163 	 0.777054
Epoch 190 	 54.880516 	 0.784200 	 0.777655
Train loss       : 54.886608
Best valid loss  : 0.779761
Best test loss   : 0.777362
Pruning          : 0.06
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,092,142
--------------------------------
Total memory      : 0.76 MB
Total Flops       : 5.44 MFlops
Total Mem (Read)  : 4.79 MB
Total Mem (Write) : 607.91 KB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 56.059307 	 0.882255 	 0.880478
Epoch 10 	 56.038738 	 0.883866 	 0.880478
Epoch 20 	 56.041340 	 0.884532 	 0.880478
Epoch 30 	 56.050274 	 0.882482 	 0.880478
Epoch 40 	 56.045433 	 0.882526 	 0.880478
Epoch 50 	 56.053246 	 0.881712 	 0.880478
Epoch 60 	 56.039486 	 0.883047 	 0.880478
[Model stopped early]
Train loss       : 56.013439
Best valid loss  : 0.881432
Best test loss   : 0.880478
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,047,388
--------------------------------
Total memory      : 0.51 MB
Total Flops       : 3.23 MFlops
Total Mem (Read)  : 4.43 MB
Total Mem (Write) : 410.77 KB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 56.008656 	 0.870628 	 0.869253
Epoch 10 	 55.839825 	 0.856030 	 0.851496
Epoch 20 	 55.611958 	 0.835632 	 0.831165
Epoch 30 	 55.553314 	 0.821519 	 0.817084
Epoch 40 	 55.509956 	 0.829866 	 0.821546
Epoch 50 	 55.470695 	 0.822440 	 0.816795
Epoch 60 	 55.404320 	 0.814989 	 0.806603
Epoch 70 	 55.360672 	 0.810563 	 0.802368
[Model stopped early]
Train loss       : 55.395050
Best valid loss  : 0.807832
Best test loss   : 0.803911
Pruning          : 0.03
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,015,320
--------------------------------
Total memory      : 0.35 MB
Total Flops       : 2.15 MFlops
Total Mem (Read)  : 4.18 MB
Total Mem (Write) : 279.34 KB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 56.037888 	 0.883426 	 0.880478
Epoch 10 	 55.977318 	 0.873542 	 0.870813
Epoch 20 	 55.952293 	 0.863996 	 0.859439
Epoch 30 	 55.937977 	 0.857966 	 0.855856
Epoch 40 	 55.921608 	 0.856748 	 0.855370
Epoch 50 	 55.918312 	 0.858916 	 0.857447
[Model stopped early]
Train loss       : 55.915585
Best valid loss  : 0.852114
Best test loss   : 0.851183
Pruning          : 0.02
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 991,581
--------------------------------
Total memory      : 0.26 MB
Total Flops       : 1.72 MFlops
Total Mem (Read)  : 4.03 MB
Total Mem (Write) : 213.57 KB
[Supermasks testing]
[Untrained loss : 0.8805]
[Starting training]
Epoch 0 	 56.051765 	 0.883467 	 0.880478
Epoch 10 	 56.050640 	 0.882196 	 0.880478
Epoch 20 	 56.031670 	 0.882444 	 0.880478
Epoch 30 	 56.049152 	 0.882725 	 0.880478
Epoch 40 	 56.042084 	 0.883313 	 0.880478
Epoch 50 	 56.037994 	 0.883359 	 0.880478
Epoch 60 	 56.049896 	 0.882797 	 0.880478
Epoch 70 	 56.043579 	 0.884897 	 0.880478
[Model stopped early]
Train loss       : 56.044689
Best valid loss  : 0.881737
Best test loss   : 0.880478
Pruning          : 0.02
