Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40871918.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, kiwisolver, python-dateutil, cycler, pyparsing, matplotlib, google-pasta, keras-preprocessing, h5py, keras-applications, opt-einsum, termcolor, protobuf, markdown, werkzeug, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, urllib3, idna, chardet, certifi, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, absl-py, tensorboard, tensorflow-estimator, wrapt, astor, gast, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871918.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-22 07:23:57.779412: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-22 07:23:57.791014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_sing_ae_cnn_xavier_trimming_activation_reinit_global_0.
*******
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5714]
[Starting training]
Epoch 0 	 0.468305 	 0.424968 	 0.410278
Epoch 10 	 0.212524 	 0.211936 	 0.205551
Epoch 20 	 0.172229 	 0.177874 	 0.174674
Epoch 30 	 0.159680 	 0.175287 	 0.169057
Epoch 40 	 0.152512 	 0.164668 	 0.156892
Epoch 50 	 0.141573 	 0.155783 	 0.148985
Epoch 60 	 0.138965 	 0.155278 	 0.148720
Epoch 70 	 0.131593 	 0.148416 	 0.142428
Epoch 80 	 0.130290 	 0.146956 	 0.142677
Epoch 90 	 0.128953 	 0.145158 	 0.141481
Epoch 100 	 0.114539 	 0.135483 	 0.130810
Epoch 110 	 0.113005 	 0.136755 	 0.131024
Epoch 120 	 0.104714 	 0.132390 	 0.126073
Epoch 130 	 0.103142 	 0.134038 	 0.126414
Epoch 140 	 0.102597 	 0.130504 	 0.124643
Epoch 150 	 0.099310 	 0.128362 	 0.122857
Train loss       : 0.097713
Best valid loss  : 0.126973
Best test loss   : 0.122365
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 13,719,559
--------------------------------
Total memory      : 18.12 MB
Total Flops       : 2.96 GFlops
Total Mem (Read)  : 62.82 MB
Total Mem (Write) : 17.89 MB
[Supermasks testing]
[Untrained loss : 0.5849]
[Starting training]
Epoch 0 	 0.458461 	 0.418230 	 0.406300
Epoch 10 	 0.220781 	 0.216963 	 0.209790
Epoch 20 	 0.177372 	 0.186976 	 0.180282
Epoch 30 	 0.159701 	 0.166954 	 0.164138
Epoch 40 	 0.149006 	 0.163275 	 0.157087
Epoch 50 	 0.141612 	 0.159417 	 0.151398
Epoch 60 	 0.138189 	 0.154325 	 0.147105
Epoch 70 	 0.135188 	 0.150883 	 0.144814
Epoch 80 	 0.132649 	 0.151003 	 0.143173
Epoch 90 	 0.115553 	 0.136128 	 0.132494
Epoch 100 	 0.114139 	 0.136741 	 0.131284
Epoch 110 	 0.112210 	 0.135220 	 0.129620
Epoch 120 	 0.111140 	 0.135587 	 0.129706
Epoch 130 	 0.102924 	 0.129823 	 0.124211
Epoch 140 	 0.101787 	 0.129657 	 0.123736
Epoch 150 	 0.097823 	 0.128008 	 0.122197
Train loss       : 0.095864
Best valid loss  : 0.126073
Best test loss   : 0.120917
Pruning          : 0.78
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 8,800,062
--------------------------------
Total memory      : 14.19 MB
Total Flops       : 1.86 GFlops
Total Mem (Read)  : 42.04 MB
Total Mem (Write) : 13.95 MB
[Supermasks testing]
[Untrained loss : 0.5175]
[Starting training]
Epoch 0 	 0.456198 	 0.422934 	 0.409976
Epoch 10 	 0.216702 	 0.216966 	 0.211269
Epoch 20 	 0.171412 	 0.178927 	 0.174413
Epoch 30 	 0.156525 	 0.169924 	 0.163317
Epoch 40 	 0.146358 	 0.165996 	 0.159546
Epoch 50 	 0.141844 	 0.156102 	 0.149645
Epoch 60 	 0.136550 	 0.152107 	 0.146165
Epoch 70 	 0.132283 	 0.151468 	 0.144697
Epoch 80 	 0.119524 	 0.140408 	 0.135025
Epoch 90 	 0.118053 	 0.139558 	 0.134091
Epoch 100 	 0.116760 	 0.140886 	 0.134069
Epoch 110 	 0.107949 	 0.134972 	 0.127636
Epoch 120 	 0.107370 	 0.134264 	 0.127402
Epoch 130 	 0.103230 	 0.133736 	 0.125319
Epoch 140 	 0.102752 	 0.131047 	 0.124877
Epoch 150 	 0.102440 	 0.131335 	 0.124765
Train loss       : 0.100375
Best valid loss  : 0.130369
Best test loss   : 0.123996
Pruning          : 0.61
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 5,836,515
--------------------------------
Total memory      : 11.13 MB
Total Flops       : 1.25 GFlops
Total Mem (Read)  : 29.78 MB
Total Mem (Write) : 10.9 MB
[Supermasks testing]
[Untrained loss : 0.5729]
[Starting training]
Epoch 0 	 0.463013 	 0.429893 	 0.415358
Epoch 10 	 0.225161 	 0.221800 	 0.217596
Epoch 20 	 0.176571 	 0.184273 	 0.179524
Epoch 30 	 0.160664 	 0.171025 	 0.165755
Epoch 40 	 0.151411 	 0.162556 	 0.157391
Epoch 50 	 0.143596 	 0.157217 	 0.151713
Epoch 60 	 0.139688 	 0.153716 	 0.146992
Epoch 70 	 0.136058 	 0.151978 	 0.145925
Epoch 80 	 0.135594 	 0.148322 	 0.142283
Epoch 90 	 0.130902 	 0.148657 	 0.142216
Epoch 100 	 0.116946 	 0.138657 	 0.132216
Epoch 110 	 0.116521 	 0.137537 	 0.131970
Epoch 120 	 0.108926 	 0.134677 	 0.127605
Epoch 130 	 0.108081 	 0.133170 	 0.127408
Epoch 140 	 0.107183 	 0.133659 	 0.127591
Epoch 150 	 0.106520 	 0.133832 	 0.126219
Train loss       : 0.106034
Best valid loss  : 0.131053
Best test loss   : 0.126209
Pruning          : 0.47
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,981,417
--------------------------------
Total memory      : 8.73 MB
Total Flops       : 828.12 MFlops
Total Mem (Read)  : 21.06 MB
Total Mem (Write) : 8.49 MB
[Supermasks testing]
[Untrained loss : 0.5639]
[Starting training]
Epoch 0 	 0.466976 	 0.436345 	 0.425254
Epoch 10 	 0.237818 	 0.230705 	 0.226358
Epoch 20 	 0.184550 	 0.194397 	 0.188409
Epoch 30 	 0.167424 	 0.179549 	 0.174221
Epoch 40 	 0.158072 	 0.170490 	 0.165325
Epoch 50 	 0.151763 	 0.165598 	 0.159698
Epoch 60 	 0.148021 	 0.161224 	 0.154944
Epoch 70 	 0.143435 	 0.159534 	 0.153625
Epoch 80 	 0.139372 	 0.157071 	 0.150792
Epoch 90 	 0.136127 	 0.150509 	 0.146106
Epoch 100 	 0.133556 	 0.151348 	 0.144983
Epoch 110 	 0.121262 	 0.141757 	 0.136442
Epoch 120 	 0.120041 	 0.142242 	 0.135511
Epoch 130 	 0.112575 	 0.136985 	 0.130867
Epoch 140 	 0.108810 	 0.135661 	 0.129114
Epoch 150 	 0.108112 	 0.134043 	 0.129023
Train loss       : 0.107526
Best valid loss  : 0.132300
Best test loss   : 0.129174
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,868,599
--------------------------------
Total memory      : 6.86 MB
Total Flops       : 575.31 MFlops
Total Mem (Read)  : 15.33 MB
Total Mem (Write) : 6.62 MB
[Supermasks testing]
[Untrained loss : 0.5646]
[Starting training]
Epoch 0 	 0.464328 	 0.432081 	 0.420467
Epoch 10 	 0.222532 	 0.220732 	 0.216846
Epoch 20 	 0.177435 	 0.191217 	 0.185272
Epoch 30 	 0.163360 	 0.173422 	 0.167886
Epoch 40 	 0.153546 	 0.167891 	 0.161153
Epoch 50 	 0.146503 	 0.162088 	 0.154661
Epoch 60 	 0.141970 	 0.163662 	 0.157003
Epoch 70 	 0.139535 	 0.153629 	 0.148351
Epoch 80 	 0.135842 	 0.152415 	 0.146937
Epoch 90 	 0.136208 	 0.156297 	 0.149153
Epoch 100 	 0.120203 	 0.141943 	 0.135824
Epoch 110 	 0.119014 	 0.140622 	 0.134823
Epoch 120 	 0.112096 	 0.138528 	 0.130541
Epoch 130 	 0.111183 	 0.136038 	 0.129321
Epoch 140 	 0.110111 	 0.134323 	 0.127883
Epoch 150 	 0.105740 	 0.131974 	 0.125876
Train loss       : 0.105280
Best valid loss  : 0.130727
Best test loss   : 0.125712
Pruning          : 0.29
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,046,996
--------------------------------
Total memory      : 5.40 MB
Total Flops       : 392.85 MFlops
Total Mem (Read)  : 11.11 MB
Total Mem (Write) : 5.16 MB
[Supermasks testing]
[Untrained loss : 0.5567]
[Starting training]
Epoch 0 	 0.464335 	 0.433452 	 0.423275
Epoch 10 	 0.237833 	 0.243979 	 0.238601
Epoch 20 	 0.185831 	 0.194028 	 0.189571
Epoch 30 	 0.165297 	 0.177361 	 0.169863
Epoch 40 	 0.155878 	 0.169977 	 0.162846
Epoch 50 	 0.152341 	 0.163973 	 0.158393
Epoch 60 	 0.144724 	 0.163348 	 0.156344
Epoch 70 	 0.142086 	 0.159920 	 0.152601
Epoch 80 	 0.137894 	 0.155669 	 0.149916
Epoch 90 	 0.125942 	 0.145021 	 0.140688
Epoch 100 	 0.124409 	 0.144126 	 0.138029
Epoch 110 	 0.117380 	 0.139369 	 0.135127
Epoch 120 	 0.116938 	 0.141113 	 0.134042
Epoch 130 	 0.116069 	 0.139517 	 0.134480
Epoch 140 	 0.115263 	 0.138422 	 0.132991
Epoch 150 	 0.111150 	 0.136239 	 0.131459
Train loss       : 0.110914
Best valid loss  : 0.135535
Best test loss   : 0.130821
Pruning          : 0.23
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,389,727
--------------------------------
Total memory      : 4.26 MB
Total Flops       : 265.32 MFlops
Total Mem (Read)  : 8.12 MB
Total Mem (Write) : 4.03 MB
[Supermasks testing]
[Untrained loss : 0.5739]
[Starting training]
Epoch 0 	 0.467817 	 0.434637 	 0.423875
Epoch 10 	 0.243921 	 0.240415 	 0.236268
Epoch 20 	 0.185122 	 0.192111 	 0.187280
Epoch 30 	 0.168245 	 0.179914 	 0.172559
Epoch 40 	 0.158872 	 0.169527 	 0.162880
Epoch 50 	 0.153833 	 0.163594 	 0.157629
Epoch 60 	 0.145839 	 0.161222 	 0.154671
Epoch 70 	 0.142648 	 0.155580 	 0.149688
Epoch 80 	 0.139600 	 0.154316 	 0.148000
Epoch 90 	 0.136984 	 0.151125 	 0.145558
Epoch 100 	 0.124555 	 0.144879 	 0.137144
Epoch 110 	 0.123986 	 0.145360 	 0.137305
Epoch 120 	 0.122843 	 0.142134 	 0.135719
Epoch 130 	 0.116542 	 0.137830 	 0.131605
Epoch 140 	 0.115412 	 0.138628 	 0.131233
Epoch 150 	 0.112389 	 0.135283 	 0.129312
Train loss       : 0.112232
Best valid loss  : 0.132964
Best test loss   : 0.129113
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 992,462
--------------------------------
Total memory      : 3.37 MB
Total Flops       : 183.39 MFlops
Total Mem (Read)  : 6.01 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 0.5560]
[Starting training]
Epoch 0 	 0.467684 	 0.433078 	 0.421079
Epoch 10 	 0.254086 	 0.254165 	 0.250354
Epoch 20 	 0.197792 	 0.205901 	 0.200352
Epoch 30 	 0.176157 	 0.188259 	 0.181304
Epoch 40 	 0.163108 	 0.176737 	 0.171551
Epoch 50 	 0.157219 	 0.173799 	 0.166760
Epoch 60 	 0.154620 	 0.167333 	 0.161201
Epoch 70 	 0.152365 	 0.165461 	 0.160827
Epoch 80 	 0.146383 	 0.162006 	 0.156097
Epoch 90 	 0.144833 	 0.161363 	 0.155003
Epoch 100 	 0.142483 	 0.159009 	 0.151978
Epoch 110 	 0.131762 	 0.150460 	 0.143224
Epoch 120 	 0.130840 	 0.149130 	 0.142818
Epoch 130 	 0.124890 	 0.145807 	 0.138900
Epoch 140 	 0.124390 	 0.146407 	 0.138803
Epoch 150 	 0.121338 	 0.144389 	 0.137378
Train loss       : 0.121016
Best valid loss  : 0.142320
Best test loss   : 0.137315
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 743,818
--------------------------------
Total memory      : 2.69 MB
Total Flops       : 129.6 MFlops
Total Mem (Read)  : 4.51 MB
Total Mem (Write) : 2.45 MB
[Supermasks testing]
[Untrained loss : 0.5437]
[Starting training]
Epoch 0 	 0.472459 	 0.439583 	 0.427483
Epoch 10 	 0.267218 	 0.265847 	 0.262959
Epoch 20 	 0.197045 	 0.203862 	 0.200690
Epoch 30 	 0.177625 	 0.190690 	 0.185824
Epoch 40 	 0.167979 	 0.180970 	 0.177365
Epoch 50 	 0.161555 	 0.174074 	 0.169009
Epoch 60 	 0.157187 	 0.171547 	 0.165490
Epoch 70 	 0.152931 	 0.169646 	 0.163419
Epoch 80 	 0.148174 	 0.165840 	 0.158625
Epoch 90 	 0.145580 	 0.162586 	 0.155049
Epoch 100 	 0.143719 	 0.159223 	 0.152897
Epoch 110 	 0.134883 	 0.151723 	 0.143819
Epoch 120 	 0.130905 	 0.148979 	 0.142690
Epoch 130 	 0.124879 	 0.143822 	 0.139313
Epoch 140 	 0.124303 	 0.145795 	 0.138298
Epoch 150 	 0.123560 	 0.144640 	 0.138275
Train loss       : 0.120477
Best valid loss  : 0.142759
Best test loss   : 0.136150
Pruning          : 0.11
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 546,047
--------------------------------
Total memory      : 2.15 MB
Total Flops       : 92.21 MFlops
Total Mem (Read)  : 3.43 MB
Total Mem (Write) : 1.92 MB
[Supermasks testing]
[Untrained loss : 0.5131]
[Starting training]
Epoch 0 	 0.473252 	 0.440581 	 0.430481
Epoch 10 	 0.310574 	 0.309095 	 0.303241
Epoch 20 	 0.228605 	 0.233350 	 0.228870
Epoch 30 	 0.207055 	 0.215799 	 0.211802
Epoch 40 	 0.195351 	 0.204739 	 0.202823
Epoch 50 	 0.187358 	 0.200406 	 0.195303
Epoch 60 	 0.179125 	 0.193411 	 0.187782
Epoch 70 	 0.174149 	 0.192413 	 0.184593
Epoch 80 	 0.169798 	 0.183866 	 0.178063
Epoch 90 	 0.164587 	 0.179377 	 0.173116
Epoch 100 	 0.164644 	 0.177463 	 0.173040
Epoch 110 	 0.161779 	 0.174357 	 0.168591
Epoch 120 	 0.159234 	 0.173284 	 0.168330
Epoch 130 	 0.155723 	 0.171822 	 0.164896
Epoch 140 	 0.154589 	 0.171979 	 0.164786
Epoch 150 	 0.143243 	 0.160979 	 0.155255
Train loss       : 0.143364
Best valid loss  : 0.158470
Best test loss   : 0.153792
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 419,853
--------------------------------
Total memory      : 1.73 MB
Total Flops       : 74.08 MFlops
Total Mem (Read)  : 2.78 MB
Total Mem (Write) : 1.5 MB
[Supermasks testing]
[Untrained loss : 0.5852]
[Starting training]
Epoch 0 	 0.492368 	 0.445596 	 0.433680
Epoch 10 	 0.316164 	 0.313753 	 0.307075
Epoch 20 	 0.246324 	 0.249304 	 0.246312
Epoch 30 	 0.222473 	 0.230621 	 0.227024
Epoch 40 	 0.208873 	 0.218720 	 0.215129
Epoch 50 	 0.203156 	 0.215881 	 0.211504
Epoch 60 	 0.196292 	 0.205192 	 0.202220
Epoch 70 	 0.193207 	 0.204846 	 0.200257
Epoch 80 	 0.188769 	 0.199556 	 0.196141
Epoch 90 	 0.183681 	 0.199371 	 0.194533
Epoch 100 	 0.180074 	 0.196164 	 0.189701
Epoch 110 	 0.181573 	 0.192523 	 0.188702
Epoch 120 	 0.176123 	 0.192555 	 0.185924
Epoch 130 	 0.176970 	 0.190483 	 0.184248
Epoch 140 	 0.164672 	 0.183021 	 0.176347
Epoch 150 	 0.163944 	 0.182487 	 0.175930
Train loss       : 0.163309
Best valid loss  : 0.176846
Best test loss   : 0.175157
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 343,843
--------------------------------
Total memory      : 1.40 MB
Total Flops       : 65.22 MFlops
Total Mem (Read)  : 2.35 MB
Total Mem (Write) : 1.17 MB
[Supermasks testing]
[Untrained loss : 0.5631]
[Starting training]
Epoch 0 	 0.505331 	 0.456790 	 0.445380
Epoch 10 	 0.339746 	 0.336309 	 0.327831
Epoch 20 	 0.274415 	 0.277036 	 0.272013
Epoch 30 	 0.238404 	 0.243408 	 0.241145
Epoch 40 	 0.220433 	 0.227272 	 0.225113
Epoch 50 	 0.211186 	 0.217861 	 0.217053
Epoch 60 	 0.205810 	 0.215457 	 0.212486
Epoch 70 	 0.195285 	 0.207083 	 0.202493
Epoch 80 	 0.190978 	 0.205681 	 0.201508
Epoch 90 	 0.189727 	 0.203086 	 0.200001
Epoch 100 	 0.187795 	 0.201839 	 0.198898
Epoch 110 	 0.183357 	 0.198524 	 0.194609
Epoch 120 	 0.181369 	 0.198790 	 0.194637
Epoch 130 	 0.178497 	 0.194186 	 0.192392
Epoch 140 	 0.178061 	 0.196156 	 0.192024
Epoch 150 	 0.176643 	 0.196210 	 0.191456
Train loss       : 0.176079
Best valid loss  : 0.194186
Best test loss   : 0.192392
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 288,895
--------------------------------
Total memory      : 1.15 MB
Total Flops       : 58.49 MFlops
Total Mem (Read)  : 2.02 MB
Total Mem (Write) : 938.88 KB
[Supermasks testing]
[Untrained loss : 0.5877]
[Starting training]
Epoch 0 	 0.512090 	 0.503400 	 0.489208
Epoch 10 	 0.507677 	 0.498941 	 0.488891
Epoch 20 	 0.507468 	 0.502526 	 0.488270
Epoch 30 	 0.506894 	 0.498634 	 0.487920
Epoch 40 	 0.506286 	 0.503743 	 0.487789
[Model stopped early]
Train loss       : 0.507422
Best valid loss  : 0.496875
Best test loss   : 0.487929
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 265,622
--------------------------------
Total memory      : 0.95 MB
Total Flops       : 55.79 MFlops
Total Mem (Read)  : 1.79 MB
Total Mem (Write) : 735.61 KB
[Supermasks testing]
[Untrained loss : 0.5551]
[Starting training]
Epoch 0 	 0.499920 	 0.458321 	 0.445716
Epoch 10 	 0.352013 	 0.345019 	 0.337309
Epoch 20 	 0.296515 	 0.297257 	 0.291500
Epoch 30 	 0.274990 	 0.279837 	 0.273795
Epoch 40 	 0.262020 	 0.272061 	 0.264116
Epoch 50 	 0.255909 	 0.264827 	 0.257430
Epoch 60 	 0.248340 	 0.258273 	 0.251626
Epoch 70 	 0.244080 	 0.253288 	 0.246137
Epoch 80 	 0.239866 	 0.249838 	 0.244862
Epoch 90 	 0.237408 	 0.246176 	 0.240456
Epoch 100 	 0.235987 	 0.246401 	 0.238842
Epoch 110 	 0.233163 	 0.243484 	 0.238168
Epoch 120 	 0.230900 	 0.242206 	 0.235179
Epoch 130 	 0.229104 	 0.238221 	 0.235296
Epoch 140 	 0.228409 	 0.237601 	 0.233322
Epoch 150 	 0.220418 	 0.231559 	 0.226891
Train loss       : 0.220261
Best valid loss  : 0.228559
Best test loss   : 0.226452
Pruning          : 0.03
