Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40871941.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, pillow-simd, future, torch, torchvision, tqdm, kiwisolver, cycler, python-dateutil, pyparsing, matplotlib, opt-einsum, termcolor, gast, absl-py, google-pasta, protobuf, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, idna, chardet, certifi, urllib3, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, werkzeug, markdown, tensorboard, wrapt, astor, h5py, keras-applications, tensorflow-estimator, keras-preprocessing, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871941.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-22 07:39:03.490115: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-22 07:39:03.917416: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_sing_ae_cnn_xavier_masking_gradient_min_rewind_local_0.
*******
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5496]
[Starting training]
Epoch 0 	 0.464194 	 0.411613 	 0.415515
Epoch 10 	 0.207807 	 0.207259 	 0.207007
Epoch 20 	 0.166638 	 0.172496 	 0.170690
Epoch 30 	 0.153079 	 0.161776 	 0.161089
Epoch 40 	 0.147406 	 0.155623 	 0.153820
Epoch 50 	 0.136727 	 0.147299 	 0.147447
Epoch 60 	 0.131570 	 0.146178 	 0.146545
Epoch 70 	 0.128056 	 0.140842 	 0.142242
Epoch 80 	 0.113943 	 0.131770 	 0.130895
Epoch 90 	 0.112403 	 0.129644 	 0.129327
Epoch 100 	 0.109646 	 0.128565 	 0.127989
Epoch 110 	 0.107467 	 0.127956 	 0.127739
Epoch 120 	 0.106293 	 0.130620 	 0.128576
Epoch 130 	 0.098393 	 0.123885 	 0.122548
Epoch 140 	 0.097893 	 0.122877 	 0.121670
Epoch 150 	 0.096702 	 0.122790 	 0.121362
Train loss       : 0.096582
Best valid loss  : 0.121267
Best test loss   : 0.121090
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.484667 	 0.428652 	 0.430584
Epoch 10 	 0.210940 	 0.210102 	 0.211457
Epoch 20 	 0.171847 	 0.179241 	 0.179956
Epoch 30 	 0.156814 	 0.159823 	 0.161119
Epoch 40 	 0.147625 	 0.155138 	 0.155041
Epoch 50 	 0.138986 	 0.152007 	 0.151661
Epoch 60 	 0.134413 	 0.146634 	 0.146518
Epoch 70 	 0.119097 	 0.135637 	 0.136061
Epoch 80 	 0.117917 	 0.136158 	 0.135298
Epoch 90 	 0.109768 	 0.130937 	 0.131064
Epoch 100 	 0.108641 	 0.130560 	 0.130009
Epoch 110 	 0.104909 	 0.128755 	 0.127679
Epoch 120 	 0.102682 	 0.125367 	 0.126582
Epoch 130 	 0.102205 	 0.125885 	 0.126480
Epoch 140 	 0.101051 	 0.124787 	 0.126153
Epoch 150 	 0.101105 	 0.125573 	 0.125930
Train loss       : 0.100363
Best valid loss  : 0.122838
Best test loss   : 0.125959
Pruning          : 0.70
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.480705 	 0.425708 	 0.429577
Epoch 10 	 0.223535 	 0.217748 	 0.219058
Epoch 20 	 0.176057 	 0.179604 	 0.180544
Epoch 30 	 0.161550 	 0.166126 	 0.165643
Epoch 40 	 0.147724 	 0.156645 	 0.154105
Epoch 50 	 0.141149 	 0.151447 	 0.152963
Epoch 60 	 0.136982 	 0.148725 	 0.148613
Epoch 70 	 0.135167 	 0.148646 	 0.147143
Epoch 80 	 0.129517 	 0.140676 	 0.141447
Epoch 90 	 0.127532 	 0.143802 	 0.143177
Epoch 100 	 0.126338 	 0.138733 	 0.138562
Epoch 110 	 0.114840 	 0.131046 	 0.131306
Epoch 120 	 0.109185 	 0.128021 	 0.127065
Epoch 130 	 0.108448 	 0.127404 	 0.126705
Epoch 140 	 0.105034 	 0.127136 	 0.124990
Epoch 150 	 0.104709 	 0.125826 	 0.124888
Train loss       : 0.104283
Best valid loss  : 0.123868
Best test loss   : 0.124657
Pruning          : 0.49
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.498867 	 0.442701 	 0.446238
Epoch 10 	 0.238068 	 0.235292 	 0.234816
Epoch 20 	 0.191894 	 0.198032 	 0.195756
Epoch 30 	 0.176630 	 0.181209 	 0.180165
Epoch 40 	 0.162367 	 0.169642 	 0.167473
Epoch 50 	 0.154577 	 0.162422 	 0.161380
Epoch 60 	 0.151070 	 0.157594 	 0.157447
Epoch 70 	 0.147670 	 0.159131 	 0.158238
Epoch 80 	 0.144107 	 0.153647 	 0.153402
Epoch 90 	 0.142625 	 0.153469 	 0.152263
Epoch 100 	 0.140689 	 0.153691 	 0.151700
Epoch 110 	 0.139291 	 0.151726 	 0.151297
Epoch 120 	 0.129411 	 0.141843 	 0.142945
Epoch 130 	 0.128782 	 0.140335 	 0.142453
Epoch 140 	 0.126844 	 0.140787 	 0.141459
Epoch 150 	 0.126492 	 0.141188 	 0.141503
Train loss       : 0.125523
Best valid loss  : 0.138384
Best test loss   : 0.139958
Pruning          : 0.34
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.491907 	 0.441273 	 0.448090
Epoch 10 	 0.222525 	 0.215503 	 0.219132
Epoch 20 	 0.176651 	 0.182799 	 0.183356
Epoch 30 	 0.163398 	 0.172925 	 0.172796
Epoch 40 	 0.149556 	 0.159750 	 0.160201
Epoch 50 	 0.144816 	 0.155553 	 0.157433
Epoch 60 	 0.142625 	 0.153727 	 0.154496
Epoch 70 	 0.138410 	 0.153139 	 0.153166
Epoch 80 	 0.135552 	 0.146477 	 0.148429
Epoch 90 	 0.132849 	 0.144661 	 0.144811
Epoch 100 	 0.130922 	 0.144820 	 0.144823
Epoch 110 	 0.127343 	 0.140301 	 0.141177
Epoch 120 	 0.125948 	 0.138239 	 0.137797
Epoch 130 	 0.123984 	 0.137897 	 0.137296
Epoch 140 	 0.123082 	 0.139912 	 0.137368
Epoch 150 	 0.122750 	 0.136720 	 0.135929
Train loss       : 0.112186
Best valid loss  : 0.126874
Best test loss   : 0.128019
Pruning          : 0.24
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.510924 	 0.490207 	 0.501510
Epoch 10 	 0.505423 	 0.492753 	 0.502108
Epoch 20 	 0.505036 	 0.495300 	 0.501480
Epoch 30 	 0.504767 	 0.490564 	 0.500876
[Model stopped early]
Train loss       : 0.505551
Best valid loss  : 0.486748
Best test loss   : 0.501355
Pruning          : 0.17
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.515135 	 0.494489 	 0.501528
Epoch 10 	 0.505519 	 0.495951 	 0.501531
Epoch 20 	 0.505192 	 0.490417 	 0.502076
Epoch 30 	 0.504781 	 0.494618 	 0.501072
Epoch 40 	 0.504811 	 0.491274 	 0.500880
Epoch 50 	 0.504985 	 0.490737 	 0.500892
Epoch 60 	 0.504858 	 0.492524 	 0.500834
Epoch 70 	 0.504732 	 0.496771 	 0.500836
[Model stopped early]
Train loss       : 0.504102
Best valid loss  : 0.487940
Best test loss   : 0.500895
Pruning          : 0.12
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.500408 	 0.444625 	 0.453923
Epoch 10 	 0.256319 	 0.251635 	 0.251068
Epoch 20 	 0.203724 	 0.207205 	 0.205376
Epoch 30 	 0.185843 	 0.197068 	 0.197067
Epoch 40 	 0.176910 	 0.186242 	 0.185233
Epoch 50 	 0.170665 	 0.177788 	 0.178926
Epoch 60 	 0.165964 	 0.176636 	 0.175147
Epoch 70 	 0.162682 	 0.170670 	 0.171280
Epoch 80 	 0.159092 	 0.167518 	 0.167764
Epoch 90 	 0.155313 	 0.165895 	 0.165275
Epoch 100 	 0.152257 	 0.160537 	 0.161335
Epoch 110 	 0.151705 	 0.161537 	 0.160436
Epoch 120 	 0.149617 	 0.155674 	 0.157580
Epoch 130 	 0.146393 	 0.158290 	 0.158257
Epoch 140 	 0.137276 	 0.152801 	 0.150955
Epoch 150 	 0.136385 	 0.149600 	 0.148794
Train loss       : 0.135240
Best valid loss  : 0.147274
Best test loss   : 0.148222
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.516340 	 0.490566 	 0.501818
Epoch 10 	 0.505812 	 0.494268 	 0.501275
Epoch 20 	 0.505118 	 0.493568 	 0.501305
Epoch 30 	 0.504427 	 0.494475 	 0.500870
Epoch 40 	 0.503360 	 0.493717 	 0.500847
Epoch 50 	 0.504799 	 0.492586 	 0.500873
Epoch 60 	 0.504871 	 0.494657 	 0.500896
Epoch 70 	 0.504961 	 0.489842 	 0.500839
Epoch 80 	 0.504485 	 0.493684 	 0.500840
[Model stopped early]
Train loss       : 0.504998
Best valid loss  : 0.486551
Best test loss   : 0.500835
Pruning          : 0.06
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.518950 	 0.491214 	 0.501639
Epoch 10 	 0.505124 	 0.489960 	 0.501136
Epoch 20 	 0.504956 	 0.495067 	 0.501147
Epoch 30 	 0.505166 	 0.491643 	 0.501038
[Model stopped early]
Train loss       : 0.504900
Best valid loss  : 0.488711
Best test loss   : 0.501173
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.515142 	 0.491296 	 0.501389
Epoch 10 	 0.504927 	 0.493723 	 0.501146
Epoch 20 	 0.505280 	 0.495648 	 0.501763
Epoch 30 	 0.505630 	 0.492009 	 0.501088
Epoch 40 	 0.504678 	 0.497841 	 0.500971
Epoch 50 	 0.505099 	 0.492518 	 0.500882
[Model stopped early]
Train loss       : 0.505099
Best valid loss  : 0.486690
Best test loss   : 0.501028
Pruning          : 0.03
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.532060 	 0.521311 	 0.528546
Epoch 10 	 0.505239 	 0.493812 	 0.501073
Epoch 20 	 0.505019 	 0.494786 	 0.500948
Epoch 30 	 0.504798 	 0.488659 	 0.501063
Epoch 40 	 0.504802 	 0.492204 	 0.500882
Epoch 50 	 0.504445 	 0.491260 	 0.500937
Epoch 60 	 0.504713 	 0.491934 	 0.500845
Epoch 70 	 0.504617 	 0.494823 	 0.500845
[Model stopped early]
Train loss       : 0.504831
Best valid loss  : 0.486210
Best test loss   : 0.500882
Pruning          : 0.02
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.532553 	 0.518606 	 0.528232
Epoch 10 	 0.532015 	 0.516310 	 0.528169
Epoch 20 	 0.532269 	 0.517283 	 0.528124
Epoch 30 	 0.532030 	 0.519051 	 0.528183
Epoch 40 	 0.532027 	 0.518213 	 0.528129
Epoch 50 	 0.532240 	 0.515222 	 0.528124
Epoch 60 	 0.531571 	 0.520477 	 0.528125
[Model stopped early]
Train loss       : 0.531915
Best valid loss  : 0.511882
Best test loss   : 0.528201
Pruning          : 0.01
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.531638 	 0.516539 	 0.528526
Epoch 10 	 0.504997 	 0.492189 	 0.501174
Epoch 20 	 0.504683 	 0.491159 	 0.501406
Epoch 30 	 0.504660 	 0.489978 	 0.501053
Epoch 40 	 0.503903 	 0.492354 	 0.500954
Epoch 50 	 0.504972 	 0.492039 	 0.500907
Epoch 60 	 0.505034 	 0.493589 	 0.500909
Epoch 70 	 0.504478 	 0.492218 	 0.500860
Epoch 80 	 0.504956 	 0.494615 	 0.500849
[Model stopped early]
Train loss       : 0.504956
Best valid loss  : 0.486715
Best test loss   : 0.500922
Pruning          : 0.01
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5287]
[Starting training]
Epoch 0 	 0.517316 	 0.491924 	 0.501943
Epoch 10 	 0.505670 	 0.490994 	 0.501113
Epoch 20 	 0.504896 	 0.494303 	 0.501017
Epoch 30 	 0.504833 	 0.491955 	 0.500894
Epoch 40 	 0.504514 	 0.491478 	 0.500904
Epoch 50 	 0.504841 	 0.488394 	 0.500900
Epoch 60 	 0.504726 	 0.494596 	 0.500831
[Model stopped early]
Train loss       : 0.504446
Best valid loss  : 0.486643
Best test loss   : 0.501151
Pruning          : 0.01
