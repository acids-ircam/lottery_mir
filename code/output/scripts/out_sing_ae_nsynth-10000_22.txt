Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40871934.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, six, future, torch, torchvision, tqdm, kiwisolver, pyparsing, cycler, python-dateutil, matplotlib, protobuf, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, idna, chardet, certifi, urllib3, requests, requests-oauthlib, google-auth-oauthlib, grpcio, werkzeug, markdown, absl-py, tensorboard, wrapt, astor, termcolor, keras-preprocessing, tensorflow-estimator, google-pasta, h5py, keras-applications, opt-einsum, gast, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871934.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-22 07:33:59.906735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-22 07:34:00.279665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_sing_ae_cnn_xavier_trimming_information_rewind_global_0.
*******
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5164]
[Starting training]
Epoch 0 	 0.469201 	 0.413631 	 0.414071
Epoch 10 	 0.214023 	 0.210776 	 0.208367
Epoch 20 	 0.166190 	 0.172677 	 0.171185
Epoch 30 	 0.151754 	 0.161253 	 0.163197
Epoch 40 	 0.143850 	 0.155860 	 0.155623
Epoch 50 	 0.137596 	 0.150091 	 0.148528
Epoch 60 	 0.135328 	 0.151413 	 0.149064
Epoch 70 	 0.131462 	 0.146964 	 0.145579
Epoch 80 	 0.117803 	 0.138246 	 0.136346
Epoch 90 	 0.115756 	 0.136477 	 0.135170
Epoch 100 	 0.114599 	 0.137204 	 0.134602
Epoch 110 	 0.112592 	 0.135538 	 0.132087
Epoch 120 	 0.105584 	 0.131004 	 0.128134
Epoch 130 	 0.104704 	 0.130588 	 0.127933
Epoch 140 	 0.100896 	 0.128591 	 0.126046
Epoch 150 	 0.100492 	 0.128088 	 0.125522
Train loss       : 0.098415
Best valid loss  : 0.125673
Best test loss   : 0.125505
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 14,520,216
--------------------------------
Total memory      : 18.04 MB
Total Flops       : 3.06 GFlops
Total Mem (Read)  : 63.56 MB
Total Mem (Write) : 17.81 MB
[Supermasks testing]
[Untrained loss : 0.1357]
[Starting training]
Epoch 0 	 0.131968 	 0.146630 	 0.144689
Epoch 10 	 0.126020 	 0.144917 	 0.140832
Epoch 20 	 0.125398 	 0.144472 	 0.142487
Epoch 30 	 0.126341 	 0.145491 	 0.144280
Epoch 40 	 0.111547 	 0.133380 	 0.130714
Epoch 50 	 0.110417 	 0.132293 	 0.132088
Epoch 60 	 0.108898 	 0.133786 	 0.131364
Epoch 70 	 0.108140 	 0.131044 	 0.129582
Epoch 80 	 0.107148 	 0.131468 	 0.129339
Epoch 90 	 0.099833 	 0.126481 	 0.125132
Epoch 100 	 0.099624 	 0.126282 	 0.124391
Epoch 110 	 0.096227 	 0.126155 	 0.122796
Epoch 120 	 0.095956 	 0.126254 	 0.122902
Epoch 130 	 0.094197 	 0.125007 	 0.121800
Epoch 140 	 0.093959 	 0.124021 	 0.121971
Epoch 150 	 0.093199 	 0.123432 	 0.121536
Train loss       : 0.093019
Best valid loss  : 0.121047
Best test loss   : 0.121544
Pruning          : 0.78
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 9,070,479
--------------------------------
Total memory      : 12.36 MB
Total Flops       : 1.93 GFlops
Total Mem (Read)  : 41.07 MB
Total Mem (Write) : 12.13 MB
[Supermasks testing]
[Untrained loss : 0.1368]
[Starting training]
Epoch 0 	 0.132770 	 0.149476 	 0.146998
Epoch 10 	 0.125699 	 0.146742 	 0.143849
Epoch 20 	 0.125560 	 0.144499 	 0.142210
Epoch 30 	 0.112451 	 0.136781 	 0.133985
Epoch 40 	 0.112620 	 0.137017 	 0.134219
Epoch 50 	 0.104626 	 0.129872 	 0.127887
Epoch 60 	 0.104274 	 0.129249 	 0.127873
Epoch 70 	 0.100359 	 0.127236 	 0.125547
Epoch 80 	 0.099718 	 0.127319 	 0.125139
Epoch 90 	 0.099286 	 0.126371 	 0.125022
Epoch 100 	 0.097257 	 0.127630 	 0.124314
Epoch 110 	 0.097130 	 0.125954 	 0.124065
Epoch 120 	 0.096209 	 0.125073 	 0.123645
Epoch 130 	 0.095747 	 0.124713 	 0.123574
Epoch 140 	 0.095512 	 0.124985 	 0.123549
Epoch 150 	 0.095237 	 0.125224 	 0.123498
Train loss       : 0.095228
Best valid loss  : 0.124281
Best test loss   : 0.123521
Pruning          : 0.61
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,285,096
--------------------------------
Total memory      : 7.90 MB
Total Flops       : 665.38 MFlops
Total Mem (Read)  : 17.65 MB
Total Mem (Write) : 7.67 MB
[Supermasks testing]
[Untrained loss : 0.1369]
[Starting training]
Epoch 0 	 0.131659 	 0.147503 	 0.145772
Epoch 10 	 0.127677 	 0.147331 	 0.143575
Epoch 20 	 0.128873 	 0.149399 	 0.147801
Epoch 30 	 0.125593 	 0.143467 	 0.143038
Epoch 40 	 0.122654 	 0.140022 	 0.138316
Epoch 50 	 0.121150 	 0.139432 	 0.139120
Epoch 60 	 0.120993 	 0.139625 	 0.139781
Epoch 70 	 0.108391 	 0.130077 	 0.128663
Epoch 80 	 0.107253 	 0.129716 	 0.128840
Epoch 90 	 0.106742 	 0.130410 	 0.128287
Epoch 100 	 0.104930 	 0.129691 	 0.127643
Epoch 110 	 0.098858 	 0.125064 	 0.122871
Epoch 120 	 0.098536 	 0.126339 	 0.123989
Epoch 130 	 0.095398 	 0.122738 	 0.121608
Epoch 140 	 0.094780 	 0.124043 	 0.121608
Epoch 150 	 0.093210 	 0.122396 	 0.120624
Train loss       : 0.092530
Best valid loss  : 0.120685
Best test loss   : 0.121342
Pruning          : 0.47
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,170,337
--------------------------------
Total memory      : 3.83 MB
Total Flops       : 186.21 MFlops
Total Mem (Read)  : 6.34 MB
Total Mem (Write) : 3.59 MB
[Supermasks testing]
[Untrained loss : 0.1392]
[Starting training]
Epoch 0 	 0.133558 	 0.146559 	 0.145516
Epoch 10 	 0.128700 	 0.146106 	 0.142430
Epoch 20 	 0.125831 	 0.140247 	 0.140374
Epoch 30 	 0.112516 	 0.134193 	 0.133268
Epoch 40 	 0.112191 	 0.133769 	 0.133369
Epoch 50 	 0.104984 	 0.130060 	 0.128450
Epoch 60 	 0.104354 	 0.129931 	 0.128322
Epoch 70 	 0.103399 	 0.129811 	 0.127032
Epoch 80 	 0.102377 	 0.126330 	 0.126895
Epoch 90 	 0.101927 	 0.129045 	 0.127059
Epoch 100 	 0.098184 	 0.127263 	 0.124530
Epoch 110 	 0.096684 	 0.125371 	 0.123492
Epoch 120 	 0.095968 	 0.125048 	 0.123650
Epoch 130 	 0.095193 	 0.125134 	 0.123064
Epoch 140 	 0.095005 	 0.124968 	 0.122985
Epoch 150 	 0.094419 	 0.125768 	 0.122894
Train loss       : 0.094295
Best valid loss  : 0.122981
Best test loss   : 0.122860
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 699,835
--------------------------------
Total memory      : 3.03 MB
Total Flops       : 78.79 MFlops
Total Mem (Read)  : 3.92 MB
Total Mem (Write) : 2.8 MB
[Supermasks testing]
[Untrained loss : 0.4574]
[Starting training]
Epoch 0 	 0.213132 	 0.174295 	 0.173420
Epoch 10 	 0.132844 	 0.148659 	 0.147567
Epoch 20 	 0.129157 	 0.147495 	 0.146184
Epoch 30 	 0.128374 	 0.144127 	 0.144455
Epoch 40 	 0.127900 	 0.147575 	 0.145899
Epoch 50 	 0.112289 	 0.136794 	 0.135664
Epoch 60 	 0.112283 	 0.135274 	 0.133864
Epoch 70 	 0.104864 	 0.131994 	 0.128575
Epoch 80 	 0.104219 	 0.128986 	 0.128617
Epoch 90 	 0.100759 	 0.128571 	 0.127082
Epoch 100 	 0.100238 	 0.127710 	 0.126723
Epoch 110 	 0.098468 	 0.127150 	 0.125904
Epoch 120 	 0.097845 	 0.126099 	 0.125853
[Model stopped early]
Train loss       : 0.097816
Best valid loss  : 0.125420
Best test loss   : 0.126757
Pruning          : 0.29
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 502,179
--------------------------------
Total memory      : 2.42 MB
Total Flops       : 32.94 MFlops
Total Mem (Read)  : 2.61 MB
Total Mem (Write) : 2.19 MB
[Supermasks testing]
[Untrained loss : 0.5140]
[Starting training]
Epoch 0 	 0.389117 	 0.320426 	 0.319716
Epoch 10 	 0.198082 	 0.202846 	 0.201669
Epoch 20 	 0.180177 	 0.189521 	 0.187731
Epoch 30 	 0.169406 	 0.179604 	 0.179967
Epoch 40 	 0.163354 	 0.179610 	 0.180024
Epoch 50 	 0.159682 	 0.175136 	 0.175740
Epoch 60 	 0.154643 	 0.167501 	 0.167440
Epoch 70 	 0.152982 	 0.171247 	 0.171378
Epoch 80 	 0.141153 	 0.159988 	 0.159042
Epoch 90 	 0.139505 	 0.157927 	 0.156294
Epoch 100 	 0.134141 	 0.153765 	 0.153414
Epoch 110 	 0.130945 	 0.153406 	 0.150669
Epoch 120 	 0.130251 	 0.152566 	 0.150132
Epoch 130 	 0.129889 	 0.152358 	 0.149906
Epoch 140 	 0.128328 	 0.151778 	 0.149159
Epoch 150 	 0.127392 	 0.152081 	 0.149012
Train loss       : 0.127090
Best valid loss  : 0.149120
Best test loss   : 0.148718
Pruning          : 0.23
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 347,795
--------------------------------
Total memory      : 1.94 MB
Total Flops       : 16.91 MFlops
Total Mem (Read)  : 1.96 MB
Total Mem (Write) : 1.71 MB
[Supermasks testing]
[Untrained loss : 0.5173]
[Starting training]
Epoch 0 	 0.443510 	 0.398168 	 0.399080
Epoch 10 	 0.303276 	 0.301114 	 0.300465
Epoch 20 	 0.272212 	 0.273990 	 0.270027
Epoch 30 	 0.256524 	 0.260275 	 0.257908
Epoch 40 	 0.246810 	 0.253576 	 0.250086
Epoch 50 	 0.239212 	 0.251439 	 0.250223
Epoch 60 	 0.234760 	 0.242595 	 0.240722
Epoch 70 	 0.230266 	 0.241269 	 0.237604
Epoch 80 	 0.225938 	 0.241703 	 0.238046
Epoch 90 	 0.223811 	 0.235498 	 0.234373
Epoch 100 	 0.221371 	 0.233620 	 0.232027
Epoch 110 	 0.217795 	 0.235755 	 0.234225
Epoch 120 	 0.217430 	 0.228687 	 0.228194
Epoch 130 	 0.207837 	 0.221658 	 0.220851
Epoch 140 	 0.206306 	 0.220619 	 0.220386
Epoch 150 	 0.205249 	 0.222800 	 0.219557
Train loss       : 0.205518
Best valid loss  : 0.219118
Best test loss   : 0.219221
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 250,679
--------------------------------
Total memory      : 1.57 MB
Total Flops       : 10.13 MFlops
Total Mem (Read)  : 1.55 MB
Total Mem (Write) : 1.34 MB
[Supermasks testing]
[Untrained loss : 0.5139]
[Starting training]
Epoch 0 	 0.463833 	 0.423143 	 0.420045
Epoch 10 	 0.357748 	 0.349589 	 0.352019
Epoch 20 	 0.320481 	 0.316472 	 0.319028
Epoch 30 	 0.302017 	 0.305134 	 0.304665
Epoch 40 	 0.293571 	 0.294358 	 0.296003
Epoch 50 	 0.286713 	 0.288658 	 0.287893
Epoch 60 	 0.280022 	 0.286065 	 0.282362
Epoch 70 	 0.274761 	 0.279185 	 0.279453
Epoch 80 	 0.270431 	 0.276000 	 0.274357
Epoch 90 	 0.265966 	 0.270214 	 0.268464
Epoch 100 	 0.263752 	 0.269689 	 0.267737
Epoch 110 	 0.259628 	 0.266163 	 0.263486
Epoch 120 	 0.256910 	 0.268142 	 0.264031
Epoch 130 	 0.252704 	 0.260691 	 0.257689
Epoch 140 	 0.248619 	 0.254149 	 0.253979
Epoch 150 	 0.245416 	 0.258355 	 0.252151
Train loss       : 0.243501
Best valid loss  : 0.253287
Best test loss   : 0.251865
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 189,934
--------------------------------
Total memory      : 1.28 MB
Total Flops       : 6.27 MFlops
Total Mem (Read)  : 1.24 MB
Total Mem (Write) : 1.04 MB
[Supermasks testing]
[Untrained loss : 0.5131]
[Starting training]
Epoch 0 	 0.473993 	 0.436967 	 0.437500
Epoch 10 	 0.397625 	 0.385919 	 0.384939
Epoch 20 	 0.362583 	 0.354986 	 0.355284
Epoch 30 	 0.343163 	 0.340318 	 0.339424
Epoch 40 	 0.334458 	 0.328826 	 0.329598
Epoch 50 	 0.325827 	 0.325041 	 0.326821
Epoch 60 	 0.317419 	 0.316542 	 0.317884
Epoch 70 	 0.310591 	 0.313267 	 0.311273
Epoch 80 	 0.306809 	 0.306195 	 0.306249
Epoch 90 	 0.302660 	 0.305726 	 0.304537
Epoch 100 	 0.299392 	 0.304424 	 0.302398
Epoch 110 	 0.296657 	 0.299991 	 0.297726
Epoch 120 	 0.293510 	 0.293774 	 0.292925
Epoch 130 	 0.290847 	 0.294607 	 0.291403
Epoch 140 	 0.290102 	 0.289041 	 0.289380
Epoch 150 	 0.287200 	 0.287396 	 0.287612
Train loss       : 0.280415
Best valid loss  : 0.281682
Best test loss   : 0.280704
Pruning          : 0.11
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 132,483
--------------------------------
Total memory      : 1.05 MB
Total Flops       : 4.46 MFlops
Total Mem (Read)  : 1.03 MB
Total Mem (Write) : 838.45 KB
[Supermasks testing]
[Untrained loss : 0.5148]
[Starting training]
Epoch 0 	 0.478443 	 0.446134 	 0.441869
Epoch 10 	 0.403998 	 0.397783 	 0.395153
Epoch 20 	 0.396180 	 0.389261 	 0.386726
Epoch 30 	 0.386046 	 0.377656 	 0.377075
Epoch 40 	 0.374234 	 0.366393 	 0.368160
Epoch 50 	 0.363228 	 0.360959 	 0.358383
Epoch 60 	 0.352407 	 0.349074 	 0.347319
Epoch 70 	 0.342775 	 0.337725 	 0.338897
Epoch 80 	 0.334386 	 0.331239 	 0.330410
Epoch 90 	 0.329884 	 0.326515 	 0.326455
Epoch 100 	 0.323416 	 0.319795 	 0.321294
Epoch 110 	 0.319217 	 0.321763 	 0.318599
Epoch 120 	 0.314595 	 0.311731 	 0.312764
Epoch 130 	 0.312308 	 0.311846 	 0.312236
Epoch 140 	 0.310773 	 0.310350 	 0.310366
Epoch 150 	 0.307460 	 0.307346 	 0.308041
Train loss       : 0.307522
Best valid loss  : 0.305438
Best test loss   : 0.307299
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 87,241
--------------------------------
Total memory      : 0.88 MB
Total Flops       : 3.4 MFlops
Total Mem (Read)  : 891.88 KB
Total Mem (Write) : 657.16 KB
[Supermasks testing]
[Untrained loss : 0.5162]
[Starting training]
Epoch 0 	 0.476304 	 0.435366 	 0.434039
Epoch 10 	 0.404271 	 0.394285 	 0.394583
Epoch 20 	 0.398435 	 0.386623 	 0.389001
Epoch 30 	 0.394691 	 0.382321 	 0.385756
Epoch 40 	 0.379277 	 0.369786 	 0.371931
Epoch 50 	 0.359625 	 0.357723 	 0.355423
Epoch 60 	 0.345646 	 0.342982 	 0.342995
Epoch 70 	 0.338753 	 0.334679 	 0.336766
Epoch 80 	 0.334027 	 0.328625 	 0.332547
Epoch 90 	 0.331378 	 0.330015 	 0.330708
Epoch 100 	 0.328300 	 0.324030 	 0.327829
Epoch 110 	 0.325763 	 0.322516 	 0.325579
Epoch 120 	 0.323105 	 0.321473 	 0.324148
Epoch 130 	 0.318403 	 0.314481 	 0.318017
Epoch 140 	 0.317123 	 0.312857 	 0.317475
Epoch 150 	 0.316541 	 0.313645 	 0.317631
Train loss       : 0.315719
Best valid loss  : 0.310759
Best test loss   : 0.316194
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 82,646
--------------------------------
Total memory      : 0.74 MB
Total Flops       : 2.75 MFlops
Total Mem (Read)  : 741.21 KB
Total Mem (Write) : 514.55 KB
[Supermasks testing]
[Untrained loss : 0.5161]
[Starting training]
Epoch 0 	 0.479535 	 0.443769 	 0.443137
Epoch 10 	 0.403928 	 0.394791 	 0.394238
Epoch 20 	 0.391560 	 0.379948 	 0.381683
Epoch 30 	 0.366385 	 0.356783 	 0.357612
Epoch 40 	 0.350042 	 0.339824 	 0.345021
Epoch 50 	 0.340533 	 0.333070 	 0.337662
Epoch 60 	 0.335633 	 0.332650 	 0.334940
Epoch 70 	 0.331960 	 0.329540 	 0.330669
Epoch 80 	 0.328827 	 0.326114 	 0.329670
Epoch 90 	 0.326265 	 0.320621 	 0.324738
Epoch 100 	 0.322968 	 0.320279 	 0.322477
Epoch 110 	 0.321840 	 0.319290 	 0.322366
Epoch 120 	 0.314991 	 0.310096 	 0.317029
Epoch 130 	 0.314433 	 0.313114 	 0.315349
Epoch 140 	 0.313149 	 0.310910 	 0.315914
Epoch 150 	 0.310435 	 0.310921 	 0.312585
Train loss       : 0.309327
Best valid loss  : 0.307118
Best test loss   : 0.315505
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 62,578
--------------------------------
Total memory      : 0.63 MB
Total Flops       : 2.09 MFlops
Total Mem (Read)  : 637.92 KB
Total Mem (Write) : 405.58 KB
[Supermasks testing]
[Untrained loss : 0.5163]
[Starting training]
Epoch 0 	 0.486763 	 0.451544 	 0.448175
Epoch 10 	 0.406790 	 0.393270 	 0.395447
Epoch 20 	 0.397985 	 0.389182 	 0.388524
Epoch 30 	 0.392018 	 0.382917 	 0.383458
Epoch 40 	 0.387069 	 0.380243 	 0.378780
Epoch 50 	 0.382251 	 0.372038 	 0.373390
Epoch 60 	 0.375379 	 0.371289 	 0.368119
Epoch 70 	 0.369071 	 0.360881 	 0.361550
Epoch 80 	 0.359718 	 0.354313 	 0.352863
Epoch 90 	 0.353539 	 0.347179 	 0.347222
Epoch 100 	 0.346451 	 0.342551 	 0.344886
Epoch 110 	 0.344566 	 0.339603 	 0.341477
Epoch 120 	 0.339531 	 0.339303 	 0.338481
Epoch 130 	 0.338487 	 0.333705 	 0.336680
Epoch 140 	 0.336497 	 0.330589 	 0.333339
Epoch 150 	 0.334098 	 0.330809 	 0.331504
Train loss       : 0.332210
Best valid loss  : 0.327214
Best test loss   : 0.328752
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 55,949
--------------------------------
Total memory      : 0.55 MB
Total Flops       : 1.71 MFlops
Total Mem (Read)  : 552.27 KB
Total Mem (Write) : 321.1 KB
[Supermasks testing]
[Untrained loss : 0.5166]
[Starting training]
Epoch 0 	 0.504413 	 0.457415 	 0.454861
Epoch 10 	 0.408037 	 0.402070 	 0.398306
Epoch 20 	 0.400325 	 0.393176 	 0.391076
Epoch 30 	 0.394300 	 0.386606 	 0.385821
Epoch 40 	 0.390224 	 0.379531 	 0.381293
Epoch 50 	 0.384276 	 0.375024 	 0.376553
Epoch 60 	 0.381296 	 0.372334 	 0.374550
Epoch 70 	 0.378498 	 0.371931 	 0.372780
Epoch 80 	 0.377952 	 0.370383 	 0.372567
Epoch 90 	 0.376271 	 0.370296 	 0.370741
Epoch 100 	 0.375654 	 0.370027 	 0.370520
Epoch 110 	 0.375513 	 0.367904 	 0.370135
Epoch 120 	 0.375104 	 0.368928 	 0.369914
Epoch 130 	 0.374780 	 0.367984 	 0.369783
Epoch 140 	 0.374608 	 0.367760 	 0.369749
Epoch 150 	 0.374014 	 0.367785 	 0.369689
[Model stopped early]
Train loss       : 0.374536
Best valid loss  : 0.365701
Best test loss   : 0.369861
Pruning          : 0.03
