Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40871913.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, python-dateutil, cycler, kiwisolver, pyparsing, matplotlib, google-pasta, h5py, keras-applications, termcolor, protobuf, keras-preprocessing, markdown, grpcio, oauthlib, urllib3, chardet, idna, certifi, requests, requests-oauthlib, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, google-auth-oauthlib, absl-py, werkzeug, tensorboard, tensorflow-estimator, astor, wrapt, gast, opt-einsum, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871913.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-22 07:22:37.724151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-22 07:22:38.114547: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_sing_ae_cnn_xavier_trimming_information_reinit_local_0.
*******
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5500]
[Starting training]
Epoch 0 	 0.470446 	 0.404452 	 0.420890
Epoch 10 	 0.205910 	 0.214159 	 0.216388
Epoch 20 	 0.165095 	 0.166374 	 0.170509
Epoch 30 	 0.149657 	 0.157234 	 0.161699
Epoch 40 	 0.143500 	 0.149324 	 0.155002
Epoch 50 	 0.136874 	 0.146427 	 0.150593
Epoch 60 	 0.133404 	 0.147277 	 0.149919
Epoch 70 	 0.130489 	 0.142923 	 0.144913
Epoch 80 	 0.130882 	 0.143770 	 0.146630
Epoch 90 	 0.125431 	 0.136640 	 0.141376
Epoch 100 	 0.121998 	 0.137346 	 0.140534
Epoch 110 	 0.120355 	 0.133485 	 0.137483
Epoch 120 	 0.119293 	 0.133808 	 0.136620
Epoch 130 	 0.105058 	 0.126660 	 0.129182
Epoch 140 	 0.104760 	 0.125743 	 0.127842
Epoch 150 	 0.097238 	 0.120415 	 0.123429
Train loss       : 0.096940
Best valid loss  : 0.118037
Best test loss   : 0.123262
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 13,660,755
--------------------------------
Total memory      : 18.12 MB
Total Flops       : 2.95 GFlops
Total Mem (Read)  : 62.53 MB
Total Mem (Write) : 17.89 MB
[Supermasks testing]
[Untrained loss : 0.5620]
[Starting training]
Epoch 0 	 0.460977 	 0.412210 	 0.426189
Epoch 10 	 0.214665 	 0.211895 	 0.212947
Epoch 20 	 0.170840 	 0.177062 	 0.181227
Epoch 30 	 0.158041 	 0.164096 	 0.169754
Epoch 40 	 0.149572 	 0.157092 	 0.161559
Epoch 50 	 0.144179 	 0.158792 	 0.159626
Epoch 60 	 0.138560 	 0.152716 	 0.154795
Epoch 70 	 0.137204 	 0.150992 	 0.154972
Epoch 80 	 0.131624 	 0.148160 	 0.152148
Epoch 90 	 0.129208 	 0.143876 	 0.147427
Epoch 100 	 0.114482 	 0.133974 	 0.137084
Epoch 110 	 0.113399 	 0.133564 	 0.136827
Epoch 120 	 0.105982 	 0.129276 	 0.132619
Epoch 130 	 0.104852 	 0.129897 	 0.132947
Epoch 140 	 0.100823 	 0.127352 	 0.130134
Epoch 150 	 0.098723 	 0.125739 	 0.129568
Train loss       : 0.098161
Best valid loss  : 0.124360
Best test loss   : 0.129083
Pruning          : 0.78
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 8,896,787
--------------------------------
Total memory      : 14.18 MB
Total Flops       : 1.87 GFlops
Total Mem (Read)  : 42.12 MB
Total Mem (Write) : 13.95 MB
[Supermasks testing]
[Untrained loss : 0.5717]
[Starting training]
Epoch 0 	 0.451947 	 0.405295 	 0.417166
Epoch 10 	 0.221918 	 0.217576 	 0.219004
Epoch 20 	 0.173981 	 0.175769 	 0.182712
Epoch 30 	 0.155865 	 0.165998 	 0.167439
Epoch 40 	 0.147012 	 0.160093 	 0.163766
Epoch 50 	 0.143898 	 0.155537 	 0.157321
Epoch 60 	 0.135016 	 0.150394 	 0.152063
Epoch 70 	 0.134260 	 0.150613 	 0.152142
Epoch 80 	 0.130216 	 0.141591 	 0.146007
Epoch 90 	 0.128019 	 0.142718 	 0.146644
Epoch 100 	 0.116724 	 0.133372 	 0.135034
Epoch 110 	 0.111561 	 0.129700 	 0.132010
Epoch 120 	 0.110637 	 0.128827 	 0.132499
Epoch 130 	 0.108766 	 0.131504 	 0.133354
Epoch 140 	 0.101469 	 0.125687 	 0.126808
Epoch 150 	 0.097671 	 0.123918 	 0.124770
Train loss       : 0.097134
Best valid loss  : 0.122184
Best test loss   : 0.124574
Pruning          : 0.61
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 5,874,567
--------------------------------
Total memory      : 11.12 MB
Total Flops       : 1.2 GFlops
Total Mem (Read)  : 28.85 MB
Total Mem (Write) : 10.88 MB
[Supermasks testing]
[Untrained loss : 0.5311]
[Starting training]
Epoch 0 	 0.459523 	 0.417474 	 0.433199
Epoch 10 	 0.221079 	 0.220568 	 0.223049
Epoch 20 	 0.173865 	 0.182645 	 0.185947
Epoch 30 	 0.156793 	 0.165329 	 0.169647
Epoch 40 	 0.150061 	 0.159793 	 0.162310
Epoch 50 	 0.144076 	 0.154683 	 0.159033
Epoch 60 	 0.142991 	 0.154356 	 0.159474
Epoch 70 	 0.137232 	 0.148214 	 0.154352
Epoch 80 	 0.133941 	 0.146595 	 0.152102
Epoch 90 	 0.131623 	 0.147130 	 0.148469
Epoch 100 	 0.118247 	 0.137079 	 0.138924
Epoch 110 	 0.116440 	 0.133994 	 0.139968
Epoch 120 	 0.109075 	 0.132315 	 0.134024
Epoch 130 	 0.108328 	 0.130521 	 0.132535
Epoch 140 	 0.106930 	 0.128350 	 0.133291
Epoch 150 	 0.105893 	 0.129527 	 0.131704
Train loss       : 0.105083
Best valid loss  : 0.127715
Best test loss   : 0.131329
Pruning          : 0.47
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,931,875
--------------------------------
Total memory      : 8.72 MB
Total Flops       : 774.71 MFlops
Total Mem (Read)  : 20.07 MB
Total Mem (Write) : 8.49 MB
[Supermasks testing]
[Untrained loss : 0.5569]
[Starting training]
Epoch 0 	 0.450164 	 0.408157 	 0.419608
Epoch 10 	 0.216405 	 0.214640 	 0.219314
Epoch 20 	 0.171950 	 0.176880 	 0.182085
Epoch 30 	 0.157809 	 0.165750 	 0.169563
Epoch 40 	 0.148194 	 0.159611 	 0.161259
Epoch 50 	 0.143339 	 0.155133 	 0.158009
Epoch 60 	 0.128962 	 0.143681 	 0.146612
Epoch 70 	 0.126280 	 0.142502 	 0.145882
Epoch 80 	 0.123968 	 0.141021 	 0.143619
Epoch 90 	 0.121518 	 0.138802 	 0.142877
Epoch 100 	 0.120513 	 0.140258 	 0.143088
Epoch 110 	 0.119309 	 0.138516 	 0.142036
Epoch 120 	 0.110955 	 0.135350 	 0.136168
Epoch 130 	 0.110499 	 0.131568 	 0.135571
Epoch 140 	 0.109703 	 0.131201 	 0.135136
Epoch 150 	 0.108901 	 0.131373 	 0.134116
Train loss       : 0.108160
Best valid loss  : 0.128930
Best test loss   : 0.134460
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,670,357
--------------------------------
Total memory      : 6.85 MB
Total Flops       : 507.44 MFlops
Total Mem (Read)  : 14.2 MB
Total Mem (Write) : 6.62 MB
[Supermasks testing]
[Untrained loss : 0.5758]
[Starting training]
Epoch 0 	 0.463359 	 0.418053 	 0.431739
Epoch 10 	 0.230949 	 0.230511 	 0.233370
Epoch 20 	 0.181791 	 0.187689 	 0.192336
Epoch 30 	 0.161699 	 0.170034 	 0.175467
Epoch 40 	 0.152350 	 0.156568 	 0.163286
Epoch 50 	 0.145541 	 0.153909 	 0.160741
Epoch 60 	 0.140901 	 0.147720 	 0.153240
Epoch 70 	 0.136974 	 0.149036 	 0.153019
Epoch 80 	 0.136257 	 0.148831 	 0.153555
Epoch 90 	 0.133834 	 0.141787 	 0.146517
Epoch 100 	 0.129772 	 0.141501 	 0.146649
Epoch 110 	 0.130454 	 0.145986 	 0.149725
Epoch 120 	 0.115514 	 0.132923 	 0.136629
Epoch 130 	 0.109278 	 0.128759 	 0.131965
Epoch 140 	 0.108014 	 0.128666 	 0.132344
Epoch 150 	 0.107356 	 0.127297 	 0.131586
Train loss       : 0.103898
Best valid loss  : 0.125762
Best test loss   : 0.131541
Pruning          : 0.29
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,842,777
--------------------------------
Total memory      : 5.40 MB
Total Flops       : 337.15 MFlops
Total Mem (Read)  : 10.22 MB
Total Mem (Write) : 5.16 MB
[Supermasks testing]
[Untrained loss : 0.5949]
[Starting training]
Epoch 0 	 0.467785 	 0.422654 	 0.435588
Epoch 10 	 0.236090 	 0.231158 	 0.236166
Epoch 20 	 0.181425 	 0.188026 	 0.193577
Epoch 30 	 0.163895 	 0.172128 	 0.176644
Epoch 40 	 0.156546 	 0.164098 	 0.166964
Epoch 50 	 0.150597 	 0.160830 	 0.164428
Epoch 60 	 0.146798 	 0.155238 	 0.160768
Epoch 70 	 0.143539 	 0.159367 	 0.160769
Epoch 80 	 0.127753 	 0.143374 	 0.147932
Epoch 90 	 0.126665 	 0.142563 	 0.146925
Epoch 100 	 0.119173 	 0.136479 	 0.141581
Epoch 110 	 0.118223 	 0.138611 	 0.140650
Epoch 120 	 0.114030 	 0.135544 	 0.139047
Epoch 130 	 0.113642 	 0.134401 	 0.138482
Epoch 140 	 0.112773 	 0.134474 	 0.138501
Epoch 150 	 0.111017 	 0.133833 	 0.137467
Train loss       : 0.110689
Best valid loss  : 0.132181
Best test loss   : 0.137591
Pruning          : 0.23
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,288,101
--------------------------------
Total memory      : 4.26 MB
Total Flops       : 226.68 MFlops
Total Mem (Read)  : 7.45 MB
Total Mem (Write) : 4.02 MB
[Supermasks testing]
[Untrained loss : 0.5941]
[Starting training]
Epoch 0 	 0.468385 	 0.428197 	 0.442797
Epoch 10 	 0.258702 	 0.252500 	 0.255688
Epoch 20 	 0.185313 	 0.183790 	 0.190442
Epoch 30 	 0.168486 	 0.169941 	 0.173989
Epoch 40 	 0.155979 	 0.163405 	 0.166934
Epoch 50 	 0.149658 	 0.157991 	 0.161909
Epoch 60 	 0.145145 	 0.154469 	 0.158766
Epoch 70 	 0.141735 	 0.150868 	 0.156533
Epoch 80 	 0.139097 	 0.148737 	 0.153085
Epoch 90 	 0.136359 	 0.148079 	 0.151846
Epoch 100 	 0.135000 	 0.146019 	 0.151675
Epoch 110 	 0.133164 	 0.145538 	 0.148353
Epoch 120 	 0.131747 	 0.143678 	 0.146472
Epoch 130 	 0.129942 	 0.143700 	 0.146323
Epoch 140 	 0.127554 	 0.138888 	 0.144647
Epoch 150 	 0.127820 	 0.140206 	 0.145356
Train loss       : 0.115402
Best valid loss  : 0.132269
Best test loss   : 0.135194
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 915,695
--------------------------------
Total memory      : 3.37 MB
Total Flops       : 155.1 MFlops
Total Mem (Read)  : 5.53 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 0.5976]
[Starting training]
Epoch 0 	 0.468912 	 0.429170 	 0.442277
Epoch 10 	 0.248274 	 0.245599 	 0.253018
Epoch 20 	 0.190396 	 0.194822 	 0.198284
Epoch 30 	 0.171208 	 0.177331 	 0.180568
Epoch 40 	 0.161051 	 0.171625 	 0.176025
Epoch 50 	 0.154392 	 0.164593 	 0.167935
Epoch 60 	 0.149808 	 0.158814 	 0.163487
Epoch 70 	 0.144756 	 0.156015 	 0.160226
Epoch 80 	 0.141498 	 0.153216 	 0.155622
Epoch 90 	 0.138887 	 0.151561 	 0.153365
Epoch 100 	 0.136809 	 0.147656 	 0.151328
Epoch 110 	 0.135532 	 0.148774 	 0.149937
Epoch 120 	 0.134685 	 0.145709 	 0.149742
Epoch 130 	 0.125335 	 0.139216 	 0.141015
Epoch 140 	 0.122205 	 0.137071 	 0.140590
Epoch 150 	 0.116559 	 0.131924 	 0.136629
Train loss       : 0.114022
Best valid loss  : 0.131311
Best test loss   : 0.136946
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 658,395
--------------------------------
Total memory      : 2.68 MB
Total Flops       : 107.48 MFlops
Total Mem (Read)  : 4.15 MB
Total Mem (Write) : 2.44 MB
[Supermasks testing]
[Untrained loss : 0.5405]
[Starting training]
Epoch 0 	 0.464362 	 0.425790 	 0.444831
Epoch 10 	 0.266880 	 0.260155 	 0.264115
Epoch 20 	 0.202362 	 0.202830 	 0.205783
Epoch 30 	 0.180951 	 0.184216 	 0.188896
Epoch 40 	 0.169293 	 0.174322 	 0.180656
Epoch 50 	 0.160766 	 0.169514 	 0.173550
Epoch 60 	 0.156510 	 0.162740 	 0.169872
Epoch 70 	 0.151680 	 0.160692 	 0.165448
Epoch 80 	 0.148832 	 0.157680 	 0.163362
Epoch 90 	 0.146926 	 0.161109 	 0.165049
Epoch 100 	 0.135573 	 0.149120 	 0.153075
Epoch 110 	 0.134527 	 0.149362 	 0.153856
Epoch 120 	 0.132649 	 0.146980 	 0.150995
Epoch 130 	 0.132198 	 0.146100 	 0.150629
Epoch 140 	 0.130684 	 0.143725 	 0.149628
Epoch 150 	 0.125044 	 0.141942 	 0.145553
Train loss       : 0.124525
Best valid loss  : 0.139202
Best test loss   : 0.145418
Pruning          : 0.11
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 480,315
--------------------------------
Total memory      : 2.14 MB
Total Flops       : 75.77 MFlops
Total Mem (Read)  : 3.17 MB
Total Mem (Write) : 1.91 MB
[Supermasks testing]
[Untrained loss : 0.5961]
[Starting training]
Epoch 0 	 0.474856 	 0.417011 	 0.435737
Epoch 10 	 0.267090 	 0.264986 	 0.263806
Epoch 20 	 0.210833 	 0.212215 	 0.214196
Epoch 30 	 0.188549 	 0.196629 	 0.197512
Epoch 40 	 0.175644 	 0.183935 	 0.185632
Epoch 50 	 0.167225 	 0.174095 	 0.178946
Epoch 60 	 0.160569 	 0.170405 	 0.173939
Epoch 70 	 0.157302 	 0.166110 	 0.169087
Epoch 80 	 0.153661 	 0.164884 	 0.166765
Epoch 90 	 0.149361 	 0.158661 	 0.163551
Epoch 100 	 0.146819 	 0.155758 	 0.159689
Epoch 110 	 0.143809 	 0.156268 	 0.158727
Epoch 120 	 0.142727 	 0.156163 	 0.158478
Epoch 130 	 0.133175 	 0.146971 	 0.148918
Epoch 140 	 0.132217 	 0.145176 	 0.148570
Epoch 150 	 0.131459 	 0.144383 	 0.147301
Train loss       : 0.130644
Best valid loss  : 0.143481
Best test loss   : 0.146621
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 352,407
--------------------------------
Total memory      : 1.71 MB
Total Flops       : 53.87 MFlops
Total Mem (Read)  : 2.44 MB
Total Mem (Write) : 1.48 MB
[Supermasks testing]
[Untrained loss : 0.5712]
[Starting training]
Epoch 0 	 0.471923 	 0.424649 	 0.444482
Epoch 10 	 0.303406 	 0.292930 	 0.297997
Epoch 20 	 0.229155 	 0.225681 	 0.231501
Epoch 30 	 0.199428 	 0.203870 	 0.206188
Epoch 40 	 0.183553 	 0.189904 	 0.195006
Epoch 50 	 0.174798 	 0.183046 	 0.185177
Epoch 60 	 0.168389 	 0.177892 	 0.182251
Epoch 70 	 0.163675 	 0.172028 	 0.174906
Epoch 80 	 0.156799 	 0.172068 	 0.174719
Epoch 90 	 0.154218 	 0.162336 	 0.166719
Epoch 100 	 0.151988 	 0.163484 	 0.165688
Epoch 110 	 0.149583 	 0.159566 	 0.162740
Epoch 120 	 0.147887 	 0.160077 	 0.163857
Epoch 130 	 0.145425 	 0.155551 	 0.158706
Epoch 140 	 0.143831 	 0.153719 	 0.157782
Epoch 150 	 0.141986 	 0.152218 	 0.154713
Train loss       : 0.141781
Best valid loss  : 0.151320
Best test loss   : 0.154420
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 262,565
--------------------------------
Total memory      : 1.39 MB
Total Flops       : 39.05 MFlops
Total Mem (Read)  : 1.91 MB
Total Mem (Write) : 1.16 MB
[Supermasks testing]
[Untrained loss : 0.5732]
[Starting training]
Epoch 0 	 0.475855 	 0.431079 	 0.446779
Epoch 10 	 0.308559 	 0.302258 	 0.301615
Epoch 20 	 0.241097 	 0.239741 	 0.241121
Epoch 30 	 0.216416 	 0.220049 	 0.222487
Epoch 40 	 0.201054 	 0.207041 	 0.210589
Epoch 50 	 0.194383 	 0.205582 	 0.208282
Epoch 60 	 0.185542 	 0.191579 	 0.196739
Epoch 70 	 0.180246 	 0.187490 	 0.190266
Epoch 80 	 0.175778 	 0.184936 	 0.188650
Epoch 90 	 0.171068 	 0.177464 	 0.184045
Epoch 100 	 0.167885 	 0.174848 	 0.180367
Epoch 110 	 0.167580 	 0.172396 	 0.177519
Epoch 120 	 0.164367 	 0.171098 	 0.176059
Epoch 130 	 0.162090 	 0.171377 	 0.174931
Epoch 140 	 0.160178 	 0.170197 	 0.174426
Epoch 150 	 0.158342 	 0.168355 	 0.171265
Train loss       : 0.156651
Best valid loss  : 0.164750
Best test loss   : 0.170729
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 196,095
--------------------------------
Total memory      : 1.13 MB
Total Flops       : 28.47 MFlops
Total Mem (Read)  : 1.51 MB
Total Mem (Write) : 920.85 KB
[Supermasks testing]
[Untrained loss : 0.5368]
[Starting training]
Epoch 0 	 0.480998 	 0.432497 	 0.448765
Epoch 10 	 0.331279 	 0.322343 	 0.327819
Epoch 20 	 0.265567 	 0.262078 	 0.269459
Epoch 30 	 0.234426 	 0.236969 	 0.239760
Epoch 40 	 0.213794 	 0.217194 	 0.220707
Epoch 50 	 0.202071 	 0.206223 	 0.210102
Epoch 60 	 0.194719 	 0.199144 	 0.204478
Epoch 70 	 0.188897 	 0.195472 	 0.199337
Epoch 80 	 0.186261 	 0.191734 	 0.196224
Epoch 90 	 0.182203 	 0.189821 	 0.193964
Epoch 100 	 0.179435 	 0.188625 	 0.192167
Epoch 110 	 0.177181 	 0.186612 	 0.189826
Epoch 120 	 0.175556 	 0.184706 	 0.188287
Epoch 130 	 0.173997 	 0.181099 	 0.186975
Epoch 140 	 0.166319 	 0.176938 	 0.180603
Epoch 150 	 0.165522 	 0.176109 	 0.180044
Train loss       : 0.165505
Best valid loss  : 0.173903
Best test loss   : 0.179734
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 147,027
--------------------------------
Total memory      : 0.93 MB
Total Flops       : 20.91 MFlops
Total Mem (Read)  : 1.21 MB
Total Mem (Write) : 714.55 KB
[Supermasks testing]
[Untrained loss : 0.5703]
[Starting training]
Epoch 0 	 0.482375 	 0.432708 	 0.448290
Epoch 10 	 0.341882 	 0.333610 	 0.340796
Epoch 20 	 0.275212 	 0.281220 	 0.283042
Epoch 30 	 0.248061 	 0.250165 	 0.254209
Epoch 40 	 0.233520 	 0.238230 	 0.243469
Epoch 50 	 0.223993 	 0.232367 	 0.234057
Epoch 60 	 0.218422 	 0.223159 	 0.228452
Epoch 70 	 0.214804 	 0.220838 	 0.224557
Epoch 80 	 0.210960 	 0.218392 	 0.223211
Epoch 90 	 0.207737 	 0.216540 	 0.220155
Epoch 100 	 0.203499 	 0.212150 	 0.215593
Epoch 110 	 0.201078 	 0.207711 	 0.212245
Epoch 120 	 0.198429 	 0.207285 	 0.210498
Epoch 130 	 0.194969 	 0.202945 	 0.206313
Epoch 140 	 0.192660 	 0.198341 	 0.204720
Epoch 150 	 0.189638 	 0.198701 	 0.200302
Train loss       : 0.186920
Best valid loss  : 0.193042
Best test loss   : 0.198943
Pruning          : 0.03
