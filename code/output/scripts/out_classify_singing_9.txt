Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41289075.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, future, torch, six, pillow-simd, torchvision, tqdm, cycler, pyparsing, python-dateutil, kiwisolver, matplotlib, google-pasta, termcolor, h5py, keras-applications, absl-py, keras-preprocessing, astor, grpcio, protobuf, wrapt, gast, tensorflow-estimator, opt-einsum, werkzeug, markdown, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, urllib3, chardet, idna, certifi, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/mir_eval-0.6.tar.gz
Requirement already satisfied: numpy>=1.7.0 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.4.1)
Requirement already satisfied: future in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (0.17.1)
Requirement already satisfied: six in /localscratch/esling.41289075.0/env/lib/python3.7/site-packages (from mir-eval==0.6) (1.14.0)
Building wheels for collected packages: mir-eval
  Building wheel for mir-eval (setup.py): started
  Building wheel for mir-eval (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/d1/c4/fe/5455addf1ef19661b1d6285877644eefd17d5aa49a196aa983
Successfully built mir-eval
Installing collected packages: mir-eval
Successfully installed mir-eval-0.6
2020-04-29 05:00:16.487773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-29 05:00:16.817092: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is singing_classify_cnn_xavier_trimming_activation_reinit_global_0.
*******
[Current model size]
================================
Total params      : 1,360,779
--------------------------------
Total memory      : 8.57 MB
Total Flops       : 663.37 MFlops
Total Mem (Read)  : 12.12 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.8765]
[Starting training]
Epoch 0 	 0.784926 	 0.719210 	 0.689246
Epoch 10 	 0.314453 	 0.390165 	 0.251471
Epoch 20 	 0.144531 	 0.255515 	 0.102114
Epoch 30 	 0.086052 	 0.208180 	 0.062960
Epoch 40 	 0.068819 	 0.195312 	 0.048070
Epoch 50 	 0.051700 	 0.183824 	 0.044026
Epoch 60 	 0.043084 	 0.179688 	 0.040165
Epoch 70 	 0.041016 	 0.177390 	 0.037040
Epoch 80 	 0.037224 	 0.177390 	 0.037408
Epoch 90 	 0.016544 	 0.173713 	 0.035754
Epoch 100 	 0.011604 	 0.173254 	 0.035018
Epoch 110 	 0.013442 	 0.173713 	 0.035202
[Model stopped early]
Train loss       : 0.009076
Best valid loss  : 0.169577
Best test loss   : 0.034651
Pruning          : 1.00
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 1,207,031
--------------------------------
Total memory      : 8.56 MB
Total Flops       : 663.22 MFlops
Total Mem (Read)  : 11.53 MB
Total Mem (Write) : 6.42 MB
[Supermasks testing]
[Untrained loss : 0.9421]
[Starting training]
Epoch 0 	 0.823644 	 0.750000 	 0.733456
Epoch 10 	 0.359720 	 0.409926 	 0.277206
Epoch 20 	 0.169577 	 0.269301 	 0.125827
Epoch 30 	 0.107537 	 0.202665 	 0.065257
Epoch 40 	 0.072725 	 0.198070 	 0.054320
Epoch 50 	 0.063764 	 0.184283 	 0.043015
Epoch 60 	 0.048024 	 0.174632 	 0.043015
Epoch 70 	 0.039982 	 0.169577 	 0.036765
Epoch 80 	 0.037684 	 0.176471 	 0.037684
Epoch 90 	 0.020795 	 0.175092 	 0.036949
Epoch 100 	 0.018612 	 0.173254 	 0.036029
[Model stopped early]
Train loss       : 0.015051
Best valid loss  : 0.169577
Best test loss   : 0.036765
Pruning          : 0.75
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 814,225
--------------------------------
Total memory      : 8.42 MB
Total Flops       : 515.26 MFlops
Total Mem (Read)  : 9.93 MB
Total Mem (Write) : 6.32 MB
[Supermasks testing]
[Untrained loss : 0.7949]
[Starting training]
Epoch 0 	 0.856273 	 0.784926 	 0.781985
Epoch 10 	 0.517923 	 0.556985 	 0.448438
Epoch 20 	 0.335133 	 0.401654 	 0.274265
Epoch 30 	 0.244830 	 0.324449 	 0.188603
Epoch 40 	 0.182560 	 0.262408 	 0.119669
Epoch 50 	 0.138097 	 0.233456 	 0.092096
Epoch 60 	 0.121324 	 0.221967 	 0.073805
Epoch 70 	 0.097312 	 0.218750 	 0.064706
Epoch 80 	 0.088810 	 0.198529 	 0.055515
Epoch 90 	 0.070772 	 0.187960 	 0.046324
Epoch 100 	 0.068704 	 0.193474 	 0.043199
Epoch 110 	 0.043313 	 0.191176 	 0.043750
Epoch 120 	 0.036190 	 0.178309 	 0.038051
Epoch 130 	 0.036535 	 0.181066 	 0.038603
Epoch 140 	 0.031365 	 0.180147 	 0.038051
Epoch 150 	 0.029297 	 0.176011 	 0.036581
Train loss       : 0.029871
Best valid loss  : 0.170956
Best test loss   : 0.035846
Pruning          : 0.56
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 788,535
--------------------------------
Total memory      : 8.42 MB
Total Flops       : 515.23 MFlops
Total Mem (Read)  : 9.83 MB
Total Mem (Write) : 6.31 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.866498 	 0.802390 	 0.794485
Epoch 10 	 0.662914 	 0.623621 	 0.578860
Epoch 20 	 0.536420 	 0.531710 	 0.444761
Epoch 30 	 0.467027 	 0.459559 	 0.350460
Epoch 40 	 0.431296 	 0.429228 	 0.309651
Epoch 50 	 0.400161 	 0.399357 	 0.280423
Epoch 60 	 0.387408 	 0.387868 	 0.260754
Epoch 70 	 0.371209 	 0.383732 	 0.262040
Epoch 80 	 0.351907 	 0.343750 	 0.228860
Epoch 90 	 0.351677 	 0.338695 	 0.221783
Epoch 100 	 0.339729 	 0.346048 	 0.232077
Epoch 110 	 0.339729 	 0.325827 	 0.209099
Epoch 120 	 0.310892 	 0.306985 	 0.199816
Epoch 130 	 0.310317 	 0.306985 	 0.191452
Epoch 140 	 0.299173 	 0.305147 	 0.193566
Epoch 150 	 0.293313 	 0.301930 	 0.188787
Train loss       : 0.275735
Best valid loss  : 0.289522
Best test loss   : 0.180331
Pruning          : 0.42
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 786,461
--------------------------------
Total memory      : 8.42 MB
Total Flops       : 515.23 MFlops
Total Mem (Read)  : 9.82 MB
Total Mem (Write) : 6.31 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.941866 	 0.938879 	 0.935754
Epoch 10 	 0.697266 	 0.650735 	 0.586673
Epoch 20 	 0.578355 	 0.538143 	 0.440441
Epoch 30 	 0.524931 	 0.490809 	 0.391085
Epoch 40 	 0.484030 	 0.457261 	 0.355790
Epoch 50 	 0.459329 	 0.450827 	 0.331710
Epoch 60 	 0.428194 	 0.454504 	 0.330790
Epoch 70 	 0.408663 	 0.474265 	 0.372794
Epoch 80 	 0.374081 	 0.416820 	 0.293290
Epoch 90 	 0.376494 	 0.385110 	 0.242555
Epoch 100 	 0.351907 	 0.371783 	 0.233915
Epoch 110 	 0.345818 	 0.344210 	 0.197702
Epoch 120 	 0.324793 	 0.362592 	 0.210754
Epoch 130 	 0.319049 	 0.314798 	 0.163327
Epoch 140 	 0.309513 	 0.344669 	 0.198346
Epoch 150 	 0.284007 	 0.317555 	 0.161673
Train loss       : 0.275161
Best valid loss  : 0.297794
Best test loss   : 0.143474
Pruning          : 0.32
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 784,914
--------------------------------
Total memory      : 8.41 MB
Total Flops       : 515.23 MFlops
Total Mem (Read)  : 9.81 MB
Total Mem (Write) : 6.31 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.873621 	 0.868107 	 0.847059
Epoch 10 	 0.675437 	 0.640165 	 0.574357
Epoch 20 	 0.573989 	 0.568015 	 0.460570
Epoch 30 	 0.508617 	 0.495864 	 0.379779
Epoch 40 	 0.469784 	 0.488971 	 0.356618
Epoch 50 	 0.428424 	 0.421415 	 0.291268
Epoch 60 	 0.424862 	 0.387868 	 0.269210
Epoch 70 	 0.403378 	 0.368107 	 0.251471
Epoch 80 	 0.381893 	 0.362592 	 0.246599
Epoch 90 	 0.382812 	 0.352022 	 0.222335
Epoch 100 	 0.346048 	 0.339614 	 0.219945
Epoch 110 	 0.332146 	 0.308364 	 0.175000
Epoch 120 	 0.319853 	 0.329044 	 0.202022
Epoch 130 	 0.314453 	 0.320772 	 0.185386
Epoch 140 	 0.308479 	 0.294577 	 0.158640
Epoch 150 	 0.290671 	 0.292739 	 0.142187
Train loss       : 0.286420
Best valid loss  : 0.277574
Best test loss   : 0.135846
Pruning          : 0.24
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 783,741
--------------------------------
Total memory      : 8.41 MB
Total Flops       : 515.23 MFlops
Total Mem (Read)  : 9.8 MB
Total Mem (Write) : 6.31 MB
[Supermasks testing]
[Untrained loss : 0.9550]
[Starting training]
Epoch 0 	 0.929113 	 0.892923 	 0.890993
Epoch 10 	 0.607307 	 0.595129 	 0.509559
Epoch 20 	 0.527918 	 0.519301 	 0.423346
Epoch 30 	 0.481158 	 0.461857 	 0.371324
Epoch 40 	 0.458295 	 0.445772 	 0.351838
Epoch 50 	 0.435202 	 0.423254 	 0.329412
Epoch 60 	 0.408203 	 0.397518 	 0.309007
Epoch 70 	 0.398323 	 0.397518 	 0.300184
Epoch 80 	 0.385915 	 0.388327 	 0.294669
Epoch 90 	 0.378102 	 0.387868 	 0.289706
Epoch 100 	 0.372587 	 0.385110 	 0.285478
Epoch 110 	 0.364545 	 0.380974 	 0.282904
Epoch 120 	 0.362477 	 0.373162 	 0.280147
Epoch 130 	 0.358801 	 0.374540 	 0.279596
Epoch 140 	 0.359145 	 0.378217 	 0.281434
Epoch 150 	 0.365464 	 0.371783 	 0.278676
[Model stopped early]
Train loss       : 0.358571
Best valid loss  : 0.369485
Best test loss   : 0.278309
Pruning          : 0.18
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 782,874
--------------------------------
Total memory      : 8.41 MB
Total Flops       : 515.23 MFlops
Total Mem (Read)  : 9.8 MB
Total Mem (Write) : 6.31 MB
[Supermasks testing]
[Untrained loss : 0.9550]
[Starting training]
Epoch 0 	 0.909697 	 0.908548 	 0.908180
Epoch 10 	 0.682560 	 0.663603 	 0.595956
Epoch 20 	 0.560202 	 0.558824 	 0.464338
Epoch 30 	 0.495864 	 0.495404 	 0.391636
Epoch 40 	 0.461052 	 0.466912 	 0.365533
Epoch 50 	 0.440142 	 0.441636 	 0.343566
Epoch 60 	 0.418199 	 0.417739 	 0.324449
Epoch 70 	 0.412339 	 0.423254 	 0.322794
Epoch 80 	 0.404527 	 0.399357 	 0.305331
Epoch 90 	 0.404756 	 0.403493 	 0.307904
Epoch 100 	 0.395680 	 0.395680 	 0.295313
Epoch 110 	 0.382583 	 0.393842 	 0.295680
Epoch 120 	 0.388097 	 0.393382 	 0.290901
Epoch 130 	 0.368796 	 0.375000 	 0.278952
Epoch 140 	 0.364775 	 0.377298 	 0.278585
Epoch 150 	 0.357881 	 0.363051 	 0.272702
Train loss       : 0.358915
Best valid loss  : 0.363051
Best test loss   : 0.272702
Pruning          : 0.13
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 216,927
--------------------------------
Total memory      : 8.15 MB
Total Flops       : 235.92 MFlops
Total Mem (Read)  : 7.45 MB
Total Mem (Write) : 6.11 MB
[Supermasks testing]
[Untrained loss : 0.8996]
[Starting training]
Epoch 0 	 0.960133 	 0.964614 	 0.965993
Epoch 10 	 0.758617 	 0.709099 	 0.694853
Epoch 20 	 0.719899 	 0.668658 	 0.628493
Epoch 30 	 0.664867 	 0.639246 	 0.578309
Epoch 40 	 0.623277 	 0.607537 	 0.524540
Epoch 50 	 0.579848 	 0.573989 	 0.482904
Epoch 60 	 0.541016 	 0.523897 	 0.430239
Epoch 70 	 0.509421 	 0.511949 	 0.396967
Epoch 80 	 0.502183 	 0.500000 	 0.383456
Epoch 90 	 0.477022 	 0.468290 	 0.356985
Epoch 100 	 0.471967 	 0.461397 	 0.349081
Epoch 110 	 0.458180 	 0.449449 	 0.339798
Epoch 120 	 0.441981 	 0.428768 	 0.307537
Epoch 130 	 0.431296 	 0.424173 	 0.303768
Epoch 140 	 0.421071 	 0.410386 	 0.281342
Epoch 150 	 0.412224 	 0.415441 	 0.290717
Train loss       : 0.411305
Best valid loss  : 0.393382
Best test loss   : 0.272978
Pruning          : 0.10
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 179,980
--------------------------------
Total memory      : 6.77 MB
Total Flops       : 195.39 MFlops
Total Mem (Read)  : 6.27 MB
Total Mem (Write) : 5.08 MB
[Supermasks testing]
[Untrained loss : 0.9127]
[Starting training]
Epoch 0 	 0.931411 	 0.940257 	 0.930147
Epoch 10 	 0.784812 	 0.728401 	 0.711581
Epoch 20 	 0.654067 	 0.632353 	 0.595956
Epoch 30 	 0.592371 	 0.608915 	 0.537776
Epoch 40 	 0.559168 	 0.552849 	 0.466636
Epoch 50 	 0.531710 	 0.502757 	 0.392831
Epoch 60 	 0.509191 	 0.480699 	 0.370313
Epoch 70 	 0.490464 	 0.464614 	 0.347978
Epoch 80 	 0.484260 	 0.433364 	 0.323529
Epoch 90 	 0.469554 	 0.429228 	 0.314522
Epoch 100 	 0.459559 	 0.414982 	 0.311029
Epoch 110 	 0.451517 	 0.400276 	 0.298989
Epoch 120 	 0.437500 	 0.400276 	 0.289522
Epoch 130 	 0.433134 	 0.394301 	 0.289890
Epoch 140 	 0.424632 	 0.384651 	 0.280699
Epoch 150 	 0.428539 	 0.378676 	 0.277022
Train loss       : 0.417165
Best valid loss  : 0.371783
Best test loss   : 0.268750
Pruning          : 0.08
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 153,298
--------------------------------
Total memory      : 5.76 MB
Total Flops       : 165.91 MFlops
Total Mem (Read)  : 5.41 MB
Total Mem (Write) : 4.32 MB
[Supermasks testing]
[Untrained loss : 0.9116]
[Starting training]
Epoch 0 	 0.951287 	 0.953585 	 0.954596
Epoch 10 	 0.882008 	 0.948989 	 0.947518
Epoch 20 	 0.747358 	 0.751379 	 0.715349
Epoch 30 	 0.695312 	 0.630055 	 0.569853
Epoch 40 	 0.676700 	 0.600184 	 0.509375
Epoch 50 	 0.655216 	 0.571691 	 0.481893
Epoch 60 	 0.648552 	 0.559743 	 0.462684
Epoch 70 	 0.632353 	 0.536765 	 0.445496
Epoch 80 	 0.621668 	 0.546415 	 0.455331
Epoch 90 	 0.611328 	 0.526654 	 0.443199
Epoch 100 	 0.607767 	 0.516544 	 0.426287
Epoch 110 	 0.599380 	 0.505055 	 0.410294
Epoch 120 	 0.587086 	 0.525276 	 0.436581
Epoch 130 	 0.585708 	 0.485294 	 0.388971
Epoch 140 	 0.579159 	 0.521140 	 0.415441
Epoch 150 	 0.573989 	 0.470588 	 0.378125
Train loss       : 0.569049
Best valid loss  : 0.466912
Best test loss   : 0.360846
Pruning          : 0.06
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 133,312
--------------------------------
Total memory      : 5.00 MB
Total Flops       : 143.8 MFlops
Total Mem (Read)  : 4.77 MB
Total Mem (Write) : 3.75 MB
[Supermasks testing]
[Untrained loss : 0.8996]
[Starting training]
Epoch 0 	 0.922449 	 0.914062 	 0.915993
Epoch 10 	 0.826287 	 0.789522 	 0.788143
Epoch 20 	 0.754021 	 0.717831 	 0.695037
Epoch 30 	 0.717371 	 0.684283 	 0.646140
Epoch 40 	 0.693130 	 0.663143 	 0.613695
Epoch 50 	 0.669233 	 0.630974 	 0.585202
Epoch 60 	 0.639476 	 0.611213 	 0.559007
Epoch 70 	 0.619945 	 0.597886 	 0.547794
Epoch 80 	 0.613396 	 0.548254 	 0.464890
Epoch 90 	 0.592256 	 0.521140 	 0.449449
Epoch 100 	 0.587546 	 0.519301 	 0.444577
Epoch 110 	 0.576746 	 0.509191 	 0.429963
Epoch 120 	 0.569164 	 0.501838 	 0.426930
Epoch 130 	 0.561696 	 0.518382 	 0.432353
Epoch 140 	 0.551471 	 0.500460 	 0.417739
Epoch 150 	 0.550437 	 0.487592 	 0.410846
Train loss       : 0.534812
Best valid loss  : 0.481158
Best test loss   : 0.398805
Pruning          : 0.04
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 96,559
--------------------------------
Total memory      : 3.62 MB
Total Flops       : 103.26 MFlops
Total Mem (Read)  : 3.59 MB
Total Mem (Write) : 2.71 MB
[Supermasks testing]
[Untrained loss : 0.7949]
[Starting training]
Epoch 0 	 0.904067 	 0.941636 	 0.938419
Epoch 10 	 0.748162 	 0.676011 	 0.660846
Epoch 20 	 0.681870 	 0.637868 	 0.587224
Epoch 30 	 0.638672 	 0.607537 	 0.521048
Epoch 40 	 0.596163 	 0.562040 	 0.472518
Epoch 50 	 0.570083 	 0.573529 	 0.488235
Epoch 60 	 0.551930 	 0.512408 	 0.425368
Epoch 70 	 0.544577 	 0.498162 	 0.397151
Epoch 80 	 0.526884 	 0.525735 	 0.429228
Epoch 90 	 0.516085 	 0.482996 	 0.370313
Epoch 100 	 0.518497 	 0.539522 	 0.451563
Epoch 110 	 0.501149 	 0.498162 	 0.395588
Epoch 120 	 0.496553 	 0.434743 	 0.329779
Epoch 130 	 0.504021 	 0.471967 	 0.359926
Epoch 140 	 0.492073 	 0.427390 	 0.322518
Epoch 150 	 0.490349 	 0.431066 	 0.329779
Train loss       : 0.485983
Best valid loss  : 0.413603
Best test loss   : 0.316728
Pruning          : 0.03
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 67,610
--------------------------------
Total memory      : 2.97 MB
Total Flops       : 78.92 MFlops
Total Mem (Read)  : 2.99 MB
Total Mem (Write) : 2.23 MB
[Supermasks testing]
[Untrained loss : 0.7540]
[Starting training]
Epoch 0 	 0.855928 	 0.871783 	 0.864706
Epoch 10 	 0.796071 	 0.812960 	 0.770496
Epoch 20 	 0.739545 	 0.742188 	 0.681618
Epoch 30 	 0.686811 	 0.675551 	 0.594761
Epoch 40 	 0.669462 	 0.620864 	 0.540809
Epoch 50 	 0.649012 	 0.612132 	 0.539246
Epoch 60 	 0.630400 	 0.589154 	 0.519669
Epoch 70 	 0.622472 	 0.556985 	 0.456066
Epoch 80 	 0.602826 	 0.546875 	 0.437040
Epoch 90 	 0.601448 	 0.521140 	 0.425276
Epoch 100 	 0.591337 	 0.528033 	 0.414338
Epoch 110 	 0.580078 	 0.510110 	 0.408456
Epoch 120 	 0.574563 	 0.512408 	 0.404596
Epoch 130 	 0.571461 	 0.511489 	 0.407077
Epoch 140 	 0.560662 	 0.506434 	 0.396691
Epoch 150 	 0.556181 	 0.504596 	 0.402022
[Model stopped early]
Train loss       : 0.556181
Best valid loss  : 0.493107
Best test loss   : 0.396048
Pruning          : 0.02
[Performing one full cumulative epoch]
0.0001
0.0001
[Current model size]
================================
Total params      : 47,813
--------------------------------
Total memory      : 2.09 MB
Total Flops       : 54.92 MFlops
Total Mem (Read)  : 2.26 MB
Total Mem (Write) : 1.57 MB
[Supermasks testing]
[Untrained loss : 0.9139]
[Starting training]
Epoch 0 	 0.955423 	 0.952665 	 0.950551
Epoch 10 	 0.944049 	 0.952665 	 0.947426
Epoch 20 	 0.874196 	 0.940717 	 0.928585
Epoch 30 	 0.826861 	 0.837316 	 0.791176
Epoch 40 	 0.754825 	 0.731618 	 0.679596
Epoch 50 	 0.708410 	 0.659926 	 0.590809
Epoch 60 	 0.686581 	 0.621783 	 0.564522
Epoch 70 	 0.666475 	 0.605239 	 0.545680
Epoch 80 	 0.657744 	 0.593750 	 0.536029
Epoch 90 	 0.654986 	 0.578125 	 0.519853
Epoch 100 	 0.644876 	 0.568934 	 0.513143
Epoch 110 	 0.635340 	 0.566636 	 0.509743
Epoch 120 	 0.638212 	 0.564338 	 0.507353
Epoch 130 	 0.636949 	 0.559283 	 0.504044
Epoch 140 	 0.645106 	 0.560662 	 0.499449
Epoch 150 	 0.631778 	 0.555147 	 0.497886
Train loss       : 0.623047
Best valid loss  : 0.549632
Best test loss   : 0.492739
Pruning          : 0.02
