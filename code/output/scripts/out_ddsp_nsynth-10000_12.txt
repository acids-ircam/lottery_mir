Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.41146336.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, pillow-simd, six, future, torch, torchvision, tqdm, python-dateutil, kiwisolver, pyparsing, cycler, matplotlib, tensorflow-estimator, opt-einsum, termcolor, keras-preprocessing, absl-py, google-pasta, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, protobuf, markdown, grpcio, idna, urllib3, certifi, chardet, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, tensorboard, wrapt, astor, h5py, keras-applications, gast, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: setuptools in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.41146336.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-27 03:01:59.163875: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-27 03:01:59.175231: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_ddsp_cnn_xavier_trimming_magnitude_rewind_local_0.
*******
[Current model size]
================================
Total params      : 4,590,296
--------------------------------
Total memory      : 42.47 MB
Total Flops       : 622.77 MFlops
Total Mem (Read)  : 44.86 MB
Total Mem (Write) : 35.63 MB
[Supermasks testing]
/localscratch/esling.41146336.0/env/lib/python3.7/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
[Untrained loss : 88.7847]
[Starting training]
Epoch 0 	 75.013687 	 64.599960 	 65.733597
Epoch 10 	 61.592793 	 60.584381 	 60.543545
Epoch 20 	 47.929626 	 44.407177 	 45.570862
Epoch 30 	 44.169712 	 39.969151 	 41.590416
Epoch 40 	 38.759922 	 34.027538 	 35.965130
Epoch 50 	 35.596016 	 33.278778 	 34.897152
Epoch 60 	 33.258671 	 30.566870 	 32.631260
Epoch 70 	 32.484619 	 29.942284 	 32.060879
Epoch 80 	 30.213223 	 29.875666 	 31.900827
Epoch 90 	 29.577190 	 28.497778 	 30.040302
Epoch 100 	 28.653603 	 30.248768 	 31.940945
Epoch 110 	 27.487839 	 26.807951 	 28.675581
Epoch 120 	 27.114075 	 26.768688 	 28.292881
Epoch 130 	 26.810068 	 26.560707 	 28.330921
Epoch 140 	 27.225599 	 26.951418 	 28.754198
Epoch 150 	 26.593967 	 9558649416047394816.000000 	 inf
Epoch 160 	 27.917580 	 26.853910 	 28.649446
Epoch 170 	 26.176437 	 25.919184 	 27.563419
Epoch 180 	 26.024935 	 25.911947 	 27.802794
Epoch 190 	 25.429848 	 25.734121 	 27.418482
Train loss       : 25.105465
Best valid loss  : 25.210033
Best test loss   : 27.106937
Pruning          : 1.00
0.001
0.001
[Current model size]
================================
Total params      : 2,909,528
--------------------------------
Total memory      : 35.03 MB
Total Flops       : 326.2 MFlops
Total Mem (Read)  : 31.67 MB
Total Mem (Write) : 24.49 MB
[Supermasks testing]
[Untrained loss : 217.6577]
[Starting training]
Epoch 0 	 49.351250 	 39.052074 	 40.591503
Epoch 10 	 31.069149 	 29.311916 	 31.171513
Epoch 20 	 29.674543 	 27.520100 	 29.537947
Epoch 30 	 28.174547 	 26.906258 	 28.687698
Epoch 40 	 29.016628 	 28.728161 	 30.209080
Epoch 50 	 27.054359 	 26.067442 	 27.761778
Epoch 60 	 26.349234 	 25.540649 	 27.384422
Epoch 70 	 25.947546 	 25.802868 	 27.390505
Epoch 80 	 24.979525 	 24.956600 	 26.540800
Epoch 90 	 24.799232 	 24.823906 	 26.524355
Epoch 100 	 24.358898 	 24.669859 	 26.147802
Epoch 110 	 24.222511 	 24.469334 	 26.082714
Epoch 120 	 23.978683 	 24.378792 	 25.855406
Epoch 130 	 23.950550 	 24.510099 	 25.932829
Epoch 140 	 23.860317 	 24.298664 	 25.849709
Epoch 150 	 23.798531 	 24.230637 	 25.827715
Epoch 160 	 23.688614 	 24.297659 	 25.764931
Epoch 170 	 23.684330 	 24.025156 	 25.816208
Epoch 180 	 23.671837 	 24.176229 	 25.907980
Epoch 190 	 23.647524 	 24.242018 	 25.800991
[Model stopped early]
Train loss       : 23.596735
Best valid loss  : 23.936205
Best test loss   : 25.781525
Pruning          : 0.72
0.001
0.001
[Current model size]
================================
Total params      : 1,898,752
--------------------------------
Total memory      : 29.65 MB
Total Flops       : 171.22 MFlops
Total Mem (Read)  : 22.92 MB
Total Mem (Write) : 17.34 MB
[Supermasks testing]
[Untrained loss : 94.5141]
[Starting training]
Epoch 0 	 51.229237 	 40.022861 	 41.730209
Epoch 10 	 33.044346 	 30.600374 	 32.448154
Epoch 20 	 32.346207 	 29.035305 	 30.981546
Epoch 30 	 30.089169 	 28.967495 	 31.185282
Epoch 40 	 29.156298 	 28.057238 	 30.032148
Epoch 50 	 28.896320 	 28.453192 	 30.213182
Epoch 60 	 26.808840 	 26.076366 	 28.205002
Epoch 70 	 26.557333 	 25.932117 	 27.793001
Epoch 80 	 26.071987 	 25.733255 	 27.516350
Epoch 90 	 25.795660 	 25.423283 	 27.224924
Epoch 100 	 24.976042 	 25.691042 	 27.250368
Epoch 110 	 24.699253 	 24.800081 	 26.458403
Epoch 120 	 24.744175 	 24.709539 	 26.368618
Epoch 130 	 24.673180 	 24.634981 	 26.262995
Epoch 140 	 24.171970 	 24.540127 	 26.138660
Epoch 150 	 24.174889 	 24.368822 	 26.047195
Epoch 160 	 23.933279 	 24.299204 	 25.949257
Epoch 170 	 23.929375 	 23.944582 	 25.913296
Epoch 180 	 23.912085 	 24.052149 	 25.913193
Epoch 190 	 23.806217 	 24.100924 	 25.806118
Train loss       : 23.747570
Best valid loss  : 23.858906
Best test loss   : 25.878036
Pruning          : 0.52
0.001
0.001
[Current model size]
================================
Total params      : 1,282,494
--------------------------------
Total memory      : 25.75 MB
Total Flops       : 89.46 MFlops
Total Mem (Read)  : 17.01 MB
Total Mem (Write) : 12.62 MB
[Supermasks testing]
[Untrained loss : 111.5624]
[Starting training]
Epoch 0 	 54.170334 	 44.022884 	 45.510300
Epoch 10 	 35.146549 	 31.929890 	 33.885788
Epoch 20 	 32.629910 	 31.556665 	 33.628372
Epoch 30 	 30.591082 	 28.818001 	 30.623796
Epoch 40 	 28.296677 	 27.365847 	 28.891468
Epoch 50 	 27.908724 	 27.969660 	 29.604244
Epoch 60 	 27.852867 	 26.644699 	 28.303217
Epoch 70 	 27.177057 	 26.545685 	 28.113245
Epoch 80 	 26.938421 	 25.850912 	 27.505995
Epoch 90 	 26.273184 	 25.851423 	 27.661600
Epoch 100 	 26.038687 	 25.447289 	 27.096575
Epoch 110 	 25.945744 	 25.348597 	 27.204353
Epoch 120 	 25.158037 	 24.913374 	 26.683552
Epoch 130 	 24.791037 	 24.731331 	 26.425529
Epoch 140 	 24.750332 	 24.739395 	 26.412361
Epoch 150 	 24.597961 	 24.536690 	 26.390608
Epoch 160 	 24.366667 	 24.339291 	 26.281902
Epoch 170 	 24.420385 	 24.642464 	 26.311337
Epoch 180 	 24.359774 	 24.543489 	 26.239845
Epoch 190 	 24.372734 	 24.425358 	 26.249355
Train loss       : 24.320581
Best valid loss  : 24.291548
Best test loss   : 26.277454
Pruning          : 0.37
0.001
0.001
[Current model size]
================================
Total params      : 887,398
--------------------------------
Total memory      : 22.89 MB
Total Flops       : 46.15 MFlops
Total Mem (Read)  : 12.89 MB
Total Mem (Write) : 9.4 MB
[Supermasks testing]
[Untrained loss : 86.7487]
[Starting training]
Epoch 0 	 57.770416 	 44.301838 	 45.854256
Epoch 10 	 37.453663 	 33.884003 	 35.709099
Epoch 20 	 34.268299 	 33.880558 	 35.718204
Epoch 30 	 39.474564 	 38.117920 	 39.773045
Epoch 40 	 35.327579 	 31.532925 	 33.545155
Epoch 50 	 34.410007 	 30.358114 	 32.485519
[Model stopped early]
Train loss       : 33.702316
Best valid loss  : 29.959492
Best test loss   : 32.351494
Pruning          : 0.27
0.001
0.001
[Current model size]
================================
Total params      : 630,951
--------------------------------
Total memory      : 20.84 MB
Total Flops       : 23.99 MFlops
Total Mem (Read)  : 10.04 MB
Total Mem (Write) : 7.21 MB
[Supermasks testing]
[Untrained loss : 104.9611]
[Starting training]
Epoch 0 	 61.475384 	 51.169792 	 52.494347
Epoch 10 	 39.563358 	 35.726685 	 37.635338
Epoch 20 	 36.473122 	 33.065289 	 34.937042
Epoch 30 	 35.103691 	 32.129425 	 34.026440
Epoch 40 	 33.880108 	 31.150309 	 32.835258
Epoch 50 	 33.597317 	 31.645367 	 33.491562
Epoch 60 	 33.123375 	 31.002501 	 32.867626
Epoch 70 	 31.712345 	 30.144770 	 32.188007
Epoch 80 	 31.241138 	 29.129864 	 31.480761
Epoch 90 	 31.148014 	 29.949703 	 31.926994
Epoch 100 	 31.074526 	 30.244627 	 32.033260
Epoch 110 	 29.736830 	 28.076303 	 30.130888
Epoch 120 	 29.351700 	 27.494221 	 29.655985
Epoch 130 	 29.608301 	 27.758995 	 29.683527
Epoch 140 	 27.875248 	 26.939190 	 28.774326
Epoch 150 	 27.902239 	 26.729286 	 28.543505
Epoch 160 	 27.177738 	 26.499004 	 28.139467
Epoch 170 	 27.027464 	 26.083389 	 28.106762
Epoch 180 	 26.873621 	 26.125374 	 27.870325
Epoch 190 	 26.604340 	 25.978481 	 27.758192
Train loss       : 26.568956
Best valid loss  : 25.768480
Best test loss   : 27.750723
Pruning          : 0.19
0.001
0.001
[Current model size]
================================
Total params      : 461,817
--------------------------------
Total memory      : 19.39 MB
Total Flops       : 12.83 MFlops
Total Mem (Read)  : 8.07 MB
Total Mem (Write) : 5.74 MB
[Supermasks testing]
[Untrained loss : 91.1094]
[Starting training]
Epoch 0 	 76.301460 	 59.014320 	 60.487312
Epoch 10 	 44.657803 	 41.735401 	 43.035934
Epoch 20 	 41.690109 	 35.771938 	 37.793819
Epoch 30 	 38.768295 	 35.731594 	 37.586201
Epoch 40 	 40.165741 	 35.618904 	 37.783474
Epoch 50 	 36.298496 	 33.052174 	 34.989784
Epoch 60 	 35.098236 	 33.419456 	 35.127258
Epoch 70 	 34.283154 	 31.632624 	 33.381546
Epoch 80 	 32.727840 	 32.003635 	 33.911118
Epoch 90 	 32.045616 	 29.938759 	 31.877962
Epoch 100 	 31.596638 	 29.548218 	 31.471970
Epoch 110 	 31.265217 	 29.838322 	 31.623814
Epoch 120 	 31.227875 	 29.047647 	 30.854475
Epoch 130 	 30.766724 	 29.560429 	 31.249996
Epoch 140 	 30.263256 	 28.415257 	 30.119728
Epoch 150 	 30.298342 	 29.896566 	 31.812540
Epoch 160 	 29.650707 	 28.219637 	 30.062288
Epoch 170 	 29.551971 	 28.561411 	 30.185104
Epoch 180 	 28.767588 	 27.930811 	 29.426126
Epoch 190 	 28.883583 	 27.509697 	 29.363388
Train loss       : 28.376705
Best valid loss  : 27.393387
Best test loss   : 29.099911
Pruning          : 0.14
0.001
0.001
[Current model size]
================================
Total params      : 348,567
--------------------------------
Total memory      : 18.35 MB
Total Flops       : 7.06 MFlops
Total Mem (Read)  : 6.7 MB
Total Mem (Write) : 4.71 MB
[Supermasks testing]
[Untrained loss : 356.1064]
[Starting training]
Epoch 0 	 75.494255 	 56.826519 	 55.543339
Epoch 10 	 46.181995 	 41.288166 	 42.895020
Epoch 20 	 42.466602 	 37.735600 	 39.406269
Epoch 30 	 39.747852 	 35.607662 	 37.675774
Epoch 40 	 38.351131 	 35.270954 	 37.244560
Epoch 50 	 36.156956 	 33.147087 	 35.178776
Epoch 60 	 35.589005 	 34.860435 	 36.961342
Epoch 70 	 36.789253 	 33.094955 	 34.828617
Epoch 80 	 32.744034 	 30.549870 	 32.526878
Epoch 90 	 32.053017 	 29.847403 	 31.829157
Epoch 100 	 31.712553 	 29.945293 	 31.689898
Epoch 110 	 31.271734 	 29.749788 	 31.508915
Epoch 120 	 30.631804 	 29.050137 	 30.981245
Epoch 130 	 30.433958 	 29.654306 	 31.357418
Epoch 140 	 30.169125 	 28.858961 	 30.615847
Epoch 150 	 30.214464 	 28.516808 	 30.505753
Epoch 160 	 29.825531 	 28.335526 	 30.192814
Epoch 170 	 29.718145 	 28.457153 	 30.158253
Epoch 180 	 29.546005 	 28.366625 	 30.212816
Epoch 190 	 29.374151 	 28.175081 	 29.943085
Train loss       : 29.322311
Best valid loss  : 27.918465
Best test loss   : 29.913019
Pruning          : 0.10
0.001
0.001
[Current model size]
================================
Total params      : 272,819
--------------------------------
Total memory      : 17.56 MB
Total Flops       : 3.79 MFlops
Total Mem (Read)  : 5.68 MB
Total Mem (Write) : 3.94 MB
[Supermasks testing]
[Untrained loss : 419.4984]
[Starting training]
Epoch 0 	 87.881569 	 72.406479 	 74.638496
Epoch 10 	 46.785835 	 43.149933 	 44.783558
Epoch 20 	 42.083530 	 38.056026 	 39.857628
Epoch 30 	 40.214508 	 37.754280 	 39.649319
Epoch 40 	 37.990849 	 36.445915 	 38.332245
Epoch 50 	 37.590641 	 34.810669 	 36.595764
Epoch 60 	 36.330425 	 33.749615 	 35.481163
Epoch 70 	 35.581207 	 33.166035 	 35.150330
Epoch 80 	 34.932884 	 32.408951 	 34.343590
Epoch 90 	 34.600666 	 32.625862 	 34.535294
Epoch 100 	 34.390469 	 31.905069 	 33.596779
Epoch 110 	 33.477139 	 32.894588 	 34.779377
Epoch 120 	 33.250164 	 32.369743 	 34.030869
Epoch 130 	 32.068516 	 29.955719 	 32.403431
Epoch 140 	 31.844294 	 30.278826 	 32.152718
Epoch 150 	 31.806421 	 29.841398 	 31.881176
Epoch 160 	 31.211496 	 29.444813 	 31.306726
Epoch 170 	 30.926523 	 29.375599 	 31.495008
Epoch 180 	 30.887768 	 29.201639 	 31.410740
Epoch 190 	 30.823889 	 29.232023 	 31.449501
Train loss       : 30.610561
Best valid loss  : 28.998833
Best test loss   : 31.144083
Pruning          : 0.07
0.001
0.001
[Current model size]
================================
Total params      : 220,983
--------------------------------
Total memory      : 17.11 MB
Total Flops       : 2.56 MFlops
Total Mem (Read)  : 5.08 MB
Total Mem (Write) : 3.52 MB
[Supermasks testing]
[Untrained loss : 267.0932]
[Starting training]
Epoch 0 	 88.872513 	 72.736397 	 74.608208
Epoch 10 	 52.523014 	 46.637871 	 47.104042
Epoch 20 	 49.657913 	 44.008835 	 45.314663
Epoch 30 	 46.732464 	 40.707714 	 42.804169
Epoch 40 	 44.838657 	 40.500404 	 42.006897
Epoch 50 	 42.790535 	 39.103123 	 40.958534
Epoch 60 	 41.720013 	 38.769264 	 40.392982
Epoch 70 	 41.173355 	 37.758911 	 39.625347
Epoch 80 	 40.100536 	 36.894630 	 38.611897
Epoch 90 	 38.900242 	 36.243126 	 37.735600
Epoch 100 	 38.108036 	 35.170635 	 37.014324
Epoch 110 	 37.576382 	 34.719421 	 36.614487
Epoch 120 	 36.945496 	 34.435661 	 36.368355
Epoch 130 	 36.800747 	 34.569702 	 36.536827
Epoch 140 	 36.672951 	 33.884483 	 36.218914
Epoch 150 	 36.251995 	 33.075016 	 35.258949
Epoch 160 	 35.599594 	 33.150940 	 35.295219
Epoch 170 	 34.981846 	 33.037022 	 35.159649
Epoch 180 	 34.465717 	 32.209106 	 34.321205
Epoch 190 	 33.972439 	 31.569332 	 33.832706
Train loss       : 33.886623
Best valid loss  : 31.515089
Best test loss   : 33.774612
Pruning          : 0.05
0.001
0.001
[Current model size]
================================
Total params      : 185,398
--------------------------------
Total memory      : 16.72 MB
Total Flops       : 1.66 MFlops
Total Mem (Read)  : 4.58 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 222.1425]
[Starting training]
Epoch 0 	 86.131699 	 71.767265 	 73.571678
Epoch 10 	 54.370579 	 49.547386 	 49.727036
Epoch 20 	 52.052921 	 48.769302 	 48.577675
Epoch 30 	 50.925331 	 48.040077 	 48.217560
Epoch 40 	 49.104103 	 45.768200 	 46.027397
Epoch 50 	 46.469734 	 42.921192 	 44.235493
Epoch 60 	 44.107460 	 42.662369 	 43.946709
Epoch 70 	 43.078510 	 40.632496 	 43.016415
Epoch 80 	 41.906189 	 40.285408 	 42.551804
Epoch 90 	 41.062038 	 37.403633 	 39.510086
Epoch 100 	 39.951599 	 36.739494 	 38.635403
Epoch 110 	 39.460945 	 37.237362 	 39.285172
Epoch 120 	 38.275066 	 35.966000 	 37.980324
Epoch 130 	 38.341461 	 36.589684 	 38.251503
Epoch 140 	 37.824009 	 35.765392 	 37.454491
Epoch 150 	 37.034039 	 34.751148 	 36.855129
Epoch 160 	 36.868355 	 35.051105 	 36.974270
Epoch 170 	 36.115997 	 34.831867 	 36.497749
Epoch 180 	 36.234985 	 34.786484 	 36.420059
Epoch 190 	 36.078346 	 34.371918 	 36.295670
Train loss       : 35.615669
Best valid loss  : 34.087463
Best test loss   : 36.130711
Pruning          : 0.04
0.001
0.001
[Current model size]
================================
Total params      : 156,046
--------------------------------
Total memory      : 16.63 MB
Total Flops       : 1.63 MFlops
Total Mem (Read)  : 4.41 MB
Total Mem (Write) : 3.08 MB
[Supermasks testing]
[Untrained loss : 376.0346]
[Starting training]
Epoch 0 	 87.651024 	 72.503181 	 74.616959
Epoch 10 	 57.500496 	 53.005703 	 53.264198
Epoch 20 	 52.917549 	 49.142853 	 49.057842
Epoch 30 	 50.617798 	 46.949181 	 47.031475
Epoch 40 	 47.865467 	 44.972290 	 45.389938
Epoch 50 	 45.214882 	 42.141079 	 43.085075
Epoch 60 	 43.152695 	 41.161079 	 42.921791
Epoch 70 	 42.653259 	 39.218399 	 41.169056
Epoch 80 	 42.202141 	 38.774269 	 40.283154
Epoch 90 	 41.738335 	 37.659504 	 39.947762
Epoch 100 	 41.783733 	 37.585270 	 40.076973
Epoch 110 	 40.074009 	 37.026279 	 39.037468
Epoch 120 	 39.824921 	 37.087673 	 39.181164
Epoch 130 	 39.684082 	 37.044262 	 39.140125
Epoch 140 	 39.682137 	 36.722717 	 38.738674
Epoch 150 	 38.940807 	 36.490677 	 38.360886
Epoch 160 	 38.453415 	 36.210201 	 38.245277
Epoch 170 	 38.268116 	 35.598076 	 38.031624
Epoch 180 	 38.367409 	 35.802605 	 38.015461
Epoch 190 	 38.245640 	 35.944191 	 37.932468
Train loss       : 38.218025
Best valid loss  : 34.988522
Best test loss   : 37.884640
Pruning          : 0.03
Traceback (most recent call last):
  File "main.py", line 261, in <module>
    model = pruning.reset(model)
  File "/scratch/esling/lottery/pruning.py", line 781, in reset
    replace_recurrent(m, l, m.unprune_idx[l], prev_kept)
  File "/scratch/esling/lottery/pruning.py", line 752, in replace_recurrent
    cur_ih = nn.Parameter(cur_ih[rep_id0])#torch.from_numpy(cur_ih[rep_id0]).to(self.args.device))
IndexError: too many indices for tensor of dimension 2
