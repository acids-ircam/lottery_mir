Using base prefix '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4'
New python executable in /localscratch/esling.40871926.0/env/bin/python
Installing setuptools, pip, wheel...
done.
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (19.1.1)
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting scipy (from -r requirements.txt (line 1))
Collecting torch_gpu (from -r requirements.txt (line 2))
Collecting torchvision (from -r requirements.txt (line 3))
Collecting tqdm (from -r requirements.txt (line 4))
Collecting numpy (from -r requirements.txt (line 5))
Collecting matplotlib (from -r requirements.txt (line 6))
Collecting tensorflow_gpu (from -r requirements.txt (line 7))
Collecting pandas (from -r requirements.txt (line 8))
Collecting mpmath (from -r requirements.txt (line 9))
Collecting networkx (from -r requirements.txt (line 10))
Collecting natsort (from -r requirements.txt (line 11))
Collecting joblib (from -r requirements.txt (line 12))
Collecting scikit_learn (from -r requirements.txt (line 13))
Collecting cffi (from -r requirements.txt (line 14))
Collecting pycparser (from -r requirements.txt (line 15))
Collecting audioread (from -r requirements.txt (line 16))
Collecting decorator (from -r requirements.txt (line 17))
Collecting six (from -r requirements.txt (line 18))
Collecting llvmlite (from -r requirements.txt (line 19))
Collecting numba (from -r requirements.txt (line 20))
Collecting imageio (from -r requirements.txt (line 21))
Collecting torch (from torchvision->-r requirements.txt (line 3))
Collecting pillow-simd>=4.1.1 (from torchvision->-r requirements.txt (line 3))
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))
Collecting python-dateutil>=2.1 (from matplotlib->-r requirements.txt (line 6))
Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 6))
Collecting opt-einsum>=2.3.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting astor>=0.6.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-preprocessing>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting termcolor>=1.1.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Requirement already satisfied: wheel>=0.26; python_version >= "3" in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from tensorflow_gpu->-r requirements.txt (line 7)) (0.33.4)
Collecting wrapt>=1.11.1 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting gast==0.2.2 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting absl-py>=0.7.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting protobuf>=3.8.0 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting grpcio>=1.8.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-pasta>=0.1.6 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting keras-applications>=1.0.8 (from tensorflow_gpu->-r requirements.txt (line 7))
Collecting pytz>=2017.2 (from pandas->-r requirements.txt (line 8))
Requirement already satisfied: setuptools in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from numba->-r requirements.txt (line 20)) (41.0.1)
Collecting pillow (from imageio->-r requirements.txt (line 21))
Collecting future (from torch->torchvision->-r requirements.txt (line 3))
Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting h5py (from keras-applications>=1.0.8->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow_gpu->-r requirements.txt (line 7))
Installing collected packages: numpy, scipy, torch-gpu, six, future, torch, pillow-simd, torchvision, tqdm, pyparsing, cycler, python-dateutil, kiwisolver, matplotlib, opt-einsum, protobuf, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, absl-py, grpcio, markdown, werkzeug, certifi, idna, urllib3, chardet, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow-estimator, astor, keras-preprocessing, termcolor, wrapt, gast, google-pasta, h5py, keras-applications, tensorflow-gpu, pytz, pandas, mpmath, decorator, networkx, natsort, joblib, scikit-learn, pycparser, cffi, audioread, llvmlite, numba, pillow, imageio
Successfully installed absl-py-0.7.1 astor-0.8.1 audioread-2.1.6 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.13.2 chardet-3.0.4 cycler-0.10.0 decorator-4.4.2 future-0.17.1 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 idna-2.9 imageio-2.8.0 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 llvmlite-0.31.0 markdown-3.2 matplotlib-3.2.1 mpmath-1.1.0 natsort-5.3.3 networkx-2.4 numba-0.48.0 numpy-1.18.1 oauthlib-3.1.0 opt-einsum-2.3.2 pandas-1.0.3 pillow-7.0.0 pillow-simd-7.0.0.post3 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.1 scipy-1.4.1 six-1.14.0 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 torch-1.5.0 torch-gpu-1.0.0 torchvision-0.5.0 tqdm-4.40.2 urllib3-1.25.8 werkzeug-1.0.1 wrapt-1.11.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/nnAudio-0.1.0-py3-none-any.whl
Installing collected packages: nnAudio
Successfully installed nnAudio-0.1.0
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/SoundFile-0.10.3.post1-py2.py3-none-any.whl
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from SoundFile==0.10.3.post1) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from cffi>=1.0->SoundFile==0.10.3.post1) (2.20)
Installing collected packages: SoundFile
Successfully installed SoundFile-0.10.3.post1
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/resampy-0.2.2.tar.gz
Requirement already satisfied: numpy>=1.10 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.18.1)
Requirement already satisfied: scipy>=0.13 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.4.1)
Requirement already satisfied: numba>=0.32 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (0.48.0)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from resampy==0.2.2) (1.14.0)
Requirement already satisfied: setuptools in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.2) (0.31.0)
Building wheels for collected packages: resampy
  Building wheel for resampy (setup.py): started
  Building wheel for resampy (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/7f/a4/f3/4443d9c0e651405d78bca2bc790f21cb3914c168761d6ce287
Successfully built resampy
Installing collected packages: resampy
Successfully installed resampy-0.2.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/librosa-0.7.2.tar.gz
Requirement already satisfied: audioread>=2.0.0 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (2.1.6)
Requirement already satisfied: numpy>=1.15.0 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.18.1)
Requirement already satisfied: scipy>=1.0.0 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.4.1)
Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.22.1)
Requirement already satisfied: joblib>=0.12 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.14.1)
Requirement already satisfied: decorator>=3.0.0 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (4.4.2)
Requirement already satisfied: six>=1.3 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (1.14.0)
Requirement already satisfied: resampy>=0.2.2 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.2.2)
Requirement already satisfied: numba>=0.43.0 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.48.0)
Requirement already satisfied: soundfile>=0.9.0 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from librosa==0.7.2) (0.10.3.post1)
Requirement already satisfied: setuptools in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (41.0.1)
Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)
Requirement already satisfied: cffi>=1.0 in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.13.2)
Requirement already satisfied: pycparser in /localscratch/esling.40871926.0/env/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)
Building wheels for collected packages: librosa
  Building wheel for librosa (setup.py): started
  Building wheel for librosa (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/5d/2c/70/9fd1f14d034354cd1109b858bb14d27d56a04a887581ff018b
Successfully built librosa
Installing collected packages: librosa
Successfully installed librosa-0.7.2
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /home/esling/scratch/python_libs/lmdb-0.98.tar.gz
Building wheels for collected packages: lmdb
  Building wheel for lmdb (setup.py): started
  Building wheel for lmdb (setup.py): finished with status 'done'
  Stored in directory: /home/esling/.cache/pip/wheels/46/8c/ac/27ff74457451e040aa411ff52d641111b0a2481c9f8d31ca95
Successfully built lmdb
Installing collected packages: lmdb
Successfully installed lmdb-0.98
2020-04-22 07:33:52.605454: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-04-22 07:33:53.010085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
*******
Run info.
Optimization will be on cuda.
Model is nsynth-10000_sing_ae_cnn_xavier_trimming_activation_rewind_local_0.
*******
[Current model size]
================================
Total params      : 21,213,200
--------------------------------
Total memory      : 23.17 MB
Total Flops       : 4.68 GFlops
Total Mem (Read)  : 94.21 MB
Total Mem (Write) : 22.94 MB
[Supermasks testing]
[Untrained loss : 0.5367]
[Starting training]
Epoch 0 	 0.462821 	 0.415064 	 0.424252
Epoch 10 	 0.204745 	 0.209654 	 0.212713
Epoch 20 	 0.167290 	 0.175293 	 0.179155
Epoch 30 	 0.154819 	 0.161468 	 0.164305
Epoch 40 	 0.143105 	 0.156332 	 0.156066
Epoch 50 	 0.137135 	 0.151572 	 0.152540
Epoch 60 	 0.139389 	 0.150990 	 0.150931
Epoch 70 	 0.115919 	 0.135174 	 0.135509
Epoch 80 	 0.112969 	 0.135144 	 0.135069
Epoch 90 	 0.104185 	 0.130704 	 0.128239
Epoch 100 	 0.100203 	 0.125830 	 0.126494
Epoch 110 	 0.098329 	 0.127111 	 0.126420
Epoch 120 	 0.095821 	 0.125262 	 0.124811
Epoch 130 	 0.095369 	 0.125905 	 0.124725
Epoch 140 	 0.093970 	 0.126226 	 0.124373
Epoch 150 	 0.093795 	 0.124145 	 0.124343
Train loss       : 0.093284
Best valid loss  : 0.123019
Best test loss   : 0.124195
Pruning          : 1.00
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 13,660,755
--------------------------------
Total memory      : 18.12 MB
Total Flops       : 2.95 GFlops
Total Mem (Read)  : 62.53 MB
Total Mem (Write) : 17.89 MB
[Supermasks testing]
[Untrained loss : 0.1349]
[Starting training]
Epoch 0 	 0.128427 	 0.139383 	 0.142529
Epoch 10 	 0.126189 	 0.139307 	 0.140473
Epoch 20 	 0.126639 	 0.140639 	 0.142396
Epoch 30 	 0.107874 	 0.130154 	 0.130608
Epoch 40 	 0.100087 	 0.126568 	 0.126622
Epoch 50 	 0.098897 	 0.127415 	 0.126034
Epoch 60 	 0.095317 	 0.124845 	 0.123761
Epoch 70 	 0.094519 	 0.122986 	 0.124077
Epoch 80 	 0.093768 	 0.125082 	 0.123831
Epoch 90 	 0.091777 	 0.119564 	 0.122233
Epoch 100 	 0.091372 	 0.123084 	 0.122092
Epoch 110 	 0.090304 	 0.122993 	 0.121778
Epoch 120 	 0.089878 	 0.124031 	 0.121685
[Model stopped early]
Train loss       : 0.089845
Best valid loss  : 0.119564
Best test loss   : 0.122233
Pruning          : 0.78
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 8,896,787
--------------------------------
Total memory      : 14.18 MB
Total Flops       : 1.87 GFlops
Total Mem (Read)  : 42.12 MB
Total Mem (Write) : 13.95 MB
[Supermasks testing]
[Untrained loss : 0.1349]
[Starting training]
Epoch 0 	 0.129372 	 0.139799 	 0.144936
Epoch 10 	 0.123514 	 0.144150 	 0.142881
Epoch 20 	 0.121580 	 0.140561 	 0.139368
Epoch 30 	 0.119661 	 0.140540 	 0.139957
Epoch 40 	 0.105927 	 0.131719 	 0.130563
Epoch 50 	 0.098834 	 0.125693 	 0.125274
Epoch 60 	 0.098051 	 0.125891 	 0.125200
Epoch 70 	 0.094149 	 0.123826 	 0.122543
Epoch 80 	 0.093497 	 0.123869 	 0.122273
Epoch 90 	 0.092912 	 0.124157 	 0.122474
Epoch 100 	 0.090766 	 0.122186 	 0.121354
Epoch 110 	 0.089821 	 0.123370 	 0.121054
[Model stopped early]
Train loss       : 0.089780
Best valid loss  : 0.118289
Best test loss   : 0.122690
Pruning          : 0.61
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 5,874,567
--------------------------------
Total memory      : 11.12 MB
Total Flops       : 1.2 GFlops
Total Mem (Read)  : 28.85 MB
Total Mem (Write) : 10.88 MB
[Supermasks testing]
[Untrained loss : 0.1349]
[Starting training]
Epoch 0 	 0.128490 	 0.142984 	 0.145744
Epoch 10 	 0.123696 	 0.142431 	 0.144731
Epoch 20 	 0.131441 	 0.141461 	 0.145075
Epoch 30 	 0.120118 	 0.142044 	 0.140268
Epoch 40 	 0.120401 	 0.139701 	 0.139169
Epoch 50 	 0.105617 	 0.129993 	 0.129028
Epoch 60 	 0.103710 	 0.128908 	 0.129410
Epoch 70 	 0.096953 	 0.125057 	 0.124064
Epoch 80 	 0.096276 	 0.125629 	 0.124798
Epoch 90 	 0.095548 	 0.124753 	 0.123988
Epoch 100 	 0.094139 	 0.125518 	 0.122733
Epoch 110 	 0.090554 	 0.121862 	 0.120879
Epoch 120 	 0.088948 	 0.122363 	 0.120624
Epoch 130 	 0.088266 	 0.122419 	 0.120103
Epoch 140 	 0.087762 	 0.120614 	 0.120059
Epoch 150 	 0.087409 	 0.120650 	 0.119854
Train loss       : 0.087100
Best valid loss  : 0.117089
Best test loss   : 0.119962
Pruning          : 0.47
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 3,931,875
--------------------------------
Total memory      : 8.72 MB
Total Flops       : 774.71 MFlops
Total Mem (Read)  : 20.07 MB
Total Mem (Write) : 8.49 MB
[Supermasks testing]
[Untrained loss : 0.1350]
[Starting training]
Epoch 0 	 0.128909 	 0.142793 	 0.144597
Epoch 10 	 0.124232 	 0.142768 	 0.143126
Epoch 20 	 0.123919 	 0.140523 	 0.139183
Epoch 30 	 0.119135 	 0.138605 	 0.139362
Epoch 40 	 0.118519 	 0.140059 	 0.138251
Epoch 50 	 0.104354 	 0.128929 	 0.128711
Epoch 60 	 0.104390 	 0.130382 	 0.127676
Epoch 70 	 0.102413 	 0.131069 	 0.128247
Epoch 80 	 0.095576 	 0.125334 	 0.123189
Epoch 90 	 0.091980 	 0.121523 	 0.121313
Epoch 100 	 0.091357 	 0.123217 	 0.121536
Epoch 110 	 0.089530 	 0.122028 	 0.120508
Epoch 120 	 0.089176 	 0.121046 	 0.120601
Epoch 130 	 0.088113 	 0.121821 	 0.120212
Epoch 140 	 0.087692 	 0.120810 	 0.120035
[Model stopped early]
Train loss       : 0.087647
Best valid loss  : 0.118844
Best test loss   : 0.120450
Pruning          : 0.37
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 2,670,357
--------------------------------
Total memory      : 6.85 MB
Total Flops       : 507.44 MFlops
Total Mem (Read)  : 14.2 MB
Total Mem (Write) : 6.62 MB
[Supermasks testing]
[Untrained loss : 0.1356]
[Starting training]
Epoch 0 	 0.129901 	 0.143489 	 0.143726
Epoch 10 	 0.123673 	 0.143607 	 0.142866
Epoch 20 	 0.121322 	 0.140315 	 0.142634
Epoch 30 	 0.107193 	 0.130267 	 0.130969
Epoch 40 	 0.107194 	 0.133012 	 0.131094
Epoch 50 	 0.098479 	 0.126966 	 0.125515
Epoch 60 	 0.097829 	 0.127915 	 0.125665
Epoch 70 	 0.094171 	 0.122609 	 0.123410
Epoch 80 	 0.091942 	 0.122508 	 0.122168
Epoch 90 	 0.091582 	 0.121440 	 0.122038
Epoch 100 	 0.091008 	 0.123232 	 0.121889
Epoch 110 	 0.090047 	 0.122511 	 0.121377
Epoch 120 	 0.089512 	 0.122561 	 0.121159
[Model stopped early]
Train loss       : 0.089468
Best valid loss  : 0.119028
Best test loss   : 0.122515
Pruning          : 0.29
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,842,777
--------------------------------
Total memory      : 5.40 MB
Total Flops       : 337.15 MFlops
Total Mem (Read)  : 10.22 MB
Total Mem (Write) : 5.16 MB
[Supermasks testing]
[Untrained loss : 0.1711]
[Starting training]
Epoch 0 	 0.132595 	 0.144602 	 0.144282
Epoch 10 	 0.124459 	 0.143022 	 0.142270
Epoch 20 	 0.110031 	 0.130879 	 0.132145
Epoch 30 	 0.108418 	 0.132834 	 0.132138
Epoch 40 	 0.101150 	 0.126398 	 0.127374
Epoch 50 	 0.100134 	 0.128921 	 0.126970
Epoch 60 	 0.096673 	 0.125221 	 0.124327
Epoch 70 	 0.095282 	 0.126002 	 0.123907
Epoch 80 	 0.094712 	 0.123387 	 0.123795
Epoch 90 	 0.092609 	 0.124031 	 0.122928
Epoch 100 	 0.091626 	 0.122874 	 0.122410
[Model stopped early]
Train loss       : 0.091368
Best valid loss  : 0.120407
Best test loss   : 0.123556
Pruning          : 0.23
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 1,288,101
--------------------------------
Total memory      : 4.26 MB
Total Flops       : 226.68 MFlops
Total Mem (Read)  : 7.45 MB
Total Mem (Write) : 4.02 MB
[Supermasks testing]
[Untrained loss : 0.2525]
[Starting training]
Epoch 0 	 0.151851 	 0.155834 	 0.156259
Epoch 10 	 0.128383 	 0.145547 	 0.145030
Epoch 20 	 0.111673 	 0.132671 	 0.133213
Epoch 30 	 0.110454 	 0.134103 	 0.133209
Epoch 40 	 0.104316 	 0.129489 	 0.128060
Epoch 50 	 0.101787 	 0.128768 	 0.127667
Epoch 60 	 0.097969 	 0.125991 	 0.125050
Epoch 70 	 0.097485 	 0.123959 	 0.124755
Epoch 80 	 0.095845 	 0.125169 	 0.124038
Epoch 90 	 0.095062 	 0.124597 	 0.123782
[Model stopped early]
Train loss       : 0.094177
Best valid loss  : 0.122008
Best test loss   : 0.124929
Pruning          : 0.18
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 915,695
--------------------------------
Total memory      : 3.37 MB
Total Flops       : 155.1 MFlops
Total Mem (Read)  : 5.53 MB
Total Mem (Write) : 3.14 MB
[Supermasks testing]
[Untrained loss : 0.3650]
[Starting training]
Epoch 0 	 0.183934 	 0.172057 	 0.169179
Epoch 10 	 0.132500 	 0.143257 	 0.147275
Epoch 20 	 0.127789 	 0.142632 	 0.144418
Epoch 30 	 0.122886 	 0.143607 	 0.143018
Epoch 40 	 0.121891 	 0.141402 	 0.143093
Epoch 50 	 0.110610 	 0.134647 	 0.133559
Epoch 60 	 0.109363 	 0.134649 	 0.132404
Epoch 70 	 0.107632 	 0.131542 	 0.131662
Epoch 80 	 0.101549 	 0.128924 	 0.127526
Epoch 90 	 0.099055 	 0.126738 	 0.125783
Epoch 100 	 0.098234 	 0.125946 	 0.125407
Epoch 110 	 0.096725 	 0.126071 	 0.124902
Epoch 120 	 0.095867 	 0.126016 	 0.124527
[Model stopped early]
Train loss       : 0.095865
Best valid loss  : 0.122490
Best test loss   : 0.125550
Pruning          : 0.14
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 658,395
--------------------------------
Total memory      : 2.68 MB
Total Flops       : 107.48 MFlops
Total Mem (Read)  : 4.15 MB
Total Mem (Write) : 2.44 MB
[Supermasks testing]
[Untrained loss : 0.4270]
[Starting training]
Epoch 0 	 0.213760 	 0.186179 	 0.186594
Epoch 10 	 0.134746 	 0.149928 	 0.151110
Epoch 20 	 0.130549 	 0.145074 	 0.146845
Epoch 30 	 0.128315 	 0.148436 	 0.148288
Epoch 40 	 0.114895 	 0.136496 	 0.136304
Epoch 50 	 0.114396 	 0.133860 	 0.135343
Epoch 60 	 0.107568 	 0.131842 	 0.131706
Epoch 70 	 0.107169 	 0.130457 	 0.131486
Epoch 80 	 0.106660 	 0.129335 	 0.130313
Epoch 90 	 0.105715 	 0.129990 	 0.130342
Epoch 100 	 0.105226 	 0.130825 	 0.130422
Epoch 110 	 0.102012 	 0.128815 	 0.127968
Epoch 120 	 0.100527 	 0.126801 	 0.127449
Epoch 130 	 0.099818 	 0.126964 	 0.127170
[Model stopped early]
Train loss       : 0.099799
Best valid loss  : 0.123653
Best test loss   : 0.128373
Pruning          : 0.11
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 480,315
--------------------------------
Total memory      : 2.14 MB
Total Flops       : 75.77 MFlops
Total Mem (Read)  : 3.17 MB
Total Mem (Write) : 1.91 MB
[Supermasks testing]
[Untrained loss : 0.4672]
[Starting training]
Epoch 0 	 0.243920 	 0.202143 	 0.201834
Epoch 10 	 0.141808 	 0.156344 	 0.156468
Epoch 20 	 0.136418 	 0.152273 	 0.154455
Epoch 30 	 0.134951 	 0.149160 	 0.151170
Epoch 40 	 0.131391 	 0.147019 	 0.147924
Epoch 50 	 0.131470 	 0.148041 	 0.149378
Epoch 60 	 0.128330 	 0.145544 	 0.148746
Epoch 70 	 0.125917 	 0.143715 	 0.145845
Epoch 80 	 0.128563 	 0.145240 	 0.143521
Epoch 90 	 0.114561 	 0.135826 	 0.135891
Epoch 100 	 0.114143 	 0.137516 	 0.137282
Epoch 110 	 0.113432 	 0.135745 	 0.135537
Epoch 120 	 0.112615 	 0.134994 	 0.135394
Epoch 130 	 0.107353 	 0.130191 	 0.131124
Epoch 140 	 0.104856 	 0.129041 	 0.129433
Epoch 150 	 0.104560 	 0.129653 	 0.129434
Train loss       : 0.103315
Best valid loss  : 0.127910
Best test loss   : 0.129412
Pruning          : 0.08
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 352,407
--------------------------------
Total memory      : 1.71 MB
Total Flops       : 53.87 MFlops
Total Mem (Read)  : 2.44 MB
Total Mem (Write) : 1.48 MB
[Supermasks testing]
[Untrained loss : 0.4947]
[Starting training]
Epoch 0 	 0.285102 	 0.226424 	 0.229236
Epoch 10 	 0.150720 	 0.161032 	 0.161821
Epoch 20 	 0.142365 	 0.154577 	 0.156906
Epoch 30 	 0.138042 	 0.151942 	 0.153132
Epoch 40 	 0.136758 	 0.150946 	 0.153386
Epoch 50 	 0.133943 	 0.152039 	 0.151601
Epoch 60 	 0.131474 	 0.147539 	 0.149950
Epoch 70 	 0.130330 	 0.146919 	 0.148738
Epoch 80 	 0.129608 	 0.146752 	 0.146892
Epoch 90 	 0.120540 	 0.139799 	 0.139708
Epoch 100 	 0.119951 	 0.137912 	 0.139921
Epoch 110 	 0.119387 	 0.139426 	 0.140016
Epoch 120 	 0.114620 	 0.131999 	 0.136105
Epoch 130 	 0.114141 	 0.136998 	 0.136465
Epoch 140 	 0.111936 	 0.135219 	 0.134933
Epoch 150 	 0.110563 	 0.134212 	 0.134504
[Model stopped early]
Train loss       : 0.110874
Best valid loss  : 0.131999
Best test loss   : 0.136105
Pruning          : 0.07
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 262,565
--------------------------------
Total memory      : 1.39 MB
Total Flops       : 39.05 MFlops
Total Mem (Read)  : 1.91 MB
Total Mem (Write) : 1.16 MB
[Supermasks testing]
[Untrained loss : 0.5172]
[Starting training]
Epoch 0 	 0.327237 	 0.254013 	 0.257971
Epoch 10 	 0.163424 	 0.172740 	 0.171515
Epoch 20 	 0.151046 	 0.164100 	 0.165199
Epoch 30 	 0.146681 	 0.158516 	 0.159428
Epoch 40 	 0.144519 	 0.157403 	 0.158229
Epoch 50 	 0.134016 	 0.148622 	 0.149983
Epoch 60 	 0.129011 	 0.145922 	 0.146933
Epoch 70 	 0.128687 	 0.144224 	 0.146833
Epoch 80 	 0.125898 	 0.144053 	 0.145319
Epoch 90 	 0.125807 	 0.142834 	 0.145012
Epoch 100 	 0.125237 	 0.143434 	 0.144859
Epoch 110 	 0.124879 	 0.143626 	 0.144876
Epoch 120 	 0.124383 	 0.143845 	 0.144718
Epoch 130 	 0.123248 	 0.143256 	 0.144065
Epoch 140 	 0.123110 	 0.141722 	 0.143922
Epoch 150 	 0.122762 	 0.142985 	 0.143724
Train loss       : 0.122383
Best valid loss  : 0.137949
Best test loss   : 0.144082
Pruning          : 0.05
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 196,095
--------------------------------
Total memory      : 1.13 MB
Total Flops       : 28.47 MFlops
Total Mem (Read)  : 1.51 MB
Total Mem (Write) : 920.85 KB
[Supermasks testing]
[Untrained loss : 0.5244]
[Starting training]
Epoch 0 	 0.365041 	 0.290526 	 0.291725
Epoch 10 	 0.184852 	 0.189983 	 0.192781
Epoch 20 	 0.167513 	 0.179314 	 0.180099
Epoch 30 	 0.160451 	 0.169196 	 0.170521
Epoch 40 	 0.156008 	 0.163412 	 0.166901
Epoch 50 	 0.151139 	 0.166886 	 0.166840
Epoch 60 	 0.149837 	 0.160194 	 0.160961
Epoch 70 	 0.148289 	 0.158031 	 0.159364
Epoch 80 	 0.145292 	 0.159848 	 0.159524
Epoch 90 	 0.138437 	 0.151779 	 0.153454
Epoch 100 	 0.137793 	 0.151389 	 0.153122
Epoch 110 	 0.133962 	 0.150154 	 0.151117
Epoch 120 	 0.132311 	 0.148444 	 0.149747
Epoch 130 	 0.131701 	 0.144922 	 0.149793
Epoch 140 	 0.131569 	 0.147602 	 0.149520
Epoch 150 	 0.130409 	 0.148314 	 0.149295
Train loss       : 0.129905
Best valid loss  : 0.144922
Best test loss   : 0.149793
Pruning          : 0.04
[Performing one full cumulative epoch]
0.001
0.001
[Current model size]
================================
Total params      : 147,027
--------------------------------
Total memory      : 0.93 MB
Total Flops       : 20.91 MFlops
Total Mem (Read)  : 1.21 MB
Total Mem (Write) : 714.55 KB
[Supermasks testing]
[Untrained loss : 0.5278]
[Starting training]
Epoch 0 	 0.406799 	 0.338058 	 0.341909
Epoch 10 	 0.212328 	 0.215769 	 0.220284
Epoch 20 	 0.189318 	 0.195735 	 0.198877
Epoch 30 	 0.181159 	 0.190980 	 0.191480
Epoch 40 	 0.175004 	 0.186237 	 0.187220
Epoch 50 	 0.172570 	 0.186702 	 0.186799
Epoch 60 	 0.168322 	 0.178167 	 0.181409
Epoch 70 	 0.166564 	 0.177854 	 0.180140
Epoch 80 	 0.165013 	 0.178090 	 0.178182
Epoch 90 	 0.158157 	 0.172933 	 0.173250
Epoch 100 	 0.157714 	 0.173831 	 0.173878
Epoch 110 	 0.153904 	 0.170540 	 0.170631
Epoch 120 	 0.152144 	 0.168838 	 0.169688
[Model stopped early]
Train loss       : 0.151873
Best valid loss  : 0.165719
Best test loss   : 0.173405
Pruning          : 0.03
